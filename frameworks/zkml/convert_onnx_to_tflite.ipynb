{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class ResNet20Cifar100(tf.keras.Model):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(ResNet20Cifar100, self).__init__()\n",
    "\n",
    "        # Initial Conv Layer\n",
    "        self.conv1 = layers.Conv2D(16, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        # Layer 1 Block 1\n",
    "        self.layer1_0_conv1 = layers.Conv2D(16, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer1_0_bn1 = layers.BatchNormalization()\n",
    "        self.layer1_0_relu1 = layers.ReLU()\n",
    "        self.layer1_0_conv2 = layers.Conv2D(16, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer1_0_bn2 = layers.BatchNormalization()\n",
    "        self.layer1_0_relu2 = layers.ReLU()\n",
    "\n",
    "        # Layer 1 Block 2\n",
    "        self.layer1_1_conv1 = layers.Conv2D(16, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer1_1_bn1 = layers.BatchNormalization()\n",
    "        self.layer1_1_relu1 = layers.ReLU()\n",
    "        self.layer1_1_conv2 = layers.Conv2D(16, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer1_1_bn2 = layers.BatchNormalization()\n",
    "        self.layer1_1_relu2 = layers.ReLU()\n",
    "\n",
    "        # Layer 1 Block 3\n",
    "        self.layer1_2_conv1 = layers.Conv2D(16, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer1_2_bn1 = layers.BatchNormalization()\n",
    "        self.layer1_2_relu1 = layers.ReLU()\n",
    "        self.layer1_2_conv2 = layers.Conv2D(16, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer1_2_bn2 = layers.BatchNormalization()\n",
    "        self.layer1_2_relu2 = layers.ReLU()\n",
    "\n",
    "        # Layer 2 Block 1 (with downsampling)\n",
    "        self.layer2_0_conv1 = layers.Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False)\n",
    "        self.layer2_0_bn1 = layers.BatchNormalization()\n",
    "        self.layer2_0_relu1 = layers.ReLU()\n",
    "        self.layer2_0_conv2 = layers.Conv2D(32, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer2_0_bn2 = layers.BatchNormalization()\n",
    "        self.layer2_0_relu2 = layers.ReLU()\n",
    "        self.layer2_0_downsample = models.Sequential([\n",
    "            layers.Conv2D(32, kernel_size=1, strides=2, use_bias=False),\n",
    "            layers.BatchNormalization()\n",
    "        ])\n",
    "\n",
    "        # Layer 2 Block 2\n",
    "        self.layer2_1_conv1 = layers.Conv2D(32, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer2_1_bn1 = layers.BatchNormalization()\n",
    "        self.layer2_1_relu1 = layers.ReLU()\n",
    "        self.layer2_1_conv2 = layers.Conv2D(32, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer2_1_bn2 = layers.BatchNormalization()\n",
    "        self.layer2_1_relu2 = layers.ReLU()\n",
    "\n",
    "        # Layer 2 Block 3\n",
    "        self.layer2_2_conv1 = layers.Conv2D(32, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer2_2_bn1 = layers.BatchNormalization()\n",
    "        self.layer2_2_relu1 = layers.ReLU()\n",
    "        self.layer2_2_conv2 = layers.Conv2D(32, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer2_2_bn2 = layers.BatchNormalization()\n",
    "        self.layer2_2_relu2 = layers.ReLU()\n",
    "\n",
    "        # Layer 3 Block 1 (with downsampling)\n",
    "        self.layer3_0_conv1 = layers.Conv2D(64, kernel_size=3, strides=2, padding='same', use_bias=False)\n",
    "        self.layer3_0_bn1 = layers.BatchNormalization()\n",
    "        self.layer3_0_relu1 = layers.ReLU()\n",
    "        self.layer3_0_conv2 = layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer3_0_bn2 = layers.BatchNormalization()\n",
    "        self.layer3_0_downsample = models.Sequential([\n",
    "            layers.Conv2D(64, kernel_size=1, strides=2, use_bias=False),\n",
    "            layers.BatchNormalization()\n",
    "        ])\n",
    "        self.layer3_0_relu2 = layers.ReLU()\n",
    "\n",
    "        # Layer 3 Block 2\n",
    "        self.layer3_1_conv1 = layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer3_1_bn1 = layers.BatchNormalization()\n",
    "        self.layer3_1_relu1 = layers.ReLU()\n",
    "        self.layer3_1_conv2 = layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer3_1_bn2 = layers.BatchNormalization()\n",
    "        self.layer3_1_relu2 = layers.ReLU()\n",
    "\n",
    "        # Layer 3 Block 3\n",
    "        self.layer3_2_conv1 = layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer3_2_bn1 = layers.BatchNormalization()\n",
    "        self.layer3_2_relu1 = layers.ReLU()\n",
    "        self.layer3_2_conv2 = layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.layer3_2_bn2 = layers.BatchNormalization()\n",
    "        self.layer3_2_relu2 = layers.ReLU()\n",
    "\n",
    "        # Pooling and classification layers\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Initial layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # Layer 1 Block 1\n",
    "        residual = x\n",
    "        x = self.layer1_0_conv1(x)\n",
    "        x = self.layer1_0_bn1(x, training=training)\n",
    "        x = self.layer1_0_relu1(x)\n",
    "        x = self.layer1_0_conv2(x)\n",
    "        x = self.layer1_0_bn2(x, training=training)\n",
    "        x += residual  # Add operation\n",
    "        x = self.layer1_0_relu2(x)\n",
    "\n",
    "        # Layer 1 Block 2\n",
    "        residual = x\n",
    "        x = self.layer1_1_conv1(x)\n",
    "        x = self.layer1_1_bn1(x, training=training)\n",
    "        x = self.layer1_1_relu1(x)\n",
    "        x = self.layer1_1_conv2(x)\n",
    "        x = self.layer1_1_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer1_1_relu2(x)\n",
    "\n",
    "        # Layer 1 Block 3\n",
    "        residual = x\n",
    "        x = self.layer1_2_conv1(x)\n",
    "        x = self.layer1_2_bn1(x, training=training)\n",
    "        x = self.layer1_2_relu1(x)\n",
    "        x = self.layer1_2_conv2(x)\n",
    "        x = self.layer1_2_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer1_2_relu2(x)\n",
    "\n",
    "        # Layer 2 Block 1 (with downsampling)\n",
    "        residual = self.layer2_0_downsample(x, training=training)\n",
    "        x = self.layer2_0_conv1(x)\n",
    "        x = self.layer2_0_bn1(x, training=training)\n",
    "        x = self.layer2_0_relu1(x)\n",
    "        x = self.layer2_0_conv2(x)\n",
    "        x = self.layer2_0_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer2_0_relu2(x)\n",
    "\n",
    "        # Layer 2 Block 2\n",
    "        residual = x\n",
    "        x = self.layer2_1_conv1(x)\n",
    "        x = self.layer2_1_bn1(x, training=training)\n",
    "        x = self.layer2_1_relu1(x)\n",
    "        x = self.layer2_1_conv2(x)\n",
    "        x = self.layer2_1_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer2_1_relu2(x)\n",
    "\n",
    "        # Layer 2 Block 3\n",
    "        residual = x\n",
    "        x = self.layer2_2_conv1(x)\n",
    "        x = self.layer2_2_bn1(x, training=training)\n",
    "        x = self.layer2_2_relu1(x)\n",
    "        x = self.layer2_2_conv2(x)\n",
    "        x = self.layer2_2_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer2_2_relu2(x)\n",
    "\n",
    "        # Layer 3 Block 1 (with downsampling)\n",
    "        residual = self.layer3_0_downsample(x, training=training)\n",
    "        x = self.layer3_0_conv1(x)\n",
    "        x = self.layer3_0_bn1(x, training=training)\n",
    "        x = self.layer3_0_relu1(x)\n",
    "        x = self.layer3_0_conv2(x)\n",
    "        x = self.layer3_0_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer3_0_relu2(x)\n",
    "\n",
    "        # Layer 3 Block 2\n",
    "        residual = x\n",
    "        x = self.layer3_1_conv1(x)\n",
    "        x = self.layer3_1_bn1(x, training=training)\n",
    "        x = self.layer3_1_relu1(x)\n",
    "        x = self.layer3_1_conv2(x)\n",
    "        x = self.layer3_1_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer3_1_relu2(x)\n",
    "\n",
    "        # Layer 3 Block 3\n",
    "        residual = x\n",
    "        x = self.layer3_2_conv1(x)\n",
    "        x = self.layer3_2_bn1(x, training=training)\n",
    "        x = self.layer3_2_relu1(x)\n",
    "        x = self.layer3_2_conv2(x)\n",
    "        x = self.layer3_2_bn2(x, training=training)\n",
    "        x += residual\n",
    "        x = self.layer3_2_relu2(x)\n",
    "\n",
    "        # Pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.02319046  0.09040488  0.17464879 -0.18580529 -0.2603389  -0.14727576\n",
      "  -0.11792352  0.10791898  0.12220275  0.3389794   0.1419803  -0.05223288\n",
      "   0.10334254  0.04306893  0.11316299  0.2082336   0.22006941  0.00218584\n",
      "  -0.1031503   0.2537732   0.05372621 -0.06419809  0.02358134 -0.27564636\n",
      "   0.00236805  0.05413433 -0.03560496  0.06373198 -0.24118027 -0.10048033\n",
      "   0.04051803 -0.21544896  0.16970938  0.27298257  0.1724781   0.05703834\n",
      "   0.29317352 -0.03834959 -0.02623548 -0.15549783 -0.16595185  0.18636723\n",
      "   0.16470961  0.16572204  0.01755742 -0.30014247 -0.13401368 -0.12308196\n",
      "  -0.02732134  0.29434842  0.01014673  0.01289519  0.26732656 -0.0024655\n",
      "   0.01835042 -0.15265046  0.32173222  0.1473306   0.43412974 -0.17355756\n",
      "  -0.0359081   0.15350258  0.0217164  -0.05186245  0.01286696 -0.01913833\n",
      "  -0.011057   -0.17544183 -0.04441772 -0.24213904 -0.07106921 -0.12055694\n",
      "   0.15901747 -0.08730578  0.28107846  0.07711687 -0.24941704 -0.01000583\n",
      "   0.20198253 -0.21223857  0.04014826 -0.3703138   0.15195061 -0.20178817\n",
      "   0.06778874 -0.07245029 -0.11188355 -0.20926496  0.10028038 -0.15613519\n",
      "  -0.2978078   0.26376957 -0.05353849 -0.01436861 -0.03514456 -0.15492819\n",
      "  -0.08182941  0.23181489 -0.24551912  0.09175409]], shape=(1, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `_wrapped_model` contains input name(s) resource with unsupported characters which will be renamed to res_net20_cifar100_5_dense_5_biasadd_readvariableop_resource in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 21). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpc81c0k27/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpc81c0k27/assets\n",
      "INFO:absl:Writing fingerprint to /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpc81c0k27/fingerprint.pb\n",
      "INFO:absl:Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "model = ResNet20Cifar100(num_classes=100)\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "cifar100 = tf.keras.datasets.cifar100\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Compile the model (necessary for training, but for inference, itâ€™s optional)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Perform inference with a sample image from CIFAR-100\n",
    "sample_image = train_images[0:1]  # Select one sample\n",
    "sample_image = tf.convert_to_tensor(sample_image, dtype=tf.float32)\n",
    "\n",
    "# Ensure the model is in evaluation mode (no training mode needed)\n",
    "predictions = model(sample_image, training=False)  # training=False ensures evaluation mode\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Convert the model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert() # experimental_new_converter=False\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 7.npy and forward it to the model\n",
    "import numpy as np\n",
    "data = np.load('7.npy')\n",
    "# reshape data to 4D\n",
    "data = np.reshape(data, (1, 32, 32, 3))\n",
    "\n",
    "# forward the data to the model\n",
    "data = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "predictions = model(data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/ZKML-Benchmark/ZKML-Benchmark/zkml_bench_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "{0, 27, 28, 29, 30}\n",
      "Add inputs:  [29, 27]\n",
      "True True\n",
      "{0, 32, 33, 27, 28, 29, 30, 31}\n",
      "Add inputs:  [32, 30]\n",
      "True True\n",
      "{0, 32, 33, 34, 35, 36, 27, 28, 29, 30, 31}\n",
      "Add inputs:  [35, 33]\n",
      "True True\n",
      "{0, 32, 33, 34, 35, 36, 37, 38, 39, 40, 27, 28, 29, 30, 31}\n",
      "Add inputs:  [38, 39]\n",
      "True True\n",
      "{0, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 27, 28, 29, 30, 31}\n",
      "Add inputs:  [42, 40]\n",
      "True True\n",
      "{0, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46}\n",
      "Add inputs:  [45, 43]\n",
      "True True\n",
      "{0, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50}\n",
      "Add inputs:  [48, 49]\n",
      "True True\n",
      "{0, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}\n",
      "Add inputs:  [52, 50]\n",
      "True True\n",
      "{0, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56}\n",
      "Add inputs:  [55, 53]\n",
      "True True\n",
      "[{'layer_type': 'Conv2D', 'inp_idxes': [0, 4, 1], 'inp_shapes': [[1, 32, 32, 3], [16, 3, 3, 3], [16]], 'out_idxes': [27], 'out_shapes': [[1, 32, 32, 16]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [27, 5, 1], 'inp_shapes': [[1, 32, 32, 16], [16, 3, 3, 16], [16]], 'out_idxes': [28], 'out_shapes': [[1, 32, 32, 16]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [28, 6, 1], 'inp_shapes': [[1, 32, 32, 16], [16, 3, 3, 16], [16]], 'out_idxes': [29], 'out_shapes': [[1, 32, 32, 16]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [29, 27], 'inp_shapes': [[1, 32, 32, 16], [1, 32, 32, 16]], 'out_idxes': [30], 'out_shapes': [[1, 32, 32, 16]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [30, 7, 1], 'inp_shapes': [[1, 32, 32, 16], [16, 3, 3, 16], [16]], 'out_idxes': [31], 'out_shapes': [[1, 32, 32, 16]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [31, 8, 1], 'inp_shapes': [[1, 32, 32, 16], [16, 3, 3, 16], [16]], 'out_idxes': [32], 'out_shapes': [[1, 32, 32, 16]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [32, 30], 'inp_shapes': [[1, 32, 32, 16], [1, 32, 32, 16]], 'out_idxes': [33], 'out_shapes': [[1, 32, 32, 16]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [33, 9, 1], 'inp_shapes': [[1, 32, 32, 16], [16, 3, 3, 16], [16]], 'out_idxes': [34], 'out_shapes': [[1, 32, 32, 16]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [34, 10, 1], 'inp_shapes': [[1, 32, 32, 16], [16, 3, 3, 16], [16]], 'out_idxes': [35], 'out_shapes': [[1, 32, 32, 16]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [35, 33], 'inp_shapes': [[1, 32, 32, 16], [1, 32, 32, 16]], 'out_idxes': [36], 'out_shapes': [[1, 32, 32, 16]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [36, 11, 2], 'inp_shapes': [[1, 32, 32, 16], [32, 3, 3, 16], [32]], 'out_idxes': [37], 'out_shapes': [[1, 16, 16, 32]], 'params': [0, 0, 1, 2, 2], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [37, 12, 2], 'inp_shapes': [[1, 16, 16, 32], [32, 3, 3, 32], [32]], 'out_idxes': [38], 'out_shapes': [[1, 16, 16, 32]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [36, 13, 2], 'inp_shapes': [[1, 32, 32, 16], [32, 1, 1, 16], [32]], 'out_idxes': [39], 'out_shapes': [[1, 16, 16, 32]], 'params': [0, 1, 0, 2, 2], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [38, 39], 'inp_shapes': [[1, 16, 16, 32], [1, 16, 16, 32]], 'out_idxes': [40], 'out_shapes': [[1, 16, 16, 32]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [40, 14, 2], 'inp_shapes': [[1, 16, 16, 32], [32, 3, 3, 32], [32]], 'out_idxes': [41], 'out_shapes': [[1, 16, 16, 32]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [41, 15, 2], 'inp_shapes': [[1, 16, 16, 32], [32, 3, 3, 32], [32]], 'out_idxes': [42], 'out_shapes': [[1, 16, 16, 32]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [42, 40], 'inp_shapes': [[1, 16, 16, 32], [1, 16, 16, 32]], 'out_idxes': [43], 'out_shapes': [[1, 16, 16, 32]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [43, 16, 2], 'inp_shapes': [[1, 16, 16, 32], [32, 3, 3, 32], [32]], 'out_idxes': [44], 'out_shapes': [[1, 16, 16, 32]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [44, 17, 2], 'inp_shapes': [[1, 16, 16, 32], [32, 3, 3, 32], [32]], 'out_idxes': [45], 'out_shapes': [[1, 16, 16, 32]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [45, 43], 'inp_shapes': [[1, 16, 16, 32], [1, 16, 16, 32]], 'out_idxes': [46], 'out_shapes': [[1, 16, 16, 32]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [46, 18, 3], 'inp_shapes': [[1, 16, 16, 32], [64, 3, 3, 32], [64]], 'out_idxes': [47], 'out_shapes': [[1, 8, 8, 64]], 'params': [0, 0, 1, 2, 2], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [47, 19, 3], 'inp_shapes': [[1, 8, 8, 64], [64, 3, 3, 64], [64]], 'out_idxes': [48], 'out_shapes': [[1, 8, 8, 64]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [46, 20, 3], 'inp_shapes': [[1, 16, 16, 32], [64, 1, 1, 32], [64]], 'out_idxes': [49], 'out_shapes': [[1, 8, 8, 64]], 'params': [0, 1, 0, 2, 2], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [48, 49], 'inp_shapes': [[1, 8, 8, 64], [1, 8, 8, 64]], 'out_idxes': [50], 'out_shapes': [[1, 8, 8, 64]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [50, 21, 3], 'inp_shapes': [[1, 8, 8, 64], [64, 3, 3, 64], [64]], 'out_idxes': [51], 'out_shapes': [[1, 8, 8, 64]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [51, 22, 3], 'inp_shapes': [[1, 8, 8, 64], [64, 3, 3, 64], [64]], 'out_idxes': [52], 'out_shapes': [[1, 8, 8, 64]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [52, 50], 'inp_shapes': [[1, 8, 8, 64], [1, 8, 8, 64]], 'out_idxes': [53], 'out_shapes': [[1, 8, 8, 64]], 'params': [1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [53, 23, 3], 'inp_shapes': [[1, 8, 8, 64], [64, 3, 3, 64], [64]], 'out_idxes': [54], 'out_shapes': [[1, 8, 8, 64]], 'params': [0, 0, 1, 1, 1], 'mask': []}, {'layer_type': 'Conv2D', 'inp_idxes': [54, 24, 3], 'inp_shapes': [[1, 8, 8, 64], [64, 3, 3, 64], [64]], 'out_idxes': [55], 'out_shapes': [[1, 8, 8, 64]], 'params': [0, 0, 0, 1, 1], 'mask': []}, {'layer_type': 'Add', 'inp_idxes': [55, 53], 'inp_shapes': [[1, 8, 8, 64], [1, 8, 8, 64]], 'out_idxes': [56], 'out_shapes': [[1, 8, 8, 64]], 'params': [1], 'mask': []}, {'layer_type': 'Mean', 'inp_idxes': [56, 25], 'inp_shapes': [[1, 8, 8, 64], [2]], 'out_idxes': [57], 'out_shapes': [[1, 64]], 'params': [1, 2], 'mask': []}, {'layer_type': 'FullyConnected', 'inp_idxes': [57, 26], 'inp_shapes': [[1, 64], [100, 64]], 'out_idxes': [58], 'out_shapes': [[1, 100]], 'params': [0], 'mask': []}]\n",
      "\n",
      "keep tensors: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, -1}\n",
      "skipping generated tensor: 0, b'serving_default_input_1:0'\n",
      "skipping generated tensor: 27, b'res_net20_cifar100_5/re_lu_90/Relu;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_105/Conv2D'\n",
      "skipping generated tensor: 28, b'res_net20_cifar100_5/re_lu_91/Relu;res_net20_cifar100_5/batch_normalization_106/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_106/Conv2D'\n",
      "skipping generated tensor: 29, b'res_net20_cifar100_5/batch_normalization_107/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_107/Conv2D'\n",
      "skipping generated tensor: 30, b'res_net20_cifar100_5/re_lu_92/Relu;res_net20_cifar100_5/add'\n",
      "skipping generated tensor: 31, b'res_net20_cifar100_5/re_lu_93/Relu;res_net20_cifar100_5/batch_normalization_108/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_108/Conv2D'\n",
      "skipping generated tensor: 32, b'res_net20_cifar100_5/batch_normalization_109/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_109/Conv2D'\n",
      "skipping generated tensor: 33, b'res_net20_cifar100_5/re_lu_94/Relu;res_net20_cifar100_5/add_1'\n",
      "skipping generated tensor: 34, b'res_net20_cifar100_5/re_lu_95/Relu;res_net20_cifar100_5/batch_normalization_110/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_110/Conv2D'\n",
      "skipping generated tensor: 35, b'res_net20_cifar100_5/batch_normalization_111/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_105/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_111/Conv2D'\n",
      "skipping generated tensor: 36, b'res_net20_cifar100_5/re_lu_96/Relu;res_net20_cifar100_5/add_2'\n",
      "skipping generated tensor: 37, b'res_net20_cifar100_5/re_lu_97/Relu;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_112/Conv2D'\n",
      "skipping generated tensor: 38, b'res_net20_cifar100_5/batch_normalization_113/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_113/Conv2D'\n",
      "skipping generated tensor: 39, b'res_net20_cifar100_5/sequential_10/batch_normalization_114/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/sequential_10/conv2d_114/Conv2D'\n",
      "skipping generated tensor: 40, b'res_net20_cifar100_5/re_lu_98/Relu;res_net20_cifar100_5/add_3'\n",
      "skipping generated tensor: 41, b'res_net20_cifar100_5/re_lu_99/Relu;res_net20_cifar100_5/batch_normalization_115/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_115/Conv2D'\n",
      "skipping generated tensor: 42, b'res_net20_cifar100_5/batch_normalization_116/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_116/Conv2D'\n",
      "skipping generated tensor: 43, b'res_net20_cifar100_5/re_lu_100/Relu;res_net20_cifar100_5/add_4'\n",
      "skipping generated tensor: 44, b'res_net20_cifar100_5/re_lu_101/Relu;res_net20_cifar100_5/batch_normalization_117/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_117/Conv2D'\n",
      "skipping generated tensor: 45, b'res_net20_cifar100_5/batch_normalization_118/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_112/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_118/Conv2D'\n",
      "skipping generated tensor: 46, b'res_net20_cifar100_5/re_lu_102/Relu;res_net20_cifar100_5/add_5'\n",
      "skipping generated tensor: 47, b'res_net20_cifar100_5/re_lu_103/Relu;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_119/Conv2D'\n",
      "skipping generated tensor: 48, b'res_net20_cifar100_5/batch_normalization_120/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_120/Conv2D'\n",
      "skipping generated tensor: 49, b'res_net20_cifar100_5/sequential_11/batch_normalization_121/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/sequential_11/conv2d_121/Conv2D'\n",
      "skipping generated tensor: 50, b'res_net20_cifar100_5/re_lu_104/Relu;res_net20_cifar100_5/add_6'\n",
      "skipping generated tensor: 51, b'res_net20_cifar100_5/re_lu_105/Relu;res_net20_cifar100_5/batch_normalization_122/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_122/Conv2D'\n",
      "skipping generated tensor: 52, b'res_net20_cifar100_5/batch_normalization_123/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_123/Conv2D'\n",
      "skipping generated tensor: 53, b'res_net20_cifar100_5/re_lu_106/Relu;res_net20_cifar100_5/add_7'\n",
      "skipping generated tensor: 54, b'res_net20_cifar100_5/re_lu_107/Relu;res_net20_cifar100_5/batch_normalization_124/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_124/Conv2D'\n",
      "skipping generated tensor: 55, b'res_net20_cifar100_5/batch_normalization_125/FusedBatchNormV3;res_net20_cifar100_5/batch_normalization_119/FusedBatchNormV3/ReadVariableOp;res_net20_cifar100_5/conv2d_125/Conv2D'\n",
      "skipping generated tensor: 56, b'res_net20_cifar100_5/re_lu_108/Relu;res_net20_cifar100_5/add_8'\n",
      "skipping generated tensor: 57, b'res_net20_cifar100_5/global_average_pooling2d_5/Mean'\n",
      "\n",
      "{'layer_type': 'FullyConnected', 'inp_idxes': [57, 26], 'inp_shapes': [[1, 64], [100, 64]], 'out_idxes': [58], 'out_shapes': [[1, 100]], 'params': [0], 'mask': []}\n",
      "dict_keys(['global_sf', 'k', 'num_cols', 'num_random', 'inp_idxes', 'out_idxes', 'layers', 'tensors', 'use_selectors', 'commit_before', 'commit_after'])\n",
      "[58]\n"
     ]
    }
   ],
   "source": [
    "# convert model\n",
    "# !python ./tools/converter.py --model ./model.tflite --model_output converted_model_new.msgpack --config_output config_new.msgpack --scale_factor 512 --k 20 --num_cols 20 --num_randoms 4096\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Command to run\n",
    "command = [\n",
    "    'python', './tools/converter.py', \n",
    "    '--model', './model.tflite',\n",
    "    '--model_output', 'converted_model_new.msgpack',\n",
    "    '--config_output', 'config_new.msgpack',\n",
    "    '--scale_factor', '2048',\n",
    "    '--k', '20',\n",
    "    '--num_cols', '20',\n",
    "    '--num_randoms', '4096'\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input\n",
    "import subprocess\n",
    "\n",
    "# !python ./tools/input_converter.py --model_config converted_model_new.msgpack --inputs 7.npy --output example_inp_new.msgpack\n",
    "\n",
    "# Command to run\n",
    "command = [\n",
    "    'python', './tools/input_converter.py', \n",
    "    '--model_config', 'converted_model_new.msgpack',\n",
    "    '--inputs', '7.npy',\n",
    "    '--output', 'example_inp_new.msgpack'\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 8] Exec format error: './bin/test_circuit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m command \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./bin/test_circuit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./converted_model_new.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./example_inp_new.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkzg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run the command\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    503\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1821\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1820\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1821\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 8] Exec format error: './bin/test_circuit'"
     ]
    }
   ],
   "source": [
    "# !./bin/test_circuit ./converted_model_new.msgpack ./example_inp_new.msgpack kzg\n",
    "\n",
    "\n",
    "# Command to run\n",
    "command = ['./bin/test_circuit', './converted_model_new.msgpack', './example_inp_new.msgpack', 'kzg']\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m command \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./bin/time_circuit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./converted_model_new.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./example_inp_new.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkzg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run the command\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241m.\u001b[39mrun(command, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# Command to run\n",
    "command = ['./bin/time_circuit', './converted_model_new.msgpack', './example_inp_new.msgpack', 'kzg']\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !onnx2tf -i ./model_simplified.onnx -o model_tf --not_use_onnxsim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnx\n",
    "# from onnx_tf.backend import prepare\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python.framework import convert_to_constants\n",
    "# import numpy as np\n",
    "\n",
    "# # Step 1: Convert ONNX model to TensorFlow\n",
    "# def onnx_to_tf(onnx_model_path, output_tf_model_path):\n",
    "#     onnx_model = onnx.load(onnx_model_path)\n",
    "#     tf_rep = prepare(onnx_model)\n",
    "#     tf_rep.export_graph(output_tf_model_path)\n",
    "#     print(f\"ONNX model converted to TensorFlow and saved at {output_tf_model_path}\")\n",
    "\n",
    "# # Step 2: Modify the TensorFlow computation graph\n",
    "# def modify_tf_model_graph(concrete_func):\n",
    "#     frozen_func = convert_to_constants.convert_variables_to_constants_v2(concrete_func)\n",
    "#     frozen_func.graph.as_graph_def()\n",
    "\n",
    "#     modified_graph_def = tf.compat.v1.GraphDef()\n",
    "\n",
    "#     for node in frozen_func.graph.as_graph_def().node:\n",
    "#         if \"Dense\" in node.op:  # Check for fully connected layer\n",
    "#             # Find the input to the dense layer and check its shape\n",
    "#             input_node = node.input[0]\n",
    "            \n",
    "#             # Get the input tensor shape properly using frozen_func inputs\n",
    "#             input_tensor = None\n",
    "#             for input_tensor_item in frozen_func.inputs:\n",
    "#                 if input_node in input_tensor_item.name or input_tensor_item.name.startswith(input_node):\n",
    "#                     input_tensor = input_tensor_item\n",
    "#                     break\n",
    "\n",
    "#             if input_tensor is None:\n",
    "#                 print(f\"Input tensor {input_node} not found in frozen_func.inputs. Skipping node {node.name}.\")\n",
    "#                 continue\n",
    "            \n",
    "#             input_shape = input_tensor.shape.as_list()\n",
    "\n",
    "#             if len(input_shape) > 2:\n",
    "#                 print(f\"Input to {node.name} is more than 2D: {input_shape}, adding Flatten.\")\n",
    "#                 # Modify the graph to add a Flatten operation before this node\n",
    "#                 flatten_node = tf.raw_ops.Flatten(input=input_node, name=f\"{node.name}_flatten\")\n",
    "#                 node.input[0] = flatten_node.name  # Redirect input to the Flatten node\n",
    "        \n",
    "#         modified_graph_def.node.extend([node])\n",
    "\n",
    "#     return modified_graph_def\n",
    "\n",
    "# # Step 3: Convert TensorFlow Model to TFLite\n",
    "# def tf_to_tflite(concrete_func, output_tflite_path):\n",
    "#     converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
    "#     tflite_model = converter.convert()\n",
    "\n",
    "#     # Save the TFLite model\n",
    "#     with open(output_tflite_path, 'wb') as f:\n",
    "#         f.write(tflite_model)\n",
    "#     print(f\"TensorFlow model converted to TFLite and saved at {output_tflite_path}\")\n",
    "\n",
    "# # Main function to handle everything\n",
    "# def convert_onnx_to_tflite(onnx_model_path, output_tf_model_path, output_tflite_path):\n",
    "#     # Step 1: Convert ONNX to TensorFlow\n",
    "#     onnx_to_tf(onnx_model_path, output_tf_model_path)\n",
    "\n",
    "#     # Step 2: Load the converted TensorFlow SavedModel\n",
    "#     loaded_model = tf.saved_model.load(output_tf_model_path)\n",
    "    \n",
    "#     # Get the concrete function from the loaded model\n",
    "#     concrete_func = loaded_model.signatures['serving_default']\n",
    "\n",
    "#     # Step 3: Modify the TensorFlow computation graph (add reshape/flatten layers if needed)\n",
    "#     modified_graph = modify_tf_model_graph(concrete_func)\n",
    "\n",
    "#     # Step 4: Convert the modified TensorFlow model to TFLite\n",
    "#     tf_to_tflite(concrete_func, output_tflite_path)\n",
    "\n",
    "# # Example usage\n",
    "# onnx_model_path = 'resnet20_cifar100_dense.onnx'  # Replace with your ONNX model path\n",
    "# output_tf_model_path = 'converted_model'  # Path to save the converted TensorFlow model\n",
    "# output_tflite_path = 'final_model.tflite'  # Path to save the TFLite model\n",
    "\n",
    "# convert_onnx_to_tflite(onnx_model_path, output_tf_model_path, output_tflite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# model_path = '/Users/mm6322/Phd research/ZKML-Benchmark/ZKML-Benchmark/frameworks/zkml/tmp/inputs/unpruned.pth.tar'\n",
    "# model_state_dict = torch.load(model_path)  # Load your PyTorch model\n",
    "# model \n",
    "\n",
    "# # Export to ONNX with opset version 10\n",
    "# dummy_input = torch.randn(1, 3, 224, 224)  # Adjust to your input size\n",
    "# torch.onnx.export(model, dummy_input, \"model_opset_10.onnx\", opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnx\n",
    "# from onnx import helper\n",
    "# from onnx import TensorProto\n",
    "\n",
    "# # Load the ONNX model\n",
    "# model = onnx.load('./tmp/inputs/model_simplified.onnx')\n",
    "\n",
    "# # Find the Reshape node\n",
    "# reshape_node = None\n",
    "# for node in model.graph.node:\n",
    "#     if node.op_type == 'Reshape':\n",
    "#         reshape_node = node\n",
    "#         break\n",
    "\n",
    "# if reshape_node is None:\n",
    "#     raise ValueError(\"Reshape node not found in the model.\")\n",
    "\n",
    "# # Create an Identity node after the Reshape\n",
    "# identity_output = reshape_node.output[0] + '_identity'\n",
    "# identity_node = helper.make_node(\n",
    "#     'Identity',\n",
    "#     inputs=[reshape_node.output[0]],\n",
    "#     outputs=[identity_output],\n",
    "#     name='Identity_after_Reshape'\n",
    "# )\n",
    "\n",
    "# # Update the next node to use the output of the Identity node\n",
    "# for node in model.graph.node:\n",
    "#     for idx, inp in enumerate(node.input):\n",
    "#         if inp == reshape_node.output[0]:\n",
    "#             node.input[idx] = identity_output\n",
    "\n",
    "# # Add the Identity node to the graph\n",
    "# model.graph.node.append(identity_node)\n",
    "\n",
    "# # Save the modified model\n",
    "# onnx.save(model, 'modified_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnx\n",
    "# from onnx import version_converter\n",
    "\n",
    "# # Load the ONNX model\n",
    "# onnx_model_path = '/Users/mm6322/Phd research/ZKML-Benchmark/ZKML-Benchmark/frameworks/zkml/tmp/inputs/resnet20_cifar100_dense.onnx'  # Replace with your ONNX file path\n",
    "# onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# # Check the current opset version of the model\n",
    "# current_opset = onnx_model.opset_import[0].version\n",
    "# print(f\"Current opset version: {current_opset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Convert the ONNX model to opset version 10\n",
    "# converted_model = version_converter.convert_version(onnx_model, 14)\n",
    "\n",
    "# # Save the converted model\n",
    "# converted_model_path = 'model_opset_10.onnx'  # Replace with your desired output path\n",
    "# onnx.save(converted_model, converted_model_path)\n",
    "\n",
    "# print(f\"Model saved with opset version 10 at {converted_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Load the TensorFlow SavedModel\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(\"model_tf\")\n",
    "\n",
    "# # (Optional) Enable optimizations\n",
    "# # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# # Convert the model\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # Save the TFLite model\n",
    "# with open(\"model.tflite\", \"wb\") as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from onnx_tf.backend import prepare\n",
    "# import onnx\n",
    "\n",
    "# # Load the ONNX model\n",
    "# onnx_model = onnx.load(\"./tmp/inputs/resnet20_cifar_dense.onnx\")\n",
    "\n",
    "# # Convert the ONNX model to TensorFlow\n",
    "# tf_rep = prepare(onnx_model)\n",
    "\n",
    "# # Export the TensorFlow model\n",
    "# tf_rep.export_graph(\"model_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnx\n",
    "# from onnx_tf.backend import prepare\n",
    "# import tensorflow as tf\n",
    "# import os\n",
    "\n",
    "# # Step 1: Load the ONNX model\n",
    "# onnx_model_path = './tmp/inputs/resnet20_cifar_dense.onnx' # Replace with your ONNX model path\n",
    "# onnx_model = onnx.load(onnx_model_path)\n",
    "# print(\"ONNX model loaded successfully.\")\n",
    "\n",
    "# # Step 2: Convert ONNX model to TensorFlow SavedModel\n",
    "# print(\"Converting ONNX model to TensorFlow format...\")\n",
    "# tf_rep = prepare(onnx_model)\n",
    "\n",
    "# # Define the path to save the TensorFlow SavedModel\n",
    "# tf_model_path = 'saved_model'\n",
    "# if not os.path.exists(tf_model_path):\n",
    "#     os.makedirs(tf_model_path)\n",
    "\n",
    "# # Export the TensorFlow model\n",
    "# tf_rep.export_graph(tf_model_path)\n",
    "# print(f\"TensorFlow SavedModel exported to {tf_model_path}.\")\n",
    "\n",
    "# # Step 3: Convert TensorFlow SavedModel to TFLite model\n",
    "# print(\"Converting TensorFlow SavedModel to TFLite format...\")\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "\n",
    "# # Optional: Set optimization flags (e.g., for quantization)\n",
    "# # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# # Perform the conversion\n",
    "# tflite_model = converter.convert()\n",
    "# print(\"Conversion to TFLite format completed.\")\n",
    "\n",
    "# # Step 4: Save the TFLite model\n",
    "# tflite_model_path = 'model.tflite'  # Replace with your desired TFLite model path\n",
    "# with open(tflite_model_path, 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "# print(f\"TFLite model saved to {tflite_model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zkml_bench_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
