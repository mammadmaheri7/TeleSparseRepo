{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# def get_cpu_load():\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = psutil.cpu_percent(interval=1, percpu=True)\n",
    "#     return cpu_loads\n",
    "\n",
    "# def select_k_cpus_with_lowest_load(k):\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = get_cpu_load()\n",
    "    \n",
    "#     # Create a list of tuples (core_id, load)\n",
    "#     cpu_load_tuples = list(enumerate(cpu_loads))\n",
    "    \n",
    "#     # Sort the list based on load\n",
    "#     sorted_cpu_loads = sorted(cpu_load_tuples, key=lambda x: x[1])\n",
    "    \n",
    "#     # Get the IDs of the K cores with lowest load\n",
    "#     selected_cpus = [core[0] for core in sorted_cpu_loads[:k]]\n",
    "    \n",
    "#     return selected_cpus\n",
    "\n",
    "# # Example usage\n",
    "# k = 8  # Number of CPUs with lowest load\n",
    "# selected_cpus = select_k_cpus_with_lowest_load(k)\n",
    "# print(f\"Selected {k} CPUs with lowest load:\", selected_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2a39bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "!which python3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "\n",
    "# def set_cpu_affinity(pid, cpu_list):\n",
    "#     try:\n",
    "#         p = psutil.Process(pid)\n",
    "#         p.cpu_affinity(cpu_list)\n",
    "#         print(f\"CPU affinity for process {pid} set to: {cpu_list}\")\n",
    "#     except psutil.NoSuchProcess:\n",
    "#         print(f\"Process with PID {pid} does not exist.\")\n",
    "#     except psutil.AccessDenied:\n",
    "#         print(\"Permission denied. You may need sudo privileges to set CPU affinity.\")\n",
    "\n",
    "# # Get the process ID of the current process\n",
    "# pid = os.getpid()\n",
    "\n",
    "# # Set the CPU affinity to only CPU core 0\n",
    "# cpu_list = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "# set_cpu_affinity(pid, cpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5fc387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as Fhtop\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "# check if notebook is in colab\n",
    "try:\n",
    "    # install ezkl\n",
    "    import google.colab\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
    "\n",
    "# rely on local installation of ezkl if the notebook is not in colab\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# uncomment for more descriptive logging \n",
    "import logging\n",
    "FORMAT = '%(levelname)s %(name)s %(asctime)-15s %(filename)s:%(lineno)d %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EZKL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import argparse\n",
    "\n",
    "def get_args_parser(initial_args=None):\n",
    "    parser = argparse.ArgumentParser('ConvNeXt training and evaluation script for image classification', add_help=False)\n",
    "#     parser.add_argument('--batch_size', default=256, type=int,\n",
    "#                         help='Per GPU batch size')\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--update_freq', default=1, type=int,\n",
    "                        help='gradient accumulation steps')\n",
    "\n",
    "    # Model parameters\n",
    "#     parser.add_argument('--model', default='convnext_tiny', type=str, metavar='MODEL',\n",
    "#                         help='Name of model to train')\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='image input size')\n",
    "    parser.add_argument('--layer_scale_init_value', default=1e-6, type=float,\n",
    "                        help=\"Layer scale initial values\")\n",
    "    \n",
    "    ########################## settings specific to this project ##########################\n",
    "    \n",
    "    # dropout and stochastic depth drop rate; set at most one to non-zero\n",
    "    parser.add_argument('--dropout', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    parser.add_argument('--drop_path', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    \n",
    "    # early / late dropout and stochastic depth settings\n",
    "    parser.add_argument('--drop_mode', type=str, default='standard', choices=['standard', 'early', 'late'], help='drop mode')\n",
    "    parser.add_argument('--drop_schedule', type=str, default='constant', choices=['constant', 'linear'], \n",
    "                        help='drop schedule for early dropout / s.d. only')\n",
    "    parser.add_argument('--cutoff_epoch', type=int, default=0, \n",
    "                        help='if drop_mode is early / late, this is the epoch where dropout ends / starts')\n",
    "    \n",
    "    ####################################################################################### \n",
    "    \n",
    "    # EMA related parameters\n",
    "    parser.add_argument('--model_ema', type=str2bool, default=False)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', type=str2bool, default=False, help='')\n",
    "    parser.add_argument('--model_ema_eval', type=str2bool, default=False, help='Using ema to eval during training.')\n",
    "\n",
    "    # Optimization parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "        weight decay. We use a cosine schedule for WD and using a larger decay by\n",
    "        the end of training improves performance for ViTs.\"\"\")\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=4e-3, metavar='LR',\n",
    "                        help='learning rate (default: 4e-3), with total batch size 4096')\n",
    "    parser.add_argument('--layer_decay', type=float, default=1.0)\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-6)')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=50, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--crop_pct', type=float, default=None)\n",
    "\n",
    "    # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', type=str2bool, default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "    parser.add_argument('--finetune', default='',\n",
    "                        help='finetune from checkpoint')\n",
    "    parser.add_argument('--head_init_scale', default=1.0, type=float,\n",
    "                        help='classifier head initial scale, typically adjusted in fine-tuning')\n",
    "    parser.add_argument('--model_key', default='model|module', type=str,\n",
    "                        help='which key to load from saved state dict, usually model or model_ema')\n",
    "    parser.add_argument('--model_prefix', default='', type=str)\n",
    "\n",
    "    # Dataset parameters\n",
    "#     parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "#                         help='dataset path')\n",
    "    parser.add_argument('--eval_data_path', default=None, type=str,\n",
    "                        help='dataset path for evaluation')\n",
    "    parser.add_argument('--nb_classes', default=1000, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--imagenet_default_mean_and_std', type=str2bool, default=True)\n",
    "    parser.add_argument('--data_set', default='IMNET', choices=['CIFAR', 'IMNET', 'image_folder'],\n",
    "                        type=str, help='ImageNet dataset path')\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "#     parser.add_argument('--resume', default='',\n",
    "#                         help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt_freq', default=1, type=int)\n",
    "    parser.add_argument('--save_ckpt_num', default=3, type=int)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', type=str2bool, default=False,\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', type=str2bool, default=True,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--disable_eval', type=str2bool, default=False,\n",
    "                        help='Disabling evaluation during training')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', type=str2bool, default=True,\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', type=str2bool, default=False)\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--use_amp', type=str2bool, default=False, \n",
    "                        help=\"Use PyTorch's AMP (Automatic Mixed Precision) or not\")\n",
    "\n",
    "    # Weights and Biases arguments\n",
    "    parser.add_argument('--enable_wandb', type=str2bool, default=False,\n",
    "                        help=\"enable logging to Weights and Biases\")\n",
    "    parser.add_argument('--project', default='convnext', type=str,\n",
    "                        help=\"The name of the W&B project where you're sending the new run.\")\n",
    "    parser.add_argument('--wandb_ckpt', type=str2bool, default=False,\n",
    "                        help=\"Save model checkpoints as W&B Artifacts.\")\n",
    "\n",
    "    # arguments for pruning\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=4096)\n",
    "#     parser.add_argument(\"--sparsity\", type=float, default=0.)\n",
    "    parser.add_argument(\"--prune_metric\", type=str, choices=[\"magnitude\", \"wanda\"])\n",
    "    parser.add_argument(\"--prune_granularity\", type=str)\n",
    "    parser.add_argument(\"--blocksize\", type=int, default=1)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"\n",
    "    Converts string to bool type; enables command line \n",
    "    arguments in the format of '--arg1 true --arg2 false'\n",
    "    \"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def get_default_args():\n",
    "    parser = get_args_parser()\n",
    "    default_args = {}\n",
    "    for action in parser._actions:\n",
    "        # Check if action is an argument\n",
    "        if not action.option_strings:\n",
    "            continue\n",
    "        # Use the destination as the key and the default value as the value\n",
    "        default_args[action.dest] = action.default\n",
    "    return DefaultArgs(**default_args)\n",
    "\n",
    "# Example usage:\n",
    "default_args = get_default_args()\n",
    "args = default_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== IMPORTANT: INPUT FLAGS OF PYTHON CODE HERE =====\n",
    "\n",
    "# args.model = \"vit_tiny\"\n",
    "# args.data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "\n",
    "# args.resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# #args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "# # args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/model_weights/deit/deit_tiny_patch16_224-a1311bcf.pth\"\n",
    "\n",
    "\n",
    "# args.sparsity = 0.5\n",
    "# args.batch_size = 32\n",
    "\n",
    "# args.pruning_method = \"CAP\" # DENSE,CAP,WANDA\n",
    "\n",
    "# # args.prune_metric = \"wanda\"\n",
    "# # args.prune_granularity = \"row\"\n",
    "# # pruned_model_dir = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "\n",
    "# # Prefix directory\n",
    "# args.prefix_dir = \"sparse-cap-acc/\" #sparse-cap-acc, dense-acc, sparse-wanda-acc\n",
    "# os.makedirs(args.prefix_dir, exist_ok=True)\n",
    "\n",
    "# # EZKL HPs\n",
    "# args.input_param_scale = 7\n",
    "# args.log_rows = 20\n",
    "# args.num_cols = 4\n",
    "# args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- IN_JUPYTER: True ---- \n",
      "Model: vit_tiny\n",
      "Data Path: /rds/general/user/mm6322/home/imagenet\n",
      "Resume Path: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "Sparsity: 0.5\n",
      "Batch Size: 1\n",
      "Pruning Method: CAP\n",
      "Prefix Directory: sparse-cap-acc-tmp/\n",
      "Input Parameter Scale: 7\n",
      "Log Rows: 20\n",
      "Number of Columns: 2\n",
      "Scale Rebase Multiplier: 1\n",
      "-------- \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# Check if running in Jupyter Notebook\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        IN_JUPYTER = True\n",
    "    else:\n",
    "        IN_JUPYTER = False\n",
    "except:\n",
    "    IN_JUPYTER = False\n",
    "\n",
    "# Define default values\n",
    "default_model = \"vit_tiny\"\n",
    "default_data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "default_resume = \"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "default_sparsity = 0.5\n",
    "default_batch_size = 1\n",
    "default_pruning_method = \"CAP\"\n",
    "default_prefix_dir = \"sparse-cap-acc-tmp/\"\n",
    "default_input_param_scale = 7\n",
    "default_log_rows = 20\n",
    "default_num_cols = 2\n",
    "default_scale_rebase_multiplier = 1\n",
    "\n",
    "# Parse command-line arguments if not in Jupyter Notebook\n",
    "if not IN_JUPYTER:\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('--model', default=default_model, type=str, help='Model type')\n",
    "    parser.add_argument('--data_path', default=default_data_path, type=str, help='Data path')\n",
    "    parser.add_argument('--resume', default=default_resume, type=str, help='Resume path')\n",
    "    parser.add_argument('--sparsity', default=default_sparsity, type=float, help='Sparsity value')\n",
    "    parser.add_argument('--batch_size', default=default_batch_size, type=int, help='Batch size')\n",
    "    parser.add_argument('--pruning_method', default=default_pruning_method, type=str, help='Pruning method')\n",
    "    parser.add_argument('--prefix_dir', default=default_prefix_dir, type=str, help='Prefix directory')\n",
    "    parser.add_argument('--input_param_scale', default=default_input_param_scale, type=int, help='Input parameter scale')\n",
    "    parser.add_argument('--log_rows', default=default_log_rows, type=int, help='Log rows')\n",
    "    parser.add_argument('--num_cols', default=default_num_cols, type=int, help='Number of columns')\n",
    "    parser.add_argument('--scale_rebase_multiplier', default=default_scale_rebase_multiplier, type=int, help='Scale rebase multiplier')\n",
    "\n",
    "    args = parser.parse_args(namespace=args)\n",
    "else:\n",
    "    # In Jupyter Notebook, define args with default values\n",
    "    args.model = default_model\n",
    "    args.data_path = default_data_path\n",
    "    args.resume = default_resume\n",
    "    args.sparsity = default_sparsity\n",
    "    args.batch_size = default_batch_size\n",
    "    args.pruning_method = default_pruning_method\n",
    "    args.prefix_dir = default_prefix_dir\n",
    "    args.input_param_scale = default_input_param_scale\n",
    "    args.log_rows = default_log_rows\n",
    "    args.num_cols = default_num_cols\n",
    "    args.scale_rebase_multiplier = default_scale_rebase_multiplier\n",
    "    \n",
    "    \n",
    "print(\"\\n ---- IN_JUPYTER:\",str(IN_JUPYTER),\"---- \")\n",
    "# Print values for verification\n",
    "print(\"Model:\", args.model)\n",
    "print(\"Data Path:\", args.data_path)\n",
    "print(\"Resume Path:\", args.resume)\n",
    "print(\"Sparsity:\", args.sparsity)\n",
    "print(\"Batch Size:\", args.batch_size)\n",
    "print(\"Pruning Method:\", args.pruning_method)\n",
    "print(\"Prefix Directory:\", args.prefix_dir)\n",
    "print(\"Input Parameter Scale:\", args.input_param_scale)\n",
    "print(\"Log Rows:\", args.log_rows)\n",
    "print(\"Number of Columns:\", args.num_cols)\n",
    "print(\"Scale Rebase Multiplier:\", args.scale_rebase_multiplier)\n",
    "print(\"-------- \\n\\n\\n\")\n",
    "\n",
    "os.makedirs(args.prefix_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 300\n",
      "update_freq: 1\n",
      "input_size: 224\n",
      "layer_scale_init_value: 1e-06\n",
      "dropout: 0\n",
      "drop_path: 0\n",
      "drop_mode: standard\n",
      "drop_schedule: constant\n",
      "cutoff_epoch: 0\n",
      "model_ema: False\n",
      "model_ema_decay: 0.9999\n",
      "model_ema_force_cpu: False\n",
      "model_ema_eval: False\n",
      "opt: adamw\n",
      "opt_eps: 1e-08\n",
      "opt_betas: None\n",
      "clip_grad: None\n",
      "momentum: 0.9\n",
      "weight_decay: 0.05\n",
      "weight_decay_end: None\n",
      "lr: 0.004\n",
      "layer_decay: 1.0\n",
      "min_lr: 1e-06\n",
      "warmup_epochs: 50\n",
      "warmup_steps: -1\n",
      "color_jitter: 0.4\n",
      "aa: rand-m9-mstd0.5-inc1\n",
      "smoothing: 0.1\n",
      "train_interpolation: bicubic\n",
      "crop_pct: None\n",
      "reprob: 0.25\n",
      "remode: pixel\n",
      "recount: 1\n",
      "resplit: False\n",
      "mixup: 0.8\n",
      "cutmix: 1.0\n",
      "cutmix_minmax: None\n",
      "mixup_prob: 1.0\n",
      "mixup_switch_prob: 0.5\n",
      "mixup_mode: batch\n",
      "finetune: \n",
      "head_init_scale: 1.0\n",
      "model_key: model|module\n",
      "model_prefix: \n",
      "eval_data_path: None\n",
      "nb_classes: 1000\n",
      "imagenet_default_mean_and_std: True\n",
      "data_set: IMNET\n",
      "output_dir: \n",
      "device: cuda\n",
      "seed: 0\n",
      "auto_resume: True\n",
      "save_ckpt: True\n",
      "save_ckpt_freq: 1\n",
      "save_ckpt_num: 3\n",
      "start_epoch: 0\n",
      "eval: False\n",
      "dist_eval: True\n",
      "disable_eval: False\n",
      "num_workers: 10\n",
      "pin_mem: True\n",
      "world_size: 1\n",
      "local_rank: -1\n",
      "dist_on_itp: False\n",
      "dist_url: env://\n",
      "use_amp: False\n",
      "enable_wandb: False\n",
      "project: convnext\n",
      "wandb_ckpt: False\n",
      "nsamples: 4096\n",
      "prune_metric: None\n",
      "prune_granularity: None\n",
      "blocksize: 1\n",
      "model: vit_tiny\n",
      "data_path: /rds/general/user/mm6322/home/imagenet\n",
      "resume: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "sparsity: 0.5\n",
      "batch_size: 1\n",
      "pruning_method: CAP\n",
      "prefix_dir: sparse-cap-acc-tmp/\n",
      "input_param_scale: 7\n",
      "log_rows: 20\n",
      "num_cols: 2\n",
      "scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in vars(args).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.output_dir:\n",
    "    Path(default_args_dict.output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from timm.data.constants import \\\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data import create_transform\n",
    "\n",
    "def build_dataset(is_train, args):\n",
    "    transform = build_transform(is_train, args)\n",
    "\n",
    "    print(\"Transform = \")\n",
    "    if isinstance(transform, tuple):\n",
    "        for trans in transform:\n",
    "            print(\" - - - - - - - - - - \")\n",
    "            for t in trans.transforms:\n",
    "                print(t)\n",
    "    else:\n",
    "        for t in transform.transforms:\n",
    "            print(t)\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    if args.data_set == 'CIFAR':\n",
    "        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform, download=True)\n",
    "        nb_classes = 100\n",
    "    elif args.data_set == 'IMNET':\n",
    "        print(\"reading from datapath\", args.data_path)\n",
    "        root = os.path.join(args.data_path, 'train' if is_train else 'val_dirs')\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = 1000\n",
    "    elif args.data_set == \"image_folder\":\n",
    "        root = args.data_path if is_train else args.eval_data_path\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = args.nb_classes\n",
    "        assert len(dataset.class_to_idx) == nb_classes\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    print(\"Number of the class = %d\" % nb_classes)\n",
    "\n",
    "    return dataset, nb_classes\n",
    "\n",
    "\n",
    "def build_transform(is_train, args):\n",
    "    resize_im = args.input_size > 32\n",
    "    imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n",
    "    mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n",
    "    std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n",
    "\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=args.input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            interpolation=args.train_interpolation,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                args.input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        # warping (no cropping) when evaluated at 384 or larger\n",
    "        if args.input_size >= 384:  \n",
    "            t.append(\n",
    "            transforms.Resize((args.input_size, args.input_size), \n",
    "                            interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "        )\n",
    "            print(f\"Warping {args.input_size} size input images...\")\n",
    "        else:\n",
    "            if args.crop_pct is None:\n",
    "                args.crop_pct = 224 / 256\n",
    "            size = int(args.input_size / args.crop_pct)\n",
    "            t.append(\n",
    "                # to maintain same ratio w.r.t. 224 images\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),  \n",
    "            )\n",
    "            t.append(transforms.CenterCrop(args.input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_train = torch.utils.data.DistributedSampler(\n",
    "#         dataset_train, num_replicas=1, rank=0, shuffle=True, seed=args.seed,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_train = torch.utils.data.DataLoader(\n",
    "#     dataset_train, sampler=sampler_train,\n",
    "#     batch_size=args.batch_size,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.12'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import numpy as np\n",
    "from timm.utils import get_state_dict\n",
    "\n",
    "from pathlib import Path\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "# from torch._six import inf\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "class TensorboardLogger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = SummaryWriter(logdir=log_dir)\n",
    "        self.step = 0\n",
    "\n",
    "    def set_step(self, step=None):\n",
    "        if step is not None:\n",
    "            self.step = step\n",
    "        else:\n",
    "            self.step += 1\n",
    "\n",
    "    def update(self, head='scalar', step=None, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.writer.add_scalar(head + \"/\" + k, v, self.step if step is None else step)\n",
    "\n",
    "    def flush(self):\n",
    "        self.writer.flush()\n",
    "\n",
    "\n",
    "class WandbLogger(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        try:\n",
    "            import wandb\n",
    "            self._wandb = wandb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use the Weights and Biases Logger please install wandb.\"\n",
    "                \"Run `pip install wandb` to install it.\"\n",
    "            )\n",
    "\n",
    "        # Initialize a W&B run \n",
    "        if self._wandb.run is None:\n",
    "            self._wandb.init(\n",
    "                project=args.project,\n",
    "                config=args\n",
    "            )\n",
    "\n",
    "    def log_epoch_metrics(self, metrics, commit=True):\n",
    "        \"\"\"\n",
    "        Log train/test metrics onto W&B.\n",
    "        \"\"\"\n",
    "        # Log number of model parameters as W&B summary\n",
    "        self._wandb.summary['n_parameters'] = metrics.get('n_parameters', None)\n",
    "        metrics.pop('n_parameters', None)\n",
    "\n",
    "        # Log current epoch\n",
    "        self._wandb.log({'epoch': metrics.get('epoch')}, commit=False)\n",
    "        metrics.pop('epoch')\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            if 'train' in k:\n",
    "                self._wandb.log({f'Global Train/{k}': v}, commit=False)\n",
    "            elif 'test' in k:\n",
    "                self._wandb.log({f'Global Test/{k}': v}, commit=False)\n",
    "\n",
    "        self._wandb.log({})\n",
    "\n",
    "    def log_checkpoints(self):\n",
    "        output_dir = self.args.output_dir\n",
    "        model_artifact = self._wandb.Artifact(\n",
    "            self._wandb.run.id + \"_model\", type=\"model\"\n",
    "        )\n",
    "\n",
    "        model_artifact.add_dir(output_dir)\n",
    "        self._wandb.log_artifact(model_artifact, aliases=[\"latest\", \"best\"])\n",
    "\n",
    "    def set_steps(self):\n",
    "        # Set global training step\n",
    "        self._wandb.define_metric('Rank-0 Batch Wise/*', step_metric='Rank-0 Batch Wise/global_train_step')\n",
    "        # Set epoch-wise step\n",
    "        self._wandb.define_metric('Global Train/*', step_metric='epoch')\n",
    "        self._wandb.define_metric('Global Test/*', step_metric='epoch')\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "def load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "        \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.weight$')\n",
    "    state_dict_keys = list(state_dict.keys())\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.weight\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "            \n",
    "    \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.bias$')\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.bias\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "                                              \n",
    "    \n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(model, prefix=prefix)\n",
    "\n",
    "    warn_missing_keys = []\n",
    "    ignore_missing_keys = []\n",
    "    for key in missing_keys:\n",
    "        keep_flag = True\n",
    "        for ignore_key in ignore_missing.split('|'):\n",
    "            if ignore_key in key:\n",
    "                keep_flag = False\n",
    "                break\n",
    "        if keep_flag:\n",
    "            warn_missing_keys.append(key)\n",
    "        else:\n",
    "            ignore_missing_keys.append(key)\n",
    "            \n",
    "\n",
    "    missing_keys = warn_missing_keys\n",
    "\n",
    "    if len(missing_keys) > 0:\n",
    "        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "            model.__class__.__name__, unexpected_keys))\n",
    "    if len(ignore_missing_keys) > 0:\n",
    "        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, ignore_missing_keys))\n",
    "    if len(error_msgs) > 0:\n",
    "        print('\\n'.join(error_msgs))\n",
    "\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "\n",
    "        ########################################################\n",
    "        ## Code I added \n",
    "        for param in parameters:\n",
    "            weight_copy = param.data.abs().clone()\n",
    "            mask = weight_copy.gt(0).float().cuda()\n",
    "            sparsity = mask.sum() / mask.numel()\n",
    "            if sparsity > 0.3:\n",
    "                # non-trivial sparsity \n",
    "                param.grad.data.mul_(mask)\n",
    "        ########################################################\n",
    "\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n",
    "                     start_warmup_value=0, warmup_steps=-1):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_steps > 0:\n",
    "        warmup_iters = warmup_steps\n",
    "    print(\"Set warmup steps = %d\" % warmup_iters)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = np.array(\n",
    "        [final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        to_save = {\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'scaler': loss_scaler.state_dict(),\n",
    "            'args': args,\n",
    "        }\n",
    "\n",
    "        if model_ema is not None:\n",
    "            to_save['model_ema'] = get_state_dict(model_ema)\n",
    "\n",
    "        save_on_master(to_save, checkpoint_path)\n",
    "\n",
    "    if is_main_process() and isinstance(epoch, int):\n",
    "        to_del = epoch - args.save_ckpt_num * args.save_ckpt_freq\n",
    "        old_ckpt = output_dir / ('checkpoint-%s.pth' % to_del)\n",
    "        if os.path.exists(old_ckpt):\n",
    "            os.remove(old_ckpt)\n",
    "\n",
    "\n",
    "def auto_load_model(args, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.auto_resume and len(args.resume) == 0:\n",
    "        import glob\n",
    "        all_checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint-*.pth'))\n",
    "        latest_ckpt = -1\n",
    "        for ckpt in all_checkpoints:\n",
    "            t = ckpt.split('-')[-1].split('.')[0]\n",
    "            if t.isdigit():\n",
    "                latest_ckpt = max(int(t), latest_ckpt)\n",
    "        if latest_ckpt >= 0:\n",
    "            args.resume = os.path.join(output_dir, 'checkpoint-%d.pth' % latest_ckpt)\n",
    "        print(\"Auto resume checkpoint: %s\" % args.resume)\n",
    "\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        print(\"Resume checkpoint %s\" % args.resume)\n",
    "        if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            if not isinstance(checkpoint['epoch'], str): # does not support resuming with 'best', 'best-ema'\n",
    "                args.start_epoch = checkpoint['epoch'] + 1\n",
    "            else:\n",
    "                assert args.eval, 'Does not support resuming with checkpoint-best'\n",
    "            if hasattr(args, 'model_ema') and args.model_ema:\n",
    "                if 'model_ema' in checkpoint.keys():\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model_ema'])\n",
    "                else:\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model'])\n",
    "            if 'scaler' in checkpoint:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            print(\"With optim & sched!\")\n",
    "\n",
    "def reg_scheduler(base_value, final_value, epochs, niter_per_ep, early_epochs=0, early_value=None, \n",
    "           mode='linear', early_mode='regular'):\n",
    "    early_schedule = np.array([])\n",
    "    early_iters = early_epochs * niter_per_ep\n",
    "    if early_value is None:\n",
    "        early_value = final_value\n",
    "    if early_epochs > 0:\n",
    "        print(f\"Set early value to {early_mode} {early_value}\")\n",
    "        if early_mode == 'regular':\n",
    "            early_schedule = np.array([early_value] * early_iters)\n",
    "        elif early_mode == 'linear':\n",
    "            early_schedule = np.linspace(early_value, base_value, early_iters)\n",
    "        elif early_mode == 'cosine':\n",
    "            early_schedule = np.array(\n",
    "            [base_value + 0.5 * (early_value - base_value) * (1 + math.cos(math.pi * i / early_iters)) for i in np.arange(early_iters)])\n",
    "    regular_epochs = epochs - early_epochs\n",
    "    iters = np.arange(regular_epochs * niter_per_ep)\n",
    "    schedule = np.linspace(base_value, final_value, len(iters))\n",
    "    schedule = np.concatenate((early_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def build_model(args, pretrained=False):\n",
    "    if args.model.startswith(\"convnext\"):\n",
    "        model = create_model(\n",
    "            args.model,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=args.nb_classes,\n",
    "            layer_scale_init_value=args.layer_scale_init_value,\n",
    "            head_init_scale=args.head_init_scale,\n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate=args.dropout,\n",
    "            )\n",
    "    else:\n",
    "        model = create_model(\n",
    "            args.model, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=args.nb_classes, \n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate =args.dropout\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCHS: 1\n"
     ]
    }
   ],
   "source": [
    "BATCHS = args.batch_size\n",
    "print(\"BATCHS:\",BATCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.resnet import resnet26d, resnet50d\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
    "    ),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_huge_patch16_224': _cfg(),\n",
    "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
    "    # hybrid models\n",
    "    'vit_small_resnet26d_224': _cfg(),\n",
    "    'vit_small_resnet50d_s3_224': _cfg(),\n",
    "    'vit_base_resnet26d_224': _cfg(),\n",
    "    'vit_base_resnet50d_224': _cfg(),\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        \n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "#         self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.query_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.key_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.value_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "#         query = self.query_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         key = self.key_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         value = self.value_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "        \n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        qkvs = self.qkv(x)\n",
    "        query = qkvs[:,:,0:self.dim]\n",
    "        key = qkvs[:,:,self.dim:2*self.dim]\n",
    "        value = qkvs[:,:,2*self.dim:] \n",
    "        query = query.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        key = key.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        value = value.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale     \n",
    "        attn = query.transpose(1,2) @ key.transpose(1,2).transpose(2,3)\n",
    "        attn = attn * self.scale\n",
    "\n",
    "\n",
    "        attn = attn.softmax(dim=(-1))\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        \n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        y = (attn@value.transpose(1,2)).transpose(1,2).reshape(B,N,C)\n",
    "        \n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "        y = self.proj(y)\n",
    "        y = self.proj_drop(y)\n",
    "#         return x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "#         self.norm1 = norm_layer(self.dim)\n",
    "        self.norm1 = norm_layer(self.dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(self.dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))      \n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def half_block(self,x):\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "#         assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "#             f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "#         x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x).reshape(self.batch_size,192,196,1).transpose(1,2)\n",
    "        x = x.squeeze(3)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# class HybridEmbed(nn.Module):\n",
    "#     \"\"\" CNN Feature Map Embedding\n",
    "#     Extract feature map from CNN, flatten, project to embedding dim.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         assert isinstance(backbone, nn.Module)\n",
    "#         img_size = to_2tuple(img_size)\n",
    "#         self.img_size = img_size\n",
    "#         self.backbone = backbone\n",
    "#         if feature_size is None:\n",
    "#             with torch.no_grad():\n",
    "#                 # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
    "#                 # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
    "#                 # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
    "#                 training = backbone.training\n",
    "#                 if training:\n",
    "#                     backbone.eval()\n",
    "#                 o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "#                 feature_size = o.shape[-2:]\n",
    "#                 feature_dim = o.shape[1]\n",
    "#                 backbone.train(training)\n",
    "#         else:\n",
    "#             feature_size = to_2tuple(feature_size)\n",
    "#             feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "#         self.num_patches = feature_size[0] * feature_size[1]\n",
    "#         self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)[-1]\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "#         x = self.proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        # I add these two lines\n",
    "        self.drop_rate=drop_rate\n",
    "        attn_drop_rate=drop_rate\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.depth = depth\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
    "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
    "        #self.repr_act = nn.Tanh()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    " \n",
    "        x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "        B = BATCHS\n",
    "    \n",
    "\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1)\n",
    "        \n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def split_convs(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "#       B = x.shape[0]\n",
    "        B = BATCHS\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = torch.cat((self.cls_token, x), dim=1)\n",
    "    \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "#         num_blocks = len(self.blocks) // 2\n",
    "#         for blk in self.blocks[:num_blocks]:\n",
    "#             x = blk(x)\n",
    "\n",
    "#         x = self.blocks[0].half_block(x)\n",
    "\n",
    "#         print(\"split_1, output.shape:\",x.shape, \"\\t num_blocks:\",num_blocks)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def split_2(self,x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks[0].half_block(x)\n",
    "        return x\n",
    "    \n",
    "    def split_n(self,x,n,half=None):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for layer_idx in range(n-1):\n",
    "            x = self.blocks[layer_idx](x)\n",
    "            \n",
    "        #n-th layer\n",
    "        if half:\n",
    "            x = self.blocks[n].half_block(x)\n",
    "        else:\n",
    "            x = self.blocks[n](x)\n",
    "            # last layer of transformer\n",
    "            if n == (self.depth - 1):\n",
    "                x = self.norm(x)\n",
    "                x = x[:, 0]\n",
    "                x = self.head(x)\n",
    "                \n",
    "        return x\n",
    "            \n",
    "          \n",
    "    def update_drop_path(self, drop_path_rate):\n",
    "        self.drop_path = drop_path_rate\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, self.depth)]\n",
    "        for i in range(self.depth):\n",
    "            self.blocks[i].drop_path.drop_prob = dp_rates[i]\n",
    "    \n",
    "    def update_dropout(self, drop_rate):\n",
    "        self.drop_rate = drop_rate\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = drop_rate\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# @register_model\n",
    "# def vit_tiny_tiny(pretrained=False, **kwargs):\n",
    "#     model = VisionTransformer(\n",
    "#         patch_size=16, embed_dim=48, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "@register_model\n",
    "def vit_tiny(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_small(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_base(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_large(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ce87a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(args, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.VisionTransformer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 5717416\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_40903/3668749705.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "# total_batch_size = args.batch_size * args.update_freq * utils.get_world_size()\n",
    "total_batch_size = args.batch_size * args.update_freq\n",
    "# num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n",
    "\n",
    "# At most one of dropout and stochastic depth should be enabled.\n",
    "assert(args.dropout == 0 or args.drop_path == 0)\n",
    "# ConvNeXt does not support dropout.\n",
    "assert(args.dropout == 0 if args.model.startswith(\"convnext\") else True)\n",
    "\n",
    "import re\n",
    "\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# cnt = 0\n",
    "# dl = DataLoader(dataset_train, batch_size=BATCHS, shuffle=True)\n",
    "# # data = dataset_train[cnt][0].unsqueeze(dim=0)\n",
    "# # label = dataset_train[cnt][1]\n",
    "# data,label = next(iter(dl))\n",
    "# print(data.shape)\n",
    "# print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5717416\n",
      "Number of zeros: 2654208\n",
      "Percentage of zeros: 46.42%\n"
     ]
    }
   ],
   "source": [
    "# compute the number of zeros in the model / total number of parameters\n",
    "total_params = 0\n",
    "zeros = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    zeros += (param.data == 0).sum().item()\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of zeros: {zeros}\")\n",
    "print(f\"Percentage of zeros: {zeros / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50cc9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Image in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (1.5.33)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (from Image) (10.4.0)\n",
      "Requirement already satisfied: django in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (from Image) (5.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (from Image) (1.16.0)\n",
      "Requirement already satisfied: asgiref<4,>=3.8.1 in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (from django->Image) (3.8.1)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (from django->Image) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3412f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (10.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b8d425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# data = torch.randn(BATCHS,3,224,224)\n",
    "# load JPEG image /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# result = model(data.view(data.shape[0],-1))\n",
    "print(\"data shape:\",data.shape)\n",
    "result = model(data)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = data.detach().clone()\n",
    "print(\"x.shape:\",x.shape)\n",
    "\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(    \n",
    "    model,               # model being run\n",
    "    x,                   # model input (or a tuple for multiple inputs)\n",
    "    args.prefix_dir + \"network_complete.onnx\",            # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=15,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                    'output': {0:'batch_size'},\n",
    "    },         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4686a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx:\t CONV \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.9203) \t max: tensor(13.6817)\n",
      "layer_idx:\t 0 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.0635) \t max: tensor(15.4783)\n",
      "layer_idx:\t 0 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.4229) \t max: tensor(17.4164)\n",
      "layer_idx:\t 1 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.8803) \t max: tensor(10.3352)\n",
      "layer_idx:\t 1 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.4997) \t max: tensor(9.3524)\n",
      "layer_idx:\t 2 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.2025) \t max: tensor(17.3773)\n",
      "layer_idx:\t 2 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.8159) \t max: tensor(16.2393)\n",
      "layer_idx:\t 3 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.4613) \t max: tensor(15.7312)\n",
      "layer_idx:\t 3 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.2997) \t max: tensor(14.8284)\n",
      "layer_idx:\t 4 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.5867) \t max: tensor(15.6392)\n",
      "layer_idx:\t 4 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.6180) \t max: tensor(14.4870)\n",
      "layer_idx:\t 5 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.0484) \t max: tensor(15.2520)\n",
      "layer_idx:\t 5 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.7885) \t max: tensor(14.6185)\n",
      "layer_idx:\t 6 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.8097) \t max: tensor(14.7460)\n",
      "layer_idx:\t 6 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.1806) \t max: tensor(14.0622)\n",
      "layer_idx:\t 7 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-7.9359) \t max: tensor(14.7281)\n",
      "layer_idx:\t 7 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.2735) \t max: tensor(18.5691)\n",
      "layer_idx:\t 8 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-11.1971) \t max: tensor(20.7566)\n",
      "layer_idx:\t 8 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.5824) \t max: tensor(25.8723)\n",
      "layer_idx:\t 9 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-14.3955) \t max: tensor(26.9132)\n",
      "layer_idx:\t 9 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.2325) \t max: tensor(27.7766)\n",
      "layer_idx:\t 10 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5998) \t max: tensor(29.1508)\n",
      "layer_idx:\t 10 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5393) \t max: tensor(30.7623)\n",
      "layer_idx:\t 11 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-16.1201) \t max: tensor(33.1553)\n",
      "layer_idx:\t 11 \t half: False \t inter_out.shape: torch.Size([1, 1000]) \t min: tensor(-3.9342) \t max: tensor(8.3160)\n"
     ]
    }
   ],
   "source": [
    "inter_out = model.split_convs(data)\n",
    "print(\"layer_idx:\\t\",\"CONV\",\"\\t half:\",str(False),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_out = model.split_n(data,layer_idx,half)\n",
    "        print(\"layer_idx:\\t\",layer_idx,\"\\t half:\",str(half),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for all layers\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),args.prefix_dir, \"input_convs.json\")\n",
    "data = dict(input_data = [((x).detach().numpy()).reshape([-1]).tolist()])\n",
    "json.dump( data, open(data_path, 'w' ))\n",
    "\n",
    "for i in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_i = model.split_n(x,i,half=half)\n",
    "        data_path = os.path.join(os.getcwd(),args.prefix_dir, f\"input_{i}_{str(half)}.json\")\n",
    "        data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "        json.dump( data, open(data_path, 'w' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# extract all onnx files of layers\n",
    "\n",
    "input_path = args.prefix_dir + \"network_complete.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function\n",
    "def activation_hook(layer_name, activation_stats):\n",
    "    def hook(module, input, output):\n",
    "        input_tensor = input[0]\n",
    "        activation_stats[layer_name] = {\n",
    "            # l1 norm\n",
    "            'norm': input_tensor.norm(),\n",
    "            'max': input_tensor.max(),\n",
    "            'min': input_tensor.min(),\n",
    "            'shape': input_tensor.shape\n",
    "    }\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d858f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(183.8878) tensor(11.3607) tensor(-4.8354) torch.Size([1, 197, 192])\n"
     ]
    }
   ],
   "source": [
    "# teleporing the mlp model\n",
    "original_mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "# IMPORTANT: input_x_T/F is acutally the output of that block (output of the intermediate or the full block)\n",
    "\n",
    "# find the input data for the mlp model\n",
    "input_split0_False = json.load(open(args.prefix_dir + \"input_0_True.json\"))[\"input_data\"][0]\n",
    "norm_split0_False = model.blocks[0].norm2\n",
    "input_mlp_0 = norm_split0_False(torch.tensor(input_split0_False).view(1,197,192))\n",
    "out_mlp_0 = original_mlp_0(input_mlp_0)\n",
    "out_mlp_0 = model.blocks[0].drop_path(out_mlp_0)\n",
    "out_split0_false = out_mlp_0 + torch.tensor(input_split0_False).view(1,197,192)\n",
    "\n",
    "# True output of the current block == input of the next block\n",
    "out_split0_false_orginal = json.load(open(args.prefix_dir + \"input_0_False.json\"))[\"input_data\"][0]\n",
    "\n",
    "print(input_mlp_0.norm(),input_mlp_0.max(),input_mlp_0.min(),input_mlp_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47c57606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(112.1824) tensor(15.4783) tensor(-4.0635) torch.Size([1, 197, 192])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(input_split0_False).view(1,197,192)\n",
    "print(t.norm(),t.max(),t.min(),t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ccb8e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_stats: {'relu_1': {'norm': tensor(476.6286), 'max': tensor(6.2171), 'min': tensor(-6.8579), 'shape': torch.Size([1, 197, 768])}}\n",
      "Original loss: 13.07499885559082, Original prediction error: 0\n"
     ]
    }
   ],
   "source": [
    "# Register hooks to the layers before all activation functions\n",
    "activation_stats = {}\n",
    "for i, layer in enumerate(original_mlp_0.children()):\n",
    "    if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "        layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats))\n",
    "\n",
    "# Run the mlp model\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_block0 = model.split_convs(torch.tensor(input_convs).view(1,3,224,224))\n",
    "input_block0 = input_block0.view(1,197,192)\n",
    "original_block0_false_pred = model.blocks[0](input_block0)\n",
    "print(\"activation_stats:\",activation_stats)\n",
    "\n",
    "original_loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "print(f\"Original loss: {original_loss}, Original prediction error: {0}\")\n",
    "\n",
    "# torch.onnx.export(original_mlp_0, input_mlp_0, args.prefix_dir + 'mlp0_cob_activation_norm.onnx', verbose=False, export_params=True, opset_version=15, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "# np.save(args.prefix_dir + \"input_mlp0_data.npy\", input_mlp_0.numpy())\n",
    "\n",
    "# original onnx is prefix_dir + network_split_0_False.onnx\n",
    "# original input is prefix_dir + input_0_True.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c82f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json.load(open(args.prefix_dir + \"input_0_True.json\"))['input_data'][0]\n",
    "d = torch.tensor(d).view(1,197,192)\n",
    "np.save(args.prefix_dir + \"input_block0_false.npy\", d.numpy())\n",
    "\n",
    "# original_pred = model.blocks[0].mlp(model.blocks[0].norm2(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c609058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralteleportation.models.model_zoo.mlpcob import MLPCOB\n",
    "from neuralteleportation.neuralteleportationmodel import NeuralTeleportationModel\n",
    "from neuralteleportation.layers.neuralteleportation import COBForwardMixin, FlattenCOB\n",
    "from neuralteleportation.layers.neuron import LinearCOB\n",
    "from neuralteleportation.layers.activation import ReLUCOB, SigmoidCOB, GELUCOB, LeakyReLUCOB\n",
    "from neuralteleportation.layers.dropout import DropoutCOB\n",
    "from neuralteleportation.layers.neuron import LayerNormCOB\n",
    "from neuralteleportation.layers.merge import Add\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # self.add = Add()\n",
    "        self.norm2 = LayerNormCOB(192)\n",
    "        self.fc1 = LinearCOB(192, 768, bias=True)\n",
    "        self.act = GELUCOB()\n",
    "        self.fc2 = LinearCOB(768, 192, bias=True)\n",
    "        # self.drop1 = DropoutCOB(0.1)\n",
    "        # self.drop2 = DropoutCOB(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.norm2(x)\n",
    "        x2 = self.fc1(x1)\n",
    "        x3 = self.act(x2)\n",
    "        x4 = self.fc2(x3)\n",
    "        # x4 = self.add(x, x3)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb4d7d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n"
     ]
    }
   ],
   "source": [
    "teleported_model = LinearNet()\n",
    "teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3c4edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ln_weights(LN, model, block_idx):\n",
    "    original_mlp = model.blocks[block_idx].mlp\n",
    "    original_norm2 = model.blocks[block_idx].norm2\n",
    "\n",
    "    combined_dict = {}\n",
    "    combined_dict.update(original_mlp.state_dict())\n",
    "    for k,v in original_norm2.state_dict().items():\n",
    "        combined_dict[\"norm2.\" + k] = v\n",
    "    LN.network.load_state_dict(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b463bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ln_weights(teleported_model, model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "933321f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the difference between the two predictions\n",
    "\n",
    "# model_pred = teleported_model.network(d).detach().cpu().numpy()\n",
    "# diff = np.abs(original_pred - model_pred).norm()\n",
    "# print(f\"Prediction error: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c77d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "args.cob_range = 0.2\n",
    "args.steps = 100\n",
    "args.sample_type = \"centered\"\n",
    "args.center = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f68bf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial weights\n",
    "initial_weights = teleported_model.get_weights().detach()\n",
    "initial_cob = teleported_model.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "# define global variable to store the best loss found\n",
    "global best_loss\n",
    "# initialize best_loss\n",
    "best_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d8b7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nevergrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7919e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space for the COB\n",
    "import nevergrad as ng\n",
    "\n",
    "def compute_loss(teleported_model, cob, input_data, original_pred, activation_stats):\n",
    "    # Set up model with the new COB\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "\n",
    "    # model.network.load_state_dict(original_mlp_0.state_dict())\n",
    "    load_ln_weights(teleported_model, model, 0)\n",
    "    teleported_model.set_weights(initial_weights)\n",
    "    \n",
    "    teleported_model = teleported_model.teleport(cob, reset_teleportation=True)\n",
    "\n",
    "    # Reset activation stats and run a forward pass\n",
    "    activation_stats = {}\n",
    "    for i, layer in enumerate(teleported_model.network.children()):\n",
    "            if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "                layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "    teleported_model.eval()\n",
    "    pred = teleported_model.network(input_data)\n",
    "    teleported_model.train()\n",
    "\n",
    "    # Compute loss based on activation stats\n",
    "    loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "    # second term of loss - difference between the cob and the ones tensor\n",
    "    # loss += 10 * (cob - torch.ones_like(cob)).abs().mean()\n",
    "    pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean() \n",
    "    loss += 10 * pred_error\n",
    "\n",
    "    return loss, pred_error\n",
    "\n",
    "# Define the function to minimize using Nevergrad\n",
    "def ng_loss_function(cob_flat):\n",
    "    cob = torch.tensor(cob_flat)\n",
    "    loss, pred_error = compute_loss(teleported_model, cob, input_teleported_model, original_pred, activation_stats)\n",
    "    print(f\"Loss: {loss}, Prediction Error: {pred_error}\")\n",
    "\n",
    "    global best_loss\n",
    "    if best_loss is None or loss < best_loss:\n",
    "        best_loss = loss\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b9beb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametrization = ng.p.Instrumentation(\n",
    "    ng.p.Array(shape=(initial_cob.size()),lower=0,upper=2)\n",
    ")\n",
    "# Define Nevergrad optimizer\n",
    "# optimizer = ng.optimizers.OnePlusOne(parametrization=parametrization, budget=args.steps)\n",
    "optimizer = ng.optimizers.CMA(parametrization=parametrization, budget=args.steps)\n",
    "optimizer.suggest(np.ones(initial_cob.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20256dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.074966430664062, Prediction Error: 1.0098381608258933e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.25308609008789, Prediction Error: 0.017091430723667145\n",
      "Loss: 13.43331241607666, Prediction Error: 0.017611205577850342\n",
      "Loss: 13.725078582763672, Prediction Error: 0.017194412648677826\n",
      "Loss: 13.161828994750977, Prediction Error: 0.016694089397788048\n",
      "Loss: 14.495159149169922, Prediction Error: 0.017777089029550552\n",
      "Loss: 14.281355857849121, Prediction Error: 0.016281992197036743\n",
      "Loss: 13.90745735168457, Prediction Error: 0.014813601970672607\n",
      "Loss: 12.849933624267578, Prediction Error: 0.0163401048630476\n",
      "Loss: 12.838342666625977, Prediction Error: 0.016323285177350044\n",
      "Loss: 14.865196228027344, Prediction Error: 0.016369570046663284\n",
      "Loss: 12.268608093261719, Prediction Error: 0.017180295661091805\n",
      "Loss: 13.074966430664062, Prediction Error: 1.0098381608258933e-05\n",
      "Loss: 13.717048645019531, Prediction Error: 0.017550310119986534\n",
      "Loss: 12.720988273620605, Prediction Error: 0.018887395039200783\n",
      "Loss: 14.329254150390625, Prediction Error: 0.01711711660027504\n",
      "Loss: 12.623664855957031, Prediction Error: 0.020514532923698425\n",
      "Loss: 13.508907318115234, Prediction Error: 0.01676025241613388\n",
      "Loss: 13.92625904083252, Prediction Error: 0.01851477287709713\n",
      "Loss: 12.913825988769531, Prediction Error: 0.017298709601163864\n",
      "Loss: 13.11910629272461, Prediction Error: 0.0203715693205595\n",
      "Loss: 12.411145210266113, Prediction Error: 0.02977168560028076\n",
      "Loss: 13.245626449584961, Prediction Error: 0.008312481455504894\n",
      "Loss: 12.945664405822754, Prediction Error: 0.023714033886790276\n",
      "Loss: 13.322456359863281, Prediction Error: 0.020651083439588547\n",
      "Loss: 13.059083938598633, Prediction Error: 0.023383457213640213\n",
      "Loss: 12.019968032836914, Prediction Error: 0.0219863448292017\n",
      "Loss: 12.629584312438965, Prediction Error: 0.020969310775399208\n",
      "Loss: 12.742936134338379, Prediction Error: 0.02543884702026844\n",
      "Loss: 12.96429443359375, Prediction Error: 0.020600344985723495\n",
      "Loss: 12.903974533081055, Prediction Error: 0.02194458432495594\n",
      "Loss: 12.756635665893555, Prediction Error: 0.0356794074177742\n",
      "Loss: 12.72594165802002, Prediction Error: 0.01274445466697216\n",
      "Loss: 12.511808395385742, Prediction Error: 0.026925284415483475\n",
      "Loss: 13.037384033203125, Prediction Error: 0.02553018182516098\n",
      "Loss: 12.925755500793457, Prediction Error: 0.02635890617966652\n",
      "Loss: 12.020971298217773, Prediction Error: 0.028063630685210228\n",
      "Loss: 12.564071655273438, Prediction Error: 0.027222711592912674\n",
      "Loss: 11.963958740234375, Prediction Error: 0.024500079452991486\n",
      "Loss: 12.47958755493164, Prediction Error: 0.026657845824956894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 1, 'index': 0, 'counter': 0}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n",
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 1, 'index': 1, 'counter': 1}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.133100509643555, Prediction Error: 0.02673579938709736\n",
      "Loss: 12.564189910888672, Prediction Error: 0.03294024616479874\n",
      "Loss: 12.481767654418945, Prediction Error: 0.02019248902797699\n",
      "Loss: 12.215362548828125, Prediction Error: 0.028756823390722275\n",
      "Loss: 12.773582458496094, Prediction Error: 0.026966897770762444\n",
      "Loss: 13.145721435546875, Prediction Error: 0.027310766279697418\n",
      "Loss: 11.8431978225708, Prediction Error: 0.027528682723641396\n",
      "Loss: 11.86927318572998, Prediction Error: 0.026617396622896194\n",
      "Loss: 12.149957656860352, Prediction Error: 0.02647397667169571\n",
      "Loss: 12.517233848571777, Prediction Error: 0.029777808114886284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 2, 'index': 0, 'counter': 2}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n",
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 2, 'index': 1, 'counter': 3}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.346145629882812, Prediction Error: 0.0307795200496912\n",
      "Loss: 12.109264373779297, Prediction Error: 0.034798767417669296\n",
      "Loss: 12.07075023651123, Prediction Error: 0.02289362996816635\n",
      "Loss: 12.315196990966797, Prediction Error: 0.02875778265297413\n",
      "Loss: 12.471230506896973, Prediction Error: 0.03133741393685341\n",
      "Loss: 12.924578666687012, Prediction Error: 0.02962389960885048\n",
      "Loss: 12.448529243469238, Prediction Error: 0.025912536308169365\n",
      "Loss: 12.343621253967285, Prediction Error: 0.02638860233128071\n",
      "Loss: 12.600296974182129, Prediction Error: 0.026444124057888985\n",
      "Loss: 11.762332916259766, Prediction Error: 0.03003413788974285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 3, 'index': 0, 'counter': 4}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n",
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 3, 'index': 1, 'counter': 5}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.774261474609375, Prediction Error: 0.03078257106244564\n",
      "Loss: 11.996882438659668, Prediction Error: 0.03338569402694702\n",
      "Loss: 11.767593383789062, Prediction Error: 0.02409658394753933\n",
      "Loss: 12.161890029907227, Prediction Error: 0.030847448855638504\n",
      "Loss: 12.705605506896973, Prediction Error: 0.029810983687639236\n",
      "Loss: 12.569046974182129, Prediction Error: 0.03358941897749901\n",
      "Loss: 12.290019989013672, Prediction Error: 0.03135107830166817\n",
      "Loss: 11.951154708862305, Prediction Error: 0.029243236407637596\n",
      "Loss: 12.324051856994629, Prediction Error: 0.03122837096452713\n",
      "Loss: 13.035022735595703, Prediction Error: 0.028283236548304558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 4, 'index': 0, 'counter': 4}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n",
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 4, 'index': 1, 'counter': 5}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.424371719360352, Prediction Error: 0.03277371823787689\n",
      "Loss: 12.008705139160156, Prediction Error: 0.0351751409471035\n",
      "Loss: 12.171574592590332, Prediction Error: 0.026817508041858673\n",
      "Loss: 11.917581558227539, Prediction Error: 0.03070712834596634\n",
      "Loss: 12.322484016418457, Prediction Error: 0.029692409560084343\n",
      "Loss: 12.116251945495605, Prediction Error: 0.03094472363591194\n",
      "Loss: 12.373708724975586, Prediction Error: 0.030962347984313965\n",
      "Loss: 12.327900886535645, Prediction Error: 0.032115332782268524\n",
      "Loss: 12.603950500488281, Prediction Error: 0.03243245184421539\n",
      "Loss: 12.397844314575195, Prediction Error: 0.030521031469106674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 5, 'index': 0, 'counter': 4}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n",
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 5, 'index': 1, 'counter': 5}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.768420219421387, Prediction Error: 0.026539919897913933\n",
      "Loss: 12.636152267456055, Prediction Error: 0.04016092047095299\n",
      "Loss: 11.780876159667969, Prediction Error: 0.02329344116151333\n",
      "Loss: 12.098198890686035, Prediction Error: 0.03410204127430916\n",
      "Loss: 12.021876335144043, Prediction Error: 0.03320653364062309\n",
      "Loss: 12.021363258361816, Prediction Error: 0.03152075782418251\n",
      "Loss: 12.832221984863281, Prediction Error: 0.034783653914928436\n",
      "Loss: 12.172605514526367, Prediction Error: 0.0327625572681427\n",
      "Loss: 12.158531188964844, Prediction Error: 0.03141215071082115\n",
      "Loss: 12.296829223632812, Prediction Error: 0.0327388234436512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 6, 'index': 0, 'counter': 4}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n",
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 6, 'index': 1, 'counter': 5}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.097651481628418, Prediction Error: 0.029915165156126022\n",
      "Loss: 12.148660659790039, Prediction Error: 0.0308877881616354\n",
      "Loss: 11.879117012023926, Prediction Error: 0.03331827372312546\n",
      "Loss: 11.901659965515137, Prediction Error: 0.029361451044678688\n",
      "Loss: 11.662211418151855, Prediction Error: 0.03374479338526726\n",
      "Loss: 12.676407814025879, Prediction Error: 0.033612534403800964\n",
      "Loss: 12.13623332977295, Prediction Error: 0.03155069798231125\n",
      "Loss: 12.626895904541016, Prediction Error: 0.0338093601167202\n",
      "Loss: 11.964428901672363, Prediction Error: 0.03133977949619293\n",
      "Loss: 11.882429122924805, Prediction Error: 0.03076118230819702\n",
      "Best COB found: tensor([0.9573, 1.0708, 0.8499, 1.1752, 0.8841, 1.1331, 1.2355, 0.9151, 1.0170,\n",
      "        1.1810, 0.9953, 1.0377, 1.0684, 1.0790, 1.0149, 0.9197, 1.1030, 0.9827,\n",
      "        1.1189, 0.9435, 0.9955, 0.7486, 1.1133, 0.9812, 0.9261, 1.0212, 1.1305,\n",
      "        0.8953, 0.7372, 0.8764, 1.0866, 1.0105, 1.0624, 0.9838, 1.0595, 0.9737,\n",
      "        1.0772, 0.9962, 1.1069, 0.8394, 0.9264, 1.1671, 0.9050, 1.2670, 1.0829,\n",
      "        0.9549, 1.0613, 0.8869, 0.9654, 1.1751, 0.9750, 0.8844, 1.0557, 1.2959,\n",
      "        0.8616, 0.9885, 1.1256, 1.0050, 1.3020, 1.0191, 1.0620, 0.6609, 0.8628,\n",
      "        1.0026, 1.1231, 0.9594, 0.8108, 1.2374, 1.0728, 1.1973, 0.7592, 0.9965,\n",
      "        0.9733, 0.9268, 0.8126, 0.9811, 0.8190, 0.9674, 1.1971, 1.1171, 1.1415,\n",
      "        1.0502, 1.1696, 1.0370, 1.1457, 1.1470, 0.7193, 1.0031, 1.3786, 1.0647,\n",
      "        0.8194, 1.1259, 0.9645, 0.8574, 1.3091, 1.1675, 1.1105, 1.0906, 0.9039,\n",
      "        1.0001, 0.6980, 1.0957, 1.3499, 0.9746, 1.2139, 1.0005, 0.8542, 1.1398,\n",
      "        0.9396, 0.8836, 0.8819, 1.0263, 0.9847, 0.9615, 0.8605, 0.9449, 1.0988,\n",
      "        1.1182, 1.3468, 1.0075, 0.9469, 0.8321, 0.8202, 0.9221, 0.8614, 0.9066,\n",
      "        1.1537, 0.8876, 1.1280, 1.2680, 1.0059, 1.1062, 1.0343, 1.0834, 0.9773,\n",
      "        0.9426, 1.2396, 0.8769, 1.1627, 0.8932, 0.6619, 0.9780, 1.2104, 0.9695,\n",
      "        0.9222, 0.9570, 0.8366, 0.9986, 0.9301, 0.8711, 1.0118, 1.3439, 1.0111,\n",
      "        1.0556, 0.8730, 1.0298, 0.9852, 1.3525, 1.1644, 1.2605, 0.9640, 1.0683,\n",
      "        1.1250, 0.8793, 0.9661, 1.1281, 1.0080, 0.9978, 0.8060, 0.7526, 1.2809,\n",
      "        0.8320, 1.0862, 1.0041, 1.1001, 0.7870, 0.9504, 0.9642, 1.1229, 1.0408,\n",
      "        0.9935, 0.9678, 0.9520, 1.0818, 1.0715, 1.0694, 1.0197, 0.9530, 0.9490,\n",
      "        1.1007, 1.1078, 1.0290, 1.1423, 1.0612, 0.6202, 0.7933, 1.0445, 1.0563,\n",
      "        1.0514, 1.1106, 0.7672, 1.3237, 1.1347, 1.2135, 0.7952, 1.1343, 1.1395,\n",
      "        0.9717, 0.9381, 0.8227, 0.7307, 0.9033, 0.8869, 1.0269, 1.0369, 0.9414,\n",
      "        0.8393, 0.9285, 0.9506, 1.0746, 1.2428, 1.1681, 1.2166, 1.1400, 0.7954,\n",
      "        1.1117, 1.0866, 1.1331, 1.0935, 0.9409, 0.9189, 1.2035, 0.9045, 0.7952,\n",
      "        1.0372, 0.8512, 0.8910, 0.7781, 1.0192, 0.7809, 1.2587, 0.9232, 0.9504,\n",
      "        1.0870, 1.1291, 1.0115, 1.0946, 1.0363, 1.0579, 0.5880, 1.2883, 0.9735,\n",
      "        1.0356, 0.8935, 1.0158, 0.7369, 0.9742, 0.9147, 1.2748, 0.7934, 0.8233,\n",
      "        0.8440, 0.6695, 1.0362, 0.8758, 1.0437, 1.0821, 1.0328, 0.9618, 0.8040,\n",
      "        1.1001, 1.0251, 0.8027, 0.9836, 0.8906, 0.7789, 1.1830, 0.9489, 1.2163,\n",
      "        0.9430, 1.1253, 0.8820, 0.6244, 0.9462, 0.6416, 0.7642, 1.0366, 1.0730,\n",
      "        1.0004, 0.9622, 0.9485, 1.0948, 1.0560, 0.8535, 1.1564, 0.9917, 0.9311,\n",
      "        0.7936, 0.9493, 1.0865, 0.9859, 0.9311, 1.1559, 1.0666, 1.0499, 1.0376,\n",
      "        0.9374, 0.9966, 0.8316, 1.1424, 1.0080, 1.0845, 0.8967, 0.9133, 1.0281,\n",
      "        0.9970, 1.1836, 1.1604, 1.1356, 0.9892, 1.0852, 1.0807, 1.2357, 0.8503,\n",
      "        0.9418, 1.0718, 1.0329, 1.1589, 0.8849, 1.0881, 0.9197, 0.9053, 0.9845,\n",
      "        0.9523, 0.9630, 1.1764, 0.9284, 1.0489, 1.1253, 1.2457, 0.9072, 1.0672,\n",
      "        0.9342, 1.0870, 0.7672, 1.0015, 0.7077, 1.0176, 1.1378, 1.1355, 1.0897,\n",
      "        0.8642, 0.9563, 1.1362, 0.9555, 0.9607, 0.9279, 1.0136, 0.9306, 1.0253,\n",
      "        0.9332, 0.9693, 1.2551, 0.7625, 0.9892, 0.9660, 0.9130, 0.7793, 0.8231,\n",
      "        0.9087, 1.0260, 1.3059, 0.9228, 1.0164, 0.9138, 1.2393, 0.9219, 0.8480,\n",
      "        0.9632, 0.9980, 0.9378, 1.1646, 1.0269, 0.9226, 1.1016, 0.8927, 0.8782,\n",
      "        0.8892, 0.9949, 0.8891, 0.9596, 1.3235, 0.7710, 1.1393, 1.0288, 1.0319,\n",
      "        0.8908, 1.2618, 1.0232, 1.0865, 0.9993, 0.7376, 0.8567, 0.8702, 0.9296,\n",
      "        1.4044, 1.0332, 0.8829, 1.1655, 0.9812, 1.0175, 0.9455, 0.9075, 1.0336,\n",
      "        0.9722, 0.8597, 1.4604, 1.2182, 0.8026, 0.9688, 1.0576, 1.0129, 0.9517,\n",
      "        0.9752, 1.2247, 0.8990, 0.6458, 1.1139, 0.8667, 0.9530, 1.1332, 0.7627,\n",
      "        0.9279, 1.0512, 1.0946, 1.1310, 1.2735, 1.0739, 0.8378, 1.0092, 0.8986,\n",
      "        0.8188, 1.1251, 0.9431, 1.1351, 0.8468, 0.9409, 0.8418, 1.0692, 0.7554,\n",
      "        1.0783, 1.0232, 1.1296, 0.8199, 1.1751, 1.0676, 1.1868, 0.8071, 0.8781,\n",
      "        1.0677, 1.0475, 1.1412, 0.9610, 1.1390, 1.1139, 0.9264, 1.0255, 1.1089,\n",
      "        1.1058, 1.1481, 0.9594, 1.2839, 1.2032, 0.9627, 0.8599, 1.1396, 0.9732,\n",
      "        0.8163, 1.1361, 1.0041, 0.9224, 1.0471, 1.1155, 0.8306, 0.9430, 0.8130,\n",
      "        1.0680, 1.2965, 1.0943, 1.0672, 0.8895, 0.9079, 0.9743, 1.0383, 0.9288,\n",
      "        0.9420, 1.0377, 0.8682, 0.9494, 1.0787, 1.0619, 0.8896, 0.9175, 0.7660,\n",
      "        0.8857, 1.0590, 0.9563, 0.7605, 1.0042, 0.9930, 1.0948, 1.0329, 0.8989,\n",
      "        1.0297, 0.9705, 0.9232, 0.9881, 1.0912, 0.8607, 0.7845, 1.1498, 1.1723,\n",
      "        1.0070, 1.0564, 1.0566, 0.9600, 1.0556, 1.1753, 0.9082, 1.0657, 0.7775,\n",
      "        0.9821, 1.0031, 0.9369, 1.1175, 1.1240, 1.1028, 0.7403, 0.8250, 0.9162,\n",
      "        1.0576, 0.9852, 1.3628, 1.3125, 1.0228, 1.0858, 0.8103, 1.0307, 1.1557,\n",
      "        0.8625, 0.9238, 0.8939, 0.9410, 0.7756, 1.1008, 0.8589, 1.1085, 0.7744,\n",
      "        0.8607, 0.8944, 0.9578, 1.0884, 1.1400, 1.2249, 1.1079, 1.0274, 0.9738,\n",
      "        1.1068, 0.9721, 1.0921, 1.4234, 1.0903, 0.9761, 1.0089, 0.8256, 0.9228,\n",
      "        1.0318, 0.9768, 1.1066, 0.9055, 0.8175, 1.0593, 0.9609, 0.9934, 1.1178,\n",
      "        1.0953, 0.9242, 1.0703, 1.0926, 0.8410, 1.1080, 1.1234, 1.1797, 1.0089,\n",
      "        1.0533, 1.0940, 0.9235, 0.9100, 0.9116, 0.8089, 0.8282, 1.0890, 1.0528,\n",
      "        1.0868, 0.9255, 1.1314, 1.0572, 0.9026, 1.0875, 0.8947, 0.8265, 0.9732,\n",
      "        1.1509, 1.0598, 0.8909, 0.8021, 1.1074, 1.1261, 1.1988, 0.9266, 0.9016,\n",
      "        0.9888, 0.7449, 0.7397, 1.0763, 1.0039, 0.9897, 1.2023, 0.8293, 0.9662,\n",
      "        0.9688, 1.1062, 0.9420, 0.9814, 0.8969, 0.7527, 0.8918, 1.2124, 0.9593,\n",
      "        1.1175, 1.1857, 0.7716, 1.0632, 0.8806, 1.2085, 1.0992, 1.0114, 1.2274,\n",
      "        1.1178, 0.7956, 1.1831, 0.8245, 0.9444, 0.7466, 1.2083, 1.0245, 1.0904,\n",
      "        0.9478, 0.8252, 1.0001, 0.7460, 0.8557, 1.0792, 1.1754, 1.0261, 1.0875,\n",
      "        1.1175, 0.7648, 0.8967, 1.1441, 0.5747, 0.9859, 1.2095, 0.8490, 0.8920,\n",
      "        0.7600, 0.8758, 0.9434, 0.8272, 0.9830, 1.1237, 1.1273, 1.2553, 0.8887,\n",
      "        0.7282, 1.1311, 1.2218, 0.8391, 0.9047, 0.9515, 1.0980, 1.0158, 0.6990,\n",
      "        0.8473, 1.2407, 0.9821, 1.0535, 0.9830, 0.8040, 0.9126, 0.9169, 0.7714,\n",
      "        1.0464, 1.1669, 0.9305, 1.0874, 0.8578, 1.0511, 1.2001, 0.9282, 1.0836,\n",
      "        1.0113, 1.2506, 1.0972, 1.0448, 1.1838, 0.7716, 0.9561, 0.8665, 0.9348,\n",
      "        0.9232, 0.9737, 1.1519, 0.9842, 0.9016, 0.7671, 1.0750, 0.8574, 0.8324,\n",
      "        0.9400, 0.9828, 1.0008, 0.7970, 1.0281, 0.9801, 0.8451, 1.1965, 1.2248,\n",
      "        1.0761, 1.1196, 1.1214, 1.2202, 0.8407, 1.0295, 1.0120, 1.0907, 1.0283,\n",
      "        0.7533, 1.3337, 1.0101, 0.9964, 1.2669, 1.0432, 1.0564, 0.8109, 1.0623,\n",
      "        0.9650, 1.0415, 1.1893, 1.0216, 0.9175, 0.8988, 0.8666, 0.9987, 1.0061,\n",
      "        1.0303, 1.1268, 0.9749, 1.0597, 1.0547, 1.0813, 0.9124, 1.0754, 1.0284,\n",
      "        1.0655, 0.6960, 0.9539, 1.2018, 0.9722, 0.9253, 0.9665, 1.0937, 0.8233,\n",
      "        0.8579, 1.0969, 0.7434, 1.0962, 0.8987, 1.1355, 1.0215, 0.9633, 0.9087,\n",
      "        1.1728, 1.3191, 1.0411, 1.0387, 0.8991, 1.0177, 0.8671, 1.0363, 0.9976,\n",
      "        1.0714, 1.2471, 1.2014, 1.0773, 0.7017, 0.8880, 1.0009, 1.1077, 0.9866,\n",
      "        0.9735, 0.8710, 0.9733, 0.8561, 1.0168, 1.1416, 1.0535, 1.0733, 0.9297,\n",
      "        1.0934, 1.0994, 1.0657, 1.2496, 1.0408, 1.1438, 1.0292, 1.2243, 1.0100,\n",
      "        0.9760, 1.0142, 1.1504, 1.1491, 0.9323, 0.8698, 0.9982, 1.2342, 1.0571,\n",
      "        0.9334, 0.8118, 1.0221, 1.0096, 1.0382, 1.1177, 1.2365, 1.0653, 0.9544,\n",
      "        1.0878, 1.0277, 0.8340, 1.0190, 1.0854, 1.0533, 0.7719, 0.9202, 0.7653,\n",
      "        1.0087, 0.5247, 0.9233, 1.0172, 1.0007, 0.9745, 1.1040, 1.1421, 0.8274,\n",
      "        0.8743, 1.1093, 0.8767, 0.5999, 1.1878, 0.9487, 1.0616, 0.9767, 0.8000,\n",
      "        1.0667, 0.7730, 1.2764, 0.9948, 0.9402, 0.8780, 1.1293, 1.0125, 0.9505,\n",
      "        1.0015, 1.0671, 1.1165, 0.9247, 1.0757, 1.1285, 1.3246, 0.9862, 0.8517,\n",
      "        0.9443, 1.0235, 1.0673, 0.9074, 1.1246, 0.7458, 1.0060, 0.8847, 0.9725,\n",
      "        1.1129, 0.9837, 1.0194, 0.8742, 1.1880, 1.0842, 1.1230, 1.1747, 1.2129,\n",
      "        0.9844, 0.8742, 0.9083, 1.0553, 1.0556, 0.9440, 1.1827, 1.1646, 1.0892,\n",
      "        0.8899, 1.0142, 0.8461, 0.9989, 1.1746, 0.7621, 1.0135, 1.0832, 0.7061,\n",
      "        1.2771, 0.8330, 0.9321, 0.9748, 0.9228, 0.7923, 0.9360, 0.7446, 0.8486,\n",
      "        0.9019, 0.8896, 0.9502, 0.9907, 1.0135, 0.9775, 1.1514, 0.6890, 0.8793,\n",
      "        0.9418, 0.8691, 0.9821, 1.1920, 1.2972, 0.9777, 1.1235, 1.0618, 0.8802,\n",
      "        0.9390, 0.9036, 0.8556, 1.2056, 0.8639, 0.6528], dtype=torch.float64)\n",
      "Loss associated with the best COB: 11.662211418151855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 7, 'index': 0, 'counter': 4}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n",
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/cma/evolution_strategy.py:3069: InjectionWarning: orphanated injected solution {'iteration': 7, 'index': 1, 'counter': 5}\n",
      "                        This could be a bug in the calling order/logics or due to\n",
      "                        a too small popsize used in `ask()` or when only using\n",
      "                        `ask(1)` repeatedly. Please check carefully.\n",
      "                        In case this is desired, the warning can be surpressed with\n",
      "                        ``warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)``\n",
      "                        \n",
      "  warnings.warn(\"\"\"orphanated injected solution %s\n"
     ]
    }
   ],
   "source": [
    "# define input_teleported_model (used in ng_loss_function)\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "input_teleported_model = model.split_n(input_convs,0,half=True)\n",
    "# save npy file\n",
    "np.save(args.prefix_dir + \"input_teleported_model.npy\", input_teleported_model.detach().numpy())\n",
    "\n",
    "# define original_pred (used in ng_loss_function)\n",
    "original_pred = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "\n",
    "# Perform optimization using Nevergrad\n",
    "recommendation = optimizer.minimize(ng_loss_function)\n",
    "\n",
    "# Extract the best COB found\n",
    "best_cob = recommendation.value[0][0]\n",
    "best_cob = torch.tensor(best_cob)\n",
    "\n",
    "print(f\"Best COB found: {best_cob}\")\n",
    "print(f\"Loss associated with the best COB: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00a24488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95726907 1.07081228 0.84993363 1.17524195 0.88414012 1.1331377\n",
      " 1.23553139 0.91506592 1.01697854 1.1810447  0.99525706 1.03774959\n",
      " 1.06843917 1.07904279 1.01488203 0.91974231 1.10297884 0.98271542\n",
      " 1.11888524 0.94348375 0.99551313 0.74864875 1.11332855 0.98120351\n",
      " 0.92614696 1.02124443 1.13053572 0.89531513 0.73716931 0.87643988\n",
      " 1.08658712 1.0104989  1.0624098  0.98375683 1.05947172 0.97374238\n",
      " 1.07718227 0.996183   1.10692114 0.8394101  0.92641709 1.16711508\n",
      " 0.90502611 1.2670082  1.08287365 0.95486168 1.061268   0.88690864\n",
      " 0.96535057 1.17508832 0.97500786 0.88439767 1.05569464 1.29590046\n",
      " 0.86158689 0.9884795  1.12555794 1.00501006 1.30198083 1.0191401\n",
      " 1.06203947 0.66091729 0.86283273 1.00258737 1.12307443 0.9594004\n",
      " 0.81075392 1.23738855 1.07275836 1.19732794 0.75918863 0.9964626\n",
      " 0.97332197 0.92679794 0.81258895 0.98107783 0.81895775 0.96744519\n",
      " 1.19712407 1.11707038 1.14151668 1.05024323 1.16955748 1.03699254\n",
      " 1.14568688 1.14702836 0.71931082 1.00306845 1.37863926 1.0647432\n",
      " 0.81942412 1.12592807 0.96447939 0.85744619 1.30907554 1.1675136\n",
      " 1.11054985 1.09063831 0.90387991 1.00012149 0.69796241 1.09570627\n",
      " 1.34992034 0.97463883 1.21390351 1.00048706 0.85420295 1.13979713\n",
      " 0.93959576 0.88363874 0.88190547 1.02625509 0.984732   0.96149275\n",
      " 0.86053995 0.94490311 1.09883057 1.11820672 1.34684971 1.0075372\n",
      " 0.94686008 0.83208031 0.8201922  0.92214529 0.86135608 0.90664766\n",
      " 1.15366037 0.88764073 1.1279601  1.26795189 1.00591775 1.10618706\n",
      " 1.0343469  1.08340294 0.97727095 0.942575   1.23958274 0.87689594\n",
      " 1.16271331 0.89322261 0.66186569 0.97803551 1.21038245 0.96948052\n",
      " 0.92219885 0.95702648 0.83659245 0.99862173 0.93005099 0.8711131\n",
      " 1.01175788 1.34388659 1.01107502 1.05556212 0.87296662 1.02976786\n",
      " 0.98515925 1.35248747 1.16441399 1.2604551  0.96404039 1.06832397\n",
      " 1.12504676 0.87929541 0.96607887 1.12811254 1.00797555 0.99780477\n",
      " 0.80597567 0.7525992  1.28085294 0.83197386 1.08616481 1.00413723\n",
      " 1.10007271 0.78697738 0.95044988 0.96418465 1.12290482 1.04081755\n",
      " 0.993455   0.9678099  0.95199447 1.08183521 1.07149604 1.06938635\n",
      " 1.01968157 0.95295943 0.94903198 1.10073788 1.10783173 1.0289734\n",
      " 1.14232416 1.0612103  0.62019    0.79329792 1.04453166 1.05625732\n",
      " 1.0514096  1.11061357 0.76723726 1.32371402 1.13468748 1.21348747\n",
      " 0.79521485 1.1342558  1.13948679 0.97166313 0.9380762  0.82268686\n",
      " 0.73068541 0.90333282 0.8868589  1.0269068  1.03685153 0.9413532\n",
      " 0.83930266 0.92848832 0.95055352 1.07455675 1.24275867 1.16806685\n",
      " 1.21658237 1.140015   0.79536635 1.11174031 1.08662106 1.13312817\n",
      " 1.09350984 0.94085087 0.91893031 1.20353572 0.90449025 0.79520149\n",
      " 1.03720365 0.8511592  0.8910361  0.77808066 1.01918166 0.78093388\n",
      " 1.25873968 0.92323133 0.95041893 1.08704304 1.1291299  1.01145734\n",
      " 1.09458328 1.03629179 1.05788281 0.58797599 1.28833739 0.97352086\n",
      " 1.03564581 0.89354045 1.01576707 0.73693806 0.97420949 0.91465374\n",
      " 1.27480687 0.79337792 0.82328342 0.84402558 0.66950861 1.03620536\n",
      " 0.87580113 1.0437147  1.08212224 1.03283655 0.96176077 0.80404495\n",
      " 1.10011691 1.02513367 0.80270728 0.98360032 0.89063525 0.77888053\n",
      " 1.18300447 0.94886492 1.21631033 0.94300655 1.1252678  0.88202748\n",
      " 0.62441581 0.94616219 0.64155421 0.76417902 1.03662062 1.07302439\n",
      " 1.00043649 0.96217499 0.94848624 1.09482147 1.05597417 0.85351248\n",
      " 1.15641379 0.99170345 0.93112862 0.79360633 0.94930755 1.08653713\n",
      " 0.98589688 0.93113193 1.15586844 1.06655995 1.04990116 1.03759011\n",
      " 0.93737201 0.99661686 0.83164365 1.14244783 1.00804698 1.08447135\n",
      " 0.89669123 0.91334079 1.02807791 0.99697846 1.18364752 1.16038094\n",
      " 1.13559545 0.98921985 1.08517243 1.08065894 1.23568212 0.85031242\n",
      " 0.94178917 1.07177151 1.03290215 1.15886343 0.88494302 1.0880512\n",
      " 0.91966111 0.90525347 0.9845187  0.95233974 0.963014   1.17643219\n",
      " 0.92840944 1.04887637 1.12532947 1.2456569  0.90715515 1.06715291\n",
      " 0.93420872 1.0869582  0.76715028 1.00147596 0.70769862 1.01764769\n",
      " 1.13782427 1.13547868 1.08967666 0.86423729 0.9563315  1.13619526\n",
      " 0.95551183 0.96069167 0.92792525 1.01362383 0.93059428 1.02529154\n",
      " 0.93321831 0.96927257 1.25512146 0.76245308 0.98923935 0.96601463\n",
      " 0.9130326  0.77932679 0.82309011 0.9086993  1.02601542 1.30591526\n",
      " 0.92283923 1.01643191 0.91382496 1.23928513 0.92191004 0.84797721\n",
      " 0.96324868 0.99802074 0.93780805 1.16463168 1.02694371 0.92255962\n",
      " 1.10162363 0.89273778 0.87822454 0.88917131 0.99492512 0.88908813\n",
      " 0.95955825 1.323545   0.77099053 1.13933404 1.02878943 1.03189843\n",
      " 0.89075078 1.2618071  1.02324276 1.08647814 0.99933598 0.73757993\n",
      " 0.85666984 0.87020444 0.92959585 1.40442208 1.0331674  0.88292826\n",
      " 1.1654802  0.98117885 1.0174806  0.94549774 0.90746268 1.03363992\n",
      " 0.97215864 0.85967442 1.46036473 1.21820906 0.80260981 0.96881576\n",
      " 1.0575897  1.01288288 0.95165765 0.97516773 1.22471453 0.89902306\n",
      " 0.64582695 1.11385873 0.86673332 0.95303843 1.13316867 0.76265509\n",
      " 0.92787319 1.05123025 1.09459171 1.1309744  1.27348198 1.0738831\n",
      " 0.83782561 1.00924308 0.89859769 0.81877692 1.12511176 0.94311929\n",
      " 1.13505289 0.8467638  0.94094108 0.841805   1.06922994 0.75538166\n",
      " 1.0782665  1.02319147 1.12961365 0.81986681 1.17510469 1.06762853\n",
      " 1.18680595 0.80705384 0.87813193 1.06770794 1.04746085 1.14120433\n",
      " 0.96098306 1.13899254 1.1138571  0.92635155 1.02550689 1.10890686\n",
      " 1.10584805 1.14812969 0.95939804 1.28393289 1.203224   0.96266462\n",
      " 0.85989866 1.13962948 0.97319868 0.81632109 1.13614755 1.00412116\n",
      " 0.92236412 1.04712538 1.11551817 0.83056986 0.94295104 0.81304477\n",
      " 1.06802734 1.296534   1.09434645 1.06718854 0.8894787  0.90791551\n",
      " 0.97428659 1.03830975 0.9287719  0.94199213 1.03766904 0.86817912\n",
      " 0.94940723 1.07865741 1.06186738 0.88961188 0.91745616 0.76601882\n",
      " 0.88566629 1.05904442 0.95628873 0.76047189 1.00421085 0.99301816\n",
      " 1.09482427 1.03287141 0.89893435 1.0297232  0.97045955 0.92319982\n",
      " 0.98808852 1.09121281 0.86067901 0.78450065 1.14982112 1.17230483\n",
      " 1.00699229 1.05640485 1.05657248 0.95996106 1.05555974 1.17531941\n",
      " 0.90821841 1.06565553 0.77753681 0.98208903 1.00308963 0.93686379\n",
      " 1.11754706 1.12397407 1.10277416 0.74026762 0.82503489 0.91618098\n",
      " 1.05762222 0.98521196 1.36283064 1.3124938  1.02278783 1.08579935\n",
      " 0.81030521 1.03072994 1.15565817 0.86248788 0.92378124 0.89393927\n",
      " 0.94104508 0.77560934 1.10078527 0.85893869 1.10846035 0.77439449\n",
      " 0.86071938 0.89441993 0.95783573 1.08836476 1.13995209 1.22487046\n",
      " 1.10792269 1.02738154 0.97384087 1.10683729 0.97210757 1.09211559\n",
      " 1.42344524 1.09028941 0.97605969 1.00893631 0.82555281 0.92275855\n",
      " 1.03184389 0.97678446 1.10662854 0.90546354 0.81751858 1.05930931\n",
      " 0.96090995 0.99340322 1.11780615 1.09533677 0.92421653 1.07025181\n",
      " 1.09263707 0.8409812  1.10798877 1.1233785  1.1797447  1.0088764\n",
      " 1.05327749 1.09395176 0.92348926 0.9099797  0.91158333 0.80886657\n",
      " 0.82816803 1.08904431 1.05279616 1.08678881 0.92553446 1.13141544\n",
      " 1.05721482 0.90261572 1.08751232 0.89473676 0.82651792 0.97317189\n",
      " 1.15087186 1.05984723 0.89094835 0.80212927 1.10741995 1.12605478\n",
      " 1.19876971 0.92658064 0.90163989 0.98878889 0.7449425  0.73968682\n",
      " 1.0763292  1.00390412 0.98971949 1.20233099 0.82933949 0.96622816\n",
      " 0.9688474  1.10624818 0.94197091 0.98138633 0.89691443 0.75273176\n",
      " 0.89180246 1.21239544 0.95928572 1.11750941 1.18574301 0.7715559\n",
      " 1.06317854 0.88064654 1.20845997 1.09921626 1.01136251 1.22740313\n",
      " 1.11782159 0.79562913 1.18313075 0.82449355 0.94443484 0.74655645\n",
      " 1.20833695 1.02452908 1.0904415  0.94775331 0.82524632 1.00012519\n",
      " 0.74600434 0.8557053  1.07919277 1.17544675 1.02610914 1.08749697\n",
      " 1.11751872 0.76480994 0.89670615 1.14409559 0.57468776 0.98586023\n",
      " 1.20948838 0.84897647 0.89201311 0.75997164 0.87582592 0.94338256\n",
      " 0.82717145 0.98296651 1.12373534 1.1273289  1.25534659 0.88865337\n",
      " 0.7281871  1.13107411 1.22177004 0.83912395 0.90469407 0.95145134\n",
      " 1.09803225 1.01581635 0.69902092 0.84734749 1.24071397 0.98208444\n",
      " 1.05348281 0.98302791 0.80396754 0.91261639 0.91686093 0.77135759\n",
      " 1.0463917  1.16690517 0.9304538  1.08741781 0.85781707 1.05105134\n",
      " 1.20014328 0.92820276 1.08358525 1.01131561 1.25059861 1.09721947\n",
      " 1.04480294 1.18382192 0.77160862 0.95609248 0.86649222 0.93475627\n",
      " 0.9232408  0.97371093 1.15185117 0.9842067  0.9015678  0.76710143\n",
      " 1.07496023 0.85739405 0.83238364 0.94000632 0.98279413 1.00083073\n",
      " 0.79704735 1.0280742  0.98006821 0.84508368 1.19651665 1.22482941\n",
      " 1.07608042 1.11957773 1.1214169  1.22024499 0.84074854 1.02952642\n",
      " 1.01199033 1.09072474 1.02833479 0.75333339 1.33370973 1.01008953\n",
      " 0.99635923 1.2669372  1.04317128 1.05642757 0.81090987 1.06227996\n",
      " 0.96503584 1.04145867 1.18930444 1.0216353  0.91751174 0.89875834\n",
      " 0.86662976 0.9987498  1.00609657 1.03028377 1.12678191 0.97494435\n",
      " 1.0597058  1.05465476 1.08133037 0.91237577 1.07543932 1.02839205\n",
      " 1.0654987  0.69598546 0.95385597 1.20183058 0.97218069 0.92527233\n",
      " 0.96649628 1.09374877 0.82327692 0.85786047 1.09687741 0.74338203\n",
      " 1.09619539 0.89865745 1.1354644  1.02149809 0.96328889 0.90872353\n",
      " 1.17281624 1.31914871 1.04112427 1.03872668 0.89911825 1.01765916\n",
      " 0.86706492 1.03633364 0.99761631 1.07138014 1.24712868 1.20143932\n",
      " 1.077303   0.70167731 0.88797911 1.00093216 1.10765229 0.9866407\n",
      " 0.9735466  0.87098568 0.97325733 0.85607962 1.01676202 1.14163458\n",
      " 1.05350218 1.07334841 0.92965841 1.09343316 1.09938346 1.06565616\n",
      " 1.24957343 1.04078716 1.14381877 1.02919639 1.22428752 1.01001865\n",
      " 0.97595375 1.01424339 1.15043627 1.14914343 0.93226354 0.86983044\n",
      " 0.99822872 1.23417603 1.05713157 0.93342344 0.81177242 1.02213591\n",
      " 1.00955033 1.03821298 1.11767488 1.23645723 1.06532572 0.95440212\n",
      " 1.08779091 1.02766245 0.83403439 1.01899865 1.08536981 1.05325218\n",
      " 0.77189634 0.92024368 0.76528827 1.00869969 0.52467815 0.9232547\n",
      " 1.01722952 1.00070936 0.97445108 1.10402335 1.14208781 0.82744789\n",
      " 0.87434314 1.10930314 0.87671019 0.59992626 1.18784051 0.94872872\n",
      " 1.06155204 0.97673803 0.8000017  1.066691   0.77296178 1.27637327\n",
      " 0.99476843 0.94015861 0.87797378 1.12929938 1.01246171 0.95050678\n",
      " 1.00152938 1.06708841 1.11646531 0.92469462 1.07571438 1.1285028\n",
      " 1.32460033 0.98615426 0.85169531 0.94426426 1.02350672 1.06727565\n",
      " 0.90744242 1.12462837 0.74583095 1.00597606 0.88466593 0.97246712\n",
      " 1.11287049 0.98370214 1.01937511 0.87417259 1.18797253 1.08416252\n",
      " 1.12295998 1.17469523 1.21285533 0.98437421 0.87424315 0.90834095\n",
      " 1.05526508 1.05561227 0.94401192 1.18271618 1.16455812 1.08921265\n",
      " 0.88994978 1.01417678 0.84606281 0.9989426  1.17455938 0.76206071\n",
      " 1.01352635 1.08320674 0.70607434 1.27712233 0.83303313 0.9320704\n",
      " 0.97481654 0.92276804 0.79230829 0.93603443 0.74455525 0.84856607\n",
      " 0.90187016 0.88955356 0.95019953 0.99069857 1.01353846 0.9774583\n",
      " 1.15135201 0.68899904 0.87931267 0.94184657 0.86910817 0.98205954\n",
      " 1.19199386 1.29720884 0.97771965 1.12353831 1.06182335 0.8802149\n",
      " 0.93901674 0.90363671 0.85559371 1.20564464 0.8639164  0.65278121]\n",
      "tensor(11.6622, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print best candidate\n",
    "print(recommendation.value[0][0])\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ab2066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best COB found using Nevergrad.\n"
     ]
    }
   ],
   "source": [
    " # Apply best COB and save model weights\n",
    "LN = LinearNet()\n",
    "LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "load_ln_weights(LN, model, 0)\n",
    "LN = LN.teleport(best_cob, reset_teleportation=True)\n",
    "torch.save(LN.network.state_dict(), args.prefix_dir + 'block0_cob_activation_norm_teleported.pth')\n",
    "\n",
    "# Export the optimized model to ONNX\n",
    "torch.onnx.export(LN.network, input_teleported_model, args.prefix_dir + 'block0_cob_activation_norm_teleported.onnx', verbose=False, export_params=True, opset_version=12, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "\n",
    "# Print final results\n",
    "print(\"Best COB found using Nevergrad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a63eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract onnx of the teleported model\n",
    "input_path = args.prefix_dir + \"network_split_0_False.onnx\"\n",
    "output_path = args.prefix_dir + \"block0_cob_activation_norm.onnx\"\n",
    "input_names = [\"/blocks.0/Add_2_output_0\"]\n",
    "output_names = [\"/blocks.0/mlp/fc2/Add_output_0\"]\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd08cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output comparison (element-wise):\n",
      "[[[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]]\n",
      "\n",
      "Detailed Differences:\n",
      "Max difference: 0.17111284\n",
      "Mean difference: 0.033744793\n",
      "\n",
      "If differences exist, their values:\n",
      "[0.05438678 0.04055738 0.01716256 ... 0.01435003 0.03288895 0.09628159]\n",
      "\n",
      "Output 1:\n",
      "[[[-0.09554856  0.3529476  -0.67855775 ...  0.1964173  -0.14930794\n",
      "   -0.5213456 ]\n",
      "  [-0.30882162  0.32332155 -1.1731504  ...  0.20733467 -0.15436971\n",
      "   -0.5943504 ]\n",
      "  [-0.620642    0.29548618  0.71730316 ...  0.19904283 -0.7441535\n",
      "   -0.44908482]\n",
      "  ...\n",
      "  [-0.4542219   0.12221723  0.4107418  ...  0.11069614 -0.00867207\n",
      "   -0.75793386]\n",
      "  [-0.5194204   0.06974911  0.43935725 ...  0.12730324 -0.19801784\n",
      "   -0.85665345]\n",
      "  [-0.6213888   0.10755614  0.20083559 ...  0.14149764  0.12504897\n",
      "   -0.83304274]]]\n",
      "\n",
      "Output 2:\n",
      "[[[-0.14993533  0.39350498 -0.6613952  ...  0.20598361 -0.1234247\n",
      "   -0.43900022]\n",
      "  [-0.3036442   0.34636506 -1.1049887  ...  0.21652922 -0.11604068\n",
      "   -0.49087635]\n",
      "  [-0.6253123   0.29293513  0.7562294  ...  0.21651298 -0.71308696\n",
      "   -0.36222446]\n",
      "  ...\n",
      "  [-0.4806537   0.13385466  0.42763412 ...  0.12149316  0.01471392\n",
      "   -0.66167647]\n",
      "  [-0.55217     0.09180634  0.46632445 ...  0.13775238 -0.16538444\n",
      "   -0.76108885]\n",
      "  [-0.6493276   0.13543183  0.2211605  ...  0.15584767  0.15793791\n",
      "   -0.73676115]]]\n"
     ]
    }
   ],
   "source": [
    "!python onnx_inference.py --input {args.prefix_dir + 'input_teleported_model.npy'} \\\n",
    "    --model1 {args.prefix_dir + 'block0_cob_activation_norm.onnx'} \\\n",
    "    --model2 {args.prefix_dir + 'block0_cob_activation_norm_teleported.onnx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b093927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.input_param_scale = 12\n",
    "args.num_cols = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c5bbf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.input_param_scale: 12\n",
      "args.logrows: 20\n",
      "args.num_cols: 2\n",
      "args.scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "# print important args related to ezkl\n",
    "print(\"args.input_param_scale:\",args.input_param_scale)\n",
    "print(\"args.logrows:\",args.log_rows)\n",
    "print(\"args.num_cols:\",args.num_cols)\n",
    "print(\"args.scale_rebase_multiplier:\",args.scale_rebase_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-08-27 20:30:11,956 execute.rs:742 SRS already exists at that path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Future pending cb=[<builtins.PyDoneCallback object at 0x30d1359d0>()]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ezkl\n",
    "\n",
    "run_args = ezkl.PyRunArgs()\n",
    "run_args.input_visibility = \"public\"\n",
    "# TODO: change that to fixed\n",
    "run_args.param_visibility = \"private\"\n",
    "run_args.output_visibility = \"public\"\n",
    "run_args.input_scale = args.input_param_scale\n",
    "run_args.param_scale = args.input_param_scale\n",
    "run_args.logrows = args.log_rows\n",
    "run_args.num_inner_cols = args.num_cols\n",
    "run_args.scale_rebase_multiplier = args.scale_rebase_multiplier\n",
    "\n",
    "run_args.variables = [('batch_size', BATCHS)]\n",
    "\n",
    "ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source ~/.config/envman/PATH.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17c8bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-08-27 20:30:13,287 execute.rs:640 read 134217988 bytes from file (vector of len = 134217988)\n",
      "INFO ezkl.execute 2024-08-27 20:30:14,358 execute.rs:647 file hash: 54ef75911da76d7a6b7ea341998aaf66cb06c679c53e0a88a4fe070dd3add963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a6698b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_param_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7723bc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Model: block0_cob_activation_norm ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-08-27 20:30:32,383 execute.rs:1044 num calibration batches: 1\n",
      "INFO tract_linalg.arm64 2024-08-27 20:30:32,395 arm64.rs:212 CPU optimisation: AppleM\n",
      "INFO tract_linalg.arm64 2024-08-27 20:30:32,397 arm64.rs:294 ARMv8.2 mmm_f16 and mmv_f16 activated\n",
      "INFO tract_linalg.arm64 2024-08-27 20:30:32,402 arm64.rs:315 ARMv8.2 tanh_f16 and sigmoid_f16 activated\n",
      "INFO tract_linalg.arm64 2024-08-27 20:30:32,404 arm64.rs:328 AMX optimisation activated\n",
      "WARNING ezkl.circuit.table 2024-08-27 20:33:37,691 table.rs:187 Using 5 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-08-27 20:33:37,734 table.rs:187 Using 5 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-08-27 20:33:37,744 table.rs:187 Using 10 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-08-27 20:33:37,806 table.rs:187 Using 10 columns for non-linearity table.\n",
      "ERROR ezkl.graph.model 2024-08-27 20:34:52,136 model.rs:1246 value (-53438638232309530861080368209599660033) out of range: (-8388608, 8388608)\n",
      "ERROR ezkl.execute 2024-08-27 20:34:52,210 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "WARNING ezkl.execute 2024-08-27 20:34:52,260 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 12, param_scale: 12, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+----------------+----------------+-------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error     | median_error   | max_error   | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+----------------+----------------+-------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.00004112916 | -0.00089766085 | 0.004924506 | -0.0043368638 | 0.00070580316  | 0.00089766085    | 0.004924506   | 0             | 0.0000008355524    | -0.0022828628      | 0.012695337            |\n",
      "+----------------+----------------+-------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n",
      "INFO ezkl.execute 2024-08-27 20:35:12,615 execute.rs:1044 num calibration batches: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Model: block0_cob_activation_norm_teleported ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ezkl.circuit.table 2024-08-27 20:38:50,183 table.rs:187 Using 5 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-08-27 20:38:50,208 table.rs:187 Using 5 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-08-27 20:38:50,213 table.rs:187 Using 9 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-08-27 20:38:50,217 table.rs:187 Using 9 columns for non-linearity table.\n",
      "ERROR ezkl.graph.model 2024-08-27 20:39:59,885 model.rs:1246 value (-53438638232309522540734099838136745985) out of range: (-8388608, 8388608)\n",
      "ERROR ezkl.execute 2024-08-27 20:39:59,920 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "WARNING ezkl.execute 2024-08-27 20:39:59,946 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 12, param_scale: 12, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+----------------+-------------+--------------+----------------+------------------+---------------+-------------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error   | max_error   | min_error    | mean_abs_error | median_abs_error | max_abs_error | min_abs_error     | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+----------------+-------------+--------------+----------------+------------------+---------------+-------------------+--------------------+--------------------+------------------------+\n",
      "| -0.000024726396 | -0.00053845346 | 0.004788637 | -0.003921896 | 0.0006812009   | 0.00053845346    | 0.004788637   | 0.000000059604645 | 0.00000078209655   | -0.00011264061     | 0.016232874            |\n",
      "+-----------------+----------------+-------------+--------------+----------------+------------------+---------------+-------------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!RUST_LOG=trace\n",
    "\n",
    "rng = ['block0_cob_activation_norm','block0_cob_activation_norm_teleported']\n",
    "\n",
    "# Generate the calibration data\n",
    "# x = input_mlp_0.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "data = dict(input_data=[x])\n",
    "cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "for model in rng:\n",
    "    print(\"==== Model:\",model, \"====\")\n",
    "    model_path = args.prefix_dir + model + \".onnx\"\n",
    "    settings_path = f'{args.prefix_dir} settings_{model}.json'\n",
    "    \n",
    "    # provide witness data\n",
    "    # x = input_mlp_0.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "    # data = {'input_data': [x]}\n",
    "    # with open(args.prefix_dir + 'input.json', 'w') as f:\n",
    "    #     json.dump(data, f)\n",
    "\n",
    "    # TODO: Dictionary outputs\n",
    "    res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "    assert res == True\n",
    "\n",
    "    # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier],max_logrows=args.log_rows)\n",
    "    # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier])\n",
    "    res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale])\n",
    "    assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "322e5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_40903/1891351780.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "# recreate and initialize the model\n",
    "model = build_model(args, pretrained=False)\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e265b52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "\n",
    "op - original_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ee44e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0337, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tout = LN.network(input_teleported_model)\n",
    "(original_pred - tout).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsitude the teleported_model weights in the original model\n",
    "\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# state_dict which the keys are related to mlp\n",
    "state_dic = LN.network.state_dict()\n",
    "\n",
    "# remove the norm2 keys\n",
    "state_dic = {k: v for k, v in state_dic.items() if 'norm2' not in k}\n",
    "\n",
    "model.blocks[0].mlp.load_state_dict(state_dic)\n",
    "\n",
    "state_dic = LN.network.state_dict()\n",
    "state_dic = {k.replace('norm2.',''): v for k, v in state_dic.items() if 'norm2' in k}\n",
    "model.blocks[0].norm2.load_state_dict(state_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error: 0.033744122833013535\n"
     ]
    }
   ],
   "source": [
    "new_pred = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "# diff new_pred and original_pred\n",
    "diff = (original_pred - new_pred).abs().mean()\n",
    "print(f\"Prediction error: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated model .pth\n",
    "torch.save(model.state_dict(), args.resume.replace(\".pth\",\"_teleported.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a9036fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# export the updated model to onnx\n",
    "torch.onnx.export(model, x,\\\n",
    "                args.prefix_dir + 'complete_model_teleported.onnx', \\\n",
    "                verbose=False, export_params=True, opset_version=15, do_constant_folding=True, \\\n",
    "                input_names=['input'], output_names=['output'], \\\n",
    "                dynamic_axes={'input' : {0 : 'batch_size'},'output': {0:'batch_size'},},\n",
    ")\n",
    "\n",
    "# define the shape\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2f467b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "# export the splits of the model\n",
    "\n",
    "input_path = args.prefix_dir + \"complete_model_teleported.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs_teleported.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}_teleported.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "477ae23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "\n",
    "# TODO:\n",
    "# define the data_loader_test an iterator which only returns the img\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "# contain data and label in the dataset\n",
    "# label is shape 1,1000 which is one hot encoded\n",
    "label = torch.tensor(1).unsqueeze(0) \n",
    "data_set_test = torch.utils.data.TensorDataset(data,label)\n",
    "data_loader_test = torch.utils.data.DataLoader(data_set_test, batch_size=BATCHS, shuffle=True)\n",
    "# data_loader_test = torch.utils.data.DataLoader(da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3c08c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import json\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_ezkl_output(witness_file, settings_file):\n",
    "    # convert the quantized ezkl output to float value\n",
    "    witness_output = json.load(open(witness_file))\n",
    "    outputs = witness_output['outputs']\n",
    "    with open(settings_file) as f:\n",
    "        settings = json.load(f)\n",
    "    ezkl_outputs = [[ezkl.felt_to_float(\n",
    "        outputs[i][j], settings['model_output_scales'][i]) for j in range(len(outputs[i]))] for i in range(len(outputs))]\n",
    "    return ezkl_outputs\n",
    "\n",
    "\n",
    "def get_onnx_output(model_file, input_file):\n",
    "    # generate the ML model output from the ONNX file\n",
    "    onnx_model = onnx.load(model_file)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    with open(input_file) as f:\n",
    "        inputs = json.load(f)\n",
    "    # reshape the input to the model\n",
    "    num_inputs = len(onnx_model.graph.input)\n",
    "\n",
    "    onnx_input = dict()\n",
    "    for i in range(num_inputs):\n",
    "        input_node = onnx_model.graph.input[i]\n",
    "        dims = []\n",
    "        elem_type = input_node.type.tensor_type.elem_type\n",
    "#         print(\"elem_type: \", elem_type)\n",
    "        for dim in input_node.type.tensor_type.shape.dim:\n",
    "            if dim.dim_value == 0:\n",
    "                dims.append(1)\n",
    "            else:\n",
    "                dims.append(dim.dim_value)\n",
    "        if elem_type == 6:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int32).reshape(dims)\n",
    "        elif elem_type == 7:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int64).reshape(dims)\n",
    "        elif elem_type == 9:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                bool).reshape(dims)\n",
    "        else:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.float32).reshape(dims)\n",
    "        onnx_input[input_node.name] = inputs_onnx\n",
    "    try:\n",
    "        onnx_session = onnxruntime.InferenceSession(model_file)\n",
    "        onnx_output = onnx_session.run(None, onnx_input)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "        # onnx_output = inputs['output_data']\n",
    "#     print(\"onnx \", onnx_output)\n",
    "    return onnx_output[0]\n",
    "\n",
    "\n",
    "def compare_outputs(zk_output, onnx_output):\n",
    "    # calculate percentage difference between the 2 outputs (which are lists)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    contains_sublist = any(isinstance(sub, list) for sub in zk_output)\n",
    "    zip_object = zip(np.array(zk_output),\n",
    "                     np.array(onnx_output))\n",
    "    \n",
    "    num_eq_zk_onnx = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for (i, (list1_i, list2_i)) in enumerate(zip_object):\n",
    "        diff = list1_i - list2_i\n",
    "        # iterate and print the diffs  if they are greater than 0.0\n",
    "        res.append(np.linalg.norm(diff,axis=(-1)))\n",
    "        print(\"= index: \",i, \"\\t diff-norm: \",np.linalg.norm(diff,axis=(-1)),\"\\t zk_output: \",list1_i.shape, \"\\t onnx_output: \",list2_i.shape)\n",
    "        \n",
    "        if np.argmax(list1_i) == np.argmax(list2_i):\n",
    "            num_eq_zk_onnx += 1\n",
    "        num_total += 1\n",
    "    \n",
    "    print(\"Accuracy (zk_onnx): \\t\", num_eq_zk_onnx / num_total)\n",
    "    acc_zk_onnx = num_eq_zk_onnx / num_total\n",
    "    return res, acc_zk_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b841355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t image.shape: torch.Size([1, 3, 224, 224])\n",
      "=====\n",
      "= index:  0 \t diff-norm:  2.5331572517661853 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label): \t 0.0\n",
      "Accuracy (onnx_label): \t 0.0\n",
      "=====\n",
      "\n",
      "\n",
      "= index:  0 \t diff-norm:  2.5331572517661853 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label) 0.0\n",
      "mean norm diff:  2.5331572517661853\n",
      "max norm diff:  2.5331572517661853\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ezkl_outputs = np.empty((0,1000))\n",
    "onnx_outputs = np.empty((0,1000))\n",
    "labels = np.array([])\n",
    "\n",
    "# Open log file for writing\n",
    "log_file_path = os.path.join(args.prefix_dir, \"log\", \"output_log.txt\")\n",
    "os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "\n",
    "    # Generate dataNum_input_convs.json  \n",
    "    for index, (image, label) in enumerate(data_loader_test):\n",
    "        \n",
    "        print(\"index:\",index,\"\\t image.shape:\",image.shape)\n",
    "        log_file.write(f\"index: {index}\\timage.shape: {image.shape}\\n\")\n",
    "        log_file.flush()\n",
    "        \n",
    "        os.makedirs(args.prefix_dir + \"ezkl_inputs/\"+str(index),exist_ok=True)\n",
    "\n",
    "        # remove batch dimension\n",
    "        output = model(image)\n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        pre_witness_path = None\n",
    "\n",
    "        # computing witness (last witness is important)\n",
    "        for i in [\"convs\"] + [t for t in range(model.depth)]:\n",
    "            for half in [\"True\",\"False\"]:\n",
    "\n",
    "                if i==\"convs\" and half==\"False\":\n",
    "                    continue\n",
    "\n",
    "                # Define paths\n",
    "                if i == \"convs\":\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}.json\"\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_{half}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}_{half}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}_{half}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}_{half}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}_{half}.json\"\n",
    "\n",
    "                # Generating input data\n",
    "                if i == \"convs\":\n",
    "                    data = dict(input_data = [((image).detach().numpy()).reshape([-1]).tolist()])\n",
    "                else:\n",
    "                    inter_i = model.split_n(image,i,half=half)\n",
    "                    data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "                json.dump(data, open(data_path, 'w' ))\n",
    "\n",
    "\n",
    "                # Swapping (output pre_witness -> cur_input of data_path)\n",
    "                if i != \"convs\":\n",
    "                    with open(pre_witness_path, 'r') as prev_witness_file:\n",
    "                        prev_witness_data = json.load(prev_witness_file)\n",
    "                        outputs = prev_witness_data['outputs']\n",
    "                        tmp = {\"input_data\": outputs}\n",
    "                        with open(data_path, 'w') as data_file:\n",
    "                            json.dump(tmp, data_file) \n",
    "\n",
    "                # Generate setting\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "                \n",
    "\n",
    "                # Calibrating setting\n",
    "#                 res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\", \\\n",
    "#                                               scales=[run_args.input_scale],max_logrows=run_args.logrows, scale_rebase_multiplier=[1],lookup_safety_margin=1)\n",
    "                # Compile circuit\n",
    "                res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "                assert res == True\n",
    "\n",
    "                # Generating witness\n",
    "                res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "                assert os.path.isfile(witness_path)\n",
    "\n",
    "                # Update input_scale\n",
    "                settings = json.load(open(settings_path, 'r'))\n",
    "                run_args.input_scale = settings[\"model_output_scales\"][0]\n",
    "\n",
    "                # Update pre_witness_path\n",
    "                pre_witness_path = witness_path\n",
    "\n",
    "        # check accuracy\n",
    "        model_file = args.prefix_dir + \"network_complete.onnx\" \n",
    "        input_file = args.prefix_dir + f\"ezkl_inputs/{index}/input_convs.json\"\n",
    "        witness_file = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{model.depth-1}_False.json\"\n",
    "        settings_file = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{model.depth-1}_False.json\"\n",
    "\n",
    "        # get the ezkl output\n",
    "        ezkl_output = get_ezkl_output(witness_file, settings_file)\n",
    "        ezkl_output = np.array(ezkl_output).reshape(BATCHS,1000)\n",
    "        # get the onnx output\n",
    "        onnx_output = get_onnx_output(model_file, input_file)\n",
    "\n",
    "        ezkl_outputs = np.concatenate((ezkl_outputs,ezkl_output),axis=0)\n",
    "        onnx_outputs = np.concatenate((onnx_outputs,onnx_output),axis=0)\n",
    "        labels = np.concatenate((labels,label),axis=0)\n",
    "\n",
    "        print(\"=====\")\n",
    "        log_file.write(\"=====\\n\")\n",
    "        _,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "        log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "        \n",
    "        acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "        print(\"Accuracy (zk_label): \\t\", acc_zk_label)\n",
    "        log_file.write(f\"Accuracy (zk_label): \\t {acc_zk_label}\\n\")\n",
    "        \n",
    "        acc_onnx_label = np.sum( (labels) == np.argmax(onnx_outputs,axis=(-1)) ) / len(labels)\n",
    "        print(\"Accuracy (onnx_label): \\t\", acc_onnx_label)\n",
    "        log_file.write(f\"Accuracy (onnx_label): \\t {acc_onnx_label}\\n\")\n",
    "        \n",
    "        print(\"=====\\n\\n\")\n",
    "        log_file.write(\"=====\\n\\n\")\n",
    "        log_file.flush()\n",
    "    \n",
    "    \n",
    "    # compare the outputs\n",
    "    percentage_difference,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "\n",
    "    # compare zk_output and truth_label\n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label)\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label) {acc_zk_label}\\n\")\n",
    "\n",
    "    # print the percentage difference\n",
    "    mean_percentage_difference = np.mean(np.abs(percentage_difference))\n",
    "    max_percentage_difference = np.max(np.abs(percentage_difference))\n",
    "    print(\"mean norm diff: \", mean_percentage_difference)\n",
    "    print(\"max norm diff: \", max_percentage_difference)\n",
    "    log_file.write(f\"mean norm diff: {mean_percentage_difference}\\n\")\n",
    "    log_file.write(f\"max norm diff: {max_percentage_difference}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c6310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralteleportation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
