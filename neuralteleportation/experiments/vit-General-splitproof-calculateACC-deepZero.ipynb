{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0deecdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# def get_cpu_load():\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = psutil.cpu_percent(interval=1, percpu=True)\n",
    "#     return cpu_loads\n",
    "\n",
    "# def select_k_cpus_with_lowest_load(k):\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = get_cpu_load()\n",
    "    \n",
    "#     # Create a list of tuples (core_id, load)\n",
    "#     cpu_load_tuples = list(enumerate(cpu_loads))\n",
    "    \n",
    "#     # Sort the list based on load\n",
    "#     sorted_cpu_loads = sorted(cpu_load_tuples, key=lambda x: x[1])\n",
    "    \n",
    "#     # Get the IDs of the K cores with lowest load\n",
    "#     selected_cpus = [core[0] for core in sorted_cpu_loads[:k]]\n",
    "    \n",
    "#     return selected_cpus\n",
    "\n",
    "# # Example usage\n",
    "# k = 8  # Number of CPUs with lowest load\n",
    "# selected_cpus = select_k_cpus_with_lowest_load(k)\n",
    "# print(f\"Selected {k} CPUs with lowest load:\", selected_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88b3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "\n",
    "# def set_cpu_affinity(pid, cpu_list):\n",
    "#     try:\n",
    "#         p = psutil.Process(pid)\n",
    "#         p.cpu_affinity(cpu_list)\n",
    "#         print(f\"CPU affinity for process {pid} set to: {cpu_list}\")\n",
    "#     except psutil.NoSuchProcess:\n",
    "#         print(f\"Process with PID {pid} does not exist.\")\n",
    "#     except psutil.AccessDenied:\n",
    "#         print(\"Permission denied. You may need sudo privileges to set CPU affinity.\")\n",
    "\n",
    "# # Get the process ID of the current process\n",
    "# pid = os.getpid()\n",
    "\n",
    "# # Set the CPU affinity to only CPU core 0\n",
    "# cpu_list = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "# set_cpu_affinity(pid, cpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b1662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9b448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c43754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b5e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Pytorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653a3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as Fhtop\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "# check if notebook is in colab\n",
    "try:\n",
    "    # install ezkl\n",
    "    import google.colab\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
    "\n",
    "# rely on local installation of ezkl if the notebook is not in colab\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# uncomment for more descriptive logging \n",
    "import logging\n",
    "FORMAT = '%(levelname)s %(name)s %(asctime)-15s %(filename)s:%(lineno)d %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9938e",
   "metadata": {},
   "source": [
    "## EZKL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa48d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import argparse\n",
    "\n",
    "def get_args_parser(initial_args=None):\n",
    "    parser = argparse.ArgumentParser('ConvNeXt training and evaluation script for image classification', add_help=False)\n",
    "#     parser.add_argument('--batch_size', default=256, type=int,\n",
    "#                         help='Per GPU batch size')\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--update_freq', default=1, type=int,\n",
    "                        help='gradient accumulation steps')\n",
    "\n",
    "    # Model parameters\n",
    "#     parser.add_argument('--model', default='convnext_tiny', type=str, metavar='MODEL',\n",
    "#                         help='Name of model to train')\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='image input size')\n",
    "    parser.add_argument('--layer_scale_init_value', default=1e-6, type=float,\n",
    "                        help=\"Layer scale initial values\")\n",
    "    \n",
    "    ########################## settings specific to this project ##########################\n",
    "    \n",
    "    # dropout and stochastic depth drop rate; set at most one to non-zero\n",
    "    parser.add_argument('--dropout', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    parser.add_argument('--drop_path', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    \n",
    "    # early / late dropout and stochastic depth settings\n",
    "    parser.add_argument('--drop_mode', type=str, default='standard', choices=['standard', 'early', 'late'], help='drop mode')\n",
    "    parser.add_argument('--drop_schedule', type=str, default='constant', choices=['constant', 'linear'], \n",
    "                        help='drop schedule for early dropout / s.d. only')\n",
    "    parser.add_argument('--cutoff_epoch', type=int, default=0, \n",
    "                        help='if drop_mode is early / late, this is the epoch where dropout ends / starts')\n",
    "    \n",
    "    ####################################################################################### \n",
    "    \n",
    "    # EMA related parameters\n",
    "    parser.add_argument('--model_ema', type=str2bool, default=False)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', type=str2bool, default=False, help='')\n",
    "    parser.add_argument('--model_ema_eval', type=str2bool, default=False, help='Using ema to eval during training.')\n",
    "\n",
    "    # Optimization parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "        weight decay. We use a cosine schedule for WD and using a larger decay by\n",
    "        the end of training improves performance for ViTs.\"\"\")\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=4e-3, metavar='LR',\n",
    "                        help='learning rate (default: 4e-3), with total batch size 4096')\n",
    "    parser.add_argument('--layer_decay', type=float, default=1.0)\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-6)')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=50, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--crop_pct', type=float, default=None)\n",
    "\n",
    "    # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', type=str2bool, default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "    parser.add_argument('--finetune', default='',\n",
    "                        help='finetune from checkpoint')\n",
    "    parser.add_argument('--head_init_scale', default=1.0, type=float,\n",
    "                        help='classifier head initial scale, typically adjusted in fine-tuning')\n",
    "    parser.add_argument('--model_key', default='model|module', type=str,\n",
    "                        help='which key to load from saved state dict, usually model or model_ema')\n",
    "    parser.add_argument('--model_prefix', default='', type=str)\n",
    "\n",
    "    # Dataset parameters\n",
    "#     parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "#                         help='dataset path')\n",
    "    parser.add_argument('--eval_data_path', default=None, type=str,\n",
    "                        help='dataset path for evaluation')\n",
    "    parser.add_argument('--nb_classes', default=1000, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--imagenet_default_mean_and_std', type=str2bool, default=True)\n",
    "    parser.add_argument('--data_set', default='IMNET', choices=['CIFAR', 'IMNET', 'image_folder'],\n",
    "                        type=str, help='ImageNet dataset path')\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "#     parser.add_argument('--resume', default='',\n",
    "#                         help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt_freq', default=1, type=int)\n",
    "    parser.add_argument('--save_ckpt_num', default=3, type=int)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', type=str2bool, default=False,\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', type=str2bool, default=True,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--disable_eval', type=str2bool, default=False,\n",
    "                        help='Disabling evaluation during training')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', type=str2bool, default=True,\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', type=str2bool, default=False)\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--use_amp', type=str2bool, default=False, \n",
    "                        help=\"Use PyTorch's AMP (Automatic Mixed Precision) or not\")\n",
    "\n",
    "    # Weights and Biases arguments\n",
    "    parser.add_argument('--enable_wandb', type=str2bool, default=False,\n",
    "                        help=\"enable logging to Weights and Biases\")\n",
    "    parser.add_argument('--project', default='convnext', type=str,\n",
    "                        help=\"The name of the W&B project where you're sending the new run.\")\n",
    "    parser.add_argument('--wandb_ckpt', type=str2bool, default=False,\n",
    "                        help=\"Save model checkpoints as W&B Artifacts.\")\n",
    "\n",
    "    # arguments for pruning\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=4096)\n",
    "#     parser.add_argument(\"--sparsity\", type=float, default=0.)\n",
    "    parser.add_argument(\"--prune_metric\", type=str, choices=[\"magnitude\", \"wanda\"])\n",
    "    parser.add_argument(\"--prune_granularity\", type=str)\n",
    "    parser.add_argument(\"--blocksize\", type=int, default=1)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"\n",
    "    Converts string to bool type; enables command line \n",
    "    arguments in the format of '--arg1 true --arg2 false'\n",
    "    \"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dcab1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def get_default_args():\n",
    "    parser = get_args_parser()\n",
    "    default_args = {}\n",
    "    for action in parser._actions:\n",
    "        # Check if action is an argument\n",
    "        if not action.option_strings:\n",
    "            continue\n",
    "        # Use the destination as the key and the default value as the value\n",
    "        default_args[action.dest] = action.default\n",
    "    return DefaultArgs(**default_args)\n",
    "\n",
    "# Example usage:\n",
    "default_args = get_default_args()\n",
    "args = default_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7973753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== IMPORTANT: INPUT FLAGS OF PYTHON CODE HERE =====\n",
    "\n",
    "# args.model = \"vit_tiny\"\n",
    "# args.data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "\n",
    "# args.resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# #args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "# # args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/model_weights/deit/deit_tiny_patch16_224-a1311bcf.pth\"\n",
    "\n",
    "\n",
    "# args.sparsity = 0.5\n",
    "# args.batch_size = 32\n",
    "\n",
    "# args.pruning_method = \"CAP\" # DENSE,CAP,WANDA\n",
    "\n",
    "# # args.prune_metric = \"wanda\"\n",
    "# # args.prune_granularity = \"row\"\n",
    "# # pruned_model_dir = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "\n",
    "# # Prefix directory\n",
    "# args.prefix_dir = \"sparse-cap-acc/\" #sparse-cap-acc, dense-acc, sparse-wanda-acc\n",
    "# os.makedirs(args.prefix_dir, exist_ok=True)\n",
    "\n",
    "# # EZKL HPs\n",
    "# args.input_param_scale = 7\n",
    "# args.log_rows = 20\n",
    "# args.num_cols = 4\n",
    "# args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65b9fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- IN_JUPYTER: True ---- \n",
      "Model: vit_tiny\n",
      "Data Path: /rds/general/user/mm6322/home/imagenet\n",
      "Resume Path: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "Sparsity: 0.5\n",
      "Batch Size: 1\n",
      "Pruning Method: CAP\n",
      "Prefix Directory: sparse-cap-acc-tmp/\n",
      "Input Parameter Scale: 7\n",
      "Log Rows: 20\n",
      "Number of Columns: 2\n",
      "Scale Rebase Multiplier: 1\n",
      "-------- \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# Check if running in Jupyter Notebook\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        IN_JUPYTER = True\n",
    "    else:\n",
    "        IN_JUPYTER = False\n",
    "except:\n",
    "    IN_JUPYTER = False\n",
    "\n",
    "# Define default values\n",
    "default_model = \"vit_tiny\"\n",
    "default_data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "default_resume = \"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# default_resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "default_sparsity = 0.5\n",
    "default_batch_size = 1\n",
    "default_pruning_method = \"CAP\"\n",
    "default_prefix_dir = \"sparse-cap-acc-tmp/\"\n",
    "default_input_param_scale = 7\n",
    "default_log_rows = 20\n",
    "default_num_cols = 2\n",
    "default_scale_rebase_multiplier = 1\n",
    "\n",
    "# Parse command-line arguments if not in Jupyter Notebook\n",
    "if not IN_JUPYTER:\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('--model', default=default_model, type=str, help='Model type')\n",
    "    parser.add_argument('--data_path', default=default_data_path, type=str, help='Data path')\n",
    "    parser.add_argument('--resume', default=default_resume, type=str, help='Resume path')\n",
    "    parser.add_argument('--sparsity', default=default_sparsity, type=float, help='Sparsity value')\n",
    "    parser.add_argument('--batch_size', default=default_batch_size, type=int, help='Batch size')\n",
    "    parser.add_argument('--pruning_method', default=default_pruning_method, type=str, help='Pruning method')\n",
    "    parser.add_argument('--prefix_dir', default=default_prefix_dir, type=str, help='Prefix directory')\n",
    "    parser.add_argument('--input_param_scale', default=default_input_param_scale, type=int, help='Input parameter scale')\n",
    "    parser.add_argument('--log_rows', default=default_log_rows, type=int, help='Log rows')\n",
    "    parser.add_argument('--num_cols', default=default_num_cols, type=int, help='Number of columns')\n",
    "    parser.add_argument('--scale_rebase_multiplier', default=default_scale_rebase_multiplier, type=int, help='Scale rebase multiplier')\n",
    "\n",
    "    args = parser.parse_args(namespace=args)\n",
    "else:\n",
    "    # In Jupyter Notebook, define args with default values\n",
    "    args.model = default_model\n",
    "    args.data_path = default_data_path\n",
    "    args.resume = default_resume\n",
    "    args.sparsity = default_sparsity\n",
    "    args.batch_size = default_batch_size\n",
    "    args.pruning_method = default_pruning_method\n",
    "    args.prefix_dir = default_prefix_dir\n",
    "    args.input_param_scale = default_input_param_scale\n",
    "    args.log_rows = default_log_rows\n",
    "    args.num_cols = default_num_cols\n",
    "    args.scale_rebase_multiplier = default_scale_rebase_multiplier\n",
    "    \n",
    "    \n",
    "print(\"\\n ---- IN_JUPYTER:\",str(IN_JUPYTER),\"---- \")\n",
    "# Print values for verification\n",
    "print(\"Model:\", args.model)\n",
    "print(\"Data Path:\", args.data_path)\n",
    "print(\"Resume Path:\", args.resume)\n",
    "print(\"Sparsity:\", args.sparsity)\n",
    "print(\"Batch Size:\", args.batch_size)\n",
    "print(\"Pruning Method:\", args.pruning_method)\n",
    "print(\"Prefix Directory:\", args.prefix_dir)\n",
    "print(\"Input Parameter Scale:\", args.input_param_scale)\n",
    "print(\"Log Rows:\", args.log_rows)\n",
    "print(\"Number of Columns:\", args.num_cols)\n",
    "print(\"Scale Rebase Multiplier:\", args.scale_rebase_multiplier)\n",
    "print(\"-------- \\n\\n\\n\")\n",
    "\n",
    "os.makedirs(args.prefix_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7100bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 300\n",
      "update_freq: 1\n",
      "input_size: 224\n",
      "layer_scale_init_value: 1e-06\n",
      "dropout: 0\n",
      "drop_path: 0\n",
      "drop_mode: standard\n",
      "drop_schedule: constant\n",
      "cutoff_epoch: 0\n",
      "model_ema: False\n",
      "model_ema_decay: 0.9999\n",
      "model_ema_force_cpu: False\n",
      "model_ema_eval: False\n",
      "opt: adamw\n",
      "opt_eps: 1e-08\n",
      "opt_betas: None\n",
      "clip_grad: None\n",
      "momentum: 0.9\n",
      "weight_decay: 0.05\n",
      "weight_decay_end: None\n",
      "lr: 0.004\n",
      "layer_decay: 1.0\n",
      "min_lr: 1e-06\n",
      "warmup_epochs: 50\n",
      "warmup_steps: -1\n",
      "color_jitter: 0.4\n",
      "aa: rand-m9-mstd0.5-inc1\n",
      "smoothing: 0.1\n",
      "train_interpolation: bicubic\n",
      "crop_pct: None\n",
      "reprob: 0.25\n",
      "remode: pixel\n",
      "recount: 1\n",
      "resplit: False\n",
      "mixup: 0.8\n",
      "cutmix: 1.0\n",
      "cutmix_minmax: None\n",
      "mixup_prob: 1.0\n",
      "mixup_switch_prob: 0.5\n",
      "mixup_mode: batch\n",
      "finetune: \n",
      "head_init_scale: 1.0\n",
      "model_key: model|module\n",
      "model_prefix: \n",
      "eval_data_path: None\n",
      "nb_classes: 1000\n",
      "imagenet_default_mean_and_std: True\n",
      "data_set: IMNET\n",
      "output_dir: \n",
      "device: cuda\n",
      "seed: 0\n",
      "auto_resume: True\n",
      "save_ckpt: True\n",
      "save_ckpt_freq: 1\n",
      "save_ckpt_num: 3\n",
      "start_epoch: 0\n",
      "eval: False\n",
      "dist_eval: True\n",
      "disable_eval: False\n",
      "num_workers: 10\n",
      "pin_mem: True\n",
      "world_size: 1\n",
      "local_rank: -1\n",
      "dist_on_itp: False\n",
      "dist_url: env://\n",
      "use_amp: False\n",
      "enable_wandb: False\n",
      "project: convnext\n",
      "wandb_ckpt: False\n",
      "nsamples: 4096\n",
      "prune_metric: None\n",
      "prune_granularity: None\n",
      "blocksize: 1\n",
      "model: vit_tiny\n",
      "data_path: /rds/general/user/mm6322/home/imagenet\n",
      "resume: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "sparsity: 0.5\n",
      "batch_size: 1\n",
      "pruning_method: CAP\n",
      "prefix_dir: sparse-cap-acc-tmp/\n",
      "input_param_scale: 7\n",
      "log_rows: 20\n",
      "num_cols: 2\n",
      "scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in vars(args).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eacb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.output_dir:\n",
    "    Path(default_args_dict.output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa529814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from timm.data.constants import \\\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data import create_transform\n",
    "\n",
    "def build_dataset(is_train, args):\n",
    "    transform = build_transform(is_train, args)\n",
    "\n",
    "    print(\"Transform = \")\n",
    "    if isinstance(transform, tuple):\n",
    "        for trans in transform:\n",
    "            print(\" - - - - - - - - - - \")\n",
    "            for t in trans.transforms:\n",
    "                print(t)\n",
    "    else:\n",
    "        for t in transform.transforms:\n",
    "            print(t)\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    if args.data_set == 'CIFAR':\n",
    "        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform, download=True)\n",
    "        nb_classes = 100\n",
    "    elif args.data_set == 'IMNET':\n",
    "        print(\"reading from datapath\", args.data_path)\n",
    "        root = os.path.join(args.data_path, 'train' if is_train else 'val_dirs')\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = 1000\n",
    "    elif args.data_set == \"image_folder\":\n",
    "        root = args.data_path if is_train else args.eval_data_path\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = args.nb_classes\n",
    "        assert len(dataset.class_to_idx) == nb_classes\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    print(\"Number of the class = %d\" % nb_classes)\n",
    "\n",
    "    return dataset, nb_classes\n",
    "\n",
    "\n",
    "def build_transform(is_train, args):\n",
    "    resize_im = args.input_size > 32\n",
    "    imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n",
    "    mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n",
    "    std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n",
    "\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=args.input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            interpolation=args.train_interpolation,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                args.input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        # warping (no cropping) when evaluated at 384 or larger\n",
    "        if args.input_size >= 384:  \n",
    "            t.append(\n",
    "            transforms.Resize((args.input_size, args.input_size), \n",
    "                            interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "        )\n",
    "            print(f\"Warping {args.input_size} size input images...\")\n",
    "        else:\n",
    "            if args.crop_pct is None:\n",
    "                args.crop_pct = 224 / 256\n",
    "            size = int(args.input_size / args.crop_pct)\n",
    "            t.append(\n",
    "                # to maintain same ratio w.r.t. 224 images\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),  \n",
    "            )\n",
    "            t.append(transforms.CenterCrop(args.input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fbfe49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b27181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_train = torch.utils.data.DistributedSampler(\n",
    "#         dataset_train, num_replicas=1, rank=0, shuffle=True, seed=args.seed,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad0d0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_train = torch.utils.data.DataLoader(\n",
    "#     dataset_train, sampler=sampler_train,\n",
    "#     batch_size=args.batch_size,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68603205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.12'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b915ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import numpy as np\n",
    "from timm.utils import get_state_dict\n",
    "\n",
    "from pathlib import Path\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "# from torch._six import inf\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "class TensorboardLogger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = SummaryWriter(logdir=log_dir)\n",
    "        self.step = 0\n",
    "\n",
    "    def set_step(self, step=None):\n",
    "        if step is not None:\n",
    "            self.step = step\n",
    "        else:\n",
    "            self.step += 1\n",
    "\n",
    "    def update(self, head='scalar', step=None, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.writer.add_scalar(head + \"/\" + k, v, self.step if step is None else step)\n",
    "\n",
    "    def flush(self):\n",
    "        self.writer.flush()\n",
    "\n",
    "\n",
    "class WandbLogger(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        try:\n",
    "            import wandb\n",
    "            self._wandb = wandb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use the Weights and Biases Logger please install wandb.\"\n",
    "                \"Run `pip install wandb` to install it.\"\n",
    "            )\n",
    "\n",
    "        # Initialize a W&B run \n",
    "        if self._wandb.run is None:\n",
    "            self._wandb.init(\n",
    "                project=args.project,\n",
    "                config=args\n",
    "            )\n",
    "\n",
    "    def log_epoch_metrics(self, metrics, commit=True):\n",
    "        \"\"\"\n",
    "        Log train/test metrics onto W&B.\n",
    "        \"\"\"\n",
    "        # Log number of model parameters as W&B summary\n",
    "        self._wandb.summary['n_parameters'] = metrics.get('n_parameters', None)\n",
    "        metrics.pop('n_parameters', None)\n",
    "\n",
    "        # Log current epoch\n",
    "        self._wandb.log({'epoch': metrics.get('epoch')}, commit=False)\n",
    "        metrics.pop('epoch')\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            if 'train' in k:\n",
    "                self._wandb.log({f'Global Train/{k}': v}, commit=False)\n",
    "            elif 'test' in k:\n",
    "                self._wandb.log({f'Global Test/{k}': v}, commit=False)\n",
    "\n",
    "        self._wandb.log({})\n",
    "\n",
    "    def log_checkpoints(self):\n",
    "        output_dir = self.args.output_dir\n",
    "        model_artifact = self._wandb.Artifact(\n",
    "            self._wandb.run.id + \"_model\", type=\"model\"\n",
    "        )\n",
    "\n",
    "        model_artifact.add_dir(output_dir)\n",
    "        self._wandb.log_artifact(model_artifact, aliases=[\"latest\", \"best\"])\n",
    "\n",
    "    def set_steps(self):\n",
    "        # Set global training step\n",
    "        self._wandb.define_metric('Rank-0 Batch Wise/*', step_metric='Rank-0 Batch Wise/global_train_step')\n",
    "        # Set epoch-wise step\n",
    "        self._wandb.define_metric('Global Train/*', step_metric='epoch')\n",
    "        self._wandb.define_metric('Global Test/*', step_metric='epoch')\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "def load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "        \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.weight$')\n",
    "    state_dict_keys = list(state_dict.keys())\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.weight\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "            \n",
    "    \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.bias$')\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.bias\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "                                              \n",
    "    \n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(model, prefix=prefix)\n",
    "\n",
    "    warn_missing_keys = []\n",
    "    ignore_missing_keys = []\n",
    "    for key in missing_keys:\n",
    "        keep_flag = True\n",
    "        for ignore_key in ignore_missing.split('|'):\n",
    "            if ignore_key in key:\n",
    "                keep_flag = False\n",
    "                break\n",
    "        if keep_flag:\n",
    "            warn_missing_keys.append(key)\n",
    "        else:\n",
    "            ignore_missing_keys.append(key)\n",
    "            \n",
    "\n",
    "    missing_keys = warn_missing_keys\n",
    "\n",
    "    if len(missing_keys) > 0:\n",
    "        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "            model.__class__.__name__, unexpected_keys))\n",
    "    if len(ignore_missing_keys) > 0:\n",
    "        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, ignore_missing_keys))\n",
    "    if len(error_msgs) > 0:\n",
    "        print('\\n'.join(error_msgs))\n",
    "\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "\n",
    "        ########################################################\n",
    "        ## Code I added \n",
    "        for param in parameters:\n",
    "            weight_copy = param.data.abs().clone()\n",
    "            mask = weight_copy.gt(0).float().cuda()\n",
    "            sparsity = mask.sum() / mask.numel()\n",
    "            if sparsity > 0.3:\n",
    "                # non-trivial sparsity \n",
    "                param.grad.data.mul_(mask)\n",
    "        ########################################################\n",
    "\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n",
    "                     start_warmup_value=0, warmup_steps=-1):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_steps > 0:\n",
    "        warmup_iters = warmup_steps\n",
    "    print(\"Set warmup steps = %d\" % warmup_iters)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = np.array(\n",
    "        [final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        to_save = {\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'scaler': loss_scaler.state_dict(),\n",
    "            'args': args,\n",
    "        }\n",
    "\n",
    "        if model_ema is not None:\n",
    "            to_save['model_ema'] = get_state_dict(model_ema)\n",
    "\n",
    "        save_on_master(to_save, checkpoint_path)\n",
    "\n",
    "    if is_main_process() and isinstance(epoch, int):\n",
    "        to_del = epoch - args.save_ckpt_num * args.save_ckpt_freq\n",
    "        old_ckpt = output_dir / ('checkpoint-%s.pth' % to_del)\n",
    "        if os.path.exists(old_ckpt):\n",
    "            os.remove(old_ckpt)\n",
    "\n",
    "\n",
    "def auto_load_model(args, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.auto_resume and len(args.resume) == 0:\n",
    "        import glob\n",
    "        all_checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint-*.pth'))\n",
    "        latest_ckpt = -1\n",
    "        for ckpt in all_checkpoints:\n",
    "            t = ckpt.split('-')[-1].split('.')[0]\n",
    "            if t.isdigit():\n",
    "                latest_ckpt = max(int(t), latest_ckpt)\n",
    "        if latest_ckpt >= 0:\n",
    "            args.resume = os.path.join(output_dir, 'checkpoint-%d.pth' % latest_ckpt)\n",
    "        print(\"Auto resume checkpoint: %s\" % args.resume)\n",
    "\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        print(\"Resume checkpoint %s\" % args.resume)\n",
    "        if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            if not isinstance(checkpoint['epoch'], str): # does not support resuming with 'best', 'best-ema'\n",
    "                args.start_epoch = checkpoint['epoch'] + 1\n",
    "            else:\n",
    "                assert args.eval, 'Does not support resuming with checkpoint-best'\n",
    "            if hasattr(args, 'model_ema') and args.model_ema:\n",
    "                if 'model_ema' in checkpoint.keys():\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model_ema'])\n",
    "                else:\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model'])\n",
    "            if 'scaler' in checkpoint:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            print(\"With optim & sched!\")\n",
    "\n",
    "def reg_scheduler(base_value, final_value, epochs, niter_per_ep, early_epochs=0, early_value=None, \n",
    "           mode='linear', early_mode='regular'):\n",
    "    early_schedule = np.array([])\n",
    "    early_iters = early_epochs * niter_per_ep\n",
    "    if early_value is None:\n",
    "        early_value = final_value\n",
    "    if early_epochs > 0:\n",
    "        print(f\"Set early value to {early_mode} {early_value}\")\n",
    "        if early_mode == 'regular':\n",
    "            early_schedule = np.array([early_value] * early_iters)\n",
    "        elif early_mode == 'linear':\n",
    "            early_schedule = np.linspace(early_value, base_value, early_iters)\n",
    "        elif early_mode == 'cosine':\n",
    "            early_schedule = np.array(\n",
    "            [base_value + 0.5 * (early_value - base_value) * (1 + math.cos(math.pi * i / early_iters)) for i in np.arange(early_iters)])\n",
    "    regular_epochs = epochs - early_epochs\n",
    "    iters = np.arange(regular_epochs * niter_per_ep)\n",
    "    schedule = np.linspace(base_value, final_value, len(iters))\n",
    "    schedule = np.concatenate((early_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def build_model(args, pretrained=False):\n",
    "    if args.model.startswith(\"convnext\"):\n",
    "        model = create_model(\n",
    "            args.model,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=args.nb_classes,\n",
    "            layer_scale_init_value=args.layer_scale_init_value,\n",
    "            head_init_scale=args.head_init_scale,\n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate=args.dropout,\n",
    "            )\n",
    "    else:\n",
    "        model = create_model(\n",
    "            args.model, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=args.nb_classes, \n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate =args.dropout\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97c8ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCHS: 1\n"
     ]
    }
   ],
   "source": [
    "BATCHS = args.batch_size\n",
    "print(\"BATCHS:\",BATCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c57ca80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.resnet import resnet26d, resnet50d\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
    "    ),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_huge_patch16_224': _cfg(),\n",
    "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
    "    # hybrid models\n",
    "    'vit_small_resnet26d_224': _cfg(),\n",
    "    'vit_small_resnet50d_s3_224': _cfg(),\n",
    "    'vit_base_resnet26d_224': _cfg(),\n",
    "    'vit_base_resnet50d_224': _cfg(),\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        \n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "#         self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.query_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.key_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.value_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "#         query = self.query_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         key = self.key_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         value = self.value_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "        \n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        qkvs = self.qkv(x)\n",
    "        query = qkvs[:,:,0:self.dim]\n",
    "        key = qkvs[:,:,self.dim:2*self.dim]\n",
    "        value = qkvs[:,:,2*self.dim:] \n",
    "        query = query.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        key = key.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        value = value.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale     \n",
    "        attn = query.transpose(1,2) @ key.transpose(1,2).transpose(2,3)\n",
    "        attn = attn * self.scale\n",
    "\n",
    "\n",
    "        attn = attn.softmax(dim=(-1))\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        \n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        y = (attn@value.transpose(1,2)).transpose(1,2).reshape(B,N,C)\n",
    "        \n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "        y = self.proj(y)\n",
    "        y = self.proj_drop(y)\n",
    "#         return x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "#         self.norm1 = norm_layer(self.dim)\n",
    "        self.norm1 = norm_layer(self.dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(self.dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))      \n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def half_block(self,x):\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "#         assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "#             f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "#         x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x).reshape(self.batch_size,192,196,1).transpose(1,2)\n",
    "        x = x.squeeze(3)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# class HybridEmbed(nn.Module):\n",
    "#     \"\"\" CNN Feature Map Embedding\n",
    "#     Extract feature map from CNN, flatten, project to embedding dim.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         assert isinstance(backbone, nn.Module)\n",
    "#         img_size = to_2tuple(img_size)\n",
    "#         self.img_size = img_size\n",
    "#         self.backbone = backbone\n",
    "#         if feature_size is None:\n",
    "#             with torch.no_grad():\n",
    "#                 # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
    "#                 # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
    "#                 # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
    "#                 training = backbone.training\n",
    "#                 if training:\n",
    "#                     backbone.eval()\n",
    "#                 o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "#                 feature_size = o.shape[-2:]\n",
    "#                 feature_dim = o.shape[1]\n",
    "#                 backbone.train(training)\n",
    "#         else:\n",
    "#             feature_size = to_2tuple(feature_size)\n",
    "#             feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "#         self.num_patches = feature_size[0] * feature_size[1]\n",
    "#         self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)[-1]\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "#         x = self.proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        # I add these two lines\n",
    "        self.drop_rate=drop_rate\n",
    "        attn_drop_rate=drop_rate\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.depth = depth\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
    "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
    "        #self.repr_act = nn.Tanh()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    " \n",
    "        x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "        B = BATCHS\n",
    "    \n",
    "\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1)\n",
    "        \n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def split_convs(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "#       B = x.shape[0]\n",
    "        B = BATCHS\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = torch.cat((self.cls_token, x), dim=1)\n",
    "    \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "#         num_blocks = len(self.blocks) // 2\n",
    "#         for blk in self.blocks[:num_blocks]:\n",
    "#             x = blk(x)\n",
    "\n",
    "#         x = self.blocks[0].half_block(x)\n",
    "\n",
    "#         print(\"split_1, output.shape:\",x.shape, \"\\t num_blocks:\",num_blocks)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def split_2(self,x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks[0].half_block(x)\n",
    "        return x\n",
    "    \n",
    "    def split_n(self,x,n,half=None):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for layer_idx in range(n-1):\n",
    "            x = self.blocks[layer_idx](x)\n",
    "            \n",
    "        #n-th layer\n",
    "        if half:\n",
    "            x = self.blocks[n].half_block(x)\n",
    "        else:\n",
    "            x = self.blocks[n](x)\n",
    "            # last layer of transformer\n",
    "            if n == (self.depth - 1):\n",
    "                x = self.norm(x)\n",
    "                x = x[:, 0]\n",
    "                x = self.head(x)\n",
    "                \n",
    "        return x\n",
    "            \n",
    "          \n",
    "    def update_drop_path(self, drop_path_rate):\n",
    "        self.drop_path = drop_path_rate\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, self.depth)]\n",
    "        for i in range(self.depth):\n",
    "            self.blocks[i].drop_path.drop_prob = dp_rates[i]\n",
    "    \n",
    "    def update_dropout(self, drop_rate):\n",
    "        self.drop_rate = drop_rate\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = drop_rate\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# @register_model\n",
    "# def vit_tiny_tiny(pretrained=False, **kwargs):\n",
    "#     model = VisionTransformer(\n",
    "#         patch_size=16, embed_dim=48, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "@register_model\n",
    "def vit_tiny(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_small(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_base(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_large(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e0b5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(args, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37b6630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.VisionTransformer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a10f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 5717416\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b184e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_78050/3668749705.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "# total_batch_size = args.batch_size * args.update_freq * utils.get_world_size()\n",
    "total_batch_size = args.batch_size * args.update_freq\n",
    "# num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n",
    "\n",
    "# At most one of dropout and stochastic depth should be enabled.\n",
    "assert(args.dropout == 0 or args.drop_path == 0)\n",
    "# ConvNeXt does not support dropout.\n",
    "assert(args.dropout == 0 if args.model.startswith(\"convnext\") else True)\n",
    "\n",
    "import re\n",
    "\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9937f1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state_dict', 'optimizer', 'loss_scaler', 'epoch', 'manager'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b86338cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# cnt = 0\n",
    "# dl = DataLoader(dataset_train, batch_size=BATCHS, shuffle=True)\n",
    "# # data = dataset_train[cnt][0].unsqueeze(dim=0)\n",
    "# # label = dataset_train[cnt][1]\n",
    "# data,label = next(iter(dl))\n",
    "# print(data.shape)\n",
    "# print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e7e6057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5717416\n",
      "Number of zeros: 2654208\n",
      "Percentage of zeros: 46.42%\n"
     ]
    }
   ],
   "source": [
    "# compute the number of zeros in the model / total number of parameters\n",
    "total_params = 0\n",
    "zeros = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    zeros += (param.data == 0).sum().item()\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of zeros: {zeros}\")\n",
    "print(f\"Percentage of zeros: {zeros / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f529fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c34d803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "# load JPEG image /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = Image.open(\"/rds/general/user/mm6322/home/imagenet/val/n12620546/ILSVRC2012_val_00011901.JPEG\")\n",
    "\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3d55646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# result = model(data.view(data.shape[0],-1))\n",
    "print(\"data shape:\",data.shape)\n",
    "result = model(data)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7128164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = data.detach().clone()\n",
    "print(\"x.shape:\",x.shape)\n",
    "\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(    \n",
    "    model,               # model being run\n",
    "    x,                   # model input (or a tuple for multiple inputs)\n",
    "    args.prefix_dir + \"network_complete.onnx\",            # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=15,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                    'output': {0:'batch_size'},\n",
    "    },         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc81f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx:\t CONV \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.9203) \t max: tensor(13.6817)\n",
      "layer_idx:\t 0 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.0635) \t max: tensor(15.4783)\n",
      "layer_idx:\t 0 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.4229) \t max: tensor(17.4164)\n",
      "layer_idx:\t 1 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.8803) \t max: tensor(10.3352)\n",
      "layer_idx:\t 1 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.4997) \t max: tensor(9.3524)\n",
      "layer_idx:\t 2 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.2025) \t max: tensor(17.3773)\n",
      "layer_idx:\t 2 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.8159) \t max: tensor(16.2393)\n",
      "layer_idx:\t 3 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.4613) \t max: tensor(15.7312)\n",
      "layer_idx:\t 3 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.2997) \t max: tensor(14.8284)\n",
      "layer_idx:\t 4 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.5867) \t max: tensor(15.6392)\n",
      "layer_idx:\t 4 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.6180) \t max: tensor(14.4870)\n",
      "layer_idx:\t 5 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.0484) \t max: tensor(15.2520)\n",
      "layer_idx:\t 5 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.7885) \t max: tensor(14.6185)\n",
      "layer_idx:\t 6 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.8097) \t max: tensor(14.7460)\n",
      "layer_idx:\t 6 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.1806) \t max: tensor(14.0622)\n",
      "layer_idx:\t 7 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-7.9359) \t max: tensor(14.7281)\n",
      "layer_idx:\t 7 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.2735) \t max: tensor(18.5691)\n",
      "layer_idx:\t 8 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-11.1971) \t max: tensor(20.7566)\n",
      "layer_idx:\t 8 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.5824) \t max: tensor(25.8723)\n",
      "layer_idx:\t 9 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-14.3955) \t max: tensor(26.9132)\n",
      "layer_idx:\t 9 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.2325) \t max: tensor(27.7766)\n",
      "layer_idx:\t 10 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5998) \t max: tensor(29.1508)\n",
      "layer_idx:\t 10 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5393) \t max: tensor(30.7623)\n",
      "layer_idx:\t 11 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-16.1201) \t max: tensor(33.1553)\n",
      "layer_idx:\t 11 \t half: False \t inter_out.shape: torch.Size([1, 1000]) \t min: tensor(-3.9342) \t max: tensor(8.3160)\n"
     ]
    }
   ],
   "source": [
    "inter_out = model.split_convs(data)\n",
    "print(\"layer_idx:\\t\",\"CONV\",\"\\t half:\",str(False),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_out = model.split_n(data,layer_idx,half)\n",
    "        print(\"layer_idx:\\t\",layer_idx,\"\\t half:\",str(half),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7357391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5df87c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for all layers\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),args.prefix_dir, \"input_convs.json\")\n",
    "data = dict(input_data = [((x).detach().numpy()).reshape([-1]).tolist()])\n",
    "json.dump( data, open(data_path, 'w' ))\n",
    "\n",
    "for i in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_i = model.split_n(x,i,half=half)\n",
    "        data_path = os.path.join(os.getcwd(),args.prefix_dir, f\"input_{i}_{str(half)}.json\")\n",
    "        data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "        json.dump( data, open(data_path, 'w' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90bb8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# extract all onnx files of layers\n",
    "\n",
    "input_path = args.prefix_dir + \"network_complete.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6857199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function\n",
    "def activation_hook(layer_name, activation_stats):\n",
    "    def hook(module, input, output):\n",
    "        input_tensor = input[0]\n",
    "        activation_stats[layer_name] = {\n",
    "            # l1 norm\n",
    "            'norm': input_tensor.norm(),\n",
    "            'max': input_tensor.max(),\n",
    "            'min': input_tensor.min(),\n",
    "            'shape': input_tensor.shape\n",
    "    }\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0384aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # teleporing the mlp model\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "# # IMPORTANT: input_x_T/F is acutally the output of that block (output of the intermediate or the full block)\n",
    "\n",
    "# # find the input data for the mlp model\n",
    "# input_split0_False = json.load(open(args.prefix_dir + \"input_0_True.json\"))[\"input_data\"][0]\n",
    "# norm_split0_False = model.blocks[0].norm2\n",
    "# input_mlp_0 = norm_split0_False(torch.tensor(input_split0_False).view(1,197,192))\n",
    "# out_mlp_0 = original_mlp_0(input_mlp_0)\n",
    "# out_mlp_0 = model.blocks[0].drop_path(out_mlp_0)\n",
    "# out_split0_false = out_mlp_0 + torch.tensor(input_split0_False).view(1,197,192)\n",
    "\n",
    "# # True output of the current block == input of the next block\n",
    "# out_split0_false_orginal = json.load(open(args.prefix_dir + \"input_0_False.json\"))[\"input_data\"][0]\n",
    "\n",
    "# print(input_mlp_0.norm(),input_mlp_0.max(),input_mlp_0.min(),input_mlp_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "329141b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor(input_split0_False).view(1,197,192)\n",
    "# print(t.norm(),t.max(),t.min(),t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "799d8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register hooks to the layers before all activation functions\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "# activation_stats = {}\n",
    "# for i, layer in enumerate(original_mlp_0.children()):\n",
    "#     if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#         layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats))\n",
    "\n",
    "# # Run the mlp model\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_block0 = model.split_convs(torch.tensor(input_convs).view(1,3,224,224))\n",
    "# input_block0 = input_block0.view(1,197,192)\n",
    "# original_block0_false_pred = model.blocks[0](input_block0)\n",
    "# print(\"activation_stats:\",activation_stats)\n",
    "\n",
    "# original_loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# print(f\"Original loss: {original_loss}, Original prediction error: {0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93768c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = json.load(open(args.prefix_dir + \"input_0_True.json\"))['input_data'][0]\n",
    "# d = torch.tensor(d).view(1,197,192)\n",
    "# np.save(args.prefix_dir + \"input_block0_false.npy\", d.numpy())\n",
    "\n",
    "# # original_pred = model.blocks[0].mlp(model.blocks[0].norm2(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "955895b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralteleportation.models.model_zoo.mlpcob import MLPCOB\n",
    "from neuralteleportation.neuralteleportationmodel import NeuralTeleportationModel\n",
    "from neuralteleportation.layers.neuralteleportation import COBForwardMixin, FlattenCOB\n",
    "from neuralteleportation.layers.neuron import LinearCOB\n",
    "from neuralteleportation.layers.activation import ReLUCOB, SigmoidCOB, GELUCOB, LeakyReLUCOB\n",
    "from neuralteleportation.layers.dropout import DropoutCOB\n",
    "from neuralteleportation.layers.neuron import LayerNormCOB\n",
    "from neuralteleportation.layers.merge import Add\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # self.add = Add()\n",
    "        self.norm2 = LayerNormCOB(192)\n",
    "        self.fc1 = LinearCOB(192, 768, bias=True)\n",
    "        self.act = GELUCOB()\n",
    "        self.fc2 = LinearCOB(768, 192, bias=True)\n",
    "        # self.drop1 = DropoutCOB(0.1)\n",
    "        # self.drop2 = DropoutCOB(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.norm2(x)\n",
    "        x2 = self.fc1(x1)\n",
    "        x3 = self.act(x2)\n",
    "        x4 = self.fc2(x3)\n",
    "        # x4 = self.add(x, x3)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c65859c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teleported_model = LinearNet()\n",
    "# teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78d5eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ln_weights(LN, model, block_idx):\n",
    "    original_mlp = model.blocks[block_idx].mlp\n",
    "    original_norm2 = model.blocks[block_idx].norm2\n",
    "\n",
    "    combined_dict = {}\n",
    "    combined_dict.update(original_mlp.state_dict())\n",
    "    for k,v in original_norm2.state_dict().items():\n",
    "        combined_dict[\"norm2.\" + k] = v\n",
    "    LN.network.load_state_dict(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1049193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variable to store the best loss found\n",
    "global best_loss\n",
    "global cor_best_pred_error\n",
    "global cor_best_range\n",
    "# initialize best_loss\n",
    "best_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "982a3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def f_ack(cob,input_data=None, original_pred=None, layer_idx=None ,original_loss=None, tm=None):    \n",
    "    \n",
    "    # # Set up model with the new COB\n",
    "    # teleported_model = LinearNet()\n",
    "    # teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    # deep copy tm to teleported_model\n",
    "    # teleported_model = copy.deepcopy(tm)\n",
    "    teleported_model = tm\n",
    "    # Load the weights of the original model\n",
    "    # load_ln_weights(teleported_model, model, layer_idx)\n",
    "    # teleport to the ones tensor\n",
    "    # teleported_model = teleported_model.teleport(torch.ones_like(cob), reset_teleportation=True)\n",
    "\n",
    "    # apply the COB\n",
    "    teleported_model = teleported_model.teleport(cob, reset_teleportation=False)\n",
    "\n",
    "    # Reset activation stats and run a forward pass\n",
    "    activation_stats = {}\n",
    "    hook_handles = []\n",
    "    for i, layer in enumerate(teleported_model.network.children()):\n",
    "            if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "                handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "                hook_handles.append(handle)\n",
    "    teleported_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = teleported_model.network(input_data)\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "    # Calculate the range loss\n",
    "    loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "    loss /= original_loss\n",
    "    # Calculate the prediction error\n",
    "    pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "    pred_error /= np.abs(original_pred).mean()\n",
    "    total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "    if random.random() < 0.0005:\n",
    "         print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "    \n",
    "    # Undo the teleportation\n",
    "    teleported_model.undo_teleportation()\n",
    "\n",
    "    # activation_stats.clear()\n",
    "    # del teleported_model, activation_stats, pred, loss, pred_error, cob\n",
    "    return total_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "272f78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy the model\n",
    "# import copy\n",
    "# import itertools\n",
    "# import functools\n",
    "\n",
    "# layer_idx = 1\n",
    "\n",
    "# # copy the model (the new_model will be used to apply the cob to prevent the original model from being modified)\n",
    "# new_model = copy.deepcopy(model)\n",
    "\n",
    "# # Register hooks to the layers before all activation functions\n",
    "# original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "# activation_stats_idx = {}\n",
    "# for i,layer in enumerate(original_mlp_idx.children()):\n",
    "#     if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#         layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "\n",
    "# # run the mlp model to find original_loss\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "# print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "\n",
    "# # calculate the original loss\n",
    "# original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "# print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "# # Load the teleported model\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# # get initial weights and cob\n",
    "# initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "# initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "# global best_loss\n",
    "# best_loss = 1e9\n",
    "\n",
    "# # Load the input data to calculate the original_pred\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "# input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "# input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "# original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "# # Apply best COB and save model weights\n",
    "# LN = LinearNet()\n",
    "# LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "# s = initial_cob_idx.size()\n",
    "\n",
    "# # add inputs to the function\n",
    "# ackley = functools.partial(\n",
    "#     f_ack,\n",
    "#     input_data=input_teleported_model,\n",
    "#     original_pred=original_pred,\n",
    "#     layer_idx=layer_idx,\n",
    "#     original_loss = original_loss_idx,\n",
    "#     tm = LN\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d068e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dic: cob\n",
    "@torch.no_grad()\n",
    "def rge(func, params_dict, sample_size, step_size, base=None):\n",
    "    if base == None:\n",
    "        base = func(params_dict[\"cob\"])\n",
    "    grads_dict = {}\n",
    "    for _ in range(sample_size):\n",
    "        perturbs_dict, perturbed_params_dict = {}, {}\n",
    "        for key, param in params_dict.items():\n",
    "            perturb = torch.randn_like(param)\n",
    "            perturb /= (torch.norm(perturb) + 1e-8)\n",
    "            perturb *= step_size\n",
    "            perturbs_dict[key] = perturb\n",
    "            perturbed_params_dict[key] = perturb + param\n",
    "        directional_derivative = (func(perturbed_params_dict[\"cob\"]) - base) / step_size\n",
    "        if len(grads_dict.keys()) == len(params_dict.keys()):\n",
    "            for key, perturb in perturbs_dict.items():\n",
    "                grads_dict[key] += perturb * directional_derivative / sample_size\n",
    "        else:\n",
    "            for key, perturb in perturbs_dict.items():\n",
    "                grads_dict[key] = perturb * directional_derivative / sample_size\n",
    "    return grads_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8979722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.pred_mul = 5\n",
    "# initial_cob_idx = torch.ones(s)\n",
    "# args.steps = int(s[0] * 10)\n",
    "# args.cob_lr = 10\n",
    "# args.zoo_sample_size = 100\n",
    "# args.zoo_step_size = 0.1\n",
    "# best_loss = 1e9\n",
    "\n",
    "# # grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, 10, 0.1)\n",
    "\n",
    "# # training the cob\n",
    "# # loop over the steps\n",
    "# for step in range(args.steps):\n",
    "#     # get the gradient of the cob\n",
    "#     grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, args.zoo_sample_size, args.zoo_step_size)\n",
    "#     # update the cob\n",
    "#     initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "#     # calculate the loss\n",
    "#     loss = ackley(initial_cob_idx)\n",
    "#     # update the best loss\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_cob = initial_cob_idx\n",
    "#         print(f\"Step: {step} \\t Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "958f1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply best_cob to find the pred_error and range_loss\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# teleported_model_idx = teleported_model_idx.teleport(best_cob, reset_teleportation=True)\n",
    "\n",
    "# # Reset activation stats and run a forward pass\n",
    "# activation_stats = {}\n",
    "# hook_handles = []\n",
    "# for i, layer in enumerate(teleported_model_idx.network.children()):\n",
    "#         if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "#             handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "#             hook_handles.append(handle)\n",
    "# teleported_model_idx.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = teleported_model_idx.network(input_teleported_model)\n",
    "# for handle in hook_handles:\n",
    "#     handle.remove()\n",
    "\n",
    "# # Calculate the range loss\n",
    "# loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# loss /= original_loss_idx\n",
    "# # Calculate the prediction error\n",
    "# pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "# pred_error /= np.abs(original_pred).mean()\n",
    "# total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "# print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "# print(f\"Total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b101ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cge(func, params_dict, mask_dict, step_size, base=None):\n",
    "    if base == None:\n",
    "        base = func(params_dict[\"cob\"])\n",
    "    grads_dict = {}\n",
    "    for key, param in params_dict.items():\n",
    "        if 'orig' in key:\n",
    "            mask_key = key.replace('orig', 'mask')\n",
    "            mask_flat = mask_dict[mask_key].flatten()\n",
    "        else:\n",
    "            mask_flat = torch.ones_like(param).flatten()\n",
    "        directional_derivative = torch.zeros_like(param)\n",
    "        directional_derivative_flat = directional_derivative.flatten()\n",
    "        for idx in mask_flat.nonzero().flatten():\n",
    "            perturbed_params_dict = copy.deepcopy(params_dict)\n",
    "            p_flat = perturbed_params_dict[key].flatten()\n",
    "            p_flat[idx] += step_size\n",
    "            directional_derivative_flat[idx] = (func(perturbed_params_dict[\"cob\"]) - base) / step_size\n",
    "        grads_dict[key] = directional_derivative.to(param.device)\n",
    "    return grads_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a51bd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.pred_mul = 10\n",
    "# args.steps = 800\n",
    "# initial_cob_idx = torch.ones(s)\n",
    "# # args.steps = int(s[0] * 10)\n",
    "# args.cob_lr = 0.1 # was 0.1\n",
    "# args.zoo_step_size = 0.001 # was 0.005 -> loss turned out to 57.2 (?)\n",
    "# #                                0.001 -> 56.2 (step 446)\n",
    "# #                                0.0001 -> 56.3 (lots of steps)\n",
    "# best_loss = 1e9\n",
    "\n",
    "\n",
    "# # grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, 10, 0.1)\n",
    "\n",
    "# # training the cob\n",
    "# # loop over the steps\n",
    "# for step in range(args.steps):\n",
    "#     # get the gradient of the cob\n",
    "#     grad_cob = cge(ackley, {\"cob\": initial_cob_idx}, None, args.zoo_step_size)\n",
    "#     # update the cob\n",
    "#     initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "#     # calculate the loss\n",
    "#     loss = ackley(initial_cob_idx)\n",
    "#     # update the best loss\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_cob = initial_cob_idx\n",
    "#         print(f\"Step: {step} \\t Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4386ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply best_cob to find the pred_error and range_loss\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# teleported_model_idx = teleported_model_idx.teleport(best_cob, reset_teleportation=True)\n",
    "\n",
    "# # Reset activation stats and run a forward pass\n",
    "# activation_stats = {}\n",
    "# hook_handles = []\n",
    "# for i, layer in enumerate(teleported_model_idx.network.children()):\n",
    "#         if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "#             handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "#             hook_handles.append(handle)\n",
    "# teleported_model_idx.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = teleported_model_idx.network(input_teleported_model)\n",
    "# for handle in hook_handles:\n",
    "#     handle.remove()\n",
    "\n",
    "# # Calculate the range loss\n",
    "# loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# loss /= original_loss_idx\n",
    "# # Calculate the prediction error\n",
    "# pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "# pred_error /= np.abs(original_pred).mean()\n",
    "# total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "# print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "# print(f\"Total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cfa6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file and write the header if it doesn't exist\n",
    "import csv\n",
    "\n",
    "csv_file_path = args.prefix_dir + 'all_settings_deepzero.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            'EXPERIMENT SETTINGS',\n",
    "            'Model_Number', 'Layer_Index',\n",
    "            # 'Activation_Loss', 'Prediction_Error',\n",
    "            'Activation_Loss',\n",
    "            'Input_Scale', 'Param_Scale', 'Scale_Rebase_Multiplier',\n",
    "            'Lookup_Range_0', 'Lookup_Range_1', 'Logrows', 'Num_Rows', 'Num_cols', 'Total_Assignments',\n",
    "            'Total_Constant_Size', 'Model_Output_Scales', \n",
    "            'Required_Range_Checks_0','Required_Range_Checks_1', 'Proof_Time_Seconds', 'Max_Memory' ,'Mean_Squared_Error', 'Mean_Abs_Percent_Error',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d930fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= START =========\n",
      "input_param_scale: 7, num_cols: 64, max_log_rows: -1, param_visibility: fixed, lookup_margin: 2\n",
      "Experiment settings: fixed/7/64/-1/-1/2 and layer_idx: 0 already exists in the csv file.\n",
      "layer_idx: 0 , \t  activation_stats: {'relu_1': {'norm': tensor(476.6286), 'max': tensor(6.2171), 'min': tensor(-6.8579), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(13.0750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n",
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_78050/346902198.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block0_cob_activation_norm_teleported.pth already exists.\n",
      "==== Model: block0_cob_activation_norm  in Layer: 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-19 00:54:46,221 execute.rs:1044 num calibration batches: 1\n",
      "INFO tract_linalg.arm64 2024-09-19 00:54:46,234 arm64.rs:212 CPU optimisation: AppleM\n",
      "INFO tract_linalg.arm64 2024-09-19 00:54:46,236 arm64.rs:294 ARMv8.2 mmm_f16 and mmv_f16 activated\n",
      "INFO tract_linalg.arm64 2024-09-19 00:54:46,237 arm64.rs:315 ARMv8.2 tanh_f16 and sigmoid_f16 activated\n",
      "INFO tract_linalg.arm64 2024-09-19 00:54:46,239 arm64.rs:328 AMX optimisation activated\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import copy\n",
    "import ezkl\n",
    "\n",
    "array_param_visibility = [\"fixed\"]\n",
    "# array_input_param_scale = [16,17,18,19,20]\n",
    "array_input_param_scale = [7,10,11,12,13,14,15]\n",
    "# array_num_cols = [64,32,16]\n",
    "array_num_cols = [64,32,16,8,4,2]\n",
    "# array_max_log_rows = [16,17,18,19,20]\n",
    "array_max_log_rows = [-1,11,12,13,14,15,16,17,18,19,20]\n",
    "array_scale_rebase = [1]\n",
    "# array_scale_rebase = [-1]\n",
    "array_lookup_margin = [2]\n",
    "\n",
    "# iterate over all the possible combinations\n",
    "combinations = list(itertools.product(array_param_visibility, array_input_param_scale, array_num_cols, array_max_log_rows, array_scale_rebase, array_lookup_margin))\n",
    "global best_loss\n",
    "# the layer_idx which the logrows are equal to 20 are not gonna be teleported\n",
    "list_of_no_teleportation = [1,3,4,5]\n",
    "\n",
    "# with no gradient pytorch\n",
    "with torch.no_grad():\n",
    "    # iterate over all the possible combinations\n",
    "    for p in combinations:\n",
    "        param_visibility, input_param_scale, num_cols, max_log_rows, scale_rebase, lookup_margin = p\n",
    "        # string experiment_settings as comma separated values\n",
    "        experiment_settings = f\"{param_visibility}/{input_param_scale}/{num_cols}/{max_log_rows}/{scale_rebase}/{lookup_margin}\"\n",
    "        print(\"========= START =========\")\n",
    "        print(f\"input_param_scale: {input_param_scale}, num_cols: {num_cols}, max_log_rows: {max_log_rows}, param_visibility: {param_visibility}, lookup_margin: {lookup_margin}\")\n",
    "\n",
    "        # copy the model\n",
    "        new_model = copy.deepcopy(model)\n",
    "        \n",
    "        # generate compression-model and setting for all vit layers\n",
    "        # for layer_idx in range(model.depth):\n",
    "        # TODO: uncomment the line above and comment the line below\n",
    "        for layer_idx in [0]:\n",
    "\n",
    "            # if layer_idx >= 4:\n",
    "            #     continue\n",
    "\n",
    "            args.pred_mul = 10\n",
    "            # args.pred_mul = 20\n",
    "            args.steps = 800\n",
    "            # args.steps = 400\n",
    "            args.cob_lr = 0.1\n",
    "            args.zoo_step_size = 0.001 \n",
    "\n",
    "            # check the experiment_settings and layer_idx exists in the csv file\n",
    "            with open(csv_file_path, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                exist_flag = False\n",
    "                for row in reader:\n",
    "                    if row[0] == experiment_settings and int(row[2]) == layer_idx:\n",
    "                        print(f\"Experiment settings: {experiment_settings} and layer_idx: {layer_idx} already exists in the csv file.\")\n",
    "                        exist_flag = True\n",
    "                        break\n",
    "            # if exist_flag:\n",
    "            #     continue\n",
    "\n",
    "            # Hook for the intermediate output of the block\n",
    "            original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "            activation_stats_idx = {}\n",
    "            for i,layer in enumerate(original_mlp_idx.children()):\n",
    "                if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "                    layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "            # run the mlp model to find original_loss\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "            \n",
    "            # print activation stats\n",
    "            print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "            original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "            print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "            # # Load the teleported model\n",
    "            # teleported_model_idx = LinearNet()\n",
    "            # teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "            # load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "\n",
    "            # # get initial weights and cob\n",
    "            # initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "            # initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "            # track best loss\n",
    "            best_loss = 1e9\n",
    "\n",
    "            # define input_teleported_model (used in ng_loss_function)\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "            input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "            # save npy file using in python checking script\n",
    "            np.save(args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\", input_teleported_model.detach().numpy())\n",
    "            # define original_pred (used in ng_loss_function)\n",
    "            input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "            np.save(args.prefix_dir + f\"input_org_{layer_idx}.npy\", input_org.detach().numpy())\n",
    "            original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "            # Apply best COB and save model weights\n",
    "            LN = LinearNet()\n",
    "            LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "            load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "            if layer_idx in list_of_no_teleportation:\n",
    "                print(\"====== NO OPTIMIZATION SINCE NO TELEPORTATION =====\")\n",
    "                best_loss = torch.tensor(best_loss).detach().cpu()\n",
    "                LN = LN.teleport(torch.ones_like(torch.ones(960)), reset_teleportation=True)\n",
    "                torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "            # check whether the teleportation .pth already exists\n",
    "            elif os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "                print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "                LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "                best_loss = torch.tensor(best_loss).detach().cpu()\n",
    "            else:\n",
    "                # zero-order derivative optimization\n",
    "                # if args.teleport_gaussian and layer_idx != 2 and layer_idx != 3:\n",
    "                initial_cob_idx = torch.ones(960)\n",
    "                # add inputs to the function\n",
    "                ackley = functools.partial(\n",
    "                    f_ack,\n",
    "                    input_data=input_teleported_model,\n",
    "                    original_pred=original_pred,\n",
    "                    layer_idx=layer_idx,\n",
    "                    original_loss = original_loss_idx,\\\n",
    "                    tm = LN\n",
    "                )\n",
    "\n",
    "                # training to find best_cob\n",
    "                best_cob = None\n",
    "                for step in range(args.steps):\n",
    "                    # get the gradient of the cob\n",
    "                    grad_cob = cge(ackley, {\"cob\": initial_cob_idx}, None, args.zoo_step_size)\n",
    "                    # update the cob\n",
    "                    initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "                    # calculate the loss\n",
    "                    loss = ackley(initial_cob_idx)\n",
    "                    # update the best loss\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_cob = initial_cob_idx\n",
    "                        print(f\"Step: {step} \\t Loss: {loss}\")\n",
    "\n",
    "                print(\"BEST LOSS:\",best_loss)\n",
    "\n",
    "                # Apply best COB and save model weights\n",
    "                if layer_idx in list_of_no_teleportation:\n",
    "                    print(\"====== NO TELEPORTATION =====\")\n",
    "                else:\n",
    "                    LN = LN.teleport(best_cob, reset_teleportation=True)\n",
    "                # save the .pth of the teleported model\n",
    "                torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "\n",
    "            # Apply the teleportation to the new_model (Using for computing the next layer inputs)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k: v for k, v in sd.items() if 'norm2' not in k}\n",
    "            new_model.blocks[layer_idx].mlp.load_state_dict(sd)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k.replace('norm2.',''): v for k, v in sd.items() if 'norm2' in k}\n",
    "            new_model.blocks[layer_idx].norm2.load_state_dict(sd)\n",
    "\n",
    "            # Export the optimized model to ONNX\n",
    "            torch.onnx.export(LN.network, input_teleported_model, args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.onnx', verbose=False, export_params=True, opset_version=15, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "\n",
    "            # check the validation of the teleportation\n",
    "            \n",
    "            # 1.extract onnx corrosponding to the teleported model (in original onnx)\n",
    "            input_path = args.prefix_dir + f\"network_split_{layer_idx}_False.onnx\"\n",
    "            output_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "            input_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "            output_names = [f\"/blocks.{layer_idx}/mlp/fc2/Add_output_0\"]\n",
    "            onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "            \n",
    "            # # 2. run the python code\n",
    "            # print(\"===== RUNNING PYTHON CODE =====\")\n",
    "            # a = args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\"\n",
    "            # b = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "            # !python onnx_inference.py \\\n",
    "            #     --input {a} --model1 {output_path} --model2 {b}\n",
    "            # print(\"===== PYTHON CODE FINISHED =====\")\n",
    "            # time.sleep(5)\n",
    "\n",
    "            # ezkl to find the resources possible reduction\n",
    "            run_args = ezkl.PyRunArgs()\n",
    "            run_args.input_visibility = \"public\"\n",
    "            run_args.param_visibility = param_visibility\n",
    "            run_args.output_visibility = \"public\"\n",
    "            run_args.input_scale = input_param_scale\n",
    "            run_args.param_scale = input_param_scale\n",
    "\n",
    "            # run_args.logrows = args.log_rows\n",
    "            run_args.num_inner_cols = num_cols\n",
    "            run_args.variables = [('batch_size', BATCHS)]\n",
    "            if max_log_rows != (-1):\n",
    "                run_args.logrows = max_log_rows\n",
    "            if scale_rebase != (-1):\n",
    "                run_args.scale_rebase_multiplier = scale_rebase\n",
    "\n",
    "            # get SRS\n",
    "            # ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)\n",
    "\n",
    "            name0 = f'block{layer_idx}_cob_activation_norm'\n",
    "            name1 = f'block{layer_idx}_cob_activation_norm_teleported'\n",
    "            rng = [name0,name1]\n",
    "\n",
    "            for m in rng:\n",
    "                print(\"==== Model:\",m, \" in Layer:\",layer_idx,\"====\")\n",
    "\n",
    "                if m == name0:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "                    x = input_org.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "                    x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "\n",
    "                # Generate the calibration data\n",
    "                data = dict(input_data=[x])\n",
    "                # cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "                cal_path = args.prefix_dir + f'cal_data_{m}.json'\n",
    "                json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "\n",
    "                settings_path = args.prefix_dir + f'settings_{m}_{layer_idx}.json'\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "\n",
    "                try:\n",
    "                    if scale_rebase != (-1) and max_log_rows != (-1):\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin, scale_rebase_multiplier=[scale_rebase],max_logrows=max_log_rows)\n",
    "                    elif max_log_rows != (-1):\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin, max_logrows=max_log_rows)\n",
    "                    elif scale_rebase != (-1):\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin, scale_rebase_multiplier=[scale_rebase])\n",
    "                    else:\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin)\n",
    "                except:\n",
    "                    print(\"ERROR in calibration: \",m,layer_idx)\n",
    "                    res = False\n",
    "                    # write in the csv file\n",
    "                    with open(csv_file_path, mode='a', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        l = original_loss_idx if m == name0 else best_loss\n",
    "                        writer.writerow([\n",
    "                            experiment_settings,\n",
    "                            m,layer_idx,\n",
    "                            l,\n",
    "                            None, None, None,\n",
    "                            None, None, None, None, None, None,\n",
    "                            None, None, None, None, None, None, None, None\n",
    "                        ])\n",
    "                    continue\n",
    "\n",
    "                # extract the resources\n",
    "                settings = json.load(open(settings_path))\n",
    "                rg = settings.get('run_args', {})\n",
    "                input_scale = rg.get('input_scale', None)\n",
    "                param_scale = rg.get('param_scale', None)\n",
    "                output_scale = settings.get('model_output_scales', None)\n",
    "                scale_rebase_multiplier = rg.get('scale_rebase_multiplier', None)\n",
    "                lookup_range = rg.get('lookup_range', [None, None])\n",
    "                logrows = rg.get('logrows', None)\n",
    "                num_rows = settings.get('num_rows', None)\n",
    "                total_assignments = settings.get('total_assignments', None)\n",
    "                num_cols = rg.get('num_inner_cols', None)\n",
    "                total_constant_size = settings.get('total_const_size', None)\n",
    "                \n",
    "                print(logrows, num_rows, num_cols ,lookup_range, scale_rebase_multiplier, output_scale)\n",
    "                print(\"===============================\")\n",
    "\n",
    "                # activation loss is equal to the original loss if m is equal to name0 else it is the best loss\n",
    "                activation_loss = original_loss_idx if m == name0 else best_loss\n",
    "\n",
    "                # write the results to the csv file\n",
    "                with open(csv_file_path, mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([\n",
    "                        experiment_settings,\n",
    "                        m,layer_idx,\n",
    "                        activation_loss.item(),\n",
    "                        input_scale, param_scale, scale_rebase_multiplier,\n",
    "                        lookup_range[0], lookup_range[1], logrows, num_rows, num_cols, total_assignments,\n",
    "                        total_constant_size, output_scale,\n",
    "                          None, None, None, None, None, None\n",
    "                    ])\n",
    "            \n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48d49153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the teleported model onnx\n",
    "in_path = args.prefix_dir + \"block0_cob_activation_norm_teleported.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70b8ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract onnx of the teleported model\n",
    "# input_path = args.prefix_dir + \"network_split_0_False.onnx\"\n",
    "# output_path = args.prefix_dir + \"block0_cob_activation_norm.onnx\"\n",
    "# input_names = [\"/blocks.0/Add_2_output_0\"]\n",
    "# output_names = [\"/blocks.0/mlp/fc2/Add_output_0\"]\n",
    "# onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90a991e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python onnx_inference.py --input {args.prefix_dir + 'input_teleported_model.npy'} \\\n",
    "#     --model1 {args.prefix_dir + 'block0_cob_activation_norm.onnx'} \\\n",
    "#     --model2 {args.prefix_dir + 'block0_cob_activation_norm_teleported.onnx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2300b600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.input_param_scale: 7\n",
      "args.logrows: 20\n",
      "args.num_cols: 2\n",
      "args.scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "# print important args related to ezkl\n",
    "print(\"args.input_param_scale:\",args.input_param_scale)\n",
    "print(\"args.logrows:\",args.log_rows)\n",
    "print(\"args.num_cols:\",args.num_cols)\n",
    "print(\"args.scale_rebase_multiplier:\",args.scale_rebase_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8b8f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-17 05:01:45,845 execute.rs:742 SRS already exists at that path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Future pending cb=[<builtins.PyDoneCallback object at 0x30d8921f0>()]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ezkl\n",
    "\n",
    "run_args = ezkl.PyRunArgs()\n",
    "run_args.input_visibility = \"public\"\n",
    "# TODO: change that to fixed\n",
    "run_args.param_visibility = \"fixed\"\n",
    "run_args.output_visibility = \"public\"\n",
    "run_args.input_scale = args.input_param_scale\n",
    "run_args.param_scale = args.input_param_scale\n",
    "run_args.logrows = args.log_rows\n",
    "run_args.num_inner_cols = args.num_cols\n",
    "run_args.scale_rebase_multiplier = args.scale_rebase_multiplier\n",
    "run_args.variables = [('batch_size', BATCHS)]\n",
    "\n",
    "ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "001240b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ezkl version: 100.100.101\n"
     ]
    }
   ],
   "source": [
    "# print ezkl version\n",
    "print(\"ezkl version:\",ezkl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9ac162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source ~/.config/envman/PATH.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e1f7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99c9df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !RUST_LOG=trace\n",
    "\n",
    "# rng = ['block0_cob_activation_norm','block0_cob_activation_norm_teleported']\n",
    "\n",
    "# # Generate the calibration data\n",
    "# x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "# data = dict(input_data=[x])\n",
    "# cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "# json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "# for model in rng:\n",
    "\n",
    "#     print(\"==== Model:\",model, \"====\")\n",
    "\n",
    "#     model_path = args.prefix_dir + model + \".onnx\"\n",
    "#     settings_path = f'{args.prefix_dir} settings_{model}.json'\n",
    "\n",
    "#     res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "#     assert res == True\n",
    "\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier],max_logrows=args.log_rows)\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier])\n",
    "#     res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale])\n",
    "#     assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fcfc0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-17 05:01:45,959 execute.rs:640 read 134217988 bytes from file (vector of len = 134217988)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_14386/2906073395.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "# recreate and initialize the model\n",
    "model = build_model(args, pretrained=False)\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# transfer the model to the cpu\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c53a0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "\n",
    "# op - original_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d05b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tout = LN.network(input_teleported_model)\n",
    "# (original_pred - tout).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "926f9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-17 05:01:47,352 execute.rs:647 file hash: 54ef75911da76d7a6b7ea341998aaf66cb06c679c53e0a88a4fe070dd3add963\n"
     ]
    }
   ],
   "source": [
    "# save all the original predictions in original_pred_array\n",
    "original_pred_array = []\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):    \n",
    "    input_teleported_model = model.split_n(input_convs,layer_idx,half=True)\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_array.append(original_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f90ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_14386/3146665136.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 0.016365107148885727\n",
      "Layer 1 error: 6.8462086346698925e-06\n",
      "Layer 2 error: 0.024285390973091125\n",
      "Layer 3 error: 1.3144852346158586e-05\n",
      "Layer 4 error: 1.2152940144005697e-05\n",
      "Layer 5 error: 1.4087309864407871e-05\n",
      "Layer 6 error: 0.03299365192651749\n",
      "Layer 7 error: 0.03475141152739525\n",
      "Layer 8 error: 0.0362166091799736\n",
      "Layer 9 error: 0.039391372352838516\n",
      "Layer 10 error: 0.04061271250247955\n",
      "Layer 11 error: 0.037957821041345596\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (correct input (not teleported) for each layer)\n",
    "\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    input_layer = model.split_n(input_convs,layer_idx,half=True)\n",
    "    org_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_layer))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "\n",
    "    teleported_pred = teleported_model.network(input_layer)\n",
    "    error = (org_pred - teleported_pred).abs().mean()\n",
    "    error /= org_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82c28ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 0.016365107148885727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_14386/1111111726.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 error: 6.8462086346698925e-06\n",
      "Layer 2 error: 0.03295620530843735\n",
      "Layer 3 error: 0.023675469681620598\n",
      "Layer 4 error: 0.023965148255228996\n",
      "Layer 5 error: 0.027395593002438545\n",
      "Layer 6 error: 0.04228731244802475\n",
      "Layer 7 error: 0.046812836080789566\n",
      "Layer 8 error: 0.05110516771674156\n",
      "Layer 9 error: 0.056092362850904465\n",
      "Layer 10 error: 0.05844700336456299\n",
      "Layer 11 error: 0.05109034851193428\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (teleported input for each layer)\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(new_model.depth):\n",
    "    input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "    input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "    teleported_pred = teleported_model.network(input_teleported_model)\n",
    "    \n",
    "    error = (original_pred - teleported_pred).abs().mean()\n",
    "    error /= original_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25440b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block0_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 0: 0.01636327989399433\n",
      "block1_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_14386/3984726627.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block2_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 2: 0.032954853028059006\n",
      "block3_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 3: 0.023675505071878433\n",
      "block4_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 4: 0.03997843340039253\n",
      "block5_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 5: 0.04379794001579285\n",
      "block6_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 6: 0.0518336296081543\n",
      "block7_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 7: 0.05893596634268761\n",
      "block8_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 8: 0.06905277818441391\n",
      "block9_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 9: 0.07727934420108795\n",
      "block10_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 10: 0.08275893330574036\n",
      "block11_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 11: 0.0732683539390564\n"
     ]
    }
   ],
   "source": [
    "teleportation_applied_layers = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "# the copy_model is used to consider the previous layer teleportation to the input of the current layer (only layers in the teleportation_applied_layers are gonna be teleported)\n",
    "copy_model = copy.deepcopy(model)\n",
    "copy_model.eval()\n",
    "for param in copy_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for layer_idx in teleportation_applied_layers:\n",
    "    # compute the original_pred_idx - IMPORTANT: PRVIOUS TELEPORTED LAYERS EFFECT THE INPUT OF THE CURRENT LAYER => pred_error WOULD BE DIFFERENT\n",
    "    input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "    input_convs = torch.tensor(input_convs).view(BATCHS,3,224,224)\n",
    "    input_teleported_model = copy_model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    # original_pred_idx = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_idx = original_pred_array[layer_idx]\n",
    "\n",
    "    LN = LinearNet()\n",
    "    LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "    # check whether the teleportation .pth already exists\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "        LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        # raise error\n",
    "        raise Exception(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exists.\")\n",
    "\n",
    "    # subsitude the teleported_model weights in the original model\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k: v for k, v in state_dic.items() if 'norm2' not in k}\n",
    "    copy_model.blocks[layer_idx].mlp.load_state_dict(state_dic)\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k.replace('norm2.',''): v for k, v in state_dic.items() if 'norm2' in k}\n",
    "    copy_model.blocks[layer_idx].norm2.load_state_dict(state_dic)\n",
    "\n",
    "    # compute the prediction error\n",
    "    new_pred = copy_model.blocks[layer_idx].mlp(copy_model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    diff = (original_pred_idx - new_pred).abs().mean()\n",
    "    print(f\"Prediction error in layer {layer_idx}: {diff/original_pred_idx.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1de950bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the updated model .pth\n",
    "torch.save(copy_model.state_dict(), args.resume.replace(\".pth\",\"_teleported.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "efdc76c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# export the updated model to onnx\n",
    "torch.onnx.export(copy_model, x,\\\n",
    "                args.prefix_dir + 'complete_model_teleported.onnx', \\\n",
    "                verbose=False, export_params=True, opset_version=15, do_constant_folding=True, \\\n",
    "                input_names=['input'], output_names=['output'], \\\n",
    "                dynamic_axes={'input' : {0 : 'batch_size'},'output': {0:'batch_size'},},\n",
    ")\n",
    "\n",
    "# define the shape\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc44d1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "# export the splits of the model\n",
    "\n",
    "input_path = args.prefix_dir + \"complete_model_teleported.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs_teleported.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}_teleported.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ac388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "\n",
    "# TODO:\n",
    "# define the data_loader_test an iterator which only returns the img\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "# contain data and label in the dataset\n",
    "# label is shape 1,1000 which is one hot encoded\n",
    "label = torch.tensor(1).unsqueeze(0) \n",
    "data_set_test = torch.utils.data.TensorDataset(data,label)\n",
    "data_loader_test = torch.utils.data.DataLoader(data_set_test, batch_size=BATCHS, shuffle=True)\n",
    "# data_loader_test = torch.utils.data.DataLoader(da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93d7b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import json\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_ezkl_output(witness_file, settings_file):\n",
    "    # convert the quantized ezkl output to float value\n",
    "    witness_output = json.load(open(witness_file))\n",
    "    outputs = witness_output['outputs']\n",
    "    with open(settings_file) as f:\n",
    "        settings = json.load(f)\n",
    "    ezkl_outputs = [[ezkl.felt_to_float(\n",
    "        outputs[i][j], settings['model_output_scales'][i]) for j in range(len(outputs[i]))] for i in range(len(outputs))]\n",
    "    return ezkl_outputs\n",
    "\n",
    "\n",
    "def get_onnx_output(model_file, input_file):\n",
    "    # generate the ML model output from the ONNX file\n",
    "    onnx_model = onnx.load(model_file)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    with open(input_file) as f:\n",
    "        inputs = json.load(f)\n",
    "    # reshape the input to the model\n",
    "    num_inputs = len(onnx_model.graph.input)\n",
    "\n",
    "    onnx_input = dict()\n",
    "    for i in range(num_inputs):\n",
    "        input_node = onnx_model.graph.input[i]\n",
    "        dims = []\n",
    "        elem_type = input_node.type.tensor_type.elem_type\n",
    "#         print(\"elem_type: \", elem_type)\n",
    "        for dim in input_node.type.tensor_type.shape.dim:\n",
    "            if dim.dim_value == 0:\n",
    "                dims.append(1)\n",
    "            else:\n",
    "                dims.append(dim.dim_value)\n",
    "        if elem_type == 6:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int32).reshape(dims)\n",
    "        elif elem_type == 7:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int64).reshape(dims)\n",
    "        elif elem_type == 9:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                bool).reshape(dims)\n",
    "        else:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.float32).reshape(dims)\n",
    "        onnx_input[input_node.name] = inputs_onnx\n",
    "    try:\n",
    "        onnx_session = onnxruntime.InferenceSession(model_file)\n",
    "        onnx_output = onnx_session.run(None, onnx_input)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "        # onnx_output = inputs['output_data']\n",
    "#     print(\"onnx \", onnx_output)\n",
    "    return onnx_output[0]\n",
    "\n",
    "\n",
    "def compare_outputs(zk_output, onnx_output):\n",
    "    # calculate percentage difference between the 2 outputs (which are lists)\n",
    "    res = []\n",
    "    contains_sublist = any(isinstance(sub, list) for sub in zk_output)\n",
    "    zip_object = zip(np.array(zk_output),\n",
    "                     np.array(onnx_output))\n",
    "    \n",
    "    num_eq_zk_onnx = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for (i, (list1_i, list2_i)) in enumerate(zip_object):\n",
    "        diff = list1_i - list2_i\n",
    "        # iterate and print the diffs  if they are greater than 0.0\n",
    "        res.append(np.linalg.norm(diff,axis=(-1)))\n",
    "        print(\"= index: \",i, \"\\t diff-norm: \",np.linalg.norm(diff,axis=(-1)),\"\\t zk_output: \",list1_i.shape, \"\\t onnx_output: \",list2_i.shape)\n",
    "        \n",
    "        if np.argmax(list1_i) == np.argmax(list2_i):\n",
    "            num_eq_zk_onnx += 1\n",
    "        num_total += 1\n",
    "    \n",
    "    print(\"Accuracy (zk_onnx): \\t\", num_eq_zk_onnx / num_total)\n",
    "    acc_zk_onnx = num_eq_zk_onnx / num_total\n",
    "    return res, acc_zk_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10db441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = img.resize((224,224))\n",
    "# data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "\n",
    "# print(\"data.shape:\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f4a205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_pp max: tensor(8.7408)\n",
      "new_pp argmax: tensor(180)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    new_pp = new_model(img_input)\n",
    "    # print maximum value of the new_pp\n",
    "    print(\"new_pp max:\",torch.max(new_pp))\n",
    "    # print index of the maximum value of the new_pp\n",
    "    print(\"new_pp argmax:\",torch.argmax(new_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17c22f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_pp max: tensor(8.7614)\n",
      "org_pp argmax: tensor(180)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    org_pp = model(img_input)\n",
    "    # print maximum value of the new_pp\n",
    "    print(\"org_pp max:\",torch.max(org_pp))\n",
    "    # print index of the maximum value of the new_pp\n",
    "    print(\"org_pp argmax:\",torch.argmax(org_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3710ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b3924e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy_pp max: tensor(8.7877)\n",
      "copy_pp argmax: tensor(180)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    copy_pp = copy_model(img_input)\n",
    "    print(\"copy_pp max:\",torch.max(copy_pp))\n",
    "    print(\"copy_pp argmax:\",torch.argmax(copy_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d24dd226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top5_org: torch.return_types.topk(\n",
      "values=tensor([[8.7614, 8.3986, 7.9848, 7.1704, 6.1396]]),\n",
      "indices=tensor([[180, 242, 243, 246, 179]]))\n",
      "top5_copy: torch.return_types.topk(\n",
      "values=tensor([[8.7877, 8.4531, 8.0164, 7.3628, 6.1197]]),\n",
      "indices=tensor([[180, 242, 243, 246, 179]]))\n"
     ]
    }
   ],
   "source": [
    "# print top-5 indexes of org_pp and copy_pp and the corrosponding values\n",
    "top5_org = torch.topk(org_pp, 5)\n",
    "top5_copy = torch.topk(copy_pp, 5)\n",
    "print(\"top5_org:\",top5_org)\n",
    "print(\"top5_copy:\",top5_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "856322aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args.input_scale = 16\n",
    "run_args.param_scale = 16\n",
    "run_args.num_inner_cols = 64\n",
    "run_args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a6bd224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t image.shape: torch.Size([1, 3, 224, 224])\n",
      "=====\n",
      "= index:  0 \t diff-norm:  3.7335980104036293 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label): \t 0.0\n",
      "Accuracy (onnx_label): \t 0.0\n",
      "=====\n",
      "\n",
      "\n",
      "= index:  0 \t diff-norm:  3.7335980104036293 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label) 0.0\n",
      "mean norm diff:  3.7335980104036293\n",
      "max norm diff:  3.7335980104036293\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ezkl_outputs = np.empty((0,1000))\n",
    "onnx_outputs = np.empty((0,1000))\n",
    "labels = np.array([])\n",
    "\n",
    "# Open log file for writing\n",
    "log_file_path = os.path.join(args.prefix_dir, \"log\", \"output_log.txt\")\n",
    "os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "\n",
    "    # Generate dataNum_input_convs.json  \n",
    "    for index, (image, label) in enumerate(data_loader_test):\n",
    "        # image = data\n",
    "        # label = torch.tensor(1).unsqueeze(0)\n",
    "        # index = 0\n",
    "        \n",
    "        print(\"index:\",index,\"\\t image.shape:\",image.shape)\n",
    "        log_file.write(f\"index: {index}\\timage.shape: {image.shape}\\n\")\n",
    "        log_file.flush()\n",
    "        \n",
    "        os.makedirs(args.prefix_dir + \"ezkl_inputs/\"+str(index),exist_ok=True)\n",
    "\n",
    "        # remove batch dimension\n",
    "        output = model(image)\n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        pre_witness_path = None\n",
    "\n",
    "        # computing witness (last witness is important)\n",
    "        for i in [\"convs\"] + [t for t in range(model.depth)]:\n",
    "            for half in [\"True\",\"False\"]:\n",
    "\n",
    "                if i==\"convs\" and half==\"False\":\n",
    "                    continue\n",
    "\n",
    "                # Define paths\n",
    "                if i == \"convs\":\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}.json\"\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_{half}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}_{half}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}_{half}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}_{half}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}_{half}.json\"\n",
    "\n",
    "                # Generating input data\n",
    "                if i == \"convs\":\n",
    "                    data = dict(input_data = [((image).detach().numpy()).reshape([-1]).tolist()])\n",
    "                else:\n",
    "                    inter_i = model.split_n(image,i,half=half)\n",
    "                    data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "                json.dump(data, open(data_path, 'w' ))\n",
    "\n",
    "\n",
    "                # Swapping (output pre_witness -> cur_input of data_path)\n",
    "                if i != \"convs\":\n",
    "                    with open(pre_witness_path, 'r') as prev_witness_file:\n",
    "                        prev_witness_data = json.load(prev_witness_file)\n",
    "                        outputs = prev_witness_data['outputs']\n",
    "                        tmp = {\"input_data\": outputs}\n",
    "                        with open(data_path, 'w') as data_file:\n",
    "                            json.dump(tmp, data_file) \n",
    "\n",
    "                # Generate setting\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "                \n",
    "\n",
    "                # Calibrating setting\n",
    "                # res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\", \\\n",
    "                #                                   scales=[run_args.input_scale],max_logrows=run_args.logrows, scale_rebase_multiplier=[1],lookup_safety_margin=1)\n",
    "                \n",
    "                # Compile circuit\n",
    "                res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "                assert res == True\n",
    "\n",
    "                # Generating witness\n",
    "                res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "                assert os.path.isfile(witness_path)\n",
    "\n",
    "                # Update input_scale\n",
    "                settings = json.load(open(settings_path, 'r'))\n",
    "                run_args.input_scale = settings[\"model_output_scales\"][0]\n",
    "\n",
    "                # Update pre_witness_path\n",
    "                pre_witness_path = witness_path\n",
    "\n",
    "    # check accuracy\n",
    "    model_file = args.prefix_dir + \"network_complete.onnx\" \n",
    "    input_file = args.prefix_dir + f\"ezkl_inputs/{index}/input_convs.json\"\n",
    "    witness_file = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{model.depth-1}_False.json\"\n",
    "    settings_file = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{model.depth-1}_False.json\"\n",
    "\n",
    "    # get the ezkl output\n",
    "    ezkl_output = get_ezkl_output(witness_file, settings_file)\n",
    "    ezkl_output = np.array(ezkl_output).reshape(BATCHS,1000)\n",
    "    # get the onnx output\n",
    "    onnx_output = get_onnx_output(model_file, input_file)\n",
    "\n",
    "    ezkl_outputs = np.concatenate((ezkl_outputs,ezkl_output),axis=0)\n",
    "    onnx_outputs = np.concatenate((onnx_outputs,onnx_output),axis=0)\n",
    "    labels = np.concatenate((labels,label),axis=0)\n",
    "\n",
    "    print(\"=====\")\n",
    "    log_file.write(\"=====\\n\")\n",
    "    _,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "    \n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label): \\t\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label): \\t {acc_zk_label}\\n\")\n",
    "    \n",
    "    acc_onnx_label = np.sum( (labels) == np.argmax(onnx_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (onnx_label): \\t\", acc_onnx_label)\n",
    "    log_file.write(f\"Accuracy (onnx_label): \\t {acc_onnx_label}\\n\")\n",
    "    \n",
    "    print(\"=====\\n\\n\")\n",
    "    log_file.write(\"=====\\n\\n\")\n",
    "    log_file.flush()\n",
    "    \n",
    "    \n",
    "    # compare the outputs\n",
    "    percentage_difference,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "\n",
    "    # compare zk_output and truth_label\n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label)\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label) {acc_zk_label}\\n\")\n",
    "\n",
    "    # print the percentage difference\n",
    "    mean_percentage_difference = np.mean(np.abs(percentage_difference))\n",
    "    max_percentage_difference = np.max(np.abs(percentage_difference))\n",
    "    print(\"mean norm diff: \", mean_percentage_difference)\n",
    "    print(\"max norm diff: \", max_percentage_difference)\n",
    "    log_file.write(f\"mean norm diff: {mean_percentage_difference}\\n\")\n",
    "    log_file.write(f\"max norm diff: {max_percentage_difference}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5389361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac8208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralteleportation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
