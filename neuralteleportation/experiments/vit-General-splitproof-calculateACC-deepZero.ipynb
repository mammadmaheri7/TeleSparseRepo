{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# def get_cpu_load():\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = psutil.cpu_percent(interval=1, percpu=True)\n",
    "#     return cpu_loads\n",
    "\n",
    "# def select_k_cpus_with_lowest_load(k):\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = get_cpu_load()\n",
    "    \n",
    "#     # Create a list of tuples (core_id, load)\n",
    "#     cpu_load_tuples = list(enumerate(cpu_loads))\n",
    "    \n",
    "#     # Sort the list based on load\n",
    "#     sorted_cpu_loads = sorted(cpu_load_tuples, key=lambda x: x[1])\n",
    "    \n",
    "#     # Get the IDs of the K cores with lowest load\n",
    "#     selected_cpus = [core[0] for core in sorted_cpu_loads[:k]]\n",
    "    \n",
    "#     return selected_cpus\n",
    "\n",
    "# # Example usage\n",
    "# k = 8  # Number of CPUs with lowest load\n",
    "# selected_cpus = select_k_cpus_with_lowest_load(k)\n",
    "# print(f\"Selected {k} CPUs with lowest load:\", selected_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88b3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "\n",
    "# def set_cpu_affinity(pid, cpu_list):\n",
    "#     try:\n",
    "#         p = psutil.Process(pid)\n",
    "#         p.cpu_affinity(cpu_list)\n",
    "#         print(f\"CPU affinity for process {pid} set to: {cpu_list}\")\n",
    "#     except psutil.NoSuchProcess:\n",
    "#         print(f\"Process with PID {pid} does not exist.\")\n",
    "#     except psutil.AccessDenied:\n",
    "#         print(\"Permission denied. You may need sudo privileges to set CPU affinity.\")\n",
    "\n",
    "# # Get the process ID of the current process\n",
    "# pid = os.getpid()\n",
    "\n",
    "# # Set the CPU affinity to only CPU core 0\n",
    "# cpu_list = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "# set_cpu_affinity(pid, cpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as Fhtop\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "# check if notebook is in colab\n",
    "try:\n",
    "    # install ezkl\n",
    "    import google.colab\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
    "\n",
    "# rely on local installation of ezkl if the notebook is not in colab\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# uncomment for more descriptive logging \n",
    "import logging\n",
    "FORMAT = '%(levelname)s %(name)s %(asctime)-15s %(filename)s:%(lineno)d %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EZKL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import argparse\n",
    "\n",
    "def get_args_parser(initial_args=None):\n",
    "    parser = argparse.ArgumentParser('ConvNeXt training and evaluation script for image classification', add_help=False)\n",
    "#     parser.add_argument('--batch_size', default=256, type=int,\n",
    "#                         help='Per GPU batch size')\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--update_freq', default=1, type=int,\n",
    "                        help='gradient accumulation steps')\n",
    "\n",
    "    # Model parameters\n",
    "#     parser.add_argument('--model', default='convnext_tiny', type=str, metavar='MODEL',\n",
    "#                         help='Name of model to train')\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='image input size')\n",
    "    parser.add_argument('--layer_scale_init_value', default=1e-6, type=float,\n",
    "                        help=\"Layer scale initial values\")\n",
    "    \n",
    "    ########################## settings specific to this project ##########################\n",
    "    \n",
    "    # dropout and stochastic depth drop rate; set at most one to non-zero\n",
    "    parser.add_argument('--dropout', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    parser.add_argument('--drop_path', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    \n",
    "    # early / late dropout and stochastic depth settings\n",
    "    parser.add_argument('--drop_mode', type=str, default='standard', choices=['standard', 'early', 'late'], help='drop mode')\n",
    "    parser.add_argument('--drop_schedule', type=str, default='constant', choices=['constant', 'linear'], \n",
    "                        help='drop schedule for early dropout / s.d. only')\n",
    "    parser.add_argument('--cutoff_epoch', type=int, default=0, \n",
    "                        help='if drop_mode is early / late, this is the epoch where dropout ends / starts')\n",
    "    \n",
    "    ####################################################################################### \n",
    "    \n",
    "    # EMA related parameters\n",
    "    parser.add_argument('--model_ema', type=str2bool, default=False)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', type=str2bool, default=False, help='')\n",
    "    parser.add_argument('--model_ema_eval', type=str2bool, default=False, help='Using ema to eval during training.')\n",
    "\n",
    "    # Optimization parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "        weight decay. We use a cosine schedule for WD and using a larger decay by\n",
    "        the end of training improves performance for ViTs.\"\"\")\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=4e-3, metavar='LR',\n",
    "                        help='learning rate (default: 4e-3), with total batch size 4096')\n",
    "    parser.add_argument('--layer_decay', type=float, default=1.0)\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-6)')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=50, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--crop_pct', type=float, default=None)\n",
    "\n",
    "    # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', type=str2bool, default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "    parser.add_argument('--finetune', default='',\n",
    "                        help='finetune from checkpoint')\n",
    "    parser.add_argument('--head_init_scale', default=1.0, type=float,\n",
    "                        help='classifier head initial scale, typically adjusted in fine-tuning')\n",
    "    parser.add_argument('--model_key', default='model|module', type=str,\n",
    "                        help='which key to load from saved state dict, usually model or model_ema')\n",
    "    parser.add_argument('--model_prefix', default='', type=str)\n",
    "\n",
    "    # Dataset parameters\n",
    "#     parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "#                         help='dataset path')\n",
    "    parser.add_argument('--eval_data_path', default=None, type=str,\n",
    "                        help='dataset path for evaluation')\n",
    "    parser.add_argument('--nb_classes', default=1000, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--imagenet_default_mean_and_std', type=str2bool, default=True)\n",
    "    parser.add_argument('--data_set', default='IMNET', choices=['CIFAR', 'IMNET', 'image_folder'],\n",
    "                        type=str, help='ImageNet dataset path')\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "#     parser.add_argument('--resume', default='',\n",
    "#                         help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt_freq', default=1, type=int)\n",
    "    parser.add_argument('--save_ckpt_num', default=3, type=int)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', type=str2bool, default=False,\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', type=str2bool, default=True,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--disable_eval', type=str2bool, default=False,\n",
    "                        help='Disabling evaluation during training')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', type=str2bool, default=True,\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', type=str2bool, default=False)\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--use_amp', type=str2bool, default=False, \n",
    "                        help=\"Use PyTorch's AMP (Automatic Mixed Precision) or not\")\n",
    "\n",
    "    # Weights and Biases arguments\n",
    "    parser.add_argument('--enable_wandb', type=str2bool, default=False,\n",
    "                        help=\"enable logging to Weights and Biases\")\n",
    "    parser.add_argument('--project', default='convnext', type=str,\n",
    "                        help=\"The name of the W&B project where you're sending the new run.\")\n",
    "    parser.add_argument('--wandb_ckpt', type=str2bool, default=False,\n",
    "                        help=\"Save model checkpoints as W&B Artifacts.\")\n",
    "\n",
    "    # arguments for pruning\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=4096)\n",
    "#     parser.add_argument(\"--sparsity\", type=float, default=0.)\n",
    "    parser.add_argument(\"--prune_metric\", type=str, choices=[\"magnitude\", \"wanda\"])\n",
    "    parser.add_argument(\"--prune_granularity\", type=str)\n",
    "    parser.add_argument(\"--blocksize\", type=int, default=1)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"\n",
    "    Converts string to bool type; enables command line \n",
    "    arguments in the format of '--arg1 true --arg2 false'\n",
    "    \"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def get_default_args():\n",
    "    parser = get_args_parser()\n",
    "    default_args = {}\n",
    "    for action in parser._actions:\n",
    "        # Check if action is an argument\n",
    "        if not action.option_strings:\n",
    "            continue\n",
    "        # Use the destination as the key and the default value as the value\n",
    "        default_args[action.dest] = action.default\n",
    "    return DefaultArgs(**default_args)\n",
    "\n",
    "# Example usage:\n",
    "default_args = get_default_args()\n",
    "args = default_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== IMPORTANT: INPUT FLAGS OF PYTHON CODE HERE =====\n",
    "\n",
    "# args.model = \"vit_tiny\"\n",
    "# args.data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "\n",
    "# args.resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# #args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "# # args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/model_weights/deit/deit_tiny_patch16_224-a1311bcf.pth\"\n",
    "\n",
    "\n",
    "# args.sparsity = 0.5\n",
    "# args.batch_size = 32\n",
    "\n",
    "# args.pruning_method = \"CAP\" # DENSE,CAP,WANDA\n",
    "\n",
    "# # args.prune_metric = \"wanda\"\n",
    "# # args.prune_granularity = \"row\"\n",
    "# # pruned_model_dir = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "\n",
    "# # Prefix directory\n",
    "# args.prefix_dir = \"sparse-cap-acc/\" #sparse-cap-acc, dense-acc, sparse-wanda-acc\n",
    "# os.makedirs(args.prefix_dir, exist_ok=True)\n",
    "\n",
    "# # EZKL HPs\n",
    "# args.input_param_scale = 7\n",
    "# args.log_rows = 20\n",
    "# args.num_cols = 4\n",
    "# args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- IN_JUPYTER: True ---- \n",
      "Model: vit_tiny\n",
      "Data Path: /rds/general/user/mm6322/home/imagenet\n",
      "Resume Path: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "Sparsity: 0.5\n",
      "Batch Size: 1\n",
      "Pruning Method: CAP\n",
      "Prefix Directory: sparse-cap-acc-tmp/\n",
      "Input Parameter Scale: 7\n",
      "Log Rows: 20\n",
      "Number of Columns: 2\n",
      "Scale Rebase Multiplier: 1\n",
      "-------- \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# Check if running in Jupyter Notebook\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        IN_JUPYTER = True\n",
    "    else:\n",
    "        IN_JUPYTER = False\n",
    "except:\n",
    "    IN_JUPYTER = False\n",
    "\n",
    "# Define default values\n",
    "default_model = \"vit_tiny\"\n",
    "default_data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "default_resume = \"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# default_resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "default_sparsity = 0.5\n",
    "default_batch_size = 1\n",
    "default_pruning_method = \"CAP\"\n",
    "default_prefix_dir = \"sparse-cap-acc-tmp/\"\n",
    "default_input_param_scale = 7\n",
    "default_log_rows = 20\n",
    "default_num_cols = 2\n",
    "default_scale_rebase_multiplier = 1\n",
    "\n",
    "# Parse command-line arguments if not in Jupyter Notebook\n",
    "if not IN_JUPYTER:\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('--model', default=default_model, type=str, help='Model type')\n",
    "    parser.add_argument('--data_path', default=default_data_path, type=str, help='Data path')\n",
    "    parser.add_argument('--resume', default=default_resume, type=str, help='Resume path')\n",
    "    parser.add_argument('--sparsity', default=default_sparsity, type=float, help='Sparsity value')\n",
    "    parser.add_argument('--batch_size', default=default_batch_size, type=int, help='Batch size')\n",
    "    parser.add_argument('--pruning_method', default=default_pruning_method, type=str, help='Pruning method')\n",
    "    parser.add_argument('--prefix_dir', default=default_prefix_dir, type=str, help='Prefix directory')\n",
    "    parser.add_argument('--input_param_scale', default=default_input_param_scale, type=int, help='Input parameter scale')\n",
    "    parser.add_argument('--log_rows', default=default_log_rows, type=int, help='Log rows')\n",
    "    parser.add_argument('--num_cols', default=default_num_cols, type=int, help='Number of columns')\n",
    "    parser.add_argument('--scale_rebase_multiplier', default=default_scale_rebase_multiplier, type=int, help='Scale rebase multiplier')\n",
    "\n",
    "    args = parser.parse_args(namespace=args)\n",
    "else:\n",
    "    # In Jupyter Notebook, define args with default values\n",
    "    args.model = default_model\n",
    "    args.data_path = default_data_path\n",
    "    args.resume = default_resume\n",
    "    args.sparsity = default_sparsity\n",
    "    args.batch_size = default_batch_size\n",
    "    args.pruning_method = default_pruning_method\n",
    "    args.prefix_dir = default_prefix_dir\n",
    "    args.input_param_scale = default_input_param_scale\n",
    "    args.log_rows = default_log_rows\n",
    "    args.num_cols = default_num_cols\n",
    "    args.scale_rebase_multiplier = default_scale_rebase_multiplier\n",
    "    \n",
    "    \n",
    "print(\"\\n ---- IN_JUPYTER:\",str(IN_JUPYTER),\"---- \")\n",
    "# Print values for verification\n",
    "print(\"Model:\", args.model)\n",
    "print(\"Data Path:\", args.data_path)\n",
    "print(\"Resume Path:\", args.resume)\n",
    "print(\"Sparsity:\", args.sparsity)\n",
    "print(\"Batch Size:\", args.batch_size)\n",
    "print(\"Pruning Method:\", args.pruning_method)\n",
    "print(\"Prefix Directory:\", args.prefix_dir)\n",
    "print(\"Input Parameter Scale:\", args.input_param_scale)\n",
    "print(\"Log Rows:\", args.log_rows)\n",
    "print(\"Number of Columns:\", args.num_cols)\n",
    "print(\"Scale Rebase Multiplier:\", args.scale_rebase_multiplier)\n",
    "print(\"-------- \\n\\n\\n\")\n",
    "\n",
    "os.makedirs(args.prefix_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 300\n",
      "update_freq: 1\n",
      "input_size: 224\n",
      "layer_scale_init_value: 1e-06\n",
      "dropout: 0\n",
      "drop_path: 0\n",
      "drop_mode: standard\n",
      "drop_schedule: constant\n",
      "cutoff_epoch: 0\n",
      "model_ema: False\n",
      "model_ema_decay: 0.9999\n",
      "model_ema_force_cpu: False\n",
      "model_ema_eval: False\n",
      "opt: adamw\n",
      "opt_eps: 1e-08\n",
      "opt_betas: None\n",
      "clip_grad: None\n",
      "momentum: 0.9\n",
      "weight_decay: 0.05\n",
      "weight_decay_end: None\n",
      "lr: 0.004\n",
      "layer_decay: 1.0\n",
      "min_lr: 1e-06\n",
      "warmup_epochs: 50\n",
      "warmup_steps: -1\n",
      "color_jitter: 0.4\n",
      "aa: rand-m9-mstd0.5-inc1\n",
      "smoothing: 0.1\n",
      "train_interpolation: bicubic\n",
      "crop_pct: None\n",
      "reprob: 0.25\n",
      "remode: pixel\n",
      "recount: 1\n",
      "resplit: False\n",
      "mixup: 0.8\n",
      "cutmix: 1.0\n",
      "cutmix_minmax: None\n",
      "mixup_prob: 1.0\n",
      "mixup_switch_prob: 0.5\n",
      "mixup_mode: batch\n",
      "finetune: \n",
      "head_init_scale: 1.0\n",
      "model_key: model|module\n",
      "model_prefix: \n",
      "eval_data_path: None\n",
      "nb_classes: 1000\n",
      "imagenet_default_mean_and_std: True\n",
      "data_set: IMNET\n",
      "output_dir: \n",
      "device: cuda\n",
      "seed: 0\n",
      "auto_resume: True\n",
      "save_ckpt: True\n",
      "save_ckpt_freq: 1\n",
      "save_ckpt_num: 3\n",
      "start_epoch: 0\n",
      "eval: False\n",
      "dist_eval: True\n",
      "disable_eval: False\n",
      "num_workers: 10\n",
      "pin_mem: True\n",
      "world_size: 1\n",
      "local_rank: -1\n",
      "dist_on_itp: False\n",
      "dist_url: env://\n",
      "use_amp: False\n",
      "enable_wandb: False\n",
      "project: convnext\n",
      "wandb_ckpt: False\n",
      "nsamples: 4096\n",
      "prune_metric: None\n",
      "prune_granularity: None\n",
      "blocksize: 1\n",
      "model: vit_tiny\n",
      "data_path: /rds/general/user/mm6322/home/imagenet\n",
      "resume: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "sparsity: 0.5\n",
      "batch_size: 1\n",
      "pruning_method: CAP\n",
      "prefix_dir: sparse-cap-acc-tmp/\n",
      "input_param_scale: 7\n",
      "log_rows: 20\n",
      "num_cols: 2\n",
      "scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in vars(args).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.output_dir:\n",
    "    Path(default_args_dict.output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from timm.data.constants import \\\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data import create_transform\n",
    "\n",
    "def build_dataset(is_train, args):\n",
    "    transform = build_transform(is_train, args)\n",
    "\n",
    "    print(\"Transform = \")\n",
    "    if isinstance(transform, tuple):\n",
    "        for trans in transform:\n",
    "            print(\" - - - - - - - - - - \")\n",
    "            for t in trans.transforms:\n",
    "                print(t)\n",
    "    else:\n",
    "        for t in transform.transforms:\n",
    "            print(t)\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    if args.data_set == 'CIFAR':\n",
    "        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform, download=True)\n",
    "        nb_classes = 100\n",
    "    elif args.data_set == 'IMNET':\n",
    "        print(\"reading from datapath\", args.data_path)\n",
    "        root = os.path.join(args.data_path, 'train' if is_train else 'val_dirs')\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = 1000\n",
    "    elif args.data_set == \"image_folder\":\n",
    "        root = args.data_path if is_train else args.eval_data_path\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = args.nb_classes\n",
    "        assert len(dataset.class_to_idx) == nb_classes\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    print(\"Number of the class = %d\" % nb_classes)\n",
    "\n",
    "    return dataset, nb_classes\n",
    "\n",
    "\n",
    "def build_transform(is_train, args):\n",
    "    resize_im = args.input_size > 32\n",
    "    imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n",
    "    mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n",
    "    std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n",
    "\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=args.input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            interpolation=args.train_interpolation,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                args.input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        # warping (no cropping) when evaluated at 384 or larger\n",
    "        if args.input_size >= 384:  \n",
    "            t.append(\n",
    "            transforms.Resize((args.input_size, args.input_size), \n",
    "                            interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "        )\n",
    "            print(f\"Warping {args.input_size} size input images...\")\n",
    "        else:\n",
    "            if args.crop_pct is None:\n",
    "                args.crop_pct = 224 / 256\n",
    "            size = int(args.input_size / args.crop_pct)\n",
    "            t.append(\n",
    "                # to maintain same ratio w.r.t. 224 images\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),  \n",
    "            )\n",
    "            t.append(transforms.CenterCrop(args.input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_train = torch.utils.data.DistributedSampler(\n",
    "#         dataset_train, num_replicas=1, rank=0, shuffle=True, seed=args.seed,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_train = torch.utils.data.DataLoader(\n",
    "#     dataset_train, sampler=sampler_train,\n",
    "#     batch_size=args.batch_size,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.12'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import numpy as np\n",
    "from timm.utils import get_state_dict\n",
    "\n",
    "from pathlib import Path\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "# from torch._six import inf\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "class TensorboardLogger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = SummaryWriter(logdir=log_dir)\n",
    "        self.step = 0\n",
    "\n",
    "    def set_step(self, step=None):\n",
    "        if step is not None:\n",
    "            self.step = step\n",
    "        else:\n",
    "            self.step += 1\n",
    "\n",
    "    def update(self, head='scalar', step=None, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.writer.add_scalar(head + \"/\" + k, v, self.step if step is None else step)\n",
    "\n",
    "    def flush(self):\n",
    "        self.writer.flush()\n",
    "\n",
    "\n",
    "class WandbLogger(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        try:\n",
    "            import wandb\n",
    "            self._wandb = wandb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use the Weights and Biases Logger please install wandb.\"\n",
    "                \"Run `pip install wandb` to install it.\"\n",
    "            )\n",
    "\n",
    "        # Initialize a W&B run \n",
    "        if self._wandb.run is None:\n",
    "            self._wandb.init(\n",
    "                project=args.project,\n",
    "                config=args\n",
    "            )\n",
    "\n",
    "    def log_epoch_metrics(self, metrics, commit=True):\n",
    "        \"\"\"\n",
    "        Log train/test metrics onto W&B.\n",
    "        \"\"\"\n",
    "        # Log number of model parameters as W&B summary\n",
    "        self._wandb.summary['n_parameters'] = metrics.get('n_parameters', None)\n",
    "        metrics.pop('n_parameters', None)\n",
    "\n",
    "        # Log current epoch\n",
    "        self._wandb.log({'epoch': metrics.get('epoch')}, commit=False)\n",
    "        metrics.pop('epoch')\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            if 'train' in k:\n",
    "                self._wandb.log({f'Global Train/{k}': v}, commit=False)\n",
    "            elif 'test' in k:\n",
    "                self._wandb.log({f'Global Test/{k}': v}, commit=False)\n",
    "\n",
    "        self._wandb.log({})\n",
    "\n",
    "    def log_checkpoints(self):\n",
    "        output_dir = self.args.output_dir\n",
    "        model_artifact = self._wandb.Artifact(\n",
    "            self._wandb.run.id + \"_model\", type=\"model\"\n",
    "        )\n",
    "\n",
    "        model_artifact.add_dir(output_dir)\n",
    "        self._wandb.log_artifact(model_artifact, aliases=[\"latest\", \"best\"])\n",
    "\n",
    "    def set_steps(self):\n",
    "        # Set global training step\n",
    "        self._wandb.define_metric('Rank-0 Batch Wise/*', step_metric='Rank-0 Batch Wise/global_train_step')\n",
    "        # Set epoch-wise step\n",
    "        self._wandb.define_metric('Global Train/*', step_metric='epoch')\n",
    "        self._wandb.define_metric('Global Test/*', step_metric='epoch')\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "def load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "        \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.weight$')\n",
    "    state_dict_keys = list(state_dict.keys())\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.weight\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "            \n",
    "    \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.bias$')\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.bias\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "                                              \n",
    "    \n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(model, prefix=prefix)\n",
    "\n",
    "    warn_missing_keys = []\n",
    "    ignore_missing_keys = []\n",
    "    for key in missing_keys:\n",
    "        keep_flag = True\n",
    "        for ignore_key in ignore_missing.split('|'):\n",
    "            if ignore_key in key:\n",
    "                keep_flag = False\n",
    "                break\n",
    "        if keep_flag:\n",
    "            warn_missing_keys.append(key)\n",
    "        else:\n",
    "            ignore_missing_keys.append(key)\n",
    "            \n",
    "\n",
    "    missing_keys = warn_missing_keys\n",
    "\n",
    "    if len(missing_keys) > 0:\n",
    "        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "            model.__class__.__name__, unexpected_keys))\n",
    "    if len(ignore_missing_keys) > 0:\n",
    "        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, ignore_missing_keys))\n",
    "    if len(error_msgs) > 0:\n",
    "        print('\\n'.join(error_msgs))\n",
    "\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "\n",
    "        ########################################################\n",
    "        ## Code I added \n",
    "        for param in parameters:\n",
    "            weight_copy = param.data.abs().clone()\n",
    "            mask = weight_copy.gt(0).float().cuda()\n",
    "            sparsity = mask.sum() / mask.numel()\n",
    "            if sparsity > 0.3:\n",
    "                # non-trivial sparsity \n",
    "                param.grad.data.mul_(mask)\n",
    "        ########################################################\n",
    "\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n",
    "                     start_warmup_value=0, warmup_steps=-1):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_steps > 0:\n",
    "        warmup_iters = warmup_steps\n",
    "    print(\"Set warmup steps = %d\" % warmup_iters)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = np.array(\n",
    "        [final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        to_save = {\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'scaler': loss_scaler.state_dict(),\n",
    "            'args': args,\n",
    "        }\n",
    "\n",
    "        if model_ema is not None:\n",
    "            to_save['model_ema'] = get_state_dict(model_ema)\n",
    "\n",
    "        save_on_master(to_save, checkpoint_path)\n",
    "\n",
    "    if is_main_process() and isinstance(epoch, int):\n",
    "        to_del = epoch - args.save_ckpt_num * args.save_ckpt_freq\n",
    "        old_ckpt = output_dir / ('checkpoint-%s.pth' % to_del)\n",
    "        if os.path.exists(old_ckpt):\n",
    "            os.remove(old_ckpt)\n",
    "\n",
    "\n",
    "def auto_load_model(args, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.auto_resume and len(args.resume) == 0:\n",
    "        import glob\n",
    "        all_checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint-*.pth'))\n",
    "        latest_ckpt = -1\n",
    "        for ckpt in all_checkpoints:\n",
    "            t = ckpt.split('-')[-1].split('.')[0]\n",
    "            if t.isdigit():\n",
    "                latest_ckpt = max(int(t), latest_ckpt)\n",
    "        if latest_ckpt >= 0:\n",
    "            args.resume = os.path.join(output_dir, 'checkpoint-%d.pth' % latest_ckpt)\n",
    "        print(\"Auto resume checkpoint: %s\" % args.resume)\n",
    "\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        print(\"Resume checkpoint %s\" % args.resume)\n",
    "        if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            if not isinstance(checkpoint['epoch'], str): # does not support resuming with 'best', 'best-ema'\n",
    "                args.start_epoch = checkpoint['epoch'] + 1\n",
    "            else:\n",
    "                assert args.eval, 'Does not support resuming with checkpoint-best'\n",
    "            if hasattr(args, 'model_ema') and args.model_ema:\n",
    "                if 'model_ema' in checkpoint.keys():\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model_ema'])\n",
    "                else:\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model'])\n",
    "            if 'scaler' in checkpoint:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            print(\"With optim & sched!\")\n",
    "\n",
    "def reg_scheduler(base_value, final_value, epochs, niter_per_ep, early_epochs=0, early_value=None, \n",
    "           mode='linear', early_mode='regular'):\n",
    "    early_schedule = np.array([])\n",
    "    early_iters = early_epochs * niter_per_ep\n",
    "    if early_value is None:\n",
    "        early_value = final_value\n",
    "    if early_epochs > 0:\n",
    "        print(f\"Set early value to {early_mode} {early_value}\")\n",
    "        if early_mode == 'regular':\n",
    "            early_schedule = np.array([early_value] * early_iters)\n",
    "        elif early_mode == 'linear':\n",
    "            early_schedule = np.linspace(early_value, base_value, early_iters)\n",
    "        elif early_mode == 'cosine':\n",
    "            early_schedule = np.array(\n",
    "            [base_value + 0.5 * (early_value - base_value) * (1 + math.cos(math.pi * i / early_iters)) for i in np.arange(early_iters)])\n",
    "    regular_epochs = epochs - early_epochs\n",
    "    iters = np.arange(regular_epochs * niter_per_ep)\n",
    "    schedule = np.linspace(base_value, final_value, len(iters))\n",
    "    schedule = np.concatenate((early_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def build_model(args, pretrained=False):\n",
    "    if args.model.startswith(\"convnext\"):\n",
    "        model = create_model(\n",
    "            args.model,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=args.nb_classes,\n",
    "            layer_scale_init_value=args.layer_scale_init_value,\n",
    "            head_init_scale=args.head_init_scale,\n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate=args.dropout,\n",
    "            )\n",
    "    else:\n",
    "        model = create_model(\n",
    "            args.model, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=args.nb_classes, \n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate =args.dropout\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCHS: 1\n"
     ]
    }
   ],
   "source": [
    "BATCHS = args.batch_size\n",
    "print(\"BATCHS:\",BATCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.resnet import resnet26d, resnet50d\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
    "    ),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_huge_patch16_224': _cfg(),\n",
    "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
    "    # hybrid models\n",
    "    'vit_small_resnet26d_224': _cfg(),\n",
    "    'vit_small_resnet50d_s3_224': _cfg(),\n",
    "    'vit_base_resnet26d_224': _cfg(),\n",
    "    'vit_base_resnet50d_224': _cfg(),\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        \n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "#         self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.query_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.key_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.value_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "#         query = self.query_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         key = self.key_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         value = self.value_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "        \n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        qkvs = self.qkv(x)\n",
    "        query = qkvs[:,:,0:self.dim]\n",
    "        key = qkvs[:,:,self.dim:2*self.dim]\n",
    "        value = qkvs[:,:,2*self.dim:] \n",
    "        query = query.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        key = key.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        value = value.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale     \n",
    "        attn = query.transpose(1,2) @ key.transpose(1,2).transpose(2,3)\n",
    "        attn = attn * self.scale\n",
    "\n",
    "\n",
    "        attn = attn.softmax(dim=(-1))\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        \n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        y = (attn@value.transpose(1,2)).transpose(1,2).reshape(B,N,C)\n",
    "        \n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "        y = self.proj(y)\n",
    "        y = self.proj_drop(y)\n",
    "#         return x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "#         self.norm1 = norm_layer(self.dim)\n",
    "        self.norm1 = norm_layer(self.dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(self.dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))      \n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def half_block(self,x):\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "#         assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "#             f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "#         x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x).reshape(self.batch_size,192,196,1).transpose(1,2)\n",
    "        x = x.squeeze(3)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# class HybridEmbed(nn.Module):\n",
    "#     \"\"\" CNN Feature Map Embedding\n",
    "#     Extract feature map from CNN, flatten, project to embedding dim.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         assert isinstance(backbone, nn.Module)\n",
    "#         img_size = to_2tuple(img_size)\n",
    "#         self.img_size = img_size\n",
    "#         self.backbone = backbone\n",
    "#         if feature_size is None:\n",
    "#             with torch.no_grad():\n",
    "#                 # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
    "#                 # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
    "#                 # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
    "#                 training = backbone.training\n",
    "#                 if training:\n",
    "#                     backbone.eval()\n",
    "#                 o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "#                 feature_size = o.shape[-2:]\n",
    "#                 feature_dim = o.shape[1]\n",
    "#                 backbone.train(training)\n",
    "#         else:\n",
    "#             feature_size = to_2tuple(feature_size)\n",
    "#             feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "#         self.num_patches = feature_size[0] * feature_size[1]\n",
    "#         self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)[-1]\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "#         x = self.proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        # I add these two lines\n",
    "        self.drop_rate=drop_rate\n",
    "        attn_drop_rate=drop_rate\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.depth = depth\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
    "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
    "        #self.repr_act = nn.Tanh()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    " \n",
    "        x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "        B = BATCHS\n",
    "    \n",
    "\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1)\n",
    "        \n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def split_convs(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "#       B = x.shape[0]\n",
    "        B = BATCHS\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = torch.cat((self.cls_token, x), dim=1)\n",
    "    \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "#         num_blocks = len(self.blocks) // 2\n",
    "#         for blk in self.blocks[:num_blocks]:\n",
    "#             x = blk(x)\n",
    "\n",
    "#         x = self.blocks[0].half_block(x)\n",
    "\n",
    "#         print(\"split_1, output.shape:\",x.shape, \"\\t num_blocks:\",num_blocks)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def split_2(self,x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks[0].half_block(x)\n",
    "        return x\n",
    "    \n",
    "    def split_n(self,x,n,half=None):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for layer_idx in range(n-1):\n",
    "            x = self.blocks[layer_idx](x)\n",
    "            \n",
    "        #n-th layer\n",
    "        if half:\n",
    "            x = self.blocks[n].half_block(x)\n",
    "        else:\n",
    "            x = self.blocks[n](x)\n",
    "            # last layer of transformer\n",
    "            if n == (self.depth - 1):\n",
    "                x = self.norm(x)\n",
    "                x = x[:, 0]\n",
    "                x = self.head(x)\n",
    "                \n",
    "        return x\n",
    "            \n",
    "          \n",
    "    def update_drop_path(self, drop_path_rate):\n",
    "        self.drop_path = drop_path_rate\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, self.depth)]\n",
    "        for i in range(self.depth):\n",
    "            self.blocks[i].drop_path.drop_prob = dp_rates[i]\n",
    "    \n",
    "    def update_dropout(self, drop_rate):\n",
    "        self.drop_rate = drop_rate\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = drop_rate\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# @register_model\n",
    "# def vit_tiny_tiny(pretrained=False, **kwargs):\n",
    "#     model = VisionTransformer(\n",
    "#         patch_size=16, embed_dim=48, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "@register_model\n",
    "def vit_tiny(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_small(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_base(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_large(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(args, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.VisionTransformer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 5717416\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_69108/3668749705.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "# total_batch_size = args.batch_size * args.update_freq * utils.get_world_size()\n",
    "total_batch_size = args.batch_size * args.update_freq\n",
    "# num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n",
    "\n",
    "# At most one of dropout and stochastic depth should be enabled.\n",
    "assert(args.dropout == 0 or args.drop_path == 0)\n",
    "# ConvNeXt does not support dropout.\n",
    "assert(args.dropout == 0 if args.model.startswith(\"convnext\") else True)\n",
    "\n",
    "import re\n",
    "\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state_dict', 'optimizer', 'loss_scaler', 'epoch', 'manager'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# cnt = 0\n",
    "# dl = DataLoader(dataset_train, batch_size=BATCHS, shuffle=True)\n",
    "# # data = dataset_train[cnt][0].unsqueeze(dim=0)\n",
    "# # label = dataset_train[cnt][1]\n",
    "# data,label = next(iter(dl))\n",
    "# print(data.shape)\n",
    "# print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5717416\n",
      "Number of zeros: 2654208\n",
      "Percentage of zeros: 46.42%\n"
     ]
    }
   ],
   "source": [
    "# compute the number of zeros in the model / total number of parameters\n",
    "total_params = 0\n",
    "zeros = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    zeros += (param.data == 0).sum().item()\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of zeros: {zeros}\")\n",
    "print(f\"Percentage of zeros: {zeros / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "# load JPEG image /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = Image.open(\"/rds/general/user/mm6322/home/imagenet/val/n12620546/ILSVRC2012_val_00011901.JPEG\")\n",
    "\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# result = model(data.view(data.shape[0],-1))\n",
    "print(\"data shape:\",data.shape)\n",
    "result = model(data)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = data.detach().clone()\n",
    "print(\"x.shape:\",x.shape)\n",
    "\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(    \n",
    "    model,               # model being run\n",
    "    x,                   # model input (or a tuple for multiple inputs)\n",
    "    args.prefix_dir + \"network_complete.onnx\",            # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=15,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                    'output': {0:'batch_size'},\n",
    "    },         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx:\t CONV \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.9203) \t max: tensor(13.6817)\n",
      "layer_idx:\t 0 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.0635) \t max: tensor(15.4783)\n",
      "layer_idx:\t 0 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.4229) \t max: tensor(17.4164)\n",
      "layer_idx:\t 1 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.8803) \t max: tensor(10.3352)\n",
      "layer_idx:\t 1 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.4997) \t max: tensor(9.3524)\n",
      "layer_idx:\t 2 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.2025) \t max: tensor(17.3773)\n",
      "layer_idx:\t 2 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.8159) \t max: tensor(16.2393)\n",
      "layer_idx:\t 3 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.4613) \t max: tensor(15.7312)\n",
      "layer_idx:\t 3 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.2997) \t max: tensor(14.8284)\n",
      "layer_idx:\t 4 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.5867) \t max: tensor(15.6392)\n",
      "layer_idx:\t 4 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.6180) \t max: tensor(14.4870)\n",
      "layer_idx:\t 5 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.0484) \t max: tensor(15.2520)\n",
      "layer_idx:\t 5 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.7885) \t max: tensor(14.6185)\n",
      "layer_idx:\t 6 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.8097) \t max: tensor(14.7460)\n",
      "layer_idx:\t 6 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.1806) \t max: tensor(14.0622)\n",
      "layer_idx:\t 7 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-7.9359) \t max: tensor(14.7281)\n",
      "layer_idx:\t 7 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.2735) \t max: tensor(18.5691)\n",
      "layer_idx:\t 8 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-11.1971) \t max: tensor(20.7566)\n",
      "layer_idx:\t 8 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.5824) \t max: tensor(25.8723)\n",
      "layer_idx:\t 9 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-14.3955) \t max: tensor(26.9132)\n",
      "layer_idx:\t 9 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.2325) \t max: tensor(27.7766)\n",
      "layer_idx:\t 10 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5998) \t max: tensor(29.1508)\n",
      "layer_idx:\t 10 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5393) \t max: tensor(30.7623)\n",
      "layer_idx:\t 11 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-16.1201) \t max: tensor(33.1553)\n",
      "layer_idx:\t 11 \t half: False \t inter_out.shape: torch.Size([1, 1000]) \t min: tensor(-3.9342) \t max: tensor(8.3160)\n"
     ]
    }
   ],
   "source": [
    "inter_out = model.split_convs(data)\n",
    "print(\"layer_idx:\\t\",\"CONV\",\"\\t half:\",str(False),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_out = model.split_n(data,layer_idx,half)\n",
    "        print(\"layer_idx:\\t\",layer_idx,\"\\t half:\",str(half),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for all layers\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),args.prefix_dir, \"input_convs.json\")\n",
    "data = dict(input_data = [((x).detach().numpy()).reshape([-1]).tolist()])\n",
    "json.dump( data, open(data_path, 'w' ))\n",
    "\n",
    "for i in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_i = model.split_n(x,i,half=half)\n",
    "        data_path = os.path.join(os.getcwd(),args.prefix_dir, f\"input_{i}_{str(half)}.json\")\n",
    "        data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "        json.dump( data, open(data_path, 'w' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# extract all onnx files of layers\n",
    "\n",
    "input_path = args.prefix_dir + \"network_complete.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function\n",
    "def activation_hook(layer_name, activation_stats):\n",
    "    def hook(module, input, output):\n",
    "        input_tensor = input[0]\n",
    "        activation_stats[layer_name] = {\n",
    "            # l1 norm\n",
    "            'norm': input_tensor.norm(),\n",
    "            'max': input_tensor.max(),\n",
    "            'min': input_tensor.min(),\n",
    "            'shape': input_tensor.shape\n",
    "    }\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # teleporing the mlp model\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "# # IMPORTANT: input_x_T/F is acutally the output of that block (output of the intermediate or the full block)\n",
    "\n",
    "# # find the input data for the mlp model\n",
    "# input_split0_False = json.load(open(args.prefix_dir + \"input_0_True.json\"))[\"input_data\"][0]\n",
    "# norm_split0_False = model.blocks[0].norm2\n",
    "# input_mlp_0 = norm_split0_False(torch.tensor(input_split0_False).view(1,197,192))\n",
    "# out_mlp_0 = original_mlp_0(input_mlp_0)\n",
    "# out_mlp_0 = model.blocks[0].drop_path(out_mlp_0)\n",
    "# out_split0_false = out_mlp_0 + torch.tensor(input_split0_False).view(1,197,192)\n",
    "\n",
    "# # True output of the current block == input of the next block\n",
    "# out_split0_false_orginal = json.load(open(args.prefix_dir + \"input_0_False.json\"))[\"input_data\"][0]\n",
    "\n",
    "# print(input_mlp_0.norm(),input_mlp_0.max(),input_mlp_0.min(),input_mlp_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor(input_split0_False).view(1,197,192)\n",
    "# print(t.norm(),t.max(),t.min(),t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register hooks to the layers before all activation functions\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "# activation_stats = {}\n",
    "# for i, layer in enumerate(original_mlp_0.children()):\n",
    "#     if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#         layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats))\n",
    "\n",
    "# # Run the mlp model\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_block0 = model.split_convs(torch.tensor(input_convs).view(1,3,224,224))\n",
    "# input_block0 = input_block0.view(1,197,192)\n",
    "# original_block0_false_pred = model.blocks[0](input_block0)\n",
    "# print(\"activation_stats:\",activation_stats)\n",
    "\n",
    "# original_loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# print(f\"Original loss: {original_loss}, Original prediction error: {0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = json.load(open(args.prefix_dir + \"input_0_True.json\"))['input_data'][0]\n",
    "# d = torch.tensor(d).view(1,197,192)\n",
    "# np.save(args.prefix_dir + \"input_block0_false.npy\", d.numpy())\n",
    "\n",
    "# # original_pred = model.blocks[0].mlp(model.blocks[0].norm2(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralteleportation.models.model_zoo.mlpcob import MLPCOB\n",
    "from neuralteleportation.neuralteleportationmodel import NeuralTeleportationModel\n",
    "from neuralteleportation.layers.neuralteleportation import COBForwardMixin, FlattenCOB\n",
    "from neuralteleportation.layers.neuron import LinearCOB\n",
    "from neuralteleportation.layers.activation import ReLUCOB, SigmoidCOB, GELUCOB, LeakyReLUCOB\n",
    "from neuralteleportation.layers.dropout import DropoutCOB\n",
    "from neuralteleportation.layers.neuron import LayerNormCOB\n",
    "from neuralteleportation.layers.merge import Add\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # self.add = Add()\n",
    "        self.norm2 = LayerNormCOB(192)\n",
    "        self.fc1 = LinearCOB(192, 768, bias=True)\n",
    "        self.act = GELUCOB()\n",
    "        self.fc2 = LinearCOB(768, 192, bias=True)\n",
    "        # self.drop1 = DropoutCOB(0.1)\n",
    "        # self.drop2 = DropoutCOB(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.norm2(x)\n",
    "        x2 = self.fc1(x1)\n",
    "        x3 = self.act(x2)\n",
    "        x4 = self.fc2(x3)\n",
    "        # x4 = self.add(x, x3)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teleported_model = LinearNet()\n",
    "# teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ln_weights(LN, model, block_idx):\n",
    "    original_mlp = model.blocks[block_idx].mlp\n",
    "    original_norm2 = model.blocks[block_idx].norm2\n",
    "\n",
    "    combined_dict = {}\n",
    "    combined_dict.update(original_mlp.state_dict())\n",
    "    for k,v in original_norm2.state_dict().items():\n",
    "        combined_dict[\"norm2.\" + k] = v\n",
    "    LN.network.load_state_dict(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variable to store the best loss found\n",
    "global best_loss\n",
    "global cor_best_pred_error\n",
    "global cor_best_range\n",
    "# initialize best_loss\n",
    "best_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "982a3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def f_ack(cob,input_data=None, original_pred=None, layer_idx=None ,original_loss=None, tm=None):    \n",
    "    \n",
    "    # # Set up model with the new COB\n",
    "    # teleported_model = LinearNet()\n",
    "    # teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    # deep copy tm to teleported_model\n",
    "    teleported_model = copy.deepcopy(tm)\n",
    "    # Load the weights of the original model\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    # teleport to the ones tensor\n",
    "    teleported_model = teleported_model.teleport(torch.ones_like(cob), reset_teleportation=True)\n",
    "\n",
    "    # apply the COB\n",
    "    teleported_model = teleported_model.teleport(cob, reset_teleportation=True)\n",
    "\n",
    "    # Reset activation stats and run a forward pass\n",
    "    activation_stats = {}\n",
    "    hook_handles = []\n",
    "    for i, layer in enumerate(teleported_model.network.children()):\n",
    "            if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "                handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "                hook_handles.append(handle)\n",
    "    teleported_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = teleported_model.network(input_data)\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "    # Calculate the range loss\n",
    "    loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "    loss /= original_loss\n",
    "    # Calculate the prediction error\n",
    "    pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "    pred_error /= np.abs(original_pred).mean()\n",
    "    total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "    if random.random() < 0.0005:\n",
    "         print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "    \n",
    "    activation_stats.clear()\n",
    "    del teleported_model, activation_stats, pred, loss, pred_error, cob\n",
    "    return total_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "272f78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy the model\n",
    "# import copy\n",
    "# import itertools\n",
    "# import functools\n",
    "\n",
    "# layer_idx = 1\n",
    "\n",
    "# # copy the model (the new_model will be used to apply the cob to prevent the original model from being modified)\n",
    "# new_model = copy.deepcopy(model)\n",
    "\n",
    "# # Register hooks to the layers before all activation functions\n",
    "# original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "# activation_stats_idx = {}\n",
    "# for i,layer in enumerate(original_mlp_idx.children()):\n",
    "#     if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#         layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "\n",
    "# # run the mlp model to find original_loss\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "# print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "\n",
    "# # calculate the original loss\n",
    "# original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "# print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "# # Load the teleported model\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# # get initial weights and cob\n",
    "# initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "# initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "# global best_loss\n",
    "# best_loss = 1e9\n",
    "\n",
    "# # Load the input data to calculate the original_pred\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "# input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "# input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "# original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "# # Apply best COB and save model weights\n",
    "# LN = LinearNet()\n",
    "# LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "# s = initial_cob_idx.size()\n",
    "\n",
    "# # add inputs to the function\n",
    "# ackley = functools.partial(\n",
    "#     f_ack,\n",
    "#     input_data=input_teleported_model,\n",
    "#     original_pred=original_pred,\n",
    "#     layer_idx=layer_idx,\n",
    "#     original_loss = original_loss_idx,\n",
    "#     tm = LN\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d068e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dic: cob\n",
    "@torch.no_grad()\n",
    "def rge(func, params_dict, sample_size, step_size, base=None):\n",
    "    if base == None:\n",
    "        base = func(params_dict[\"cob\"])\n",
    "    grads_dict = {}\n",
    "    for _ in range(sample_size):\n",
    "        perturbs_dict, perturbed_params_dict = {}, {}\n",
    "        for key, param in params_dict.items():\n",
    "            perturb = torch.randn_like(param)\n",
    "            perturb /= (torch.norm(perturb) + 1e-8)\n",
    "            perturb *= step_size\n",
    "            perturbs_dict[key] = perturb\n",
    "            perturbed_params_dict[key] = perturb + param\n",
    "        directional_derivative = (func(perturbed_params_dict[\"cob\"]) - base) / step_size\n",
    "        if len(grads_dict.keys()) == len(params_dict.keys()):\n",
    "            for key, perturb in perturbs_dict.items():\n",
    "                grads_dict[key] += perturb * directional_derivative / sample_size\n",
    "        else:\n",
    "            for key, perturb in perturbs_dict.items():\n",
    "                grads_dict[key] = perturb * directional_derivative / sample_size\n",
    "    return grads_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8979722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.pred_mul = 5\n",
    "# initial_cob_idx = torch.ones(s)\n",
    "# args.steps = int(s[0] * 10)\n",
    "# args.cob_lr = 10\n",
    "# args.zoo_sample_size = 100\n",
    "# args.zoo_step_size = 0.1\n",
    "# best_loss = 1e9\n",
    "\n",
    "# # grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, 10, 0.1)\n",
    "\n",
    "# # training the cob\n",
    "# # loop over the steps\n",
    "# for step in range(args.steps):\n",
    "#     # get the gradient of the cob\n",
    "#     grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, args.zoo_sample_size, args.zoo_step_size)\n",
    "#     # update the cob\n",
    "#     initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "#     # calculate the loss\n",
    "#     loss = ackley(initial_cob_idx)\n",
    "#     # update the best loss\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_cob = initial_cob_idx\n",
    "#         print(f\"Step: {step} \\t Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "958f1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply best_cob to find the pred_error and range_loss\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# teleported_model_idx = teleported_model_idx.teleport(best_cob, reset_teleportation=True)\n",
    "\n",
    "# # Reset activation stats and run a forward pass\n",
    "# activation_stats = {}\n",
    "# hook_handles = []\n",
    "# for i, layer in enumerate(teleported_model_idx.network.children()):\n",
    "#         if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "#             handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "#             hook_handles.append(handle)\n",
    "# teleported_model_idx.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = teleported_model_idx.network(input_teleported_model)\n",
    "# for handle in hook_handles:\n",
    "#     handle.remove()\n",
    "\n",
    "# # Calculate the range loss\n",
    "# loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# loss /= original_loss_idx\n",
    "# # Calculate the prediction error\n",
    "# pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "# pred_error /= np.abs(original_pred).mean()\n",
    "# total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "# print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "# print(f\"Total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b101ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cge(func, params_dict, mask_dict, step_size, base=None):\n",
    "    if base == None:\n",
    "        base = func(params_dict[\"cob\"])\n",
    "    grads_dict = {}\n",
    "    for key, param in params_dict.items():\n",
    "        if 'orig' in key:\n",
    "            mask_key = key.replace('orig', 'mask')\n",
    "            mask_flat = mask_dict[mask_key].flatten()\n",
    "        else:\n",
    "            mask_flat = torch.ones_like(param).flatten()\n",
    "        directional_derivative = torch.zeros_like(param)\n",
    "        directional_derivative_flat = directional_derivative.flatten()\n",
    "        for idx in mask_flat.nonzero().flatten():\n",
    "            perturbed_params_dict = copy.deepcopy(params_dict)\n",
    "            p_flat = perturbed_params_dict[key].flatten()\n",
    "            p_flat[idx] += step_size\n",
    "            directional_derivative_flat[idx] = (func(perturbed_params_dict[\"cob\"]) - base) / step_size\n",
    "        grads_dict[key] = directional_derivative.to(param.device)\n",
    "    return grads_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a51bd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.pred_mul = 10\n",
    "# args.steps = 800\n",
    "# initial_cob_idx = torch.ones(s)\n",
    "# # args.steps = int(s[0] * 10)\n",
    "# args.cob_lr = 0.1 # was 0.1\n",
    "# args.zoo_step_size = 0.001 # was 0.005 -> loss turned out to 57.2 (?)\n",
    "# #                                0.001 -> 56.2 (step 446)\n",
    "# #                                0.0001 -> 56.3 (lots of steps)\n",
    "# best_loss = 1e9\n",
    "\n",
    "\n",
    "# # grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, 10, 0.1)\n",
    "\n",
    "# # training the cob\n",
    "# # loop over the steps\n",
    "# for step in range(args.steps):\n",
    "#     # get the gradient of the cob\n",
    "#     grad_cob = cge(ackley, {\"cob\": initial_cob_idx}, None, args.zoo_step_size)\n",
    "#     # update the cob\n",
    "#     initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "#     # calculate the loss\n",
    "#     loss = ackley(initial_cob_idx)\n",
    "#     # update the best loss\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_cob = initial_cob_idx\n",
    "#         print(f\"Step: {step} \\t Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4386ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply best_cob to find the pred_error and range_loss\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# teleported_model_idx = teleported_model_idx.teleport(best_cob, reset_teleportation=True)\n",
    "\n",
    "# # Reset activation stats and run a forward pass\n",
    "# activation_stats = {}\n",
    "# hook_handles = []\n",
    "# for i, layer in enumerate(teleported_model_idx.network.children()):\n",
    "#         if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "#             handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "#             hook_handles.append(handle)\n",
    "# teleported_model_idx.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = teleported_model_idx.network(input_teleported_model)\n",
    "# for handle in hook_handles:\n",
    "#     handle.remove()\n",
    "\n",
    "# # Calculate the range loss\n",
    "# loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# loss /= original_loss_idx\n",
    "# # Calculate the prediction error\n",
    "# pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "# pred_error /= np.abs(original_pred).mean()\n",
    "# total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "# print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "# print(f\"Total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file and write the header if it doesn't exist\n",
    "import csv\n",
    "\n",
    "csv_file_path = args.prefix_dir + 'all_settings_deepzero.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            'EXPERIMENT SETTINGS',\n",
    "            'Model_Number', 'Layer_Index',\n",
    "            # 'Activation_Loss', 'Prediction_Error',\n",
    "            'Activation_Loss',\n",
    "            'Input_Scale', 'Param_Scale', 'Scale_Rebase_Multiplier',\n",
    "            'Lookup_Range_0', 'Lookup_Range_1', 'Logrows', 'Num_Rows', 'Num_cols', 'Total_Assignments',\n",
    "            'Total_Constant_Size', 'Model_Output_Scales', \n",
    "            'Required_Range_Checks_0','Required_Range_Checks_1', 'Proof_Time_Seconds', 'Max_Memory' ,'Mean_Squared_Error', 'Mean_Abs_Percent_Error',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= START =========\n",
      "input_param_scale: 16, num_cols: 64, max_log_rows: -1, param_visibility: fixed, lookup_margin: 2\n",
      "layer_idx: 0 , \t  activation_stats: {'relu_1': {'norm': tensor(476.6286), 'max': tensor(6.2171), 'min': tensor(-6.8579), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(13.0750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \t Loss: 1.0335593223571777\n",
      "Step: 1 \t Loss: 0.9670328497886658\n",
      "pred_error: 0.004030668176710606 \t range_loss: 0.9267262816429138\n",
      "pred_error: 0.004024708177894354 \t range_loss: 0.9267262816429138\n",
      "Step: 2 \t Loss: 0.9376171827316284\n",
      "Step: 3 \t Loss: 0.9279061555862427\n",
      "pred_error: 0.004524803254753351 \t range_loss: 0.8826581239700317\n",
      "Step: 4 \t Loss: 0.8983767628669739\n",
      "Step: 5 \t Loss: 0.8915551900863647\n",
      "Step: 6 \t Loss: 0.8797388076782227\n",
      "pred_error: 0.004548515193164349 \t range_loss: 0.8342570066452026\n",
      "Step: 7 \t Loss: 0.8648985624313354\n",
      "pred_error: 0.004696848336607218 \t range_loss: 0.8179301023483276\n",
      "Step: 8 \t Loss: 0.8569036722183228\n",
      "Step: 9 \t Loss: 0.8502700924873352\n",
      "pred_error: 0.005080545321106911 \t range_loss: 0.7994646430015564\n",
      "Step: 10 \t Loss: 0.8377221822738647\n",
      "pred_error: 0.004485889337956905 \t range_loss: 0.7928633093833923\n",
      "Step: 12 \t Loss: 0.8341931104660034\n",
      "Step: 13 \t Loss: 0.8321618437767029\n",
      "pred_error: 0.005026698112487793 \t range_loss: 0.781894862651825\n",
      "Step: 14 \t Loss: 0.8311120271682739\n",
      "Step: 15 \t Loss: 0.8224620223045349\n",
      "pred_error: 0.004825321491807699 \t range_loss: 0.7742236852645874\n",
      "Step: 16 \t Loss: 0.8142070770263672\n",
      "pred_error: 0.004936108831316233 \t range_loss: 0.7648478746414185\n",
      "Step: 17 \t Loss: 0.8134172558784485\n",
      "pred_error: 0.004962406121194363 \t range_loss: 0.7637930512428284\n",
      "Step: 18 \t Loss: 0.8122773766517639\n",
      "Step: 20 \t Loss: 0.8095033764839172\n",
      "pred_error: 0.005014901515096426 \t range_loss: 0.759351372718811\n",
      "pred_error: 0.005015227943658829 \t range_loss: 0.759351372718811\n",
      "Step: 21 \t Loss: 0.8081076741218567\n",
      "Step: 22 \t Loss: 0.8024575114250183\n",
      "pred_error: 0.005187300965189934 \t range_loss: 0.7505838871002197\n",
      "Step: 23 \t Loss: 0.799782395362854\n",
      "pred_error: 0.005149335600435734 \t range_loss: 0.748285174369812\n",
      "Step: 24 \t Loss: 0.7985395789146423\n",
      "Step: 25 \t Loss: 0.7935965657234192\n",
      "pred_error: 0.0050703128799796104 \t range_loss: 0.7428943514823914\n",
      "Step: 27 \t Loss: 0.792155385017395\n",
      "Step: 31 \t Loss: 0.7897515892982483\n",
      "Step: 32 \t Loss: 0.7879810333251953\n",
      "Step: 33 \t Loss: 0.7865201830863953\n",
      "pred_error: 0.005504977889358997 \t range_loss: 0.7314704060554504\n",
      "pred_error: 0.0055032167583703995 \t range_loss: 0.7314704060554504\n",
      "Step: 34 \t Loss: 0.7828879952430725\n",
      "Step: 35 \t Loss: 0.7809505462646484\n",
      "pred_error: 0.005263956263661385 \t range_loss: 0.7283105254173279\n",
      "pred_error: 0.005810377188026905 \t range_loss: 0.7234750986099243\n",
      "pred_error: 0.005810850765556097 \t range_loss: 0.7234750986099243\n",
      "Step: 40 \t Loss: 0.7777238488197327\n",
      "pred_error: 0.005762322805821896 \t range_loss: 0.721572756767273\n",
      "pred_error: 0.005762950517237186 \t range_loss: 0.721572756767273\n",
      "Step: 42 \t Loss: 0.776839017868042\n",
      "Step: 43 \t Loss: 0.7752469182014465\n",
      "Step: 44 \t Loss: 0.7740675210952759\n",
      "pred_error: 0.005826242733746767 \t range_loss: 0.7157770991325378\n",
      "Step: 45 \t Loss: 0.7721742987632751\n",
      "Step: 46 \t Loss: 0.7698358297348022\n",
      "Step: 49 \t Loss: 0.7686330676078796\n",
      "Step: 52 \t Loss: 0.7681944370269775\n",
      "Step: 53 \t Loss: 0.7675420045852661\n",
      "Step: 54 \t Loss: 0.7669731974601746\n",
      "pred_error: 0.005727182608097792 \t range_loss: 0.7097013592720032\n",
      "pred_error: 0.006138282362371683 \t range_loss: 0.7106888890266418\n",
      "pred_error: 0.006138065829873085 \t range_loss: 0.7106888890266418\n",
      "pred_error: 0.006136700976639986 \t range_loss: 0.7106888890266418\n",
      "Step: 59 \t Loss: 0.7668614387512207\n",
      "pred_error: 0.006162232719361782 \t range_loss: 0.705226480960846\n",
      "Step: 60 \t Loss: 0.7647147178649902\n",
      "Step: 61 \t Loss: 0.7624703049659729\n",
      "pred_error: 0.006224832031875849 \t range_loss: 0.7021462321281433\n",
      "Step: 65 \t Loss: 0.7617834806442261\n",
      "Step: 66 \t Loss: 0.7608795166015625\n",
      "Step: 67 \t Loss: 0.7595295906066895\n",
      "pred_error: 0.006412657443434 \t range_loss: 0.697838306427002\n",
      "pred_error: 0.006459846161305904 \t range_loss: 0.6974664926528931\n",
      "pred_error: 0.006459646858274937 \t range_loss: 0.6974664926528931\n",
      "Step: 72 \t Loss: 0.7594578266143799\n",
      "pred_error: 0.006405191961675882 \t range_loss: 0.695405900478363\n",
      "pred_error: 0.006405150983482599 \t range_loss: 0.695405900478363\n",
      "pred_error: 0.006652092095464468 \t range_loss: 0.6932243704795837\n",
      "Step: 74 \t Loss: 0.7589011192321777\n",
      "pred_error: 0.00655766949057579 \t range_loss: 0.6933222413063049\n",
      "Step: 76 \t Loss: 0.7580229043960571\n",
      "Step: 77 \t Loss: 0.7573918104171753\n",
      "pred_error: 0.006729960907250643 \t range_loss: 0.6916006803512573\n",
      "pred_error: 0.00692320941016078 \t range_loss: 0.69001305103302\n",
      "Step: 83 \t Loss: 0.7565837502479553\n",
      "pred_error: 0.006913369987159967 \t range_loss: 0.6875980496406555\n",
      "pred_error: 0.007230523508042097 \t range_loss: 0.6863990426063538\n",
      "Step: 86 \t Loss: 0.755270779132843\n",
      "pred_error: 0.007121911272406578 \t range_loss: 0.6840505003929138\n",
      "Step: 87 \t Loss: 0.754051923751831\n",
      "pred_error: 0.007012542802840471 \t range_loss: 0.6841012239456177\n",
      "Step: 90 \t Loss: 0.7540135979652405\n",
      "Step: 92 \t Loss: 0.7539271116256714\n",
      "Step: 93 \t Loss: 0.7531043887138367\n",
      "pred_error: 0.007563682738691568 \t range_loss: 0.6813624501228333\n",
      "Step: 96 \t Loss: 0.7527326941490173\n",
      "Step: 97 \t Loss: 0.7514830827713013\n",
      "Step: 98 \t Loss: 0.750637412071228\n",
      "pred_error: 0.007392142433673143 \t range_loss: 0.6767159700393677\n",
      "pred_error: 0.007567411754280329 \t range_loss: 0.6763530373573303\n",
      "pred_error: 0.007567004766315222 \t range_loss: 0.6763530373573303\n",
      "pred_error: 0.007534750271588564 \t range_loss: 0.6758363842964172\n",
      "pred_error: 0.007952400483191013 \t range_loss: 0.6738681793212891\n",
      "pred_error: 0.00795074738562107 \t range_loss: 0.6743318438529968\n",
      "pred_error: 0.0077635860070586205 \t range_loss: 0.6737955212593079\n",
      "Step: 110 \t Loss: 0.7501866817474365\n",
      "pred_error: 0.00772463483735919 \t range_loss: 0.672941267490387\n",
      "Step: 111 \t Loss: 0.7493324279785156\n",
      "pred_error: 0.007728962227702141 \t range_loss: 0.6726041436195374\n",
      "pred_error: 0.007728962227702141 \t range_loss: 0.6726041436195374\n",
      "pred_error: 0.007728962227702141 \t range_loss: 0.6726041436195374\n",
      "Step: 116 \t Loss: 0.7492306232452393\n",
      "Step: 117 \t Loss: 0.7473916411399841\n",
      "pred_error: 0.008150935173034668 \t range_loss: 0.6685811281204224\n",
      "Step: 121 \t Loss: 0.7471197843551636\n",
      "pred_error: 0.008335026912391186 \t range_loss: 0.6637703776359558\n",
      "pred_error: 0.008335406892001629 \t range_loss: 0.6637703776359558\n",
      "Step: 122 \t Loss: 0.7464628219604492\n",
      "Step: 123 \t Loss: 0.7460709810256958\n",
      "pred_error: 0.008347699418663979 \t range_loss: 0.6625946164131165\n",
      "pred_error: 0.008232488296926022 \t range_loss: 0.6638121008872986\n",
      "pred_error: 0.008513878099620342 \t range_loss: 0.663038432598114\n",
      "pred_error: 0.0083834333345294 \t range_loss: 0.6622863411903381\n",
      "pred_error: 0.008913589641451836 \t range_loss: 0.6588041186332703\n",
      "Step: 136 \t Loss: 0.7460252046585083\n",
      "pred_error: 0.008770209737122059 \t range_loss: 0.6583250761032104\n",
      "pred_error: 0.008814744651317596 \t range_loss: 0.6578884124755859\n",
      "Step: 138 \t Loss: 0.7458817958831787\n",
      "pred_error: 0.009127642028033733 \t range_loss: 0.6554301381111145\n",
      "pred_error: 0.009163309819996357 \t range_loss: 0.6549354195594788\n",
      "Step: 142 \t Loss: 0.7448061108589172\n",
      "pred_error: 0.009031611494719982 \t range_loss: 0.6544848084449768\n",
      "Step: 143 \t Loss: 0.7444702386856079\n",
      "pred_error: 0.00899513065814972 \t range_loss: 0.6544895172119141\n",
      "Step: 146 \t Loss: 0.7427117824554443\n",
      "pred_error: 0.008890260010957718 \t range_loss: 0.6542590260505676\n",
      "pred_error: 0.00906805507838726 \t range_loss: 0.6532198190689087\n",
      "pred_error: 0.009283472783863544 \t range_loss: 0.6526501178741455\n",
      "pred_error: 0.009289476089179516 \t range_loss: 0.6528149843215942\n",
      "pred_error: 0.009545192122459412 \t range_loss: 0.64993816614151\n",
      "pred_error: 0.009545049630105495 \t range_loss: 0.64993816614151\n",
      "pred_error: 0.009503558278083801 \t range_loss: 0.648636519908905\n",
      "Step: 161 \t Loss: 0.7426820993423462\n",
      "Step: 162 \t Loss: 0.7416975498199463\n",
      "pred_error: 0.009375158697366714 \t range_loss: 0.6479478478431702\n",
      "pred_error: 0.009524671360850334 \t range_loss: 0.6475080847740173\n",
      "Step: 165 \t Loss: 0.7413416504859924\n",
      "pred_error: 0.009845025837421417 \t range_loss: 0.6459479331970215\n",
      "pred_error: 0.010016185231506824 \t range_loss: 0.6454762816429138\n",
      "pred_error: 0.010009069927036762 \t range_loss: 0.6446516513824463\n",
      "Step: 176 \t Loss: 0.7406759858131409\n",
      "pred_error: 0.009781364351511002 \t range_loss: 0.6428641676902771\n",
      "Step: 177 \t Loss: 0.7396206259727478\n",
      "pred_error: 0.00977516919374466 \t range_loss: 0.6432984471321106\n",
      "pred_error: 0.00990218110382557 \t range_loss: 0.642494797706604\n",
      "pred_error: 0.009966911748051643 \t range_loss: 0.6408196091651917\n",
      "pred_error: 0.010289078578352928 \t range_loss: 0.6407533884048462\n",
      "pred_error: 0.010107787325978279 \t range_loss: 0.6403332352638245\n",
      "Step: 191 \t Loss: 0.738563060760498\n",
      "pred_error: 0.010003939270973206 \t range_loss: 0.6407985091209412\n",
      "pred_error: 0.010229899547994137 \t range_loss: 0.6398040056228638\n",
      "pred_error: 0.010229834355413914 \t range_loss: 0.6398040056228638\n",
      "pred_error: 0.010484977625310421 \t range_loss: 0.6387413740158081\n",
      "pred_error: 0.010381053201854229 \t range_loss: 0.6377007961273193\n",
      "pred_error: 0.010381054133176804 \t range_loss: 0.6377007961273193\n",
      "pred_error: 0.010380970314145088 \t range_loss: 0.6377007961273193\n",
      "pred_error: 0.010586315765976906 \t range_loss: 0.637030303478241\n",
      "pred_error: 0.010647852905094624 \t range_loss: 0.6362623572349548\n",
      "pred_error: 0.010648034512996674 \t range_loss: 0.6362623572349548\n",
      "pred_error: 0.01055224984884262 \t range_loss: 0.6352893114089966\n",
      "pred_error: 0.010567822493612766 \t range_loss: 0.635493814945221\n",
      "pred_error: 0.010832495056092739 \t range_loss: 0.634903073310852\n",
      "pred_error: 0.010805884376168251 \t range_loss: 0.6344645023345947\n",
      "pred_error: 0.010723350569605827 \t range_loss: 0.6335641145706177\n",
      "Step: 216 \t Loss: 0.7376683950424194\n",
      "pred_error: 0.01046648807823658 \t range_loss: 0.6343848705291748\n",
      "pred_error: 0.010721348226070404 \t range_loss: 0.6341837048530579\n",
      "pred_error: 0.010721316561102867 \t range_loss: 0.6341837048530579\n",
      "pred_error: 0.011020942591130733 \t range_loss: 0.6320355534553528\n",
      "pred_error: 0.0110203567892313 \t range_loss: 0.6320355534553528\n",
      "pred_error: 0.011001869104802608 \t range_loss: 0.6322193145751953\n",
      "pred_error: 0.01101245079189539 \t range_loss: 0.6315013766288757\n",
      "pred_error: 0.011063831858336926 \t range_loss: 0.6301459074020386\n",
      "pred_error: 0.010971457697451115 \t range_loss: 0.6280493140220642\n",
      "pred_error: 0.011224796064198017 \t range_loss: 0.6275858879089355\n",
      "pred_error: 0.011361411772668362 \t range_loss: 0.6270519495010376\n",
      "pred_error: 0.01126501988619566 \t range_loss: 0.6266791224479675\n",
      "pred_error: 0.011237461119890213 \t range_loss: 0.6276302933692932\n",
      "pred_error: 0.011270737275481224 \t range_loss: 0.6251259446144104\n",
      "pred_error: 0.011270850896835327 \t range_loss: 0.6251259446144104\n",
      "pred_error: 0.01129462942481041 \t range_loss: 0.6254522800445557\n",
      "pred_error: 0.011323502287268639 \t range_loss: 0.6251044869422913\n",
      "pred_error: 0.011691218242049217 \t range_loss: 0.6255896687507629\n",
      "pred_error: 0.011691820807754993 \t range_loss: 0.6255896687507629\n",
      "pred_error: 0.011612516827881336 \t range_loss: 0.6241227388381958\n",
      "pred_error: 0.011558325961232185 \t range_loss: 0.623203694820404\n",
      "Step: 253 \t Loss: 0.7375650405883789\n",
      "pred_error: 0.01143852062523365 \t range_loss: 0.6231735348701477\n",
      "Step: 259 \t Loss: 0.7373089790344238\n",
      "pred_error: 0.011630292981863022 \t range_loss: 0.6231741309165955\n",
      "pred_error: 0.012153656221926212 \t range_loss: 0.6214455962181091\n",
      "pred_error: 0.0121536273509264 \t range_loss: 0.6214455962181091\n",
      "pred_error: 0.01195905264467001 \t range_loss: 0.6203461289405823\n",
      "pred_error: 0.01196605060249567 \t range_loss: 0.6197996735572815\n",
      "Step: 265 \t Loss: 0.737248957157135\n",
      "pred_error: 0.011770825833082199 \t range_loss: 0.6195404529571533\n",
      "Step: 266 \t Loss: 0.7358878254890442\n",
      "pred_error: 0.012100884690880775 \t range_loss: 0.6193268299102783\n",
      "pred_error: 0.012100559659302235 \t range_loss: 0.6193268299102783\n",
      "pred_error: 0.01216938253492117 \t range_loss: 0.6216180324554443\n",
      "pred_error: 0.012220409698784351 \t range_loss: 0.6197748780250549\n",
      "pred_error: 0.012048451229929924 \t range_loss: 0.618838906288147\n",
      "pred_error: 0.012048429809510708 \t range_loss: 0.618838906288147\n",
      "pred_error: 0.012010150589048862 \t range_loss: 0.6177142858505249\n",
      "pred_error: 0.01221917662769556 \t range_loss: 0.616562008857727\n",
      "pred_error: 0.012339195236563683 \t range_loss: 0.6152665019035339\n",
      "pred_error: 0.01240389235317707 \t range_loss: 0.6146356463432312\n",
      "pred_error: 0.012401829473674297 \t range_loss: 0.6146356463432312\n",
      "pred_error: 0.012677463702857494 \t range_loss: 0.6135332584381104\n",
      "pred_error: 0.012677324004471302 \t range_loss: 0.6135332584381104\n",
      "pred_error: 0.012760991230607033 \t range_loss: 0.6122552156448364\n",
      "pred_error: 0.012761111371219158 \t range_loss: 0.6122552156448364\n",
      "pred_error: 0.012692759744822979 \t range_loss: 0.6134268045425415\n",
      "pred_error: 0.012692618183791637 \t range_loss: 0.6134268045425415\n",
      "pred_error: 0.012651494704186916 \t range_loss: 0.6130204200744629\n",
      "pred_error: 0.012795478105545044 \t range_loss: 0.6118665337562561\n",
      "pred_error: 0.012671426869928837 \t range_loss: 0.6115729808807373\n",
      "pred_error: 0.0126718208193779 \t range_loss: 0.6115729808807373\n",
      "pred_error: 0.012585722841322422 \t range_loss: 0.6111810207366943\n",
      "pred_error: 0.012906472198665142 \t range_loss: 0.6128859519958496\n",
      "pred_error: 0.012578262947499752 \t range_loss: 0.6138591170310974\n",
      "pred_error: 0.012823148630559444 \t range_loss: 0.6110113263130188\n",
      "pred_error: 0.012637744657695293 \t range_loss: 0.6104114055633545\n",
      "pred_error: 0.012886185199022293 \t range_loss: 0.611330509185791\n",
      "pred_error: 0.012953659519553185 \t range_loss: 0.610561728477478\n",
      "pred_error: 0.01308883260935545 \t range_loss: 0.6089420914649963\n",
      "pred_error: 0.01300894282758236 \t range_loss: 0.6088723540306091\n",
      "pred_error: 0.012872095219790936 \t range_loss: 0.6109221577644348\n",
      "pred_error: 0.013288917951285839 \t range_loss: 0.6084429621696472\n",
      "pred_error: 0.013002617284655571 \t range_loss: 0.6086201071739197\n",
      "pred_error: 0.013175548985600471 \t range_loss: 0.6077342629432678\n",
      "pred_error: 0.013175652362406254 \t range_loss: 0.6077342629432678\n",
      "pred_error: 0.013167002238333225 \t range_loss: 0.6078664660453796\n",
      "pred_error: 0.013166932389140129 \t range_loss: 0.6078664660453796\n",
      "pred_error: 0.01299804262816906 \t range_loss: 0.6092425584793091\n",
      "pred_error: 0.01311231218278408 \t range_loss: 0.6076806783676147\n",
      "pred_error: 0.013308374211192131 \t range_loss: 0.6076066493988037\n",
      "pred_error: 0.013308273628354073 \t range_loss: 0.6076066493988037\n",
      "pred_error: 0.013195053674280643 \t range_loss: 0.6077128648757935\n",
      "pred_error: 0.013220181688666344 \t range_loss: 0.6073115468025208\n",
      "pred_error: 0.01323204580694437 \t range_loss: 0.6064826250076294\n",
      "pred_error: 0.013240253552794456 \t range_loss: 0.6071745157241821\n",
      "pred_error: 0.01336201373487711 \t range_loss: 0.6061771512031555\n",
      "pred_error: 0.013129751197993755 \t range_loss: 0.6052058339118958\n",
      "pred_error: 0.01348316203802824 \t range_loss: 0.6059550642967224\n",
      "pred_error: 0.013239898718893528 \t range_loss: 0.6060246825218201\n",
      "pred_error: 0.013278004713356495 \t range_loss: 0.6056088209152222\n",
      "pred_error: 0.013278004713356495 \t range_loss: 0.6056088209152222\n",
      "pred_error: 0.013208223506808281 \t range_loss: 0.6058984398841858\n",
      "pred_error: 0.013270271942019463 \t range_loss: 0.6074648499488831\n",
      "pred_error: 0.013847755268216133 \t range_loss: 0.6054403781890869\n",
      "pred_error: 0.013425321318209171 \t range_loss: 0.6034825444221497\n",
      "pred_error: 0.013596823439002037 \t range_loss: 0.6028593182563782\n",
      "pred_error: 0.013584386557340622 \t range_loss: 0.6031476855278015\n",
      "pred_error: 0.013551723212003708 \t range_loss: 0.6056182980537415\n",
      "pred_error: 0.013563238084316254 \t range_loss: 0.6027901768684387\n",
      "pred_error: 0.013646571896970272 \t range_loss: 0.6034053564071655\n",
      "pred_error: 0.013646695762872696 \t range_loss: 0.6034053564071655\n",
      "pred_error: 0.013796046376228333 \t range_loss: 0.6016473770141602\n",
      "pred_error: 0.01382436417043209 \t range_loss: 0.6000014543533325\n",
      "pred_error: 0.014203407801687717 \t range_loss: 0.5995970368385315\n",
      "pred_error: 0.014024683274328709 \t range_loss: 0.6009558439254761\n",
      "pred_error: 0.014156480319797993 \t range_loss: 0.5990440249443054\n",
      "pred_error: 0.014049503952264786 \t range_loss: 0.599816620349884\n",
      "pred_error: 0.014049840159714222 \t range_loss: 0.599816620349884\n",
      "pred_error: 0.014014133252203465 \t range_loss: 0.6001981496810913\n",
      "pred_error: 0.014014191925525665 \t range_loss: 0.6001981496810913\n",
      "pred_error: 0.014011003077030182 \t range_loss: 0.6001397371292114\n",
      "pred_error: 0.014083980582654476 \t range_loss: 0.6014149188995361\n",
      "pred_error: 0.0141720836982131 \t range_loss: 0.6002328395843506\n",
      "pred_error: 0.014017793349921703 \t range_loss: 0.6007215976715088\n",
      "pred_error: 0.01413493137806654 \t range_loss: 0.5999066829681396\n",
      "pred_error: 0.013921658508479595 \t range_loss: 0.599272608757019\n",
      "pred_error: 0.014130055904388428 \t range_loss: 0.5991377830505371\n",
      "pred_error: 0.014164191670715809 \t range_loss: 0.5992828607559204\n",
      "pred_error: 0.014166343957185745 \t range_loss: 0.5988742113113403\n",
      "pred_error: 0.014428672380745411 \t range_loss: 0.5987458825111389\n",
      "pred_error: 0.014203375205397606 \t range_loss: 0.5972179174423218\n",
      "pred_error: 0.014351487159729004 \t range_loss: 0.5962267518043518\n",
      "pred_error: 0.01420560572296381 \t range_loss: 0.5979794263839722\n",
      "pred_error: 0.014319214969873428 \t range_loss: 0.5972950458526611\n",
      "pred_error: 0.01417002733796835 \t range_loss: 0.5974116921424866\n",
      "pred_error: 0.014169965870678425 \t range_loss: 0.5974116921424866\n",
      "pred_error: 0.01426009088754654 \t range_loss: 0.5971798300743103\n",
      "pred_error: 0.014210215769708157 \t range_loss: 0.5979363918304443\n",
      "pred_error: 0.014301945455372334 \t range_loss: 0.599185585975647\n",
      "pred_error: 0.014307920821011066 \t range_loss: 0.598466157913208\n",
      "pred_error: 0.014340093359351158 \t range_loss: 0.5976592302322388\n",
      "pred_error: 0.014331175945699215 \t range_loss: 0.5977761745452881\n",
      "pred_error: 0.014331345446407795 \t range_loss: 0.5977761745452881\n",
      "pred_error: 0.014331299811601639 \t range_loss: 0.5977761745452881\n",
      "pred_error: 0.014377950690686703 \t range_loss: 0.5960498452186584\n",
      "pred_error: 0.014270659536123276 \t range_loss: 0.5956783294677734\n",
      "pred_error: 0.014461648650467396 \t range_loss: 0.5964584350585938\n",
      "pred_error: 0.014348025433719158 \t range_loss: 0.5976713299751282\n",
      "pred_error: 0.01450737938284874 \t range_loss: 0.5968909859657288\n",
      "pred_error: 0.014507242478430271 \t range_loss: 0.5968909859657288\n",
      "pred_error: 0.014506877399981022 \t range_loss: 0.5968909859657288\n",
      "pred_error: 0.014390113763511181 \t range_loss: 0.5972870588302612\n",
      "pred_error: 0.014386325143277645 \t range_loss: 0.5978299379348755\n",
      "pred_error: 0.014318358153104782 \t range_loss: 0.5969008803367615\n",
      "pred_error: 0.01451217383146286 \t range_loss: 0.595592200756073\n",
      "pred_error: 0.014294213615357876 \t range_loss: 0.5955368280410767\n",
      "pred_error: 0.014280236326158047 \t range_loss: 0.5968204140663147\n",
      "pred_error: 0.014279934577643871 \t range_loss: 0.5968204140663147\n",
      "pred_error: 0.014436324127018452 \t range_loss: 0.5960221290588379\n",
      "pred_error: 0.014415011741220951 \t range_loss: 0.5961034893989563\n",
      "pred_error: 0.01441490650177002 \t range_loss: 0.5961034893989563\n",
      "pred_error: 0.014486994594335556 \t range_loss: 0.5967537760734558\n",
      "pred_error: 0.014463288709521294 \t range_loss: 0.5967622995376587\n",
      "pred_error: 0.014465906657278538 \t range_loss: 0.5958425998687744\n",
      "pred_error: 0.014456886798143387 \t range_loss: 0.5954310894012451\n",
      "pred_error: 0.014602281153202057 \t range_loss: 0.5960269570350647\n",
      "pred_error: 0.014314144849777222 \t range_loss: 0.5979084968566895\n",
      "pred_error: 0.0146565530449152 \t range_loss: 0.5958123803138733\n",
      "pred_error: 0.014694769866764545 \t range_loss: 0.595611035823822\n",
      "pred_error: 0.014635021798312664 \t range_loss: 0.5952140688896179\n",
      "pred_error: 0.014635017141699791 \t range_loss: 0.5952140688896179\n",
      "pred_error: 0.014494186267256737 \t range_loss: 0.5943934321403503\n",
      "pred_error: 0.014915425330400467 \t range_loss: 0.594392716884613\n",
      "pred_error: 0.014915362000465393 \t range_loss: 0.594392716884613\n",
      "pred_error: 0.014753242023289204 \t range_loss: 0.5937996506690979\n",
      "pred_error: 0.014565341174602509 \t range_loss: 0.5963733792304993\n",
      "pred_error: 0.014919474720954895 \t range_loss: 0.5922366380691528\n",
      "pred_error: 0.014890133403241634 \t range_loss: 0.5918223857879639\n",
      "pred_error: 0.014816971495747566 \t range_loss: 0.5920633673667908\n",
      "pred_error: 0.015035377815365791 \t range_loss: 0.59121173620224\n",
      "pred_error: 0.015024518594145775 \t range_loss: 0.5912612676620483\n",
      "pred_error: 0.015192522667348385 \t range_loss: 0.5903387069702148\n",
      "pred_error: 0.015192066319286823 \t range_loss: 0.5903387069702148\n",
      "pred_error: 0.015173686668276787 \t range_loss: 0.590038001537323\n",
      "pred_error: 0.015200253576040268 \t range_loss: 0.5888335704803467\n",
      "pred_error: 0.015071762725710869 \t range_loss: 0.5895190834999084\n",
      "pred_error: 0.015222063288092613 \t range_loss: 0.590229332447052\n",
      "pred_error: 0.015388227999210358 \t range_loss: 0.5897672176361084\n",
      "pred_error: 0.01538883801549673 \t range_loss: 0.5894507765769958\n",
      "pred_error: 0.015159606002271175 \t range_loss: 0.590178370475769\n",
      "pred_error: 0.01515963301062584 \t range_loss: 0.590178370475769\n",
      "pred_error: 0.015185357071459293 \t range_loss: 0.5889140963554382\n",
      "pred_error: 0.015185318887233734 \t range_loss: 0.5889140963554382\n",
      "pred_error: 0.015439143404364586 \t range_loss: 0.588241457939148\n",
      "pred_error: 0.01543908379971981 \t range_loss: 0.588241457939148\n",
      "pred_error: 0.015414300374686718 \t range_loss: 0.5871677994728088\n",
      "pred_error: 0.01531689241528511 \t range_loss: 0.5877144932746887\n",
      "pred_error: 0.01558482926338911 \t range_loss: 0.5901397466659546\n",
      "pred_error: 0.01558496430516243 \t range_loss: 0.5901397466659546\n",
      "pred_error: 0.01533882413059473 \t range_loss: 0.5875644683837891\n",
      "pred_error: 0.015450597740709782 \t range_loss: 0.5912263989448547\n",
      "pred_error: 0.01562422327697277 \t range_loss: 0.5897422432899475\n",
      "pred_error: 0.015336478129029274 \t range_loss: 0.5892080664634705\n",
      "pred_error: 0.015326233580708504 \t range_loss: 0.5884779095649719\n",
      "pred_error: 0.015326826833188534 \t range_loss: 0.5884779095649719\n",
      "pred_error: 0.015326841734349728 \t range_loss: 0.5884779095649719\n",
      "pred_error: 0.015139657072722912 \t range_loss: 0.5901273488998413\n",
      "pred_error: 0.015336478129029274 \t range_loss: 0.5882918238639832\n",
      "pred_error: 0.015420201234519482 \t range_loss: 0.587411105632782\n",
      "pred_error: 0.015170427039265633 \t range_loss: 0.5871737003326416\n",
      "pred_error: 0.015417678281664848 \t range_loss: 0.5878323316574097\n",
      "pred_error: 0.015349030494689941 \t range_loss: 0.5868827104568481\n",
      "pred_error: 0.0154170086607337 \t range_loss: 0.5885753631591797\n",
      "pred_error: 0.01549291331321001 \t range_loss: 0.5868552923202515\n",
      "pred_error: 0.015492583625018597 \t range_loss: 0.5868552923202515\n",
      "pred_error: 0.01558738574385643 \t range_loss: 0.5856637954711914\n",
      "pred_error: 0.01558634452521801 \t range_loss: 0.5856637954711914\n",
      "pred_error: 0.015720775350928307 \t range_loss: 0.5873839855194092\n",
      "pred_error: 0.015767375007271767 \t range_loss: 0.5856771469116211\n",
      "pred_error: 0.015856867656111717 \t range_loss: 0.5865095257759094\n",
      "pred_error: 0.015857188031077385 \t range_loss: 0.5865095257759094\n",
      "pred_error: 0.015868380665779114 \t range_loss: 0.5861742496490479\n",
      "pred_error: 0.015476838685572147 \t range_loss: 0.5847434997558594\n",
      "pred_error: 0.01586333103477955 \t range_loss: 0.5845580101013184\n",
      "pred_error: 0.01615217514336109 \t range_loss: 0.5854744911193848\n",
      "pred_error: 0.01575397700071335 \t range_loss: 0.5833304524421692\n",
      "pred_error: 0.015766343101859093 \t range_loss: 0.5835721492767334\n",
      "pred_error: 0.015766343101859093 \t range_loss: 0.5835721492767334\n",
      "pred_error: 0.016067929565906525 \t range_loss: 0.5845454931259155\n",
      "pred_error: 0.01618211902678013 \t range_loss: 0.5845469832420349\n",
      "pred_error: 0.016009610146284103 \t range_loss: 0.5836970210075378\n",
      "pred_error: 0.016009584069252014 \t range_loss: 0.5836970210075378\n",
      "pred_error: 0.015828441828489304 \t range_loss: 0.5845954418182373\n",
      "pred_error: 0.0157551821321249 \t range_loss: 0.584097146987915\n",
      "pred_error: 0.015953563153743744 \t range_loss: 0.5835181474685669\n",
      "pred_error: 0.015884393826127052 \t range_loss: 0.583564043045044\n",
      "pred_error: 0.015949465334415436 \t range_loss: 0.5835022926330566\n",
      "pred_error: 0.01599673181772232 \t range_loss: 0.5839152336120605\n",
      "pred_error: 0.016012107953429222 \t range_loss: 0.5833054184913635\n",
      "pred_error: 0.016170203685760498 \t range_loss: 0.5826582312583923\n",
      "pred_error: 0.016123177483677864 \t range_loss: 0.582267701625824\n",
      "pred_error: 0.015913359820842743 \t range_loss: 0.5830869078636169\n",
      "pred_error: 0.016115669161081314 \t range_loss: 0.5842291116714478\n",
      "pred_error: 0.016115641221404076 \t range_loss: 0.5842291116714478\n",
      "pred_error: 0.016201414167881012 \t range_loss: 0.5839048027992249\n",
      "pred_error: 0.016229288652539253 \t range_loss: 0.583026111125946\n",
      "pred_error: 0.016230043023824692 \t range_loss: 0.583026111125946\n",
      "pred_error: 0.016074612736701965 \t range_loss: 0.5818248391151428\n",
      "pred_error: 0.016074689105153084 \t range_loss: 0.5818248391151428\n",
      "pred_error: 0.015933414921164513 \t range_loss: 0.5816589593887329\n",
      "pred_error: 0.015933357179164886 \t range_loss: 0.5816589593887329\n",
      "pred_error: 0.015962734818458557 \t range_loss: 0.5827910304069519\n",
      "pred_error: 0.015989435836672783 \t range_loss: 0.5837210416793823\n",
      "pred_error: 0.01612897962331772 \t range_loss: 0.5834800004959106\n",
      "pred_error: 0.016128920018672943 \t range_loss: 0.5834800004959106\n",
      "pred_error: 0.01599670574069023 \t range_loss: 0.5835567116737366\n",
      "pred_error: 0.016116052865982056 \t range_loss: 0.5833185911178589\n",
      "pred_error: 0.01611609198153019 \t range_loss: 0.5833185911178589\n",
      "pred_error: 0.01608094945549965 \t range_loss: 0.582139790058136\n",
      "pred_error: 0.01608096808195114 \t range_loss: 0.582139790058136\n",
      "pred_error: 0.016018809750676155 \t range_loss: 0.5817804336547852\n",
      "pred_error: 0.016252649948000908 \t range_loss: 0.5829734206199646\n",
      "pred_error: 0.01622929237782955 \t range_loss: 0.5838047862052917\n",
      "pred_error: 0.01617507077753544 \t range_loss: 0.5825098156929016\n",
      "pred_error: 0.01605900190770626 \t range_loss: 0.5819423198699951\n",
      "pred_error: 0.01614200323820114 \t range_loss: 0.5827339291572571\n",
      "pred_error: 0.01614203490316868 \t range_loss: 0.5827339291572571\n",
      "pred_error: 0.01600339449942112 \t range_loss: 0.5826178789138794\n",
      "pred_error: 0.016003096476197243 \t range_loss: 0.5826178789138794\n",
      "pred_error: 0.01626081019639969 \t range_loss: 0.5811434388160706\n",
      "pred_error: 0.01620698906481266 \t range_loss: 0.5823230147361755\n",
      "pred_error: 0.01635884679853916 \t range_loss: 0.5829495787620544\n",
      "pred_error: 0.016505593433976173 \t range_loss: 0.5807339549064636\n",
      "pred_error: 0.016299394890666008 \t range_loss: 0.580129861831665\n",
      "pred_error: 0.01630038395524025 \t range_loss: 0.580129861831665\n",
      "pred_error: 0.016528887674212456 \t range_loss: 0.5807691216468811\n",
      "pred_error: 0.016499584540724754 \t range_loss: 0.5809696316719055\n",
      "pred_error: 0.01638571172952652 \t range_loss: 0.5808502435684204\n",
      "pred_error: 0.01626788079738617 \t range_loss: 0.5801975131034851\n",
      "pred_error: 0.016262400895357132 \t range_loss: 0.5826756358146667\n",
      "pred_error: 0.016406072303652763 \t range_loss: 0.5800962448120117\n",
      "pred_error: 0.016404297202825546 \t range_loss: 0.5797508358955383\n",
      "pred_error: 0.016312912106513977 \t range_loss: 0.5808117389678955\n",
      "pred_error: 0.016415627673268318 \t range_loss: 0.5808410048484802\n",
      "pred_error: 0.01641615480184555 \t range_loss: 0.5808410048484802\n",
      "BEST LOSS: 0.7358878\n",
      "==== Model: block0_cob_activation_norm  in Layer: 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 12:20:04,547 execute.rs:1044 num calibration batches: 1\n",
      "INFO tract_linalg.arm64 2024-09-15 12:20:04,567 arm64.rs:212 CPU optimisation: AppleM\n",
      "INFO tract_linalg.arm64 2024-09-15 12:20:04,568 arm64.rs:294 ARMv8.2 mmm_f16 and mmv_f16 activated\n",
      "INFO tract_linalg.arm64 2024-09-15 12:20:04,570 arm64.rs:315 ARMv8.2 tanh_f16 and sigmoid_f16 activated\n",
      "INFO tract_linalg.arm64 2024-09-15 12:20:04,571 arm64.rs:328 AMX optimisation activated\n",
      "ERROR ezkl.graph.model 2024-09-15 12:22:13,293 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 12:22:13,299 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 12:22:34,418 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 12:22:34,424 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 12:22:34,442 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 12:22:34,462 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 12:22:34,475 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error     | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000000483782 | 0.00004838407 | 0.00029587746 | -0.0002837777 | 0.00004309618  | 0.00004838407    | 0.00029587746 | 0             | 0.000000003106742  | 0.00026246207      | 0.0009010576           |\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 644820 64 [-635610, 576214] 1 [16]\n",
      "===============================\n",
      "==== Model: block0_cob_activation_norm_teleported  in Layer: 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 12:22:55,783 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 12:25:02,840 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 12:25:02,846 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 12:25:23,466 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 12:25:23,471 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 12:25:23,493 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 12:25:23,515 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 12:25:23,529 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.00000019549545 | -0.000022426248 | 0.00029560924 | -0.00025856495 | 0.000043380584 | 0.000022426248   | 0.00029560924 | 0             | 0.0000000031457021 | 0.00003521087      | 0.0006645845           |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 644823 64 [-411124, 291680] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 1 , \t  activation_stats: {'relu_1': {'norm': tensor(319.0262), 'max': tensor(6.5916), 'min': tensor(-4.5777), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(11.1694)\n",
      "Step: 0 \t Loss: 0.9807444214820862\n",
      "pred_error: 0.0024482784792780876 \t range_loss: 0.9562616348266602\n",
      "pred_error: 0.002448328537866473 \t range_loss: 0.9562616348266602\n",
      "Step: 1 \t Loss: 0.9294413328170776\n",
      "Step: 2 \t Loss: 0.8871041536331177\n",
      "Step: 3 \t Loss: 0.8467554450035095\n",
      "pred_error: 0.0009139413014054298 \t range_loss: 0.8376167416572571\n",
      "Step: 4 \t Loss: 0.8030611872673035\n",
      "Step: 5 \t Loss: 0.7834885120391846\n",
      "Step: 6 \t Loss: 0.7692751884460449\n",
      "Step: 7 \t Loss: 0.767705500125885\n",
      "Step: 8 \t Loss: 0.7564604878425598\n",
      "pred_error: 0.001065212651155889 \t range_loss: 0.745802104473114\n",
      "Step: 9 \t Loss: 0.7469931244850159\n",
      "pred_error: 0.0011322301579639316 \t range_loss: 0.735670804977417\n",
      "Step: 10 \t Loss: 0.744818925857544\n",
      "Step: 11 \t Loss: 0.7348767518997192\n",
      "Step: 12 \t Loss: 0.7223331332206726\n",
      "Step: 13 \t Loss: 0.7165736556053162\n",
      "pred_error: 0.0016589784063398838 \t range_loss: 0.7012101411819458\n",
      "pred_error: 0.0016584473196417093 \t range_loss: 0.7012101411819458\n",
      "Step: 15 \t Loss: 0.713829517364502\n",
      "Step: 16 \t Loss: 0.7102242708206177\n",
      "Step: 17 \t Loss: 0.7074376940727234\n",
      "Step: 18 \t Loss: 0.7043792605400085\n",
      "Step: 20 \t Loss: 0.7011570930480957\n",
      "Step: 21 \t Loss: 0.6998950242996216\n",
      "pred_error: 0.002199584851041436 \t range_loss: 0.6778980493545532\n",
      "Step: 22 \t Loss: 0.6975765228271484\n",
      "pred_error: 0.0022535822354257107 \t range_loss: 0.675042450428009\n",
      "pred_error: 0.0022533638402819633 \t range_loss: 0.675042450428009\n",
      "Step: 23 \t Loss: 0.6926772594451904\n",
      "pred_error: 0.0023091158363968134 \t range_loss: 0.6695861220359802\n",
      "pred_error: 0.002290133386850357 \t range_loss: 0.6702169179916382\n",
      "pred_error: 0.0022904451470822096 \t range_loss: 0.6702169179916382\n",
      "Step: 25 \t Loss: 0.6914239525794983\n",
      "Step: 27 \t Loss: 0.6896468997001648\n",
      "Step: 28 \t Loss: 0.6863309741020203\n",
      "Step: 29 \t Loss: 0.6849570274353027\n",
      "pred_error: 0.0026202115695923567 \t range_loss: 0.6587549448013306\n",
      "Step: 30 \t Loss: 0.6825107336044312\n",
      "Step: 31 \t Loss: 0.6814500093460083\n",
      "Step: 32 \t Loss: 0.6808673739433289\n",
      "Step: 33 \t Loss: 0.6794034242630005\n",
      "Step: 34 \t Loss: 0.6790241003036499\n",
      "Step: 35 \t Loss: 0.6780623197555542\n",
      "pred_error: 0.00301651144400239 \t range_loss: 0.6478971838951111\n",
      "Step: 38 \t Loss: 0.6765112280845642\n",
      "Step: 39 \t Loss: 0.6741671562194824\n",
      "pred_error: 0.0031972252763807774 \t range_loss: 0.6421950459480286\n",
      "Step: 40 \t Loss: 0.6701967716217041\n",
      "pred_error: 0.0032708654180169106 \t range_loss: 0.6374902725219727\n",
      "Step: 41 \t Loss: 0.6699948310852051\n",
      "Step: 43 \t Loss: 0.6697825789451599\n",
      "Step: 44 \t Loss: 0.6694616675376892\n",
      "Step: 45 \t Loss: 0.6686689853668213\n",
      "Step: 46 \t Loss: 0.6668352484703064\n",
      "pred_error: 0.003545894520357251 \t range_loss: 0.6313802003860474\n",
      "Step: 48 \t Loss: 0.6654295325279236\n",
      "pred_error: 0.003664765041321516 \t range_loss: 0.628781795501709\n",
      "Step: 49 \t Loss: 0.6648326516151428\n",
      "Step: 51 \t Loss: 0.6639174222946167\n",
      "pred_error: 0.003864928148686886 \t range_loss: 0.6252685189247131\n",
      "Step: 52 \t Loss: 0.6624491810798645\n",
      "Step: 53 \t Loss: 0.6602554321289062\n",
      "pred_error: 0.0038447408005595207 \t range_loss: 0.6218067407608032\n",
      "pred_error: 0.0038450038991868496 \t range_loss: 0.6218067407608032\n",
      "Step: 54 \t Loss: 0.6599336862564087\n",
      "Step: 55 \t Loss: 0.6595270037651062\n",
      "pred_error: 0.0043007684871554375 \t range_loss: 0.6174433827400208\n",
      "Step: 59 \t Loss: 0.6568195223808289\n",
      "pred_error: 0.004227671772241592 \t range_loss: 0.6145424246788025\n",
      "pred_error: 0.004227899480611086 \t range_loss: 0.6145424246788025\n",
      "Step: 60 \t Loss: 0.6567404866218567\n",
      "Step: 61 \t Loss: 0.6559375524520874\n",
      "Step: 62 \t Loss: 0.655314028263092\n",
      "Step: 63 \t Loss: 0.6540483832359314\n",
      "pred_error: 0.004188177641481161 \t range_loss: 0.612164318561554\n",
      "Step: 64 \t Loss: 0.6539485454559326\n",
      "Step: 65 \t Loss: 0.6531426310539246\n",
      "pred_error: 0.004510131198912859 \t range_loss: 0.6081804633140564\n",
      "Step: 67 \t Loss: 0.6525241732597351\n",
      "Step: 68 \t Loss: 0.6516871452331543\n",
      "pred_error: 0.004486463963985443 \t range_loss: 0.6068220138549805\n",
      "Step: 69 \t Loss: 0.6516745090484619\n",
      "pred_error: 0.004593151155859232 \t range_loss: 0.6057422161102295\n",
      "pred_error: 0.004599482286721468 \t range_loss: 0.6057901382446289\n",
      "Step: 71 \t Loss: 0.6516402959823608\n",
      "Step: 72 \t Loss: 0.6506165266036987\n",
      "Step: 73 \t Loss: 0.6491016745567322\n",
      "pred_error: 0.0047003584913909435 \t range_loss: 0.6020981073379517\n",
      "Step: 76 \t Loss: 0.6475529074668884\n",
      "Step: 77 \t Loss: 0.6467745304107666\n",
      "pred_error: 0.004772352520376444 \t range_loss: 0.5990518927574158\n",
      "pred_error: 0.004772236105054617 \t range_loss: 0.5990518927574158\n",
      "pred_error: 0.0048646461218595505 \t range_loss: 0.5993961095809937\n",
      "pred_error: 0.004958116449415684 \t range_loss: 0.599177360534668\n",
      "pred_error: 0.0050010052509605885 \t range_loss: 0.5986472368240356\n",
      "Step: 83 \t Loss: 0.6460866928100586\n",
      "Step: 84 \t Loss: 0.6455795168876648\n",
      "Step: 85 \t Loss: 0.6448544263839722\n",
      "pred_error: 0.00501118041574955 \t range_loss: 0.5947424173355103\n",
      "pred_error: 0.00501128938049078 \t range_loss: 0.5947424173355103\n",
      "Step: 86 \t Loss: 0.6444723010063171\n",
      "Step: 87 \t Loss: 0.6435617208480835\n",
      "pred_error: 0.005263398867100477 \t range_loss: 0.5920690894126892\n",
      "pred_error: 0.005438620690256357 \t range_loss: 0.5904843807220459\n",
      "Step: 92 \t Loss: 0.6434115767478943\n",
      "pred_error: 0.005369721911847591 \t range_loss: 0.5897093415260315\n",
      "Step: 93 \t Loss: 0.6411844491958618\n",
      "pred_error: 0.005234838463366032 \t range_loss: 0.5888357162475586\n",
      "pred_error: 0.005235082004219294 \t range_loss: 0.5888357162475586\n",
      "pred_error: 0.005379680078476667 \t range_loss: 0.5888053178787231\n",
      "Step: 100 \t Loss: 0.6392894983291626\n",
      "Step: 101 \t Loss: 0.6381147503852844\n",
      "pred_error: 0.0055551230907440186 \t range_loss: 0.5828542709350586\n",
      "pred_error: 0.005860318895429373 \t range_loss: 0.5833261609077454\n",
      "pred_error: 0.005820338148623705 \t range_loss: 0.5823137760162354\n",
      "pred_error: 0.005798704922199249 \t range_loss: 0.5816483497619629\n",
      "pred_error: 0.00590034294873476 \t range_loss: 0.5795978903770447\n",
      "Step: 110 \t Loss: 0.6372896432876587\n",
      "Step: 111 \t Loss: 0.637119710445404\n",
      "pred_error: 0.005976621061563492 \t range_loss: 0.5779463648796082\n",
      "Step: 114 \t Loss: 0.636662483215332\n",
      "pred_error: 0.006198610179126263 \t range_loss: 0.5755805373191833\n",
      "pred_error: 0.0062948549166321754 \t range_loss: 0.5739343762397766\n",
      "Step: 121 \t Loss: 0.6363372802734375\n",
      "Step: 123 \t Loss: 0.6357779502868652\n",
      "pred_error: 0.006291672121733427 \t range_loss: 0.5728611350059509\n",
      "pred_error: 0.006291520781815052 \t range_loss: 0.5728611350059509\n",
      "Step: 124 \t Loss: 0.6349906921386719\n",
      "pred_error: 0.006378638558089733 \t range_loss: 0.5730662941932678\n",
      "pred_error: 0.006454181857407093 \t range_loss: 0.5725182294845581\n",
      "pred_error: 0.006935169454663992 \t range_loss: 0.5711052417755127\n",
      "Step: 131 \t Loss: 0.6341359615325928\n",
      "Step: 132 \t Loss: 0.6335700154304504\n",
      "pred_error: 0.006402270868420601 \t range_loss: 0.5695477724075317\n",
      "Step: 133 \t Loss: 0.6327654123306274\n",
      "pred_error: 0.006693169008940458 \t range_loss: 0.5671578645706177\n",
      "pred_error: 0.0066842674277722836 \t range_loss: 0.5662143230438232\n",
      "pred_error: 0.00716735702008009 \t range_loss: 0.56640625\n",
      "pred_error: 0.006893927231431007 \t range_loss: 0.5654045939445496\n",
      "Step: 149 \t Loss: 0.6320505738258362\n",
      "Step: 150 \t Loss: 0.6309309005737305\n",
      "pred_error: 0.007026953157037497 \t range_loss: 0.5606613755226135\n",
      "pred_error: 0.007026953157037497 \t range_loss: 0.5606613755226135\n",
      "pred_error: 0.007018969859927893 \t range_loss: 0.5608707666397095\n",
      "pred_error: 0.0072637624107301235 \t range_loss: 0.5614950060844421\n",
      "pred_error: 0.007263492792844772 \t range_loss: 0.5614950060844421\n",
      "Step: 155 \t Loss: 0.6306232213973999\n",
      "pred_error: 0.007143798749893904 \t range_loss: 0.5591832995414734\n",
      "Step: 157 \t Loss: 0.6303415894508362\n",
      "pred_error: 0.007138997316360474 \t range_loss: 0.558952271938324\n",
      "pred_error: 0.007138888817280531 \t range_loss: 0.558952271938324\n",
      "Step: 158 \t Loss: 0.6301247477531433\n",
      "pred_error: 0.007227854337543249 \t range_loss: 0.5586469769477844\n",
      "Step: 161 \t Loss: 0.6296282410621643\n",
      "pred_error: 0.007167079485952854 \t range_loss: 0.5579587817192078\n",
      "pred_error: 0.007740493398159742 \t range_loss: 0.5578709840774536\n",
      "pred_error: 0.007740492466837168 \t range_loss: 0.5578709840774536\n",
      "pred_error: 0.00774047477170825 \t range_loss: 0.5578709840774536\n",
      "pred_error: 0.007221083156764507 \t range_loss: 0.55814528465271\n",
      "pred_error: 0.007376800756901503 \t range_loss: 0.5571138262748718\n",
      "pred_error: 0.007510692346841097 \t range_loss: 0.5552189946174622\n",
      "Step: 174 \t Loss: 0.6292515397071838\n",
      "Step: 175 \t Loss: 0.6290836334228516\n",
      "Step: 178 \t Loss: 0.6290715932846069\n",
      "pred_error: 0.007521436084061861 \t range_loss: 0.5538595914840698\n",
      "Step: 179 \t Loss: 0.6286944150924683\n",
      "pred_error: 0.007628330960869789 \t range_loss: 0.5545462369918823\n",
      "pred_error: 0.007676134817302227 \t range_loss: 0.5534154176712036\n",
      "pred_error: 0.0076761082746088505 \t range_loss: 0.5534154176712036\n",
      "pred_error: 0.0077110761776566505 \t range_loss: 0.5527935028076172\n",
      "pred_error: 0.007681619375944138 \t range_loss: 0.5527281761169434\n",
      "Step: 185 \t Loss: 0.6286261677742004\n",
      "pred_error: 0.0076527646742761135 \t range_loss: 0.5520977973937988\n",
      "Step: 186 \t Loss: 0.6284536123275757\n",
      "Step: 187 \t Loss: 0.6270486116409302\n",
      "pred_error: 0.007598872762173414 \t range_loss: 0.551059365272522\n",
      "Step: 188 \t Loss: 0.6267310380935669\n",
      "pred_error: 0.00757948774844408 \t range_loss: 0.5515459179878235\n",
      "pred_error: 0.007905920967459679 \t range_loss: 0.5515430569648743\n",
      "pred_error: 0.007905877195298672 \t range_loss: 0.5515430569648743\n",
      "pred_error: 0.007980577647686005 \t range_loss: 0.5511491298675537\n",
      "pred_error: 0.007980615831911564 \t range_loss: 0.5511491298675537\n",
      "Step: 197 \t Loss: 0.6261232495307922\n",
      "Step: 199 \t Loss: 0.6260629296302795\n",
      "pred_error: 0.007898153737187386 \t range_loss: 0.547081470489502\n",
      "pred_error: 0.007898225449025631 \t range_loss: 0.547081470489502\n",
      "Step: 200 \t Loss: 0.6259317994117737\n",
      "pred_error: 0.007929165847599506 \t range_loss: 0.5475199222564697\n",
      "pred_error: 0.008155671879649162 \t range_loss: 0.5467311143875122\n",
      "pred_error: 0.008078247308731079 \t range_loss: 0.5467485785484314\n",
      "pred_error: 0.008210500702261925 \t range_loss: 0.545867383480072\n",
      "pred_error: 0.008210507221519947 \t range_loss: 0.545867383480072\n",
      "Step: 211 \t Loss: 0.6258898973464966\n",
      "pred_error: 0.008119958452880383 \t range_loss: 0.54497230052948\n",
      "pred_error: 0.008119974285364151 \t range_loss: 0.54497230052948\n",
      "pred_error: 0.008279520086944103 \t range_loss: 0.5438144207000732\n",
      "pred_error: 0.008170315995812416 \t range_loss: 0.5442062020301819\n",
      "pred_error: 0.008170511573553085 \t range_loss: 0.5442062020301819\n",
      "pred_error: 0.008246669545769691 \t range_loss: 0.5447620749473572\n",
      "pred_error: 0.008375909179449081 \t range_loss: 0.5434886813163757\n",
      "pred_error: 0.008949036709964275 \t range_loss: 0.5434302091598511\n",
      "pred_error: 0.00894944928586483 \t range_loss: 0.5434302091598511\n",
      "pred_error: 0.008420663885772228 \t range_loss: 0.5426880121231079\n",
      "Step: 224 \t Loss: 0.6256793737411499\n",
      "pred_error: 0.00838937982916832 \t range_loss: 0.5425893068313599\n",
      "Step: 228 \t Loss: 0.6256474852561951\n",
      "pred_error: 0.008412943221628666 \t range_loss: 0.5415141582489014\n",
      "pred_error: 0.008407389745116234 \t range_loss: 0.5416993498802185\n",
      "Step: 230 \t Loss: 0.624885082244873\n",
      "pred_error: 0.008445100858807564 \t range_loss: 0.54097980260849\n",
      "pred_error: 0.008765940554440022 \t range_loss: 0.5413377285003662\n",
      "pred_error: 0.008700210601091385 \t range_loss: 0.5404152274131775\n",
      "pred_error: 0.008700236678123474 \t range_loss: 0.5404152274131775\n",
      "pred_error: 0.008602297864854336 \t range_loss: 0.5414747595787048\n",
      "pred_error: 0.008688058704137802 \t range_loss: 0.5408259630203247\n",
      "pred_error: 0.008687853813171387 \t range_loss: 0.5408259630203247\n",
      "Step: 237 \t Loss: 0.6245149374008179\n",
      "pred_error: 0.008815609849989414 \t range_loss: 0.5385047793388367\n",
      "pred_error: 0.008662212640047073 \t range_loss: 0.5381116271018982\n",
      "Step: 244 \t Loss: 0.6239416003227234\n",
      "pred_error: 0.008793017826974392 \t range_loss: 0.5385071039199829\n",
      "pred_error: 0.008782858960330486 \t range_loss: 0.5378626585006714\n",
      "pred_error: 0.008709119632840157 \t range_loss: 0.5374885201454163\n",
      "Step: 251 \t Loss: 0.6238486766815186\n",
      "pred_error: 0.009397288784384727 \t range_loss: 0.5372072458267212\n",
      "pred_error: 0.009396483190357685 \t range_loss: 0.5372072458267212\n",
      "pred_error: 0.008826698176562786 \t range_loss: 0.5363019108772278\n",
      "pred_error: 0.009035572409629822 \t range_loss: 0.5359603762626648\n",
      "pred_error: 0.009748226031661034 \t range_loss: 0.5359430313110352\n",
      "pred_error: 0.009039151482284069 \t range_loss: 0.5359313488006592\n",
      "Step: 264 \t Loss: 0.6237512826919556\n",
      "pred_error: 0.008917643688619137 \t range_loss: 0.5345742106437683\n",
      "pred_error: 0.008917898871004581 \t range_loss: 0.5345742106437683\n",
      "Step: 266 \t Loss: 0.6235145330429077\n",
      "pred_error: 0.009027414955198765 \t range_loss: 0.5332394242286682\n",
      "Step: 267 \t Loss: 0.6228889226913452\n",
      "pred_error: 0.009016522206366062 \t range_loss: 0.5340235829353333\n",
      "pred_error: 0.009863881394267082 \t range_loss: 0.5330744385719299\n",
      "pred_error: 0.009202580899000168 \t range_loss: 0.5334357619285583\n",
      "pred_error: 0.009202392771840096 \t range_loss: 0.5334357619285583\n",
      "pred_error: 0.00920812040567398 \t range_loss: 0.5338878631591797\n",
      "pred_error: 0.009122257120907307 \t range_loss: 0.5319601893424988\n",
      "pred_error: 0.009122258052229881 \t range_loss: 0.5319601893424988\n",
      "pred_error: 0.00912250205874443 \t range_loss: 0.5319601893424988\n",
      "pred_error: 0.009191435761749744 \t range_loss: 0.5328137278556824\n",
      "pred_error: 0.009274525567889214 \t range_loss: 0.53274005651474\n",
      "pred_error: 0.009274535812437534 \t range_loss: 0.53274005651474\n",
      "pred_error: 0.009256215766072273 \t range_loss: 0.5323646664619446\n",
      "pred_error: 0.009221418760716915 \t range_loss: 0.5327383875846863\n",
      "pred_error: 0.009221351705491543 \t range_loss: 0.5327383875846863\n",
      "pred_error: 0.01009406242519617 \t range_loss: 0.5320869088172913\n",
      "pred_error: 0.009323112666606903 \t range_loss: 0.5308082103729248\n",
      "pred_error: 0.009292595088481903 \t range_loss: 0.5304116606712341\n",
      "pred_error: 0.009292620234191418 \t range_loss: 0.5304116606712341\n",
      "pred_error: 0.009276636876165867 \t range_loss: 0.5302631258964539\n",
      "pred_error: 0.009298058226704597 \t range_loss: 0.5304346680641174\n",
      "pred_error: 0.009298112243413925 \t range_loss: 0.5304346680641174\n",
      "pred_error: 0.009281846694648266 \t range_loss: 0.5306780934333801\n",
      "pred_error: 0.00951279979199171 \t range_loss: 0.5293287634849548\n",
      "pred_error: 0.009328627027571201 \t range_loss: 0.5305705666542053\n",
      "Step: 307 \t Loss: 0.6222004890441895\n",
      "pred_error: 0.009455404244363308 \t range_loss: 0.5282280445098877\n",
      "pred_error: 0.009420237503945827 \t range_loss: 0.5288671255111694\n",
      "pred_error: 0.009559225291013718 \t range_loss: 0.5272584557533264\n",
      "pred_error: 0.009456907398998737 \t range_loss: 0.5276681780815125\n",
      "Step: 319 \t Loss: 0.6221630573272705\n",
      "pred_error: 0.009511559270322323 \t range_loss: 0.5270469188690186\n",
      "pred_error: 0.009511797688901424 \t range_loss: 0.5270469188690186\n",
      "pred_error: 0.009511617012321949 \t range_loss: 0.5270469188690186\n",
      "Step: 320 \t Loss: 0.6216211318969727\n",
      "pred_error: 0.009690382517874241 \t range_loss: 0.5269429683685303\n",
      "pred_error: 0.009720761328935623 \t range_loss: 0.5269722938537598\n",
      "pred_error: 0.009720760397613049 \t range_loss: 0.5269722938537598\n",
      "pred_error: 0.009673595428466797 \t range_loss: 0.5266808867454529\n",
      "pred_error: 0.009689835831522942 \t range_loss: 0.525236964225769\n",
      "pred_error: 0.009689973667263985 \t range_loss: 0.525236964225769\n",
      "pred_error: 0.010467653162777424 \t range_loss: 0.5251711010932922\n",
      "pred_error: 0.00984003022313118 \t range_loss: 0.5244637131690979\n",
      "Step: 331 \t Loss: 0.6215054988861084\n",
      "Step: 332 \t Loss: 0.6213456392288208\n",
      "pred_error: 0.009717443957924843 \t range_loss: 0.5241711735725403\n",
      "Step: 333 \t Loss: 0.6212459802627563\n",
      "Step: 334 \t Loss: 0.6212049126625061\n",
      "pred_error: 0.009702097624540329 \t range_loss: 0.5241810083389282\n",
      "pred_error: 0.009702499024569988 \t range_loss: 0.5241810083389282\n",
      "pred_error: 0.009873542934656143 \t range_loss: 0.5243580937385559\n",
      "pred_error: 0.009956845082342625 \t range_loss: 0.5255947709083557\n",
      "pred_error: 0.009890739805996418 \t range_loss: 0.5242325067520142\n",
      "pred_error: 0.010779006406664848 \t range_loss: 0.523681104183197\n",
      "Step: 346 \t Loss: 0.6211205720901489\n",
      "pred_error: 0.009804942645132542 \t range_loss: 0.5230706334114075\n",
      "pred_error: 0.009805027395486832 \t range_loss: 0.5230706334114075\n",
      "Step: 347 \t Loss: 0.6205735206604004\n",
      "pred_error: 0.009843245148658752 \t range_loss: 0.5233327150344849\n",
      "pred_error: 0.009944107383489609 \t range_loss: 0.5237734913825989\n",
      "pred_error: 0.010065324604511261 \t range_loss: 0.5223023891448975\n",
      "pred_error: 0.009912610054016113 \t range_loss: 0.5219116806983948\n",
      "pred_error: 0.010065392591059208 \t range_loss: 0.5226927995681763\n",
      "pred_error: 0.01104213111102581 \t range_loss: 0.5203123688697815\n",
      "pred_error: 0.010230181738734245 \t range_loss: 0.5209289193153381\n",
      "pred_error: 0.010184885002672672 \t range_loss: 0.5206234455108643\n",
      "pred_error: 0.010205158032476902 \t range_loss: 0.5203050971031189\n",
      "pred_error: 0.010146607644855976 \t range_loss: 0.5200563073158264\n",
      "pred_error: 0.010140175931155682 \t range_loss: 0.5195217728614807\n",
      "pred_error: 0.010225959122180939 \t range_loss: 0.5195806622505188\n",
      "pred_error: 0.010233686305582523 \t range_loss: 0.5194011330604553\n",
      "pred_error: 0.010567726567387581 \t range_loss: 0.5200120806694031\n",
      "pred_error: 0.010282262228429317 \t range_loss: 0.5197427272796631\n",
      "pred_error: 0.010291841812431812 \t range_loss: 0.519835352897644\n",
      "Step: 385 \t Loss: 0.6205517053604126\n",
      "pred_error: 0.010185804218053818 \t range_loss: 0.5186936259269714\n",
      "Step: 388 \t Loss: 0.6204849481582642\n",
      "pred_error: 0.010232016444206238 \t range_loss: 0.5181670784950256\n",
      "Step: 389 \t Loss: 0.6204661130905151\n",
      "Step: 390 \t Loss: 0.6201853156089783\n",
      "pred_error: 0.010258904658257961 \t range_loss: 0.5188598036766052\n",
      "pred_error: 0.010388490743935108 \t range_loss: 0.5176897048950195\n",
      "pred_error: 0.010374590754508972 \t range_loss: 0.5177393555641174\n",
      "pred_error: 0.010342316702008247 \t range_loss: 0.5167945623397827\n",
      "pred_error: 0.010342328809201717 \t range_loss: 0.5167945623397827\n",
      "pred_error: 0.010342348366975784 \t range_loss: 0.5167945623397827\n",
      "pred_error: 0.01037086546421051 \t range_loss: 0.5171448588371277\n",
      "pred_error: 0.010410952381789684 \t range_loss: 0.516721785068512\n",
      "pred_error: 0.010603105649352074 \t range_loss: 0.5154407024383545\n",
      "pred_error: 0.01054105069488287 \t range_loss: 0.5157576203346252\n",
      "pred_error: 0.010618838481605053 \t range_loss: 0.515190064907074\n",
      "pred_error: 0.010648457333445549 \t range_loss: 0.514876663684845\n",
      "pred_error: 0.010564511641860008 \t range_loss: 0.5154179930686951\n",
      "pred_error: 0.010564496740698814 \t range_loss: 0.5154179930686951\n",
      "pred_error: 0.010673446580767632 \t range_loss: 0.5163079500198364\n",
      "pred_error: 0.011065632104873657 \t range_loss: 0.5156456232070923\n",
      "pred_error: 0.010863714851439 \t range_loss: 0.5143254399299622\n",
      "pred_error: 0.010741033591330051 \t range_loss: 0.5135474801063538\n",
      "Step: 421 \t Loss: 0.6191762089729309\n",
      "Step: 422 \t Loss: 0.6190332174301147\n",
      "pred_error: 0.01052890159189701 \t range_loss: 0.5137450695037842\n",
      "pred_error: 0.011601804755628109 \t range_loss: 0.5129704475402832\n",
      "pred_error: 0.010887197218835354 \t range_loss: 0.5138962864875793\n",
      "pred_error: 0.010982479900121689 \t range_loss: 0.5134034156799316\n",
      "pred_error: 0.010982641950249672 \t range_loss: 0.5134034156799316\n",
      "pred_error: 0.010849351063370705 \t range_loss: 0.5120275616645813\n",
      "pred_error: 0.010807108134031296 \t range_loss: 0.5123046636581421\n",
      "pred_error: 0.010853996500372887 \t range_loss: 0.511051595211029\n",
      "pred_error: 0.010853596962988377 \t range_loss: 0.511051595211029\n",
      "pred_error: 0.011055543087422848 \t range_loss: 0.5116749405860901\n",
      "pred_error: 0.012141157872974873 \t range_loss: 0.5117149353027344\n",
      "pred_error: 0.011061694473028183 \t range_loss: 0.5106698274612427\n",
      "pred_error: 0.010990006849169731 \t range_loss: 0.5115594863891602\n",
      "pred_error: 0.01123424619436264 \t range_loss: 0.5108094811439514\n",
      "pred_error: 0.011175641790032387 \t range_loss: 0.5108298063278198\n",
      "pred_error: 0.011147174052894115 \t range_loss: 0.5097388625144958\n",
      "pred_error: 0.011257249861955643 \t range_loss: 0.5088690519332886\n",
      "pred_error: 0.011341908946633339 \t range_loss: 0.5088028311729431\n",
      "pred_error: 0.011341935954988003 \t range_loss: 0.5088028311729431\n",
      "pred_error: 0.012615879997611046 \t range_loss: 0.5076839923858643\n",
      "pred_error: 0.011718708090484142 \t range_loss: 0.5070826411247253\n",
      "pred_error: 0.011718843132257462 \t range_loss: 0.5070826411247253\n",
      "pred_error: 0.011718854308128357 \t range_loss: 0.5070826411247253\n",
      "pred_error: 0.011456156149506569 \t range_loss: 0.507038414478302\n",
      "pred_error: 0.011456077918410301 \t range_loss: 0.507038414478302\n",
      "pred_error: 0.011428800411522388 \t range_loss: 0.5067528486251831\n",
      "pred_error: 0.011428670957684517 \t range_loss: 0.5067528486251831\n",
      "pred_error: 0.01145616453140974 \t range_loss: 0.5069764256477356\n",
      "pred_error: 0.012534490786492825 \t range_loss: 0.5067410469055176\n",
      "pred_error: 0.011335324496030807 \t range_loss: 0.5074095129966736\n",
      "pred_error: 0.011486943811178207 \t range_loss: 0.5072669982910156\n",
      "pred_error: 0.01144145242869854 \t range_loss: 0.5072208642959595\n",
      "pred_error: 0.011501244269311428 \t range_loss: 0.5067434906959534\n",
      "pred_error: 0.011501244269311428 \t range_loss: 0.5067434906959534\n",
      "pred_error: 0.011501244269311428 \t range_loss: 0.5067434906959534\n",
      "pred_error: 0.012731212191283703 \t range_loss: 0.5059739947319031\n",
      "pred_error: 0.011554666794836521 \t range_loss: 0.5058528780937195\n",
      "pred_error: 0.011464573442935944 \t range_loss: 0.5055592060089111\n",
      "pred_error: 0.011616402305662632 \t range_loss: 0.5049456357955933\n",
      "pred_error: 0.011447948403656483 \t range_loss: 0.5049005746841431\n",
      "pred_error: 0.011613661423325539 \t range_loss: 0.5051537156105042\n",
      "pred_error: 0.011884250678122044 \t range_loss: 0.5054575204849243\n",
      "pred_error: 0.011696070432662964 \t range_loss: 0.5047320127487183\n",
      "pred_error: 0.011715062893927097 \t range_loss: 0.5042566657066345\n",
      "pred_error: 0.011946132406592369 \t range_loss: 0.504697322845459\n",
      "pred_error: 0.011658751405775547 \t range_loss: 0.5042378902435303\n",
      "pred_error: 0.011585994623601437 \t range_loss: 0.5045938491821289\n",
      "pred_error: 0.011584872379899025 \t range_loss: 0.5047884583473206\n",
      "pred_error: 0.011610561050474644 \t range_loss: 0.5039229989051819\n",
      "pred_error: 0.011610453948378563 \t range_loss: 0.5039229989051819\n",
      "pred_error: 0.01180428545922041 \t range_loss: 0.5039953589439392\n",
      "pred_error: 0.011767350137233734 \t range_loss: 0.5056366324424744\n",
      "pred_error: 0.011756911873817444 \t range_loss: 0.5037025809288025\n",
      "pred_error: 0.011676120571792126 \t range_loss: 0.5036703944206238\n",
      "pred_error: 0.01167615782469511 \t range_loss: 0.5036703944206238\n",
      "pred_error: 0.011710438877344131 \t range_loss: 0.5033169388771057\n",
      "pred_error: 0.011728476732969284 \t range_loss: 0.5026072859764099\n",
      "pred_error: 0.01298364344984293 \t range_loss: 0.5024182796478271\n",
      "pred_error: 0.011887641623616219 \t range_loss: 0.5021213293075562\n",
      "pred_error: 0.012015271931886673 \t range_loss: 0.502605676651001\n",
      "pred_error: 0.013249457813799381 \t range_loss: 0.501561164855957\n",
      "pred_error: 0.013249448500573635 \t range_loss: 0.501561164855957\n",
      "pred_error: 0.012006714008748531 \t range_loss: 0.5022692680358887\n",
      "pred_error: 0.011988158337771893 \t range_loss: 0.5021988749504089\n",
      "pred_error: 0.01198781281709671 \t range_loss: 0.5013783574104309\n",
      "pred_error: 0.011972150765359402 \t range_loss: 0.5008792281150818\n",
      "pred_error: 0.012533528730273247 \t range_loss: 0.5015652179718018\n",
      "pred_error: 0.012027773074805737 \t range_loss: 0.5003442764282227\n",
      "pred_error: 0.012057083658874035 \t range_loss: 0.5003978610038757\n",
      "pred_error: 0.01296829804778099 \t range_loss: 0.5003325939178467\n",
      "pred_error: 0.012105952017009258 \t range_loss: 0.5003435015678406\n",
      "pred_error: 0.012512654066085815 \t range_loss: 0.5001280903816223\n",
      "pred_error: 0.0119452103972435 \t range_loss: 0.5005263686180115\n",
      "pred_error: 0.011945031583309174 \t range_loss: 0.5005263686180115\n",
      "pred_error: 0.011945223435759544 \t range_loss: 0.5005263686180115\n",
      "pred_error: 0.01194520853459835 \t range_loss: 0.5005263686180115\n",
      "pred_error: 0.012044203467667103 \t range_loss: 0.5014975666999817\n",
      "pred_error: 0.012192538008093834 \t range_loss: 0.5013301968574524\n",
      "pred_error: 0.012085368856787682 \t range_loss: 0.5007061958312988\n",
      "pred_error: 0.012085344642400742 \t range_loss: 0.5007061958312988\n",
      "Step: 577 \t Loss: 0.6188251376152039\n",
      "pred_error: 0.012077764607965946 \t range_loss: 0.4998689293861389\n",
      "pred_error: 0.012217393144965172 \t range_loss: 0.4998292028903961\n",
      "pred_error: 0.012230120599269867 \t range_loss: 0.49999821186065674\n",
      "pred_error: 0.012214605696499348 \t range_loss: 0.49960917234420776\n",
      "pred_error: 0.012172089889645576 \t range_loss: 0.49917519092559814\n",
      "pred_error: 0.012125738896429539 \t range_loss: 0.4981464743614197\n",
      "pred_error: 0.012200272642076015 \t range_loss: 0.4985581338405609\n",
      "pred_error: 0.01220034807920456 \t range_loss: 0.4985581338405609\n",
      "pred_error: 0.012232151813805103 \t range_loss: 0.4982253909111023\n",
      "pred_error: 0.012232051230967045 \t range_loss: 0.4982253909111023\n",
      "pred_error: 0.012201501056551933 \t range_loss: 0.49798300862312317\n",
      "pred_error: 0.012358142994344234 \t range_loss: 0.4984776973724365\n",
      "pred_error: 0.01229792833328247 \t range_loss: 0.4971942603588104\n",
      "pred_error: 0.01247765589505434 \t range_loss: 0.49598968029022217\n",
      "pred_error: 0.01247776672244072 \t range_loss: 0.49598968029022217\n",
      "pred_error: 0.012504689395427704 \t range_loss: 0.49607187509536743\n",
      "pred_error: 0.012621446512639523 \t range_loss: 0.4965042471885681\n",
      "pred_error: 0.012621252797544003 \t range_loss: 0.4965042471885681\n",
      "pred_error: 0.012618105858564377 \t range_loss: 0.4965042471885681\n",
      "pred_error: 0.012894452549517155 \t range_loss: 0.4953457713127136\n",
      "pred_error: 0.012596803717315197 \t range_loss: 0.49511009454727173\n",
      "pred_error: 0.012515891343355179 \t range_loss: 0.4950897991657257\n",
      "pred_error: 0.012516006827354431 \t range_loss: 0.4950897991657257\n",
      "pred_error: 0.01252099871635437 \t range_loss: 0.49637719988822937\n",
      "pred_error: 0.012497587129473686 \t range_loss: 0.49732956290245056\n",
      "pred_error: 0.012497512623667717 \t range_loss: 0.49732956290245056\n",
      "pred_error: 0.012528442777693272 \t range_loss: 0.49689072370529175\n",
      "pred_error: 0.012583393603563309 \t range_loss: 0.49582621455192566\n",
      "pred_error: 0.012551369145512581 \t range_loss: 0.49558740854263306\n",
      "pred_error: 0.012624344788491726 \t range_loss: 0.4959920644760132\n",
      "pred_error: 0.012624385766685009 \t range_loss: 0.4959920644760132\n",
      "pred_error: 0.012543931603431702 \t range_loss: 0.49521827697753906\n",
      "pred_error: 0.012543990276753902 \t range_loss: 0.49521827697753906\n",
      "pred_error: 0.012477513402700424 \t range_loss: 0.4950137138366699\n",
      "pred_error: 0.012477529235184193 \t range_loss: 0.4950137138366699\n",
      "pred_error: 0.013754388317465782 \t range_loss: 0.4956473410129547\n",
      "pred_error: 0.012478291988372803 \t range_loss: 0.49620527029037476\n",
      "pred_error: 0.01247832179069519 \t range_loss: 0.49620527029037476\n",
      "pred_error: 0.01256350427865982 \t range_loss: 0.49609994888305664\n",
      "pred_error: 0.01262472290545702 \t range_loss: 0.4958679676055908\n",
      "pred_error: 0.01253189705312252 \t range_loss: 0.4967203438282013\n",
      "pred_error: 0.012531846761703491 \t range_loss: 0.4967203438282013\n",
      "pred_error: 0.012525107711553574 \t range_loss: 0.4972773790359497\n",
      "pred_error: 0.012515371665358543 \t range_loss: 0.49669182300567627\n",
      "pred_error: 0.012515553273260593 \t range_loss: 0.49669182300567627\n",
      "pred_error: 0.012463493272662163 \t range_loss: 0.4954138994216919\n",
      "pred_error: 0.013275429606437683 \t range_loss: 0.49529334902763367\n",
      "pred_error: 0.012493705376982689 \t range_loss: 0.49547135829925537\n",
      "pred_error: 0.012728807516396046 \t range_loss: 0.49387872219085693\n",
      "pred_error: 0.012627027928829193 \t range_loss: 0.4941585063934326\n",
      "pred_error: 0.012627065181732178 \t range_loss: 0.4941585063934326\n",
      "pred_error: 0.012623605318367481 \t range_loss: 0.4935637414455414\n",
      "pred_error: 0.012622443959116936 \t range_loss: 0.4935637414455414\n",
      "pred_error: 0.012804714031517506 \t range_loss: 0.4933330714702606\n",
      "pred_error: 0.01278836838901043 \t range_loss: 0.4939107298851013\n",
      "Step: 690 \t Loss: 0.6187872290611267\n",
      "pred_error: 0.01265201810747385 \t range_loss: 0.4932239055633545\n",
      "pred_error: 0.01282719150185585 \t range_loss: 0.49348339438438416\n",
      "pred_error: 0.012808607891201973 \t range_loss: 0.49309518933296204\n",
      "pred_error: 0.01286489237099886 \t range_loss: 0.49278318881988525\n",
      "pred_error: 0.01284113246947527 \t range_loss: 0.49268293380737305\n",
      "pred_error: 0.012815647758543491 \t range_loss: 0.49374815821647644\n",
      "pred_error: 0.01297117117792368 \t range_loss: 0.4926804304122925\n",
      "pred_error: 0.0128563791513443 \t range_loss: 0.491834819316864\n",
      "pred_error: 0.012804490514099598 \t range_loss: 0.4917231798171997\n",
      "pred_error: 0.013977812603116035 \t range_loss: 0.4917011559009552\n",
      "pred_error: 0.012926144525408745 \t range_loss: 0.49167463183403015\n",
      "pred_error: 0.012987667694687843 \t range_loss: 0.491420179605484\n",
      "pred_error: 0.012879185378551483 \t range_loss: 0.49200165271759033\n",
      "pred_error: 0.012965510599315166 \t range_loss: 0.49161967635154724\n",
      "pred_error: 0.013062416575849056 \t range_loss: 0.49128255248069763\n",
      "pred_error: 0.013015552423894405 \t range_loss: 0.4910007119178772\n",
      "pred_error: 0.013015840202569962 \t range_loss: 0.4910007119178772\n",
      "pred_error: 0.013015803880989552 \t range_loss: 0.4910007119178772\n",
      "pred_error: 0.013378611765801907 \t range_loss: 0.49179607629776\n",
      "pred_error: 0.013038096949458122 \t range_loss: 0.4901614785194397\n",
      "pred_error: 0.013038025237619877 \t range_loss: 0.4901614785194397\n",
      "pred_error: 0.013177424669265747 \t range_loss: 0.4900690019130707\n",
      "pred_error: 0.013164177536964417 \t range_loss: 0.49091485142707825\n",
      "pred_error: 0.01450276654213667 \t range_loss: 0.4896909296512604\n",
      "pred_error: 0.013097410090267658 \t range_loss: 0.4909193217754364\n",
      "pred_error: 0.013097578659653664 \t range_loss: 0.4909193217754364\n",
      "pred_error: 0.014295225962996483 \t range_loss: 0.4910600483417511\n",
      "pred_error: 0.01441539078950882 \t range_loss: 0.48991045355796814\n",
      "pred_error: 0.013023464009165764 \t range_loss: 0.4911743402481079\n",
      "pred_error: 0.013023070991039276 \t range_loss: 0.4911743402481079\n",
      "pred_error: 0.013088168576359749 \t range_loss: 0.4904772937297821\n",
      "pred_error: 0.013075600378215313 \t range_loss: 0.4907075762748718\n",
      "pred_error: 0.013159716501832008 \t range_loss: 0.4910850524902344\n",
      "pred_error: 0.013177289627492428 \t range_loss: 0.49015122652053833\n",
      "pred_error: 0.013228020630776882 \t range_loss: 0.4900418817996979\n",
      "pred_error: 0.013179242610931396 \t range_loss: 0.489314466714859\n",
      "pred_error: 0.013330405578017235 \t range_loss: 0.48888158798217773\n",
      "pred_error: 0.013330517336726189 \t range_loss: 0.48888158798217773\n",
      "pred_error: 0.013274394907057285 \t range_loss: 0.48943647742271423\n",
      "pred_error: 0.013292505405843258 \t range_loss: 0.4890384376049042\n",
      "pred_error: 0.013292334973812103 \t range_loss: 0.4890384376049042\n",
      "pred_error: 0.013293936848640442 \t range_loss: 0.48875725269317627\n",
      "pred_error: 0.013227072544395924 \t range_loss: 0.48868221044540405\n",
      "pred_error: 0.013227083720266819 \t range_loss: 0.48868221044540405\n",
      "pred_error: 0.014484272338449955 \t range_loss: 0.489162415266037\n",
      "pred_error: 0.013211528770625591 \t range_loss: 0.49011048674583435\n",
      "pred_error: 0.013219844549894333 \t range_loss: 0.4899372458457947\n",
      "pred_error: 0.01317882165312767 \t range_loss: 0.49093884229660034\n",
      "Step: 789 \t Loss: 0.6187679767608643\n",
      "pred_error: 0.01321889366954565 \t range_loss: 0.4892268776893616\n",
      "pred_error: 0.013922109268605709 \t range_loss: 0.48911190032958984\n",
      "pred_error: 0.013921801932156086 \t range_loss: 0.48911190032958984\n",
      "BEST LOSS: 0.618768\n",
      "==== Model: block1_cob_activation_norm  in Layer: 1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 13:14:23,778 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 13:16:29,351 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 13:16:29,362 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 13:16:51,133 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 13:16:51,148 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 13:16:51,216 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 13:16:51,237 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 13:16:51,252 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000022606143 | -0.000042706728 | 0.00025475025 | -0.00023677945 | 0.000051323652 | 0.000042706728   | 0.00025475025 | 0             | 0.0000000041819175 | -0.000027651575    | 0.0006764947           |\n",
      "+-----------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 638313 64 [-424278, 610914] 1 [16]\n",
      "===============================\n",
      "==== Model: block1_cob_activation_norm_teleported  in Layer: 1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 13:17:12,887 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 13:19:18,058 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 13:19:18,072 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 13:19:37,339 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 13:19:37,343 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 13:19:37,357 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 13:19:37,384 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 13:19:37,397 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error   | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000014131263 | 0.000058695674 | 0.0002834797 | -0.00028671324 | 0.000048387603 | 0.000058695674   | 0.00028671324 | 0             | 0.000000003743601  | -0.00079742284     | 0.0015773685           |\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 638321 64 [-268004, 237998] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 2 , \t  activation_stats: {'relu_1': {'norm': tensor(414.7610), 'max': tensor(4.6655), 'min': tensor(-6.8689), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(11.5343)\n",
      "pred_error: 0.023249201476573944 \t range_loss: 1.004598617553711\n",
      "Step: 0 \t Loss: 1.1848399639129639\n",
      "pred_error: 0.023229418322443962 \t range_loss: 0.9525445699691772\n",
      "Step: 1 \t Loss: 1.1611366271972656\n",
      "Step: 2 \t Loss: 1.1420512199401855\n",
      "Step: 3 \t Loss: 1.1073503494262695\n",
      "Step: 4 \t Loss: 1.0944997072219849\n",
      "pred_error: 0.023689376190304756 \t range_loss: 0.8576059937477112\n",
      "Step: 5 \t Loss: 1.0877439975738525\n",
      "pred_error: 0.023711899295449257 \t range_loss: 0.8506283164024353\n",
      "pred_error: 0.023711739107966423 \t range_loss: 0.8506283164024353\n",
      "Step: 6 \t Loss: 1.0851325988769531\n",
      "Step: 7 \t Loss: 1.0807740688323975\n",
      "pred_error: 0.023764215409755707 \t range_loss: 0.843134880065918\n",
      "Step: 8 \t Loss: 1.0730276107788086\n",
      "Step: 9 \t Loss: 1.0624116659164429\n",
      "Step: 10 \t Loss: 1.0591211318969727\n",
      "Step: 11 \t Loss: 1.0576112270355225\n",
      "Step: 12 \t Loss: 1.0519071817398071\n",
      "Step: 13 \t Loss: 1.0493799448013306\n",
      "pred_error: 0.024068905040621758 \t range_loss: 0.8086900115013123\n",
      "Step: 14 \t Loss: 1.0464003086090088\n",
      "pred_error: 0.024046676233410835 \t range_loss: 0.805932879447937\n",
      "pred_error: 0.024046512320637703 \t range_loss: 0.805932879447937\n",
      "Step: 15 \t Loss: 1.0434564352035522\n",
      "pred_error: 0.024182051420211792 \t range_loss: 0.8016344904899597\n",
      "Step: 16 \t Loss: 1.0421451330184937\n",
      "Step: 17 \t Loss: 1.038959264755249\n",
      "Step: 18 \t Loss: 1.037550926208496\n",
      "pred_error: 0.024036696180701256 \t range_loss: 0.7971839904785156\n",
      "pred_error: 0.024217359721660614 \t range_loss: 0.7950296401977539\n",
      "Step: 19 \t Loss: 1.0372031927108765\n",
      "Step: 20 \t Loss: 1.0355781316757202\n",
      "Step: 22 \t Loss: 1.0311875343322754\n",
      "Step: 23 \t Loss: 1.025542140007019\n",
      "pred_error: 0.024374986067414284 \t range_loss: 0.781793475151062\n",
      "Step: 24 \t Loss: 1.022242546081543\n",
      "Step: 25 \t Loss: 1.0183415412902832\n",
      "pred_error: 0.024370558559894562 \t range_loss: 0.7746334671974182\n",
      "Step: 31 \t Loss: 1.0167056322097778\n",
      "Step: 32 \t Loss: 1.0135127305984497\n",
      "Step: 33 \t Loss: 1.009773850440979\n",
      "Step: 34 \t Loss: 1.0076525211334229\n",
      "Step: 36 \t Loss: 1.0071830749511719\n",
      "pred_error: 0.0249040424823761 \t range_loss: 0.7581421136856079\n",
      "Step: 37 \t Loss: 1.004750370979309\n",
      "pred_error: 0.024862948805093765 \t range_loss: 0.7561208605766296\n",
      "pred_error: 0.02515368163585663 \t range_loss: 0.7562180161476135\n",
      "Step: 40 \t Loss: 1.0038057565689087\n",
      "Step: 41 \t Loss: 1.0018219947814941\n",
      "pred_error: 0.02520957961678505 \t range_loss: 0.7497260570526123\n",
      "Step: 42 \t Loss: 1.000614881515503\n",
      "pred_error: 0.025114530697464943 \t range_loss: 0.7494673132896423\n",
      "Step: 43 \t Loss: 0.9996945858001709\n",
      "Step: 44 \t Loss: 0.9984986186027527\n",
      "pred_error: 0.025137564167380333 \t range_loss: 0.7471226453781128\n",
      "Step: 45 \t Loss: 0.9975740909576416\n",
      "Step: 46 \t Loss: 0.9965015649795532\n",
      "Step: 47 \t Loss: 0.9956628680229187\n",
      "pred_error: 0.02516525238752365 \t range_loss: 0.744008481502533\n",
      "pred_error: 0.025165466591715813 \t range_loss: 0.744008481502533\n",
      "pred_error: 0.02532585896551609 \t range_loss: 0.7427680492401123\n",
      "pred_error: 0.025325531139969826 \t range_loss: 0.7427680492401123\n",
      "pred_error: 0.025325531139969826 \t range_loss: 0.7427680492401123\n",
      "Step: 51 \t Loss: 0.9952799081802368\n",
      "pred_error: 0.0255883801728487 \t range_loss: 0.7393907308578491\n",
      "Step: 52 \t Loss: 0.9902108311653137\n",
      "pred_error: 0.025531744584441185 \t range_loss: 0.7348909974098206\n",
      "pred_error: 0.025532033294439316 \t range_loss: 0.7348909974098206\n",
      "Step: 53 \t Loss: 0.9874080419540405\n",
      "Step: 54 \t Loss: 0.9867796897888184\n",
      "Step: 62 \t Loss: 0.9847629070281982\n",
      "Step: 63 \t Loss: 0.9834339618682861\n",
      "pred_error: 0.025826217606663704 \t range_loss: 0.7251718044281006\n",
      "pred_error: 0.025826217606663704 \t range_loss: 0.7251718044281006\n",
      "Step: 64 \t Loss: 0.9804955720901489\n",
      "pred_error: 0.025695662945508957 \t range_loss: 0.7253148555755615\n",
      "pred_error: 0.025695616379380226 \t range_loss: 0.7253148555755615\n",
      "pred_error: 0.025907857343554497 \t range_loss: 0.7246453166007996\n",
      "pred_error: 0.025964010506868362 \t range_loss: 0.7212691903114319\n",
      "Step: 70 \t Loss: 0.9797138571739197\n",
      "Step: 72 \t Loss: 0.9786595106124878\n",
      "Step: 73 \t Loss: 0.9770330190658569\n",
      "pred_error: 0.026086430996656418 \t range_loss: 0.7161626219749451\n",
      "pred_error: 0.026087302714586258 \t range_loss: 0.7161626219749451\n",
      "Step: 74 \t Loss: 0.9767066836357117\n",
      "pred_error: 0.025968287140130997 \t range_loss: 0.7170183658599854\n",
      "pred_error: 0.02628333307802677 \t range_loss: 0.7163842916488647\n",
      "pred_error: 0.026283374056220055 \t range_loss: 0.7163842916488647\n",
      "Step: 79 \t Loss: 0.9764037132263184\n",
      "pred_error: 0.02624400332570076 \t range_loss: 0.7139595150947571\n",
      "pred_error: 0.026244226843118668 \t range_loss: 0.7139595150947571\n",
      "pred_error: 0.02624460868537426 \t range_loss: 0.7139595150947571\n",
      "Step: 80 \t Loss: 0.9747283458709717\n",
      "pred_error: 0.026239218190312386 \t range_loss: 0.7126782536506653\n",
      "pred_error: 0.026313958689570427 \t range_loss: 0.7130212783813477\n",
      "Step: 83 \t Loss: 0.9737710356712341\n",
      "pred_error: 0.026337288320064545 \t range_loss: 0.710397481918335\n",
      "Step: 84 \t Loss: 0.9726318717002869\n",
      "Step: 86 \t Loss: 0.9726012349128723\n",
      "Step: 87 \t Loss: 0.9714715480804443\n",
      "pred_error: 0.026477506384253502 \t range_loss: 0.7066938877105713\n",
      "Step: 88 \t Loss: 0.9692729711532593\n",
      "Step: 90 \t Loss: 0.9686295986175537\n",
      "pred_error: 0.026586802676320076 \t range_loss: 0.7037296295166016\n",
      "Step: 94 \t Loss: 0.9681792855262756\n",
      "Step: 95 \t Loss: 0.9675430059432983\n",
      "pred_error: 0.02657005563378334 \t range_loss: 0.7025840878486633\n",
      "pred_error: 0.02675609104335308 \t range_loss: 0.7017368078231812\n",
      "pred_error: 0.026832452043890953 \t range_loss: 0.7007009387016296\n",
      "pred_error: 0.026819979771971703 \t range_loss: 0.7003950476646423\n",
      "Step: 102 \t Loss: 0.9664215445518494\n",
      "pred_error: 0.027049368247389793 \t range_loss: 0.6980113387107849\n",
      "Step: 107 \t Loss: 0.9660871028900146\n",
      "Step: 108 \t Loss: 0.9646373987197876\n",
      "Step: 109 \t Loss: 0.9619364738464355\n",
      "pred_error: 0.027157841250300407 \t range_loss: 0.6921812295913696\n",
      "pred_error: 0.02730974182486534 \t range_loss: 0.6907675266265869\n",
      "pred_error: 0.027309924364089966 \t range_loss: 0.6907675266265869\n",
      "Step: 113 \t Loss: 0.9618303179740906\n",
      "pred_error: 0.0271508377045393 \t range_loss: 0.690321147441864\n",
      "pred_error: 0.027151009067893028 \t range_loss: 0.690321147441864\n",
      "Step: 114 \t Loss: 0.9602168798446655\n",
      "pred_error: 0.027259021997451782 \t range_loss: 0.690823495388031\n",
      "pred_error: 0.027259329333901405 \t range_loss: 0.690823495388031\n",
      "pred_error: 0.027397548779845238 \t range_loss: 0.6897318959236145\n",
      "pred_error: 0.02734837494790554 \t range_loss: 0.6901381611824036\n",
      "pred_error: 0.027518028393387794 \t range_loss: 0.689749002456665\n",
      "pred_error: 0.027515077963471413 \t range_loss: 0.689749002456665\n",
      "pred_error: 0.02744346112012863 \t range_loss: 0.6903640627861023\n",
      "pred_error: 0.02733694016933441 \t range_loss: 0.6884045004844666\n",
      "Step: 125 \t Loss: 0.9599010348320007\n",
      "pred_error: 0.027519486844539642 \t range_loss: 0.6857134699821472\n",
      "pred_error: 0.027519701048731804 \t range_loss: 0.6857134699821472\n",
      "Step: 132 \t Loss: 0.95912766456604\n",
      "Step: 133 \t Loss: 0.9586693048477173\n",
      "Step: 134 \t Loss: 0.9572163820266724\n",
      "pred_error: 0.027517307549715042 \t range_loss: 0.6841551065444946\n",
      "pred_error: 0.027735590934753418 \t range_loss: 0.6828704476356506\n",
      "pred_error: 0.02773856744170189 \t range_loss: 0.6824960708618164\n",
      "pred_error: 0.027738507837057114 \t range_loss: 0.6824960708618164\n",
      "pred_error: 0.0278935469686985 \t range_loss: 0.6804438233375549\n",
      "pred_error: 0.02789396420121193 \t range_loss: 0.6804438233375549\n",
      "Step: 149 \t Loss: 0.9567958116531372\n",
      "pred_error: 0.02795160375535488 \t range_loss: 0.6772816777229309\n",
      "Step: 150 \t Loss: 0.9561622142791748\n",
      "Step: 151 \t Loss: 0.9550561904907227\n",
      "Step: 152 \t Loss: 0.95389723777771\n",
      "pred_error: 0.028096294030547142 \t range_loss: 0.6780068278312683\n",
      "pred_error: 0.028163883835077286 \t range_loss: 0.6755667328834534\n",
      "pred_error: 0.02817417122423649 \t range_loss: 0.6752042770385742\n",
      "pred_error: 0.028174396604299545 \t range_loss: 0.6749675273895264\n",
      "pred_error: 0.02816394530236721 \t range_loss: 0.6738802790641785\n",
      "pred_error: 0.028377285227179527 \t range_loss: 0.6723542213439941\n",
      "pred_error: 0.028230108320713043 \t range_loss: 0.6719473004341125\n",
      "Step: 166 \t Loss: 0.9526910185813904\n",
      "pred_error: 0.028117544949054718 \t range_loss: 0.671514630317688\n",
      "pred_error: 0.0281175896525383 \t range_loss: 0.671514630317688\n",
      "pred_error: 0.028431523591279984 \t range_loss: 0.6705377101898193\n",
      "pred_error: 0.028243474662303925 \t range_loss: 0.670383632183075\n",
      "pred_error: 0.0283904317766428 \t range_loss: 0.669983446598053\n",
      "pred_error: 0.028389310464262962 \t range_loss: 0.669983446598053\n",
      "pred_error: 0.028554540127515793 \t range_loss: 0.6697293519973755\n",
      "pred_error: 0.028554340824484825 \t range_loss: 0.6697293519973755\n",
      "pred_error: 0.02859596721827984 \t range_loss: 0.6686156988143921\n",
      "pred_error: 0.028517961502075195 \t range_loss: 0.6685794591903687\n",
      "pred_error: 0.028518665581941605 \t range_loss: 0.6685794591903687\n",
      "pred_error: 0.02871454320847988 \t range_loss: 0.6673661470413208\n",
      "pred_error: 0.028714865446090698 \t range_loss: 0.6673661470413208\n",
      "pred_error: 0.028669023886322975 \t range_loss: 0.6672917604446411\n",
      "Step: 181 \t Loss: 0.9526278972625732\n",
      "pred_error: 0.028609124943614006 \t range_loss: 0.6671131253242493\n",
      "Step: 183 \t Loss: 0.9517298936843872\n",
      "pred_error: 0.0289605725556612 \t range_loss: 0.6683855056762695\n",
      "pred_error: 0.028960375115275383 \t range_loss: 0.6683855056762695\n",
      "pred_error: 0.028963565826416016 \t range_loss: 0.6650918126106262\n",
      "pred_error: 0.029097696766257286 \t range_loss: 0.6639647483825684\n",
      "pred_error: 0.029098298400640488 \t range_loss: 0.6639647483825684\n",
      "Step: 190 \t Loss: 0.9485098123550415\n",
      "pred_error: 0.028743304312229156 \t range_loss: 0.6610767245292664\n",
      "pred_error: 0.028743384405970573 \t range_loss: 0.6610767245292664\n",
      "pred_error: 0.028920847922563553 \t range_loss: 0.6611763834953308\n",
      "pred_error: 0.02912355586886406 \t range_loss: 0.6619129776954651\n",
      "pred_error: 0.02912965603172779 \t range_loss: 0.6610281467437744\n",
      "pred_error: 0.029052959755063057 \t range_loss: 0.6604834794998169\n",
      "pred_error: 0.029438743367791176 \t range_loss: 0.6603673696517944\n",
      "pred_error: 0.02938157133758068 \t range_loss: 0.6592674851417542\n",
      "pred_error: 0.029420001432299614 \t range_loss: 0.658225417137146\n",
      "pred_error: 0.0291864275932312 \t range_loss: 0.6592134833335876\n",
      "pred_error: 0.029270121827721596 \t range_loss: 0.6586989164352417\n",
      "pred_error: 0.029232513159513474 \t range_loss: 0.6586840152740479\n",
      "pred_error: 0.029437890276312828 \t range_loss: 0.65842604637146\n",
      "pred_error: 0.029388906434178352 \t range_loss: 0.656561017036438\n",
      "pred_error: 0.029667865484952927 \t range_loss: 0.6566714644432068\n",
      "pred_error: 0.02966790460050106 \t range_loss: 0.6566714644432068\n",
      "Step: 227 \t Loss: 0.9482158422470093\n",
      "Step: 230 \t Loss: 0.9480819702148438\n",
      "Step: 231 \t Loss: 0.9472758173942566\n",
      "pred_error: 0.02947976253926754 \t range_loss: 0.6524690985679626\n",
      "pred_error: 0.02942727319896221 \t range_loss: 0.654341459274292\n",
      "pred_error: 0.02958233654499054 \t range_loss: 0.6564273238182068\n",
      "pred_error: 0.029749294742941856 \t range_loss: 0.6545190811157227\n",
      "pred_error: 0.02954799123108387 \t range_loss: 0.6533636450767517\n",
      "pred_error: 0.029739078134298325 \t range_loss: 0.6533397436141968\n",
      "pred_error: 0.029770828783512115 \t range_loss: 0.6526262760162354\n",
      "pred_error: 0.029852136969566345 \t range_loss: 0.6527237892150879\n",
      "pred_error: 0.029852384701371193 \t range_loss: 0.6527237892150879\n",
      "pred_error: 0.029877623543143272 \t range_loss: 0.6523715257644653\n",
      "pred_error: 0.02969704009592533 \t range_loss: 0.6508464813232422\n",
      "pred_error: 0.029669521376490593 \t range_loss: 0.6519061326980591\n",
      "pred_error: 0.02976391464471817 \t range_loss: 0.6509376168251038\n",
      "pred_error: 0.02982063591480255 \t range_loss: 0.6503748297691345\n",
      "pred_error: 0.029820747673511505 \t range_loss: 0.6520506739616394\n",
      "pred_error: 0.02982068993151188 \t range_loss: 0.6520506739616394\n",
      "pred_error: 0.029962917789816856 \t range_loss: 0.6516302227973938\n",
      "pred_error: 0.029962031170725822 \t range_loss: 0.6516302227973938\n",
      "pred_error: 0.029911678284406662 \t range_loss: 0.6498034000396729\n",
      "pred_error: 0.029993807896971703 \t range_loss: 0.6508237719535828\n",
      "pred_error: 0.029993752017617226 \t range_loss: 0.6508237719535828\n",
      "pred_error: 0.030154258012771606 \t range_loss: 0.6513519883155823\n",
      "pred_error: 0.03003251925110817 \t range_loss: 0.6511788964271545\n",
      "pred_error: 0.029958520084619522 \t range_loss: 0.6485515832901001\n",
      "pred_error: 0.030254792422056198 \t range_loss: 0.6480726003646851\n",
      "pred_error: 0.03025474026799202 \t range_loss: 0.6480726003646851\n",
      "Step: 282 \t Loss: 0.9467617869377136\n",
      "pred_error: 0.030366530641913414 \t range_loss: 0.647403359413147\n",
      "pred_error: 0.030340390279889107 \t range_loss: 0.6452112197875977\n",
      "pred_error: 0.030188467353582382 \t range_loss: 0.6450884342193604\n",
      "pred_error: 0.03024575300514698 \t range_loss: 0.6458452939987183\n",
      "pred_error: 0.03024572879076004 \t range_loss: 0.6458452939987183\n",
      "pred_error: 0.030245965346693993 \t range_loss: 0.6458452939987183\n",
      "pred_error: 0.03024780936539173 \t range_loss: 0.6469964981079102\n",
      "pred_error: 0.030533932149410248 \t range_loss: 0.6463114023208618\n",
      "pred_error: 0.030533786863088608 \t range_loss: 0.6463114023208618\n",
      "pred_error: 0.030409611761569977 \t range_loss: 0.6459932327270508\n",
      "pred_error: 0.030461495742201805 \t range_loss: 0.6442392468452454\n",
      "pred_error: 0.030376603826880455 \t range_loss: 0.6445450782775879\n",
      "Step: 300 \t Loss: 0.9464260339736938\n",
      "Step: 301 \t Loss: 0.9447697997093201\n",
      "pred_error: 0.0301556047052145 \t range_loss: 0.6432135105133057\n",
      "Step: 302 \t Loss: 0.9447673559188843\n",
      "pred_error: 0.03026794083416462 \t range_loss: 0.6433935761451721\n",
      "pred_error: 0.030467605218291283 \t range_loss: 0.6439518332481384\n",
      "pred_error: 0.03052701987326145 \t range_loss: 0.6462169885635376\n",
      "pred_error: 0.03040117397904396 \t range_loss: 0.6439979076385498\n",
      "pred_error: 0.030371516942977905 \t range_loss: 0.6429861783981323\n",
      "pred_error: 0.030400896444916725 \t range_loss: 0.6421907544136047\n",
      "pred_error: 0.03073636256158352 \t range_loss: 0.6430643796920776\n",
      "pred_error: 0.030736342072486877 \t range_loss: 0.6430643796920776\n",
      "pred_error: 0.03085171803832054 \t range_loss: 0.6413781046867371\n",
      "pred_error: 0.030862949788570404 \t range_loss: 0.6406771540641785\n",
      "pred_error: 0.03086288832128048 \t range_loss: 0.6406771540641785\n",
      "pred_error: 0.030727548524737358 \t range_loss: 0.6399545669555664\n",
      "pred_error: 0.03100930154323578 \t range_loss: 0.6387781500816345\n",
      "pred_error: 0.03100958652794361 \t range_loss: 0.6387781500816345\n",
      "pred_error: 0.031064588576555252 \t range_loss: 0.6374916434288025\n",
      "pred_error: 0.03109736181795597 \t range_loss: 0.6377080082893372\n",
      "pred_error: 0.0310974158346653 \t range_loss: 0.6377080082893372\n",
      "pred_error: 0.03113972768187523 \t range_loss: 0.6370819807052612\n",
      "pred_error: 0.03089934028685093 \t range_loss: 0.6370484828948975\n",
      "pred_error: 0.030815478414297104 \t range_loss: 0.6378398537635803\n",
      "pred_error: 0.030815429985523224 \t range_loss: 0.6378398537635803\n",
      "pred_error: 0.031150134280323982 \t range_loss: 0.6369367837905884\n",
      "pred_error: 0.030892012640833855 \t range_loss: 0.6371535658836365\n",
      "pred_error: 0.031276389956474304 \t range_loss: 0.6357452869415283\n",
      "pred_error: 0.03107869066298008 \t range_loss: 0.6353446841239929\n",
      "pred_error: 0.03107878565788269 \t range_loss: 0.6353446841239929\n",
      "pred_error: 0.031001776456832886 \t range_loss: 0.636926531791687\n",
      "pred_error: 0.03138604760169983 \t range_loss: 0.6354078054428101\n",
      "pred_error: 0.031358975917100906 \t range_loss: 0.6361370086669922\n",
      "pred_error: 0.031201008707284927 \t range_loss: 0.6334930062294006\n",
      "pred_error: 0.031149981543421745 \t range_loss: 0.635044276714325\n",
      "pred_error: 0.03114996664226055 \t range_loss: 0.635044276714325\n",
      "pred_error: 0.0313912108540535 \t range_loss: 0.6358619332313538\n",
      "pred_error: 0.0313912108540535 \t range_loss: 0.6358619332313538\n",
      "pred_error: 0.03136671334505081 \t range_loss: 0.6374399065971375\n",
      "pred_error: 0.03146880865097046 \t range_loss: 0.6368100047111511\n",
      "pred_error: 0.031173139810562134 \t range_loss: 0.6352174282073975\n",
      "pred_error: 0.031249968335032463 \t range_loss: 0.6355099081993103\n",
      "pred_error: 0.031249891966581345 \t range_loss: 0.6355099081993103\n",
      "pred_error: 0.031247198581695557 \t range_loss: 0.6349876523017883\n",
      "pred_error: 0.03124728426337242 \t range_loss: 0.6349876523017883\n",
      "pred_error: 0.031103862449526787 \t range_loss: 0.635501503944397\n",
      "pred_error: 0.031260840594768524 \t range_loss: 0.6347401142120361\n",
      "pred_error: 0.03153875842690468 \t range_loss: 0.6346197724342346\n",
      "pred_error: 0.031455378979444504 \t range_loss: 0.6344218254089355\n",
      "pred_error: 0.03130006045103073 \t range_loss: 0.63262939453125\n",
      "pred_error: 0.03136029839515686 \t range_loss: 0.6350078582763672\n",
      "pred_error: 0.031416911631822586 \t range_loss: 0.6354455351829529\n",
      "pred_error: 0.03133440017700195 \t range_loss: 0.6349795460700989\n",
      "pred_error: 0.03133440017700195 \t range_loss: 0.6349795460700989\n",
      "pred_error: 0.03121933527290821 \t range_loss: 0.6342322826385498\n",
      "pred_error: 0.03128854185342789 \t range_loss: 0.6345876455307007\n",
      "pred_error: 0.031278081238269806 \t range_loss: 0.6337611675262451\n",
      "pred_error: 0.0312780998647213 \t range_loss: 0.6337611675262451\n",
      "pred_error: 0.0314725786447525 \t range_loss: 0.6332192420959473\n",
      "pred_error: 0.03146747499704361 \t range_loss: 0.6343685984611511\n",
      "pred_error: 0.03146654739975929 \t range_loss: 0.6343685984611511\n",
      "pred_error: 0.03146565705537796 \t range_loss: 0.6343685984611511\n",
      "pred_error: 0.03142092376947403 \t range_loss: 0.6325156092643738\n",
      "pred_error: 0.03136950358748436 \t range_loss: 0.6323288083076477\n",
      "pred_error: 0.031514495611190796 \t range_loss: 0.6314167380332947\n",
      "pred_error: 0.03151436522603035 \t range_loss: 0.6314167380332947\n",
      "pred_error: 0.031514305621385574 \t range_loss: 0.6314167380332947\n",
      "pred_error: 0.031678829342126846 \t range_loss: 0.631566047668457\n",
      "pred_error: 0.03163320943713188 \t range_loss: 0.6314303278923035\n",
      "pred_error: 0.031560152769088745 \t range_loss: 0.6320666074752808\n",
      "pred_error: 0.03156792372465134 \t range_loss: 0.6308955550193787\n",
      "pred_error: 0.03168158978223801 \t range_loss: 0.6306420564651489\n",
      "pred_error: 0.03168175742030144 \t range_loss: 0.6306420564651489\n",
      "pred_error: 0.03149405121803284 \t range_loss: 0.6310415267944336\n",
      "pred_error: 0.03150326758623123 \t range_loss: 0.6317277550697327\n",
      "pred_error: 0.031569063663482666 \t range_loss: 0.6323444247245789\n",
      "pred_error: 0.031569063663482666 \t range_loss: 0.6323444247245789\n",
      "pred_error: 0.031625062227249146 \t range_loss: 0.6329277157783508\n",
      "pred_error: 0.03153166174888611 \t range_loss: 0.6308038234710693\n",
      "Step: 456 \t Loss: 0.9442025423049927\n",
      "Step: 458 \t Loss: 0.9438042640686035\n",
      "pred_error: 0.0316634476184845 \t range_loss: 0.6302162408828735\n",
      "pred_error: 0.03168937936425209 \t range_loss: 0.6304810643196106\n",
      "pred_error: 0.03155140578746796 \t range_loss: 0.6309767365455627\n",
      "pred_error: 0.03154462203383446 \t range_loss: 0.6308257579803467\n",
      "pred_error: 0.03152012452483177 \t range_loss: 0.6312648057937622\n",
      "pred_error: 0.03158704563975334 \t range_loss: 0.6312711238861084\n",
      "pred_error: 0.0315873883664608 \t range_loss: 0.6312711238861084\n",
      "pred_error: 0.03152480721473694 \t range_loss: 0.6308267712593079\n",
      "pred_error: 0.031584613025188446 \t range_loss: 0.6318191885948181\n",
      "pred_error: 0.031674087047576904 \t range_loss: 0.630754292011261\n",
      "pred_error: 0.03167400509119034 \t range_loss: 0.630754292011261\n",
      "pred_error: 0.03171554580330849 \t range_loss: 0.6306836605072021\n",
      "pred_error: 0.031790222972631454 \t range_loss: 0.6294201016426086\n",
      "pred_error: 0.031728021800518036 \t range_loss: 0.6283439993858337\n",
      "pred_error: 0.0318526029586792 \t range_loss: 0.6292470097541809\n",
      "pred_error: 0.031944241374731064 \t range_loss: 0.6294580101966858\n",
      "pred_error: 0.03198983892798424 \t range_loss: 0.6285466551780701\n",
      "pred_error: 0.03188849985599518 \t range_loss: 0.626878023147583\n",
      "pred_error: 0.03170069307088852 \t range_loss: 0.6278442144393921\n",
      "pred_error: 0.03168847784399986 \t range_loss: 0.6274058222770691\n",
      "pred_error: 0.03168850392103195 \t range_loss: 0.6274058222770691\n",
      "pred_error: 0.0316883809864521 \t range_loss: 0.6274058222770691\n",
      "pred_error: 0.031863514333963394 \t range_loss: 0.6274641156196594\n",
      "pred_error: 0.03192894160747528 \t range_loss: 0.6289989352226257\n",
      "pred_error: 0.03220794349908829 \t range_loss: 0.6254146099090576\n",
      "pred_error: 0.03199068084359169 \t range_loss: 0.6257885694503784\n",
      "pred_error: 0.03199070319533348 \t range_loss: 0.6257885694503784\n",
      "pred_error: 0.03205861896276474 \t range_loss: 0.6260517835617065\n",
      "pred_error: 0.032102301716804504 \t range_loss: 0.6251479983329773\n",
      "pred_error: 0.0321347676217556 \t range_loss: 0.6254149079322815\n",
      "pred_error: 0.032266393303871155 \t range_loss: 0.6255989670753479\n",
      "pred_error: 0.03233272209763527 \t range_loss: 0.6265562772750854\n",
      "pred_error: 0.03229796141386032 \t range_loss: 0.6263729929924011\n",
      "pred_error: 0.032297778874635696 \t range_loss: 0.6263729929924011\n",
      "pred_error: 0.03208586946129799 \t range_loss: 0.6255044341087341\n",
      "pred_error: 0.0320858508348465 \t range_loss: 0.6255044341087341\n",
      "pred_error: 0.03210216760635376 \t range_loss: 0.6254410743713379\n",
      "pred_error: 0.03233567252755165 \t range_loss: 0.6255067586898804\n",
      "pred_error: 0.03250238671898842 \t range_loss: 0.6254538297653198\n",
      "pred_error: 0.03245672583580017 \t range_loss: 0.6247894167900085\n",
      "pred_error: 0.032194700092077255 \t range_loss: 0.624028205871582\n",
      "pred_error: 0.03208767622709274 \t range_loss: 0.6256168484687805\n",
      "pred_error: 0.03248071298003197 \t range_loss: 0.6249125003814697\n",
      "pred_error: 0.03245588764548302 \t range_loss: 0.6248504519462585\n",
      "pred_error: 0.032358817756175995 \t range_loss: 0.6228978633880615\n",
      "pred_error: 0.03237439692020416 \t range_loss: 0.6230375170707703\n",
      "pred_error: 0.032374609261751175 \t range_loss: 0.6230375170707703\n",
      "pred_error: 0.03245691582560539 \t range_loss: 0.6242752075195312\n",
      "pred_error: 0.03245687112212181 \t range_loss: 0.6242752075195312\n",
      "pred_error: 0.03254693001508713 \t range_loss: 0.6244326829910278\n",
      "pred_error: 0.032438334077596664 \t range_loss: 0.6257570385932922\n",
      "pred_error: 0.03243807330727577 \t range_loss: 0.6239882707595825\n",
      "pred_error: 0.032381195574998856 \t range_loss: 0.6238561868667603\n",
      "pred_error: 0.03219641372561455 \t range_loss: 0.624189019203186\n",
      "pred_error: 0.03220268338918686 \t range_loss: 0.623348593711853\n",
      "pred_error: 0.03233588486909866 \t range_loss: 0.624493420124054\n",
      "pred_error: 0.032326534390449524 \t range_loss: 0.6244752407073975\n",
      "pred_error: 0.03248549997806549 \t range_loss: 0.6240406036376953\n",
      "pred_error: 0.03251031041145325 \t range_loss: 0.6247517466545105\n",
      "pred_error: 0.03251023590564728 \t range_loss: 0.6247517466545105\n",
      "pred_error: 0.03242435306310654 \t range_loss: 0.6256659626960754\n",
      "pred_error: 0.0325341522693634 \t range_loss: 0.6219955682754517\n",
      "pred_error: 0.03255605697631836 \t range_loss: 0.6233904361724854\n",
      "pred_error: 0.03256234899163246 \t range_loss: 0.6221911311149597\n",
      "pred_error: 0.03232964500784874 \t range_loss: 0.6237614154815674\n",
      "pred_error: 0.03234392777085304 \t range_loss: 0.6232949495315552\n",
      "pred_error: 0.03233690932393074 \t range_loss: 0.6215920448303223\n",
      "pred_error: 0.03224501013755798 \t range_loss: 0.6224421858787537\n",
      "pred_error: 0.03265928849577904 \t range_loss: 0.6227871179580688\n",
      "pred_error: 0.032655853778123856 \t range_loss: 0.6227881908416748\n",
      "pred_error: 0.03265530988574028 \t range_loss: 0.6225934624671936\n",
      "pred_error: 0.03270880505442619 \t range_loss: 0.6222985982894897\n",
      "pred_error: 0.03270644322037697 \t range_loss: 0.6222985982894897\n",
      "pred_error: 0.032528165727853775 \t range_loss: 0.6220459938049316\n",
      "pred_error: 0.03261423856019974 \t range_loss: 0.6218216419219971\n",
      "pred_error: 0.032614223659038544 \t range_loss: 0.6218216419219971\n",
      "pred_error: 0.032526325434446335 \t range_loss: 0.6209719181060791\n",
      "pred_error: 0.03240250423550606 \t range_loss: 0.6214892268180847\n",
      "pred_error: 0.03240257129073143 \t range_loss: 0.6214892268180847\n",
      "pred_error: 0.032402537763118744 \t range_loss: 0.6214892268180847\n",
      "pred_error: 0.03232116624712944 \t range_loss: 0.6232528686523438\n",
      "pred_error: 0.03260349854826927 \t range_loss: 0.6224862337112427\n",
      "pred_error: 0.032903071492910385 \t range_loss: 0.6224706768989563\n",
      "pred_error: 0.03247097507119179 \t range_loss: 0.621155858039856\n",
      "pred_error: 0.032470811158418655 \t range_loss: 0.621155858039856\n",
      "pred_error: 0.03260532766580582 \t range_loss: 0.6222707629203796\n",
      "pred_error: 0.032611291855573654 \t range_loss: 0.6220871806144714\n",
      "pred_error: 0.03251195326447487 \t range_loss: 0.6226328015327454\n",
      "pred_error: 0.032736871391534805 \t range_loss: 0.6216403245925903\n",
      "pred_error: 0.03259248659014702 \t range_loss: 0.6208062767982483\n",
      "pred_error: 0.032639291137456894 \t range_loss: 0.6214187145233154\n",
      "pred_error: 0.0326392725110054 \t range_loss: 0.6214187145233154\n",
      "pred_error: 0.032598014920949936 \t range_loss: 0.623851478099823\n",
      "pred_error: 0.032698508352041245 \t range_loss: 0.622552752494812\n",
      "pred_error: 0.03274189680814743 \t range_loss: 0.6226637363433838\n",
      "pred_error: 0.0325973816215992 \t range_loss: 0.6216153502464294\n",
      "pred_error: 0.03244663402438164 \t range_loss: 0.6220265626907349\n",
      "pred_error: 0.03262343630194664 \t range_loss: 0.6216604709625244\n",
      "pred_error: 0.03262344002723694 \t range_loss: 0.6216604709625244\n",
      "pred_error: 0.032583724707365036 \t range_loss: 0.623001754283905\n",
      "pred_error: 0.032711464911699295 \t range_loss: 0.6209670901298523\n",
      "pred_error: 0.03272078558802605 \t range_loss: 0.6211893558502197\n",
      "pred_error: 0.03266017884016037 \t range_loss: 0.6218957304954529\n",
      "pred_error: 0.03266578167676926 \t range_loss: 0.6211771965026855\n",
      "pred_error: 0.032716892659664154 \t range_loss: 0.6203555464744568\n",
      "pred_error: 0.03263360261917114 \t range_loss: 0.6196761131286621\n",
      "pred_error: 0.03266808018088341 \t range_loss: 0.6200225353240967\n",
      "pred_error: 0.032634999603033066 \t range_loss: 0.620864748954773\n",
      "pred_error: 0.03291825205087662 \t range_loss: 0.6218029260635376\n",
      "pred_error: 0.032686010003089905 \t range_loss: 0.6201003789901733\n",
      "pred_error: 0.032568760216236115 \t range_loss: 0.6195860505104065\n",
      "pred_error: 0.03267880156636238 \t range_loss: 0.6226076483726501\n",
      "pred_error: 0.032895784825086594 \t range_loss: 0.6203101277351379\n",
      "pred_error: 0.03275616094470024 \t range_loss: 0.6201220154762268\n",
      "pred_error: 0.032721880823373795 \t range_loss: 0.6200818419456482\n",
      "pred_error: 0.03272193670272827 \t range_loss: 0.6200818419456482\n",
      "pred_error: 0.03279624879360199 \t range_loss: 0.620315432548523\n",
      "pred_error: 0.033005937933921814 \t range_loss: 0.6189521551132202\n",
      "pred_error: 0.032500796020030975 \t range_loss: 0.6201176643371582\n",
      "pred_error: 0.03278927877545357 \t range_loss: 0.6205921769142151\n",
      "pred_error: 0.032822251319885254 \t range_loss: 0.6209626197814941\n",
      "pred_error: 0.032790280878543854 \t range_loss: 0.6212745904922485\n",
      "pred_error: 0.03279169276356697 \t range_loss: 0.6212791800498962\n",
      "pred_error: 0.03275035694241524 \t range_loss: 0.6213650703430176\n",
      "pred_error: 0.032594963908195496 \t range_loss: 0.6188379526138306\n",
      "pred_error: 0.03296268731355667 \t range_loss: 0.6192322373390198\n",
      "pred_error: 0.03289619833230972 \t range_loss: 0.6205261945724487\n",
      "pred_error: 0.03289543837308884 \t range_loss: 0.6205261945724487\n",
      "pred_error: 0.033055003732442856 \t range_loss: 0.6186738014221191\n",
      "pred_error: 0.03305507451295853 \t range_loss: 0.6186738014221191\n",
      "pred_error: 0.032820045948028564 \t range_loss: 0.6182655692100525\n",
      "pred_error: 0.0328177884221077 \t range_loss: 0.6198536157608032\n",
      "pred_error: 0.03281771391630173 \t range_loss: 0.6198536157608032\n",
      "pred_error: 0.032971035689115524 \t range_loss: 0.6186705827713013\n",
      "pred_error: 0.03315727412700653 \t range_loss: 0.6191975474357605\n",
      "pred_error: 0.03313999995589256 \t range_loss: 0.6186308860778809\n",
      "Step: 749 \t Loss: 0.9436295032501221\n",
      "pred_error: 0.03264034166932106 \t range_loss: 0.6172269582748413\n",
      "pred_error: 0.033015746623277664 \t range_loss: 0.6190635561943054\n",
      "pred_error: 0.032898157835006714 \t range_loss: 0.6193658113479614\n",
      "pred_error: 0.032967668026685715 \t range_loss: 0.6197217106819153\n",
      "pred_error: 0.03298182040452957 \t range_loss: 0.6184458136558533\n",
      "pred_error: 0.03299514949321747 \t range_loss: 0.6186833381652832\n",
      "pred_error: 0.03292315453290939 \t range_loss: 0.619140625\n",
      "pred_error: 0.03286302089691162 \t range_loss: 0.6164748072624207\n",
      "Step: 779 \t Loss: 0.942825198173523\n",
      "Step: 780 \t Loss: 0.941809356212616\n",
      "pred_error: 0.032710228115320206 \t range_loss: 0.6166138052940369\n",
      "pred_error: 0.03286384791135788 \t range_loss: 0.6197213530540466\n",
      "pred_error: 0.03303329646587372 \t range_loss: 0.6189042329788208\n",
      "pred_error: 0.033180683851242065 \t range_loss: 0.618272602558136\n",
      "pred_error: 0.03282756358385086 \t range_loss: 0.6179283261299133\n",
      "pred_error: 0.03267662972211838 \t range_loss: 0.6181563138961792\n",
      "pred_error: 0.03277495130896568 \t range_loss: 0.6185352802276611\n",
      "pred_error: 0.03283264487981796 \t range_loss: 0.6198500990867615\n",
      "pred_error: 0.03311335667967796 \t range_loss: 0.6179695725440979\n",
      "pred_error: 0.033120594918727875 \t range_loss: 0.618242084980011\n",
      "BEST LOSS: 0.94180936\n",
      "==== Model: block2_cob_activation_norm  in Layer: 2 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 14:09:14,405 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 14:11:21,799 model.rs:1246 value (-617472) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 14:11:21,806 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 14:11:46,069 model.rs:1246 value (-617472) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 14:11:46,090 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 14:11:46,121 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 14:11:46,150 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 14:11:46,165 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error   | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000008162118 | 0.000015750527 | 0.00027549267 | -0.00029866397 | 0.00004002754  | 0.000015750527   | 0.00029866397 | 0             | 0.000000002682428  | 0.00010109771      | 0.00079670444          |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 671215 64 [-636614, 432394] 1 [16]\n",
      "===============================\n",
      "==== Model: block2_cob_activation_norm_teleported  in Layer: 2 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 14:12:06,193 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 14:14:16,489 model.rs:1246 value (-35584) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 14:14:16,497 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 14:14:37,877 model.rs:1246 value (-35584) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 14:14:37,881 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 14:14:37,913 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 14:14:37,931 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 14:14:37,944 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error  | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.000000007360856 | 0.00002554059 | 0.0002527237 | -0.00028014183 | 0.000040324416 | 0.00002554059    | 0.00028014183 | 0             | 0.0000000027162694 | -0.00016391218     | 0.00077374355          |\n",
      "+-------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 671223 64 [-409346, 251202] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 3 , \t  activation_stats: {'relu_1': {'norm': tensor(501.2616), 'max': tensor(3.6280), 'min': tensor(-5.4008), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(9.0288)\n",
      "pred_error: 0.07796229422092438 \t range_loss: 1.0050991773605347\n",
      "pred_error: 0.07796287536621094 \t range_loss: 1.0050991773605347\n",
      "pred_error: 0.07796136289834976 \t range_loss: 1.0050991773605347\n",
      "Step: 0 \t Loss: 1.726619839668274\n",
      "pred_error: 0.0773656964302063 \t range_loss: 0.952964186668396\n",
      "pred_error: 0.07736716419458389 \t range_loss: 0.952964186668396\n",
      "Step: 1 \t Loss: 1.6967357397079468\n",
      "Step: 2 \t Loss: 1.668107032775879\n",
      "Step: 3 \t Loss: 1.6464121341705322\n",
      "pred_error: 0.07570911943912506 \t range_loss: 0.8893206119537354\n",
      "pred_error: 0.0757092759013176 \t range_loss: 0.8893206119537354\n",
      "Step: 4 \t Loss: 1.6396706104278564\n",
      "pred_error: 0.07527218759059906 \t range_loss: 0.8869485855102539\n",
      "Step: 5 \t Loss: 1.6203818321228027\n",
      "Step: 6 \t Loss: 1.6145141124725342\n",
      "pred_error: 0.07451470196247101 \t range_loss: 0.8693562150001526\n",
      "Step: 7 \t Loss: 1.6046183109283447\n",
      "Step: 8 \t Loss: 1.5836617946624756\n",
      "pred_error: 0.07374702394008636 \t range_loss: 0.8461914658546448\n",
      "Step: 9 \t Loss: 1.5785067081451416\n",
      "Step: 10 \t Loss: 1.567042350769043\n",
      "pred_error: 0.07311023771762848 \t range_loss: 0.8359375596046448\n",
      "pred_error: 0.0731109231710434 \t range_loss: 0.8359375596046448\n",
      "Step: 11 \t Loss: 1.5601691007614136\n",
      "pred_error: 0.07287152856588364 \t range_loss: 0.8314472436904907\n",
      "Step: 12 \t Loss: 1.5521609783172607\n",
      "Step: 13 \t Loss: 1.5487616062164307\n",
      "pred_error: 0.07246321439743042 \t range_loss: 0.8241255879402161\n",
      "Step: 14 \t Loss: 1.5445637702941895\n",
      "Step: 15 \t Loss: 1.5399328470230103\n",
      "pred_error: 0.07190704345703125 \t range_loss: 0.8208624124526978\n",
      "Step: 16 \t Loss: 1.534949541091919\n",
      "Step: 17 \t Loss: 1.5287632942199707\n",
      "Step: 18 \t Loss: 1.5237128734588623\n",
      "Step: 19 \t Loss: 1.5225656032562256\n",
      "Step: 20 \t Loss: 1.5175374746322632\n",
      "pred_error: 0.07120398432016373 \t range_loss: 0.8055056929588318\n",
      "Step: 21 \t Loss: 1.5133864879608154\n",
      "pred_error: 0.07104850560426712 \t range_loss: 0.8029037714004517\n",
      "Step: 22 \t Loss: 1.5088319778442383\n",
      "pred_error: 0.07082170248031616 \t range_loss: 0.8006151914596558\n",
      "Step: 23 \t Loss: 1.502739667892456\n",
      "Step: 24 \t Loss: 1.5015900135040283\n",
      "Step: 25 \t Loss: 1.49916410446167\n",
      "Step: 26 \t Loss: 1.4957149028778076\n",
      "pred_error: 0.07032956182956696 \t range_loss: 0.7924160361289978\n",
      "pred_error: 0.07032956928014755 \t range_loss: 0.7924160361289978\n",
      "Step: 27 \t Loss: 1.494343638420105\n",
      "Step: 28 \t Loss: 1.4930567741394043\n",
      "Step: 29 \t Loss: 1.489954948425293\n",
      "Step: 30 \t Loss: 1.485121250152588\n",
      "Step: 31 \t Loss: 1.4848034381866455\n",
      "Step: 32 \t Loss: 1.482778787612915\n",
      "Step: 33 \t Loss: 1.480522871017456\n",
      "Step: 34 \t Loss: 1.4762862920761108\n",
      "Step: 35 \t Loss: 1.4712884426116943\n",
      "pred_error: 0.06937385350465775 \t range_loss: 0.7775499820709229\n",
      "pred_error: 0.06937382370233536 \t range_loss: 0.7775499820709229\n",
      "Step: 36 \t Loss: 1.46923828125\n",
      "pred_error: 0.06926821917295456 \t range_loss: 0.776555061340332\n",
      "Step: 37 \t Loss: 1.4669175148010254\n",
      "Step: 38 \t Loss: 1.4627647399902344\n",
      "pred_error: 0.06899746507406235 \t range_loss: 0.7727909088134766\n",
      "Step: 39 \t Loss: 1.4617040157318115\n",
      "Step: 40 \t Loss: 1.4615987539291382\n",
      "Step: 41 \t Loss: 1.4607794284820557\n",
      "Step: 42 \t Loss: 1.4595696926116943\n",
      "Step: 43 \t Loss: 1.4575519561767578\n",
      "pred_error: 0.069040946662426 \t range_loss: 0.7671421766281128\n",
      "Step: 44 \t Loss: 1.453326940536499\n",
      "pred_error: 0.06895007938146591 \t range_loss: 0.7638350129127502\n",
      "Step: 45 \t Loss: 1.4520207643508911\n",
      "pred_error: 0.06889177113771439 \t range_loss: 0.763100266456604\n",
      "Step: 46 \t Loss: 1.4485844373703003\n",
      "pred_error: 0.06880240142345428 \t range_loss: 0.7605614066123962\n",
      "Step: 47 \t Loss: 1.4463114738464355\n",
      "pred_error: 0.06860772520303726 \t range_loss: 0.7602353096008301\n",
      "Step: 48 \t Loss: 1.4455431699752808\n",
      "Step: 49 \t Loss: 1.4441521167755127\n",
      "pred_error: 0.0685981959104538 \t range_loss: 0.7581700682640076\n",
      "Step: 50 \t Loss: 1.442385196685791\n",
      "Step: 51 \t Loss: 1.441754698753357\n",
      "Step: 52 \t Loss: 1.4409375190734863\n",
      "pred_error: 0.06863380968570709 \t range_loss: 0.7546021342277527\n",
      "pred_error: 0.06876274198293686 \t range_loss: 0.7540370225906372\n",
      "Step: 54 \t Loss: 1.4386231899261475\n",
      "pred_error: 0.06873790919780731 \t range_loss: 0.7512430548667908\n",
      "Step: 55 \t Loss: 1.4364852905273438\n",
      "Step: 56 \t Loss: 1.4341299533843994\n",
      "pred_error: 0.0684397742152214 \t range_loss: 0.7497321367263794\n",
      "Step: 57 \t Loss: 1.431530475616455\n",
      "Step: 58 \t Loss: 1.4304792881011963\n",
      "Step: 59 \t Loss: 1.4297900199890137\n",
      "pred_error: 0.06823531538248062 \t range_loss: 0.7474339008331299\n",
      "pred_error: 0.06823547184467316 \t range_loss: 0.7474339008331299\n",
      "Step: 60 \t Loss: 1.4291577339172363\n",
      "Step: 61 \t Loss: 1.42802095413208\n",
      "Step: 62 \t Loss: 1.4262566566467285\n",
      "Step: 64 \t Loss: 1.425262689590454\n",
      "Step: 65 \t Loss: 1.4247881174087524\n",
      "Step: 66 \t Loss: 1.4232237339019775\n",
      "pred_error: 0.06816813349723816 \t range_loss: 0.7415353059768677\n",
      "Step: 67 \t Loss: 1.421800136566162\n",
      "pred_error: 0.06811899691820145 \t range_loss: 0.7406070828437805\n",
      "Step: 68 \t Loss: 1.420662760734558\n",
      "Step: 69 \t Loss: 1.4194446802139282\n",
      "pred_error: 0.06802362203598022 \t range_loss: 0.739205002784729\n",
      "pred_error: 0.06802379339933395 \t range_loss: 0.739205002784729\n",
      "pred_error: 0.06802434474229813 \t range_loss: 0.739205002784729\n",
      "Step: 70 \t Loss: 1.4172039031982422\n",
      "pred_error: 0.06790482252836227 \t range_loss: 0.7381559610366821\n",
      "Step: 71 \t Loss: 1.4155216217041016\n",
      "Step: 72 \t Loss: 1.4132087230682373\n",
      "pred_error: 0.06778262555599213 \t range_loss: 0.7353836894035339\n",
      "Step: 73 \t Loss: 1.410675048828125\n",
      "pred_error: 0.06762959808111191 \t range_loss: 0.7349505424499512\n",
      "pred_error: 0.06780420988798141 \t range_loss: 0.7339218258857727\n",
      "pred_error: 0.06772521138191223 \t range_loss: 0.7339548468589783\n",
      "Step: 77 \t Loss: 1.4104118347167969\n",
      "Step: 78 \t Loss: 1.4086501598358154\n",
      "pred_error: 0.06765761971473694 \t range_loss: 0.7320668697357178\n",
      "pred_error: 0.06766736507415771 \t range_loss: 0.7335728406906128\n",
      "Step: 82 \t Loss: 1.407273292541504\n",
      "Step: 83 \t Loss: 1.4056012630462646\n",
      "pred_error: 0.06777875125408173 \t range_loss: 0.7278164029121399\n",
      "Step: 84 \t Loss: 1.4038853645324707\n",
      "pred_error: 0.06760472059249878 \t range_loss: 0.7278381586074829\n",
      "Step: 85 \t Loss: 1.4019660949707031\n",
      "Step: 86 \t Loss: 1.401368260383606\n",
      "pred_error: 0.06759681552648544 \t range_loss: 0.7253942489624023\n",
      "pred_error: 0.06771999597549438 \t range_loss: 0.7250695824623108\n",
      "Step: 88 \t Loss: 1.4002304077148438\n",
      "Step: 89 \t Loss: 1.3993291854858398\n",
      "Step: 90 \t Loss: 1.3979301452636719\n",
      "pred_error: 0.0674971491098404 \t range_loss: 0.7229604721069336\n",
      "Step: 91 \t Loss: 1.3976404666900635\n",
      "Step: 92 \t Loss: 1.397078037261963\n",
      "Step: 94 \t Loss: 1.3965792655944824\n",
      "Step: 95 \t Loss: 1.396064043045044\n",
      "pred_error: 0.06759986281394958 \t range_loss: 0.7200653553009033\n",
      "pred_error: 0.06770096719264984 \t range_loss: 0.7193154096603394\n",
      "Step: 97 \t Loss: 1.3947820663452148\n",
      "Step: 98 \t Loss: 1.394312858581543\n",
      "pred_error: 0.06759472191333771 \t range_loss: 0.7183684706687927\n",
      "Step: 99 \t Loss: 1.394267201423645\n",
      "Step: 100 \t Loss: 1.392780065536499\n",
      "Step: 103 \t Loss: 1.3922768831253052\n",
      "Step: 106 \t Loss: 1.390430212020874\n",
      "Step: 107 \t Loss: 1.3891769647598267\n",
      "pred_error: 0.06752542406320572 \t range_loss: 0.7139212489128113\n",
      "Step: 108 \t Loss: 1.386728048324585\n",
      "Step: 109 \t Loss: 1.3844444751739502\n",
      "Step: 110 \t Loss: 1.3844020366668701\n",
      "pred_error: 0.06749477982521057 \t range_loss: 0.7123844027519226\n",
      "pred_error: 0.0674949362874031 \t range_loss: 0.7123844623565674\n",
      "pred_error: 0.06742852181196213 \t range_loss: 0.7115403413772583\n",
      "pred_error: 0.06765630841255188 \t range_loss: 0.7106055617332458\n",
      "Step: 118 \t Loss: 1.383378267288208\n",
      "Step: 119 \t Loss: 1.3825352191925049\n",
      "pred_error: 0.06748904287815094 \t range_loss: 0.7076440453529358\n",
      "pred_error: 0.06748896092176437 \t range_loss: 0.7076440453529358\n",
      "Step: 120 \t Loss: 1.381183385848999\n",
      "pred_error: 0.06737685203552246 \t range_loss: 0.707414984703064\n",
      "Step: 121 \t Loss: 1.3803483247756958\n",
      "Step: 122 \t Loss: 1.3802292346954346\n",
      "Step: 123 \t Loss: 1.3795188665390015\n",
      "pred_error: 0.06736506521701813 \t range_loss: 0.7058684825897217\n",
      "Step: 124 \t Loss: 1.3778882026672363\n",
      "pred_error: 0.06727507710456848 \t range_loss: 0.7051360011100769\n",
      "Step: 125 \t Loss: 1.3769352436065674\n",
      "pred_error: 0.06740584969520569 \t range_loss: 0.7037050724029541\n",
      "pred_error: 0.06738802045583725 \t range_loss: 0.7036471962928772\n",
      "pred_error: 0.06749199330806732 \t range_loss: 0.7024175524711609\n",
      "Step: 135 \t Loss: 1.3759973049163818\n",
      "Step: 136 \t Loss: 1.3748178482055664\n",
      "pred_error: 0.06731953471899033 \t range_loss: 0.7016282677650452\n",
      "Step: 137 \t Loss: 1.3740825653076172\n",
      "pred_error: 0.06735368818044662 \t range_loss: 0.7005456686019897\n",
      "Step: 138 \t Loss: 1.3722548484802246\n",
      "pred_error: 0.06719228625297546 \t range_loss: 0.7003322243690491\n",
      "Step: 139 \t Loss: 1.3716809749603271\n",
      "pred_error: 0.06748483330011368 \t range_loss: 0.7006009817123413\n",
      "pred_error: 0.06730998307466507 \t range_loss: 0.6995503902435303\n",
      "Step: 148 \t Loss: 1.3699944019317627\n",
      "Step: 149 \t Loss: 1.3682634830474854\n",
      "pred_error: 0.06717095524072647 \t range_loss: 0.6965504288673401\n",
      "pred_error: 0.06717110425233841 \t range_loss: 0.6965504288673401\n",
      "Step: 150 \t Loss: 1.3671164512634277\n",
      "Step: 151 \t Loss: 1.3664772510528564\n",
      "pred_error: 0.06740275025367737 \t range_loss: 0.6944298148155212\n",
      "Step: 158 \t Loss: 1.3652145862579346\n",
      "pred_error: 0.06733745336532593 \t range_loss: 0.6918400526046753\n",
      "pred_error: 0.06733725219964981 \t range_loss: 0.6918400526046753\n",
      "pred_error: 0.06733778119087219 \t range_loss: 0.6918400526046753\n",
      "Step: 159 \t Loss: 1.3641560077667236\n",
      "pred_error: 0.06744007766246796 \t range_loss: 0.6903431415557861\n",
      "pred_error: 0.06751751154661179 \t range_loss: 0.6896004676818848\n",
      "pred_error: 0.06740810722112656 \t range_loss: 0.6903648972511292\n",
      "pred_error: 0.06740733981132507 \t range_loss: 0.6903648972511292\n",
      "pred_error: 0.0675073191523552 \t range_loss: 0.6901588439941406\n",
      "pred_error: 0.06754142045974731 \t range_loss: 0.688955545425415\n",
      "pred_error: 0.06754124164581299 \t range_loss: 0.688955545425415\n",
      "Step: 171 \t Loss: 1.363555669784546\n",
      "Step: 172 \t Loss: 1.3613431453704834\n",
      "Step: 173 \t Loss: 1.3601739406585693\n",
      "pred_error: 0.0674104169011116 \t range_loss: 0.6884365081787109\n",
      "pred_error: 0.06741025298833847 \t range_loss: 0.6884365081787109\n",
      "Step: 176 \t Loss: 1.3597491979599\n",
      "pred_error: 0.0673452615737915 \t range_loss: 0.6862965822219849\n",
      "pred_error: 0.06735921651124954 \t range_loss: 0.6865078806877136\n",
      "pred_error: 0.06776174157857895 \t range_loss: 0.684870719909668\n",
      "pred_error: 0.06765129417181015 \t range_loss: 0.685611367225647\n",
      "pred_error: 0.06763274222612381 \t range_loss: 0.6849459409713745\n",
      "Step: 184 \t Loss: 1.3593155145645142\n",
      "pred_error: 0.06766422837972641 \t range_loss: 0.6828563213348389\n",
      "pred_error: 0.06766393780708313 \t range_loss: 0.6828563213348389\n",
      "Step: 186 \t Loss: 1.3591203689575195\n",
      "pred_error: 0.06798402965068817 \t range_loss: 0.6827910542488098\n",
      "Step: 192 \t Loss: 1.3581825494766235\n",
      "pred_error: 0.06767051666975021 \t range_loss: 0.6814788579940796\n",
      "Step: 193 \t Loss: 1.3574960231781006\n",
      "pred_error: 0.06767851114273071 \t range_loss: 0.6807108521461487\n",
      "pred_error: 0.06767816096544266 \t range_loss: 0.6807108521461487\n",
      "Step: 194 \t Loss: 1.3554770946502686\n",
      "pred_error: 0.06752169877290726 \t range_loss: 0.6812007427215576\n",
      "pred_error: 0.06752157211303711 \t range_loss: 0.6812007427215576\n",
      "pred_error: 0.06752119958400726 \t range_loss: 0.6812007427215576\n",
      "pred_error: 0.0675215870141983 \t range_loss: 0.6812007427215576\n",
      "Step: 198 \t Loss: 1.354945182800293\n",
      "pred_error: 0.0674726739525795 \t range_loss: 0.6802194118499756\n",
      "Step: 199 \t Loss: 1.3548160791397095\n",
      "Step: 200 \t Loss: 1.3530030250549316\n",
      "pred_error: 0.06743527203798294 \t range_loss: 0.6786429286003113\n",
      "pred_error: 0.06761825829744339 \t range_loss: 0.6789032220840454\n",
      "Step: 212 \t Loss: 1.3528165817260742\n",
      "Step: 213 \t Loss: 1.3514801263809204\n",
      "pred_error: 0.06732642650604248 \t range_loss: 0.6782158613204956\n",
      "pred_error: 0.06723412871360779 \t range_loss: 0.679651141166687\n",
      "pred_error: 0.06745554506778717 \t range_loss: 0.6786119341850281\n",
      "pred_error: 0.06753973662853241 \t range_loss: 0.677518904209137\n",
      "Step: 226 \t Loss: 1.351121425628662\n",
      "pred_error: 0.06741149723529816 \t range_loss: 0.6769948601722717\n",
      "pred_error: 0.06747038662433624 \t range_loss: 0.6780732870101929\n",
      "pred_error: 0.06747046858072281 \t range_loss: 0.6780732870101929\n",
      "Step: 229 \t Loss: 1.3507976531982422\n",
      "pred_error: 0.0674431174993515 \t range_loss: 0.67636638879776\n",
      "Step: 230 \t Loss: 1.3502082824707031\n",
      "pred_error: 0.06729757785797119 \t range_loss: 0.677232563495636\n",
      "Step: 232 \t Loss: 1.3501806259155273\n",
      "Step: 233 \t Loss: 1.3493258953094482\n",
      "pred_error: 0.0673506036400795 \t range_loss: 0.675815999507904\n",
      "Step: 234 \t Loss: 1.3491520881652832\n",
      "pred_error: 0.0673627033829689 \t range_loss: 0.6755250692367554\n",
      "pred_error: 0.06736275553703308 \t range_loss: 0.6755250692367554\n",
      "Step: 235 \t Loss: 1.347594141960144\n",
      "pred_error: 0.06724783778190613 \t range_loss: 0.6751160025596619\n",
      "pred_error: 0.0673912763595581 \t range_loss: 0.6760677695274353\n",
      "pred_error: 0.06756572425365448 \t range_loss: 0.6745958924293518\n",
      "pred_error: 0.06757206469774246 \t range_loss: 0.6742735505104065\n",
      "pred_error: 0.06757225096225739 \t range_loss: 0.6742735505104065\n",
      "pred_error: 0.06757192313671112 \t range_loss: 0.6742735505104065\n",
      "Step: 249 \t Loss: 1.3464365005493164\n",
      "pred_error: 0.06738731265068054 \t range_loss: 0.672562301158905\n",
      "pred_error: 0.06738734990358353 \t range_loss: 0.672562301158905\n",
      "pred_error: 0.06739134341478348 \t range_loss: 0.6733571887016296\n",
      "pred_error: 0.06754845380783081 \t range_loss: 0.6734420657157898\n",
      "pred_error: 0.06750249117612839 \t range_loss: 0.6719855666160583\n",
      "Step: 259 \t Loss: 1.345357894897461\n",
      "Step: 262 \t Loss: 1.3446543216705322\n",
      "Step: 263 \t Loss: 1.3442883491516113\n",
      "pred_error: 0.06755721569061279 \t range_loss: 0.6706615090370178\n",
      "pred_error: 0.06750605255365372 \t range_loss: 0.6709195375442505\n",
      "pred_error: 0.06753135472536087 \t range_loss: 0.6704481244087219\n",
      "pred_error: 0.06753132492303848 \t range_loss: 0.6704481244087219\n",
      "pred_error: 0.06753291189670563 \t range_loss: 0.6714708805084229\n",
      "pred_error: 0.06753293424844742 \t range_loss: 0.6714708805084229\n",
      "Step: 275 \t Loss: 1.3434181213378906\n",
      "pred_error: 0.06733258068561554 \t range_loss: 0.6700928807258606\n",
      "pred_error: 0.06744174659252167 \t range_loss: 0.6691811084747314\n",
      "pred_error: 0.06754473596811295 \t range_loss: 0.6702287197113037\n",
      "pred_error: 0.06774353981018066 \t range_loss: 0.6687027215957642\n",
      "pred_error: 0.06768757104873657 \t range_loss: 0.6680368781089783\n",
      "pred_error: 0.06768752634525299 \t range_loss: 0.6680368781089783\n",
      "Step: 285 \t Loss: 1.3426227569580078\n",
      "Step: 286 \t Loss: 1.340104341506958\n",
      "pred_error: 0.06731855124235153 \t range_loss: 0.6669182181358337\n",
      "Step: 287 \t Loss: 1.3391644954681396\n",
      "pred_error: 0.0674925297498703 \t range_loss: 0.6680864095687866\n",
      "pred_error: 0.0674920529127121 \t range_loss: 0.6680864095687866\n",
      "pred_error: 0.06749211251735687 \t range_loss: 0.6680864095687866\n",
      "pred_error: 0.06748385727405548 \t range_loss: 0.667905867099762\n",
      "pred_error: 0.06748378276824951 \t range_loss: 0.667905867099762\n",
      "pred_error: 0.06779897212982178 \t range_loss: 0.66574627161026\n",
      "pred_error: 0.0676770806312561 \t range_loss: 0.6643107533454895\n",
      "pred_error: 0.06761643290519714 \t range_loss: 0.6648162007331848\n",
      "pred_error: 0.06770641356706619 \t range_loss: 0.6645916104316711\n",
      "pred_error: 0.06764612346887589 \t range_loss: 0.6634482741355896\n",
      "pred_error: 0.06764590740203857 \t range_loss: 0.6634482741355896\n",
      "Step: 311 \t Loss: 1.3380038738250732\n",
      "pred_error: 0.06747256219387054 \t range_loss: 0.6652225852012634\n",
      "pred_error: 0.06765162199735641 \t range_loss: 0.6660248041152954\n",
      "pred_error: 0.06785336136817932 \t range_loss: 0.6625676155090332\n",
      "pred_error: 0.06785187870264053 \t range_loss: 0.6625676155090332\n",
      "pred_error: 0.0678069218993187 \t range_loss: 0.662962019443512\n",
      "pred_error: 0.06780674308538437 \t range_loss: 0.662962019443512\n",
      "pred_error: 0.06783051043748856 \t range_loss: 0.6620003581047058\n",
      "pred_error: 0.06800464540719986 \t range_loss: 0.6625208258628845\n",
      "pred_error: 0.06800394505262375 \t range_loss: 0.6625208258628845\n",
      "pred_error: 0.06803155690431595 \t range_loss: 0.6621661186218262\n",
      "pred_error: 0.06806930154561996 \t range_loss: 0.6618306040763855\n",
      "pred_error: 0.06761717796325684 \t range_loss: 0.6618508100509644\n",
      "pred_error: 0.06786507368087769 \t range_loss: 0.6629317402839661\n",
      "pred_error: 0.06786505877971649 \t range_loss: 0.6629317402839661\n",
      "pred_error: 0.06799478083848953 \t range_loss: 0.661943256855011\n",
      "pred_error: 0.06797419488430023 \t range_loss: 0.6612650156021118\n",
      "Step: 342 \t Loss: 1.3363139629364014\n",
      "pred_error: 0.06765425205230713 \t range_loss: 0.6597706079483032\n",
      "pred_error: 0.06765440106391907 \t range_loss: 0.6597706079483032\n",
      "Step: 343 \t Loss: 1.3345773220062256\n",
      "pred_error: 0.06796199828386307 \t range_loss: 0.6610581278800964\n",
      "pred_error: 0.06797091662883759 \t range_loss: 0.661422073841095\n",
      "pred_error: 0.06781606376171112 \t range_loss: 0.6598714590072632\n",
      "pred_error: 0.06811289489269257 \t range_loss: 0.6602440476417542\n",
      "pred_error: 0.06801579147577286 \t range_loss: 0.6580597758293152\n",
      "pred_error: 0.06784020364284515 \t range_loss: 0.6590994000434875\n",
      "pred_error: 0.06781106442213058 \t range_loss: 0.656968355178833\n",
      "Step: 366 \t Loss: 1.3338637351989746\n",
      "pred_error: 0.06761065125465393 \t range_loss: 0.6577560901641846\n",
      "pred_error: 0.06806395947933197 \t range_loss: 0.6579142808914185\n",
      "pred_error: 0.06808886677026749 \t range_loss: 0.657697856426239\n",
      "pred_error: 0.06808852404356003 \t range_loss: 0.657697856426239\n",
      "pred_error: 0.06787644326686859 \t range_loss: 0.6580660939216614\n",
      "pred_error: 0.06796236336231232 \t range_loss: 0.6561864614486694\n",
      "pred_error: 0.06806229799985886 \t range_loss: 0.6563040018081665\n",
      "pred_error: 0.0680798590183258 \t range_loss: 0.6560623645782471\n",
      "pred_error: 0.06808367371559143 \t range_loss: 0.6561893820762634\n",
      "pred_error: 0.06809264421463013 \t range_loss: 0.6550236344337463\n",
      "pred_error: 0.06809279322624207 \t range_loss: 0.6550236344337463\n",
      "pred_error: 0.06823764741420746 \t range_loss: 0.6552417278289795\n",
      "pred_error: 0.0682990550994873 \t range_loss: 0.6548852324485779\n",
      "pred_error: 0.068184494972229 \t range_loss: 0.6556223034858704\n",
      "pred_error: 0.06819376349449158 \t range_loss: 0.6547141671180725\n",
      "pred_error: 0.0681939572095871 \t range_loss: 0.6547141671180725\n",
      "pred_error: 0.06792265176773071 \t range_loss: 0.6557718515396118\n",
      "pred_error: 0.06836235523223877 \t range_loss: 0.6545087099075317\n",
      "pred_error: 0.06836206465959549 \t range_loss: 0.6545087099075317\n",
      "pred_error: 0.06833966076374054 \t range_loss: 0.6539790630340576\n",
      "pred_error: 0.068471759557724 \t range_loss: 0.6533422470092773\n",
      "pred_error: 0.06847167760133743 \t range_loss: 0.6533422470092773\n",
      "pred_error: 0.0682891234755516 \t range_loss: 0.6546121835708618\n",
      "pred_error: 0.06838910281658173 \t range_loss: 0.6544454097747803\n",
      "Step: 412 \t Loss: 1.333140254020691\n",
      "pred_error: 0.06800395995378494 \t range_loss: 0.6531007885932922\n",
      "pred_error: 0.06821190565824509 \t range_loss: 0.6549189686775208\n",
      "pred_error: 0.0682598128914833 \t range_loss: 0.6536841988563538\n",
      "pred_error: 0.06837181001901627 \t range_loss: 0.6552409529685974\n",
      "pred_error: 0.0683721974492073 \t range_loss: 0.6552409529685974\n",
      "pred_error: 0.06833859533071518 \t range_loss: 0.6552432775497437\n",
      "pred_error: 0.06819630414247513 \t range_loss: 0.6545931696891785\n",
      "pred_error: 0.0681963860988617 \t range_loss: 0.6545931696891785\n",
      "pred_error: 0.06802260875701904 \t range_loss: 0.6534199118614197\n",
      "pred_error: 0.06802263110876083 \t range_loss: 0.6534199118614197\n",
      "Step: 423 \t Loss: 1.332887887954712\n",
      "Step: 424 \t Loss: 1.3323304653167725\n",
      "pred_error: 0.06816593557596207 \t range_loss: 0.6543831825256348\n",
      "pred_error: 0.06845304369926453 \t range_loss: 0.6527297496795654\n",
      "pred_error: 0.0685463696718216 \t range_loss: 0.6524994373321533\n",
      "pred_error: 0.0683835819363594 \t range_loss: 0.65274578332901\n",
      "pred_error: 0.06851138919591904 \t range_loss: 0.6521075963973999\n",
      "pred_error: 0.06851135194301605 \t range_loss: 0.6521075963973999\n",
      "pred_error: 0.06852055341005325 \t range_loss: 0.653438925743103\n",
      "pred_error: 0.06826770305633545 \t range_loss: 0.6513124704360962\n",
      "pred_error: 0.06820973008871078 \t range_loss: 0.6520838141441345\n",
      "pred_error: 0.06835507601499557 \t range_loss: 0.6513964533805847\n",
      "pred_error: 0.06830446422100067 \t range_loss: 0.6512702107429504\n",
      "pred_error: 0.0683906301856041 \t range_loss: 0.6502861380577087\n",
      "pred_error: 0.06846064329147339 \t range_loss: 0.6498986482620239\n",
      "pred_error: 0.068460613489151 \t range_loss: 0.6498986482620239\n",
      "pred_error: 0.06830526888370514 \t range_loss: 0.6498483419418335\n",
      "pred_error: 0.06830526888370514 \t range_loss: 0.6498483419418335\n",
      "pred_error: 0.06830523163080215 \t range_loss: 0.6498483419418335\n",
      "pred_error: 0.06837780028581619 \t range_loss: 0.6500903367996216\n",
      "pred_error: 0.06844950467348099 \t range_loss: 0.6519883275032043\n",
      "pred_error: 0.0684497281908989 \t range_loss: 0.6519883275032043\n",
      "pred_error: 0.06836393475532532 \t range_loss: 0.6493868827819824\n",
      "pred_error: 0.06844595819711685 \t range_loss: 0.6490485072135925\n",
      "pred_error: 0.06835687160491943 \t range_loss: 0.6499718427658081\n",
      "pred_error: 0.06878969818353653 \t range_loss: 0.6493491530418396\n",
      "pred_error: 0.0685291588306427 \t range_loss: 0.6479132175445557\n",
      "pred_error: 0.0685710608959198 \t range_loss: 0.6490823030471802\n",
      "pred_error: 0.0685710608959198 \t range_loss: 0.6490823030471802\n",
      "pred_error: 0.06854935735464096 \t range_loss: 0.6487158536911011\n",
      "pred_error: 0.06854952126741409 \t range_loss: 0.6487158536911011\n",
      "pred_error: 0.06860386580228806 \t range_loss: 0.6482253670692444\n",
      "pred_error: 0.06860185414552689 \t range_loss: 0.6482253670692444\n",
      "Step: 505 \t Loss: 1.3312029838562012\n",
      "pred_error: 0.06837983429431915 \t range_loss: 0.647404670715332\n",
      "pred_error: 0.0686926320195198 \t range_loss: 0.6469720005989075\n",
      "pred_error: 0.06843448430299759 \t range_loss: 0.6472229957580566\n",
      "pred_error: 0.0687343180179596 \t range_loss: 0.6466550827026367\n",
      "pred_error: 0.0685974508523941 \t range_loss: 0.6458735466003418\n",
      "pred_error: 0.06859758496284485 \t range_loss: 0.6458735466003418\n",
      "Step: 515 \t Loss: 1.3311129808425903\n",
      "pred_error: 0.06879518926143646 \t range_loss: 0.6459633111953735\n",
      "pred_error: 0.06870324909687042 \t range_loss: 0.6455464363098145\n",
      "pred_error: 0.06876036524772644 \t range_loss: 0.6462475061416626\n",
      "pred_error: 0.06876038014888763 \t range_loss: 0.6462475061416626\n",
      "pred_error: 0.06874024868011475 \t range_loss: 0.6453802585601807\n",
      "pred_error: 0.06870714575052261 \t range_loss: 0.6449826955795288\n",
      "pred_error: 0.06870566308498383 \t range_loss: 0.6449826955795288\n",
      "pred_error: 0.06890050321817398 \t range_loss: 0.6464720964431763\n",
      "pred_error: 0.06890050321817398 \t range_loss: 0.6464720964431763\n",
      "pred_error: 0.06876636296510696 \t range_loss: 0.647097647190094\n",
      "pred_error: 0.06858519464731216 \t range_loss: 0.6463781595230103\n",
      "pred_error: 0.06873199343681335 \t range_loss: 0.6467992067337036\n",
      "pred_error: 0.06857914477586746 \t range_loss: 0.6480777263641357\n",
      "pred_error: 0.0684330016374588 \t range_loss: 0.6470911502838135\n",
      "Step: 544 \t Loss: 1.3298197984695435\n",
      "pred_error: 0.06852947175502777 \t range_loss: 0.6459095478057861\n",
      "pred_error: 0.06869782507419586 \t range_loss: 0.6451391577720642\n",
      "pred_error: 0.06882348656654358 \t range_loss: 0.6456091403961182\n",
      "pred_error: 0.06892940402030945 \t range_loss: 0.644757091999054\n",
      "pred_error: 0.06892933696508408 \t range_loss: 0.644757091999054\n",
      "pred_error: 0.06915323436260223 \t range_loss: 0.6452926993370056\n",
      "pred_error: 0.0688730850815773 \t range_loss: 0.643944501876831\n",
      "pred_error: 0.06895790249109268 \t range_loss: 0.6428840160369873\n",
      "pred_error: 0.06882738322019577 \t range_loss: 0.6439806222915649\n",
      "pred_error: 0.06916411221027374 \t range_loss: 0.6444211602210999\n",
      "pred_error: 0.06885252147912979 \t range_loss: 0.6439756751060486\n",
      "pred_error: 0.06885255128145218 \t range_loss: 0.6439756751060486\n",
      "pred_error: 0.0689311996102333 \t range_loss: 0.6458902955055237\n",
      "pred_error: 0.06869953870773315 \t range_loss: 0.6450315713882446\n",
      "pred_error: 0.06885139644145966 \t range_loss: 0.6449702978134155\n",
      "pred_error: 0.06887945532798767 \t range_loss: 0.6448875069618225\n",
      "pred_error: 0.06880504637956619 \t range_loss: 0.6444686651229858\n",
      "pred_error: 0.06880499422550201 \t range_loss: 0.6444686651229858\n",
      "pred_error: 0.06864556670188904 \t range_loss: 0.6444177031517029\n",
      "pred_error: 0.06873638927936554 \t range_loss: 0.6450983285903931\n",
      "pred_error: 0.06867136061191559 \t range_loss: 0.6436237692832947\n",
      "pred_error: 0.06875741481781006 \t range_loss: 0.6426720023155212\n",
      "Step: 607 \t Loss: 1.328658103942871\n",
      "pred_error: 0.06867760419845581 \t range_loss: 0.6422833800315857\n",
      "pred_error: 0.06871011853218079 \t range_loss: 0.6428960561752319\n",
      "pred_error: 0.06893935799598694 \t range_loss: 0.643925666809082\n",
      "pred_error: 0.0691213384270668 \t range_loss: 0.6434319019317627\n",
      "pred_error: 0.06912725418806076 \t range_loss: 0.6427747011184692\n",
      "pred_error: 0.06894661486148834 \t range_loss: 0.6421175003051758\n",
      "pred_error: 0.06909329444169998 \t range_loss: 0.6416715383529663\n",
      "pred_error: 0.06898102909326553 \t range_loss: 0.6408587098121643\n",
      "pred_error: 0.06892812252044678 \t range_loss: 0.6414813995361328\n",
      "pred_error: 0.06924272328615189 \t range_loss: 0.6423379182815552\n",
      "pred_error: 0.06910743564367294 \t range_loss: 0.643493115901947\n",
      "pred_error: 0.06918463855981827 \t range_loss: 0.641242265701294\n",
      "pred_error: 0.06899023056030273 \t range_loss: 0.6405593156814575\n",
      "pred_error: 0.06899021565914154 \t range_loss: 0.6405593156814575\n",
      "pred_error: 0.06898599863052368 \t range_loss: 0.6437232494354248\n",
      "pred_error: 0.0691952258348465 \t range_loss: 0.6431055665016174\n",
      "pred_error: 0.06894740462303162 \t range_loss: 0.6421861052513123\n",
      "pred_error: 0.06903084367513657 \t range_loss: 0.6421509385108948\n",
      "pred_error: 0.06903195381164551 \t range_loss: 0.6420720815658569\n",
      "pred_error: 0.06891045719385147 \t range_loss: 0.6421654224395752\n",
      "pred_error: 0.06938309222459793 \t range_loss: 0.6429075598716736\n",
      "pred_error: 0.06909414380788803 \t range_loss: 0.6420410871505737\n",
      "pred_error: 0.0689578652381897 \t range_loss: 0.6423558592796326\n",
      "pred_error: 0.06895757466554642 \t range_loss: 0.6423558592796326\n",
      "pred_error: 0.06909916549921036 \t range_loss: 0.6420252323150635\n",
      "pred_error: 0.06909909099340439 \t range_loss: 0.6420252323150635\n",
      "pred_error: 0.0691208615899086 \t range_loss: 0.6418045163154602\n",
      "pred_error: 0.06912078708410263 \t range_loss: 0.6418045163154602\n",
      "pred_error: 0.0691208764910698 \t range_loss: 0.6418045163154602\n",
      "pred_error: 0.06898002326488495 \t range_loss: 0.6414685249328613\n",
      "pred_error: 0.0688120424747467 \t range_loss: 0.6414291858673096\n",
      "pred_error: 0.06915590167045593 \t range_loss: 0.6421858072280884\n",
      "pred_error: 0.06919287890195847 \t range_loss: 0.6413055062294006\n",
      "pred_error: 0.06919700652360916 \t range_loss: 0.641425609588623\n",
      "pred_error: 0.06900901347398758 \t range_loss: 0.6415414810180664\n",
      "pred_error: 0.06895571202039719 \t range_loss: 0.6410874128341675\n",
      "pred_error: 0.06911621987819672 \t range_loss: 0.6397079229354858\n",
      "pred_error: 0.06903883069753647 \t range_loss: 0.6415073871612549\n",
      "pred_error: 0.06910870224237442 \t range_loss: 0.6402840614318848\n",
      "pred_error: 0.06910514831542969 \t range_loss: 0.6402840614318848\n",
      "pred_error: 0.06928979605436325 \t range_loss: 0.6399544477462769\n",
      "pred_error: 0.06931774318218231 \t range_loss: 0.6394938230514526\n",
      "pred_error: 0.06931889057159424 \t range_loss: 0.6394938230514526\n",
      "pred_error: 0.06912174820899963 \t range_loss: 0.6381033062934875\n",
      "pred_error: 0.06920089572668076 \t range_loss: 0.6385915875434875\n",
      "pred_error: 0.06920941174030304 \t range_loss: 0.6378487348556519\n",
      "pred_error: 0.06950702518224716 \t range_loss: 0.6386293768882751\n",
      "pred_error: 0.06957055628299713 \t range_loss: 0.6382244229316711\n",
      "pred_error: 0.06950921565294266 \t range_loss: 0.6384129524230957\n",
      "pred_error: 0.0693766176700592 \t range_loss: 0.6386857032775879\n",
      "pred_error: 0.0693766176700592 \t range_loss: 0.6386857032775879\n",
      "pred_error: 0.06937479972839355 \t range_loss: 0.6382869482040405\n",
      "pred_error: 0.06930224597454071 \t range_loss: 0.6404934525489807\n",
      "pred_error: 0.06956077367067337 \t range_loss: 0.6388669610023499\n",
      "pred_error: 0.06935115158557892 \t range_loss: 0.6379490494728088\n",
      "pred_error: 0.06918466836214066 \t range_loss: 0.6388112902641296\n",
      "pred_error: 0.06933791190385818 \t range_loss: 0.6383368968963623\n",
      "pred_error: 0.06945033371448517 \t range_loss: 0.6385650038719177\n",
      "pred_error: 0.06926003843545914 \t range_loss: 0.6380690336227417\n",
      "pred_error: 0.06941745430231094 \t range_loss: 0.6390044689178467\n",
      "pred_error: 0.06925550848245621 \t range_loss: 0.6382448077201843\n",
      "pred_error: 0.06955592334270477 \t range_loss: 0.6390260457992554\n",
      "pred_error: 0.06973698735237122 \t range_loss: 0.6395275592803955\n",
      "pred_error: 0.0697377398610115 \t range_loss: 0.6395275592803955\n",
      "pred_error: 0.06940348446369171 \t range_loss: 0.6366971135139465\n",
      "pred_error: 0.06940345466136932 \t range_loss: 0.6366971135139465\n",
      "pred_error: 0.06923554092645645 \t range_loss: 0.6371827721595764\n",
      "pred_error: 0.06923534721136093 \t range_loss: 0.6371827721595764\n",
      "pred_error: 0.06948650628328323 \t range_loss: 0.6358639597892761\n",
      "pred_error: 0.06948655843734741 \t range_loss: 0.6358639597892761\n",
      "pred_error: 0.06963592767715454 \t range_loss: 0.6364927887916565\n",
      "pred_error: 0.06963516026735306 \t range_loss: 0.6364927887916565\n",
      "pred_error: 0.0695369616150856 \t range_loss: 0.6366468071937561\n",
      "pred_error: 0.06949695944786072 \t range_loss: 0.637354850769043\n",
      "pred_error: 0.06961856782436371 \t range_loss: 0.6363819241523743\n",
      "pred_error: 0.06947807222604752 \t range_loss: 0.6375294327735901\n",
      "pred_error: 0.0694856196641922 \t range_loss: 0.6378952860832214\n",
      "BEST LOSS: 1.3286581\n",
      "==== Model: block3_cob_activation_norm  in Layer: 3 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 15:02:50,026 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 15:04:46,912 model.rs:1246 value (-590528) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 15:04:46,917 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 15:05:05,126 model.rs:1246 value (-590528) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 15:05:05,129 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 15:05:05,142 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 15:05:05,152 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 15:05:05,162 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000022542627 | -0.000016748905 | 0.00029869378 | -0.00035107136 | 0.00004945049  | 0.000016748905   | 0.00035107136 | 0             | 0.0000000041202632 | 0.0001190524       | 0.0013027088           |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 645018 64 [-500564, 336270] 1 [16]\n",
      "===============================\n",
      "==== Model: block3_cob_activation_norm_teleported  in Layer: 3 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 15:05:25,445 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 15:07:19,981 model.rs:1246 value (552192) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 15:07:19,986 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 15:07:39,722 model.rs:1246 value (552192) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 15:07:39,725 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 15:07:39,743 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 15:07:39,757 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 15:07:39,768 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000020264486 | -0.000049352646 | 0.00031274557 | -0.00039947033 | 0.00004967533  | 0.000049352646   | 0.00039947033 | 0             | 0.000000004169303  | 0.00011785378      | 0.0010370193           |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 645016 64 [-328506, 204950] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 4 , \t  activation_stats: {'relu_1': {'norm': tensor(569.6558), 'max': tensor(2.8129), 'min': tensor(-5.8987), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(8.7116)\n",
      "Step: 0 \t Loss: 1.765427589416504\n",
      "Step: 1 \t Loss: 1.716856837272644\n",
      "Step: 2 \t Loss: 1.7043430805206299\n",
      "Step: 3 \t Loss: 1.6882336139678955\n",
      "pred_error: 0.07660149037837982 \t range_loss: 0.9222047924995422\n",
      "Step: 4 \t Loss: 1.6688698530197144\n",
      "Step: 5 \t Loss: 1.6521217823028564\n",
      "Step: 6 \t Loss: 1.6512415409088135\n",
      "Step: 7 \t Loss: 1.63741934299469\n",
      "Step: 8 \t Loss: 1.629514455795288\n",
      "Step: 9 \t Loss: 1.6225757598876953\n",
      "Step: 11 \t Loss: 1.6100523471832275\n",
      "Step: 12 \t Loss: 1.6031267642974854\n",
      "pred_error: 0.074282206594944 \t range_loss: 0.8603045344352722\n",
      "Step: 13 \t Loss: 1.6006598472595215\n",
      "pred_error: 0.07470656931400299 \t range_loss: 0.8575240969657898\n",
      "Step: 15 \t Loss: 1.5976920127868652\n",
      "pred_error: 0.07431768625974655 \t range_loss: 0.8545151352882385\n",
      "pred_error: 0.07431773841381073 \t range_loss: 0.8545151352882385\n",
      "Step: 16 \t Loss: 1.5815926790237427\n",
      "pred_error: 0.07468380033969879 \t range_loss: 0.8389164209365845\n",
      "pred_error: 0.07468325644731522 \t range_loss: 0.8389164209365845\n",
      "Step: 18 \t Loss: 1.5760424137115479\n",
      "Step: 19 \t Loss: 1.569365382194519\n",
      "pred_error: 0.07330542057752609 \t range_loss: 0.8385725617408752\n",
      "pred_error: 0.07412916421890259 \t range_loss: 0.8323830962181091\n",
      "Step: 22 \t Loss: 1.5680416822433472\n",
      "Step: 23 \t Loss: 1.560689926147461\n",
      "pred_error: 0.07336197048425674 \t range_loss: 0.8270641565322876\n",
      "pred_error: 0.07336211949586868 \t range_loss: 0.8270641565322876\n",
      "pred_error: 0.07401387393474579 \t range_loss: 0.8251057863235474\n",
      "Step: 26 \t Loss: 1.5569534301757812\n",
      "Step: 27 \t Loss: 1.5534998178482056\n",
      "Step: 29 \t Loss: 1.5464913845062256\n",
      "Step: 30 \t Loss: 1.5452446937561035\n",
      "pred_error: 0.07311597466468811 \t range_loss: 0.8140848875045776\n",
      "Step: 32 \t Loss: 1.5395495891571045\n",
      "pred_error: 0.07336409389972687 \t range_loss: 0.805908203125\n",
      "Step: 33 \t Loss: 1.5374822616577148\n",
      "Step: 35 \t Loss: 1.5361301898956299\n",
      "Step: 36 \t Loss: 1.5344876050949097\n",
      "pred_error: 0.07316894829273224 \t range_loss: 0.8027982711791992\n",
      "Step: 39 \t Loss: 1.5323548316955566\n",
      "pred_error: 0.07317667454481125 \t range_loss: 0.800583004951477\n",
      "pred_error: 0.0731777474284172 \t range_loss: 0.800583004951477\n",
      "pred_error: 0.07317725569009781 \t range_loss: 0.800583004951477\n",
      "Step: 41 \t Loss: 1.530057430267334\n",
      "pred_error: 0.07344372570514679 \t range_loss: 0.7956189513206482\n",
      "Step: 42 \t Loss: 1.5261895656585693\n",
      "pred_error: 0.0730561763048172 \t range_loss: 0.7956286668777466\n",
      "pred_error: 0.07305634021759033 \t range_loss: 0.7956286668777466\n",
      "pred_error: 0.07305629551410675 \t range_loss: 0.7956286668777466\n",
      "Step: 44 \t Loss: 1.5210551023483276\n",
      "pred_error: 0.07410018891096115 \t range_loss: 0.7838401794433594\n",
      "Step: 47 \t Loss: 1.5175714492797852\n",
      "Step: 50 \t Loss: 1.515770435333252\n",
      "Step: 53 \t Loss: 1.513264536857605\n",
      "Step: 55 \t Loss: 1.513118028640747\n",
      "pred_error: 0.07422204315662384 \t range_loss: 0.7708976864814758\n",
      "Step: 56 \t Loss: 1.5062073469161987\n",
      "Step: 59 \t Loss: 1.5057220458984375\n",
      "pred_error: 0.07370148599147797 \t range_loss: 0.7687084078788757\n",
      "pred_error: 0.07421419024467468 \t range_loss: 0.7687942981719971\n",
      "Step: 62 \t Loss: 1.5048232078552246\n",
      "pred_error: 0.07427269965410233 \t range_loss: 0.7666653394699097\n",
      "Step: 65 \t Loss: 1.5005502700805664\n",
      "pred_error: 0.07411032915115356 \t range_loss: 0.7646855115890503\n",
      "Step: 68 \t Loss: 1.498531699180603\n",
      "Step: 71 \t Loss: 1.4962553977966309\n",
      "pred_error: 0.07352662831544876 \t range_loss: 0.7609900832176208\n",
      "pred_error: 0.07306423038244247 \t range_loss: 0.7699834704399109\n",
      "Step: 74 \t Loss: 1.4953944683074951\n",
      "pred_error: 0.07325893640518188 \t range_loss: 0.7688269019126892\n",
      "Step: 77 \t Loss: 1.4950120449066162\n",
      "Step: 83 \t Loss: 1.4907504320144653\n",
      "pred_error: 0.07422638684511185 \t range_loss: 0.7523178458213806\n",
      "Step: 86 \t Loss: 1.4886771440505981\n",
      "pred_error: 0.07372627407312393 \t range_loss: 0.75141441822052\n",
      "pred_error: 0.07325290143489838 \t range_loss: 0.7639060020446777\n",
      "Step: 92 \t Loss: 1.4875869750976562\n",
      "Step: 95 \t Loss: 1.4825208187103271\n",
      "Step: 104 \t Loss: 1.482036828994751\n",
      "Step: 107 \t Loss: 1.4783570766448975\n",
      "pred_error: 0.07342888414859772 \t range_loss: 0.7440676689147949\n",
      "pred_error: 0.0732511505484581 \t range_loss: 0.7555024027824402\n",
      "pred_error: 0.07381848990917206 \t range_loss: 0.7438814043998718\n",
      "pred_error: 0.07366891205310822 \t range_loss: 0.7534670829772949\n",
      "pred_error: 0.07400460541248322 \t range_loss: 0.7423523664474487\n",
      "Step: 116 \t Loss: 1.477433681488037\n",
      "pred_error: 0.07334160059690475 \t range_loss: 0.7514275312423706\n",
      "pred_error: 0.07445096969604492 \t range_loss: 0.7417652606964111\n",
      "pred_error: 0.07330194115638733 \t range_loss: 0.7494258284568787\n",
      "pred_error: 0.07330219447612762 \t range_loss: 0.7494258284568787\n",
      "pred_error: 0.07405280321836472 \t range_loss: 0.7391184568405151\n",
      "pred_error: 0.07459226250648499 \t range_loss: 0.7401924133300781\n",
      "pred_error: 0.07459279894828796 \t range_loss: 0.7401924133300781\n",
      "Step: 128 \t Loss: 1.4755632877349854\n",
      "pred_error: 0.07365158945322037 \t range_loss: 0.7390458583831787\n",
      "pred_error: 0.07393363863229752 \t range_loss: 0.7364515066146851\n",
      "Step: 134 \t Loss: 1.4737489223480225\n",
      "Step: 137 \t Loss: 1.4715994596481323\n",
      "pred_error: 0.07382818311452866 \t range_loss: 0.7333175539970398\n",
      "pred_error: 0.07346779108047485 \t range_loss: 0.7447740435600281\n",
      "Step: 140 \t Loss: 1.4712998867034912\n",
      "pred_error: 0.07369416952133179 \t range_loss: 0.7438672780990601\n",
      "pred_error: 0.07467722147703171 \t range_loss: 0.7316545844078064\n",
      "Step: 146 \t Loss: 1.4706108570098877\n",
      "pred_error: 0.07394009083509445 \t range_loss: 0.7312136292457581\n",
      "pred_error: 0.07352551072835922 \t range_loss: 0.7426086664199829\n",
      "pred_error: 0.07459083199501038 \t range_loss: 0.7306048274040222\n",
      "pred_error: 0.07417459785938263 \t range_loss: 0.7300553917884827\n",
      "pred_error: 0.0737086683511734 \t range_loss: 0.7403753399848938\n",
      "pred_error: 0.07473812997341156 \t range_loss: 0.7310311198234558\n",
      "pred_error: 0.07405606657266617 \t range_loss: 0.7304091453552246\n",
      "pred_error: 0.07494720816612244 \t range_loss: 0.7296456694602966\n",
      "Step: 158 \t Loss: 1.4698694944381714\n",
      "pred_error: 0.07410012185573578 \t range_loss: 0.7288708090782166\n",
      "pred_error: 0.07478219270706177 \t range_loss: 0.729239821434021\n",
      "Step: 161 \t Loss: 1.4680955410003662\n",
      "pred_error: 0.07372180372476578 \t range_loss: 0.7398349642753601\n",
      "pred_error: 0.07377130538225174 \t range_loss: 0.7390167713165283\n",
      "Step: 167 \t Loss: 1.4654595851898193\n",
      "pred_error: 0.0739581435918808 \t range_loss: 0.7258797287940979\n",
      "pred_error: 0.07344180345535278 \t range_loss: 0.7389280200004578\n",
      "pred_error: 0.07400572299957275 \t range_loss: 0.7272889018058777\n",
      "pred_error: 0.07378552109003067 \t range_loss: 0.7373903393745422\n",
      "pred_error: 0.0747879222035408 \t range_loss: 0.727602481842041\n",
      "pred_error: 0.07499123364686966 \t range_loss: 0.7289425730705261\n",
      "pred_error: 0.07378634065389633 \t range_loss: 0.7362185716629028\n",
      "Step: 179 \t Loss: 1.463777780532837\n",
      "pred_error: 0.07474525272846222 \t range_loss: 0.7257584929466248\n",
      "pred_error: 0.07474523782730103 \t range_loss: 0.7257584929466248\n",
      "pred_error: 0.07474546879529953 \t range_loss: 0.7257584929466248\n",
      "pred_error: 0.07474541664123535 \t range_loss: 0.7257584929466248\n",
      "pred_error: 0.07484099268913269 \t range_loss: 0.7250533103942871\n",
      "pred_error: 0.0748405009508133 \t range_loss: 0.7250533103942871\n",
      "Step: 188 \t Loss: 1.4636144638061523\n",
      "Step: 191 \t Loss: 1.46354079246521\n",
      "pred_error: 0.07418455183506012 \t range_loss: 0.7311160564422607\n",
      "pred_error: 0.07392723113298416 \t range_loss: 0.7313605546951294\n",
      "pred_error: 0.073927141726017 \t range_loss: 0.7313605546951294\n",
      "pred_error: 0.07458388805389404 \t range_loss: 0.7194685935974121\n",
      "Step: 206 \t Loss: 1.4621479511260986\n",
      "pred_error: 0.07526607811450958 \t range_loss: 0.7183663845062256\n",
      "pred_error: 0.07517647743225098 \t range_loss: 0.7174094319343567\n",
      "pred_error: 0.07423383742570877 \t range_loss: 0.7289526462554932\n",
      "pred_error: 0.07423414289951324 \t range_loss: 0.7289526462554932\n",
      "pred_error: 0.075479656457901 \t range_loss: 0.7161487936973572\n",
      "pred_error: 0.07422849535942078 \t range_loss: 0.7276442646980286\n",
      "pred_error: 0.07529779523611069 \t range_loss: 0.7163490653038025\n",
      "Step: 221 \t Loss: 1.4615700244903564\n",
      "pred_error: 0.07530081272125244 \t range_loss: 0.7161317467689514\n",
      "pred_error: 0.074812151491642 \t range_loss: 0.7157179713249207\n",
      "pred_error: 0.07439084351062775 \t range_loss: 0.7264408469200134\n",
      "pred_error: 0.07547933608293533 \t range_loss: 0.7171656489372253\n",
      "pred_error: 0.07494118809700012 \t range_loss: 0.7161186933517456\n",
      "pred_error: 0.07494115084409714 \t range_loss: 0.7161186933517456\n",
      "pred_error: 0.0743015706539154 \t range_loss: 0.7257938385009766\n",
      "pred_error: 0.07430130243301392 \t range_loss: 0.7257938385009766\n",
      "pred_error: 0.07430149614810944 \t range_loss: 0.7257938385009766\n",
      "pred_error: 0.07537079602479935 \t range_loss: 0.7139212489128113\n",
      "pred_error: 0.07537111639976501 \t range_loss: 0.7139212489128113\n",
      "Step: 233 \t Loss: 1.458695411682129\n",
      "pred_error: 0.07456668466329575 \t range_loss: 0.7130295038223267\n",
      "pred_error: 0.07462384551763535 \t range_loss: 0.7152103781700134\n",
      "pred_error: 0.07445741444826126 \t range_loss: 0.7260509729385376\n",
      "pred_error: 0.07544279098510742 \t range_loss: 0.7150919437408447\n",
      "pred_error: 0.07544248551130295 \t range_loss: 0.7150919437408447\n",
      "pred_error: 0.0744830071926117 \t range_loss: 0.7208767533302307\n",
      "pred_error: 0.07555098086595535 \t range_loss: 0.7118706703186035\n",
      "Step: 245 \t Loss: 1.4572679996490479\n",
      "pred_error: 0.07460545748472214 \t range_loss: 0.7112134099006653\n",
      "pred_error: 0.07476558536291122 \t range_loss: 0.7138970494270325\n",
      "pred_error: 0.0751166045665741 \t range_loss: 0.7107102274894714\n",
      "pred_error: 0.07577458769083023 \t range_loss: 0.7120204567909241\n",
      "pred_error: 0.07441801577806473 \t range_loss: 0.7229529619216919\n",
      "pred_error: 0.07476963102817535 \t range_loss: 0.7224744558334351\n",
      "pred_error: 0.07500594854354858 \t range_loss: 0.7096461057662964\n",
      "pred_error: 0.07500608265399933 \t range_loss: 0.7096461057662964\n",
      "pred_error: 0.07452572882175446 \t range_loss: 0.7223013043403625\n",
      "pred_error: 0.07452856749296188 \t range_loss: 0.7223013043403625\n",
      "pred_error: 0.07479266822338104 \t range_loss: 0.7093760371208191\n",
      "pred_error: 0.07456498593091965 \t range_loss: 0.7224469780921936\n",
      "pred_error: 0.07499568164348602 \t range_loss: 0.7087599039077759\n",
      "pred_error: 0.07499571144580841 \t range_loss: 0.7087599039077759\n",
      "pred_error: 0.0757208988070488 \t range_loss: 0.7096089720726013\n",
      "pred_error: 0.07488136738538742 \t range_loss: 0.7196916937828064\n",
      "pred_error: 0.07472214847803116 \t range_loss: 0.7194557785987854\n",
      "pred_error: 0.07572609186172485 \t range_loss: 0.7071180939674377\n",
      "Step: 282 \t Loss: 1.454870343208313\n",
      "pred_error: 0.0747988298535347 \t range_loss: 0.7068820595741272\n",
      "pred_error: 0.07479877024888992 \t range_loss: 0.7068820595741272\n",
      "pred_error: 0.07494867593050003 \t range_loss: 0.7077209949493408\n",
      "pred_error: 0.07481082528829575 \t range_loss: 0.7203071713447571\n",
      "pred_error: 0.07587197422981262 \t range_loss: 0.7070651650428772\n",
      "pred_error: 0.07605737447738647 \t range_loss: 0.7075657248497009\n",
      "pred_error: 0.07509475201368332 \t range_loss: 0.7054858207702637\n",
      "pred_error: 0.07502390444278717 \t range_loss: 0.7168708443641663\n",
      "pred_error: 0.07502368837594986 \t range_loss: 0.7168708443641663\n",
      "pred_error: 0.07596299797296524 \t range_loss: 0.7054664492607117\n",
      "pred_error: 0.07596581429243088 \t range_loss: 0.7054664492607117\n",
      "pred_error: 0.07596692442893982 \t range_loss: 0.7054664492607117\n",
      "pred_error: 0.07517213374376297 \t range_loss: 0.7045005559921265\n",
      "pred_error: 0.07480985671281815 \t range_loss: 0.716266393661499\n",
      "pred_error: 0.07528313249349594 \t range_loss: 0.7084181308746338\n",
      "pred_error: 0.076486736536026 \t range_loss: 0.7026000022888184\n",
      "pred_error: 0.076486736536026 \t range_loss: 0.7026000022888184\n",
      "pred_error: 0.07599855214357376 \t range_loss: 0.7042009830474854\n",
      "Step: 317 \t Loss: 1.4547711610794067\n",
      "pred_error: 0.0751749575138092 \t range_loss: 0.7030224204063416\n",
      "pred_error: 0.07600858062505722 \t range_loss: 0.707321047782898\n",
      "pred_error: 0.07617484033107758 \t range_loss: 0.7037078142166138\n",
      "pred_error: 0.075369693338871 \t range_loss: 0.7034083008766174\n",
      "pred_error: 0.07611384242773056 \t range_loss: 0.7027646899223328\n",
      "pred_error: 0.07551266252994537 \t range_loss: 0.7024361491203308\n",
      "pred_error: 0.07599476724863052 \t range_loss: 0.7071384787559509\n",
      "pred_error: 0.07599503546953201 \t range_loss: 0.7071384787559509\n",
      "pred_error: 0.07631819695234299 \t range_loss: 0.7025803327560425\n",
      "pred_error: 0.07502458989620209 \t range_loss: 0.7120270133018494\n",
      "pred_error: 0.07510436326265335 \t range_loss: 0.7128167152404785\n",
      "pred_error: 0.0751042366027832 \t range_loss: 0.7128167152404785\n",
      "pred_error: 0.07510404288768768 \t range_loss: 0.7128167152404785\n",
      "pred_error: 0.07556314766407013 \t range_loss: 0.7019910216331482\n",
      "pred_error: 0.07513701915740967 \t range_loss: 0.7131862044334412\n",
      "pred_error: 0.07622602581977844 \t range_loss: 0.7016516327857971\n",
      "pred_error: 0.07497194409370422 \t range_loss: 0.7129464745521545\n",
      "pred_error: 0.07538920640945435 \t range_loss: 0.7009696960449219\n",
      "pred_error: 0.07548529654741287 \t range_loss: 0.7023021578788757\n",
      "pred_error: 0.07548569142818451 \t range_loss: 0.7023021578788757\n",
      "pred_error: 0.07525765895843506 \t range_loss: 0.71279376745224\n",
      "pred_error: 0.07553420960903168 \t range_loss: 0.7129267454147339\n",
      "pred_error: 0.07558683305978775 \t range_loss: 0.7017410397529602\n",
      "pred_error: 0.07510163635015488 \t range_loss: 0.7133749127388\n",
      "pred_error: 0.07510162889957428 \t range_loss: 0.7133749127388\n",
      "Step: 365 \t Loss: 1.4529774188995361\n",
      "pred_error: 0.07536274939775467 \t range_loss: 0.6993482708930969\n",
      "pred_error: 0.07564707100391388 \t range_loss: 0.6988816261291504\n",
      "pred_error: 0.07564735412597656 \t range_loss: 0.6988816261291504\n",
      "pred_error: 0.07564695179462433 \t range_loss: 0.6988816261291504\n",
      "pred_error: 0.07639781385660172 \t range_loss: 0.701242208480835\n",
      "pred_error: 0.07582344114780426 \t range_loss: 0.6989041566848755\n",
      "pred_error: 0.07525454461574554 \t range_loss: 0.7112661600112915\n",
      "pred_error: 0.07558705657720566 \t range_loss: 0.698397696018219\n",
      "pred_error: 0.07536359131336212 \t range_loss: 0.7054247856140137\n",
      "pred_error: 0.07536301016807556 \t range_loss: 0.7054247856140137\n",
      "pred_error: 0.07653103023767471 \t range_loss: 0.7020555734634399\n",
      "pred_error: 0.07637679576873779 \t range_loss: 0.7006428241729736\n",
      "pred_error: 0.07551830261945724 \t range_loss: 0.7126182913780212\n",
      "pred_error: 0.07565630227327347 \t range_loss: 0.6987939476966858\n",
      "pred_error: 0.07557313144207001 \t range_loss: 0.6978082656860352\n",
      "pred_error: 0.07660763710737228 \t range_loss: 0.6979648470878601\n",
      "pred_error: 0.07658251374959946 \t range_loss: 0.6981298923492432\n",
      "pred_error: 0.07535858452320099 \t range_loss: 0.7094224691390991\n",
      "pred_error: 0.07565250247716904 \t range_loss: 0.7093972563743591\n",
      "pred_error: 0.07565256953239441 \t range_loss: 0.7093972563743591\n",
      "pred_error: 0.07603724300861359 \t range_loss: 0.6982160210609436\n",
      "pred_error: 0.07563700526952744 \t range_loss: 0.708817720413208\n",
      "pred_error: 0.07585295289754868 \t range_loss: 0.696066677570343\n",
      "pred_error: 0.07551445811986923 \t range_loss: 0.7080766558647156\n",
      "pred_error: 0.07658522576093674 \t range_loss: 0.6972758173942566\n",
      "pred_error: 0.07600731402635574 \t range_loss: 0.698178768157959\n",
      "pred_error: 0.0758368968963623 \t range_loss: 0.7074312567710876\n",
      "pred_error: 0.0757373720407486 \t range_loss: 0.6963658928871155\n",
      "pred_error: 0.0766826644539833 \t range_loss: 0.6962489485740662\n",
      "pred_error: 0.07689284533262253 \t range_loss: 0.6967939138412476\n",
      "pred_error: 0.07597897946834564 \t range_loss: 0.6966878771781921\n",
      "pred_error: 0.07561585307121277 \t range_loss: 0.7069311738014221\n",
      "pred_error: 0.07543565332889557 \t range_loss: 0.708077073097229\n",
      "pred_error: 0.07691981643438339 \t range_loss: 0.6960005164146423\n",
      "pred_error: 0.07692009210586548 \t range_loss: 0.6960005164146423\n",
      "pred_error: 0.0767955407500267 \t range_loss: 0.6954078674316406\n",
      "pred_error: 0.07679581642150879 \t range_loss: 0.6954078674316406\n",
      "pred_error: 0.07679490745067596 \t range_loss: 0.6954078674316406\n",
      "pred_error: 0.07595207542181015 \t range_loss: 0.694245457649231\n",
      "pred_error: 0.07619970291852951 \t range_loss: 0.6948844790458679\n",
      "pred_error: 0.07611209154129028 \t range_loss: 0.6950936913490295\n",
      "pred_error: 0.07687077671289444 \t range_loss: 0.6940114498138428\n",
      "pred_error: 0.0760728120803833 \t range_loss: 0.6936219334602356\n",
      "Step: 451 \t Loss: 1.4521653652191162\n",
      "pred_error: 0.07587301731109619 \t range_loss: 0.6934351325035095\n",
      "pred_error: 0.07587301731109619 \t range_loss: 0.6934351325035095\n",
      "pred_error: 0.07600624114274979 \t range_loss: 0.6942587494850159\n",
      "pred_error: 0.07600603252649307 \t range_loss: 0.6942587494850159\n",
      "pred_error: 0.07607891410589218 \t range_loss: 0.6951702833175659\n",
      "pred_error: 0.07576443254947662 \t range_loss: 0.7079958915710449\n",
      "pred_error: 0.07576445490121841 \t range_loss: 0.7079958915710449\n",
      "pred_error: 0.07576440274715424 \t range_loss: 0.7079958915710449\n",
      "pred_error: 0.07580377161502838 \t range_loss: 0.7082390785217285\n",
      "pred_error: 0.07615149766206741 \t range_loss: 0.6947723627090454\n",
      "pred_error: 0.07735955715179443 \t range_loss: 0.6941378712654114\n",
      "pred_error: 0.07638340443372726 \t range_loss: 0.6939119100570679\n",
      "pred_error: 0.07583477348089218 \t range_loss: 0.7061440944671631\n",
      "pred_error: 0.07583501189947128 \t range_loss: 0.7061440944671631\n",
      "pred_error: 0.07583519071340561 \t range_loss: 0.7061440944671631\n",
      "pred_error: 0.07583422213792801 \t range_loss: 0.7061440944671631\n",
      "pred_error: 0.07721498608589172 \t range_loss: 0.693329930305481\n",
      "pred_error: 0.07622358947992325 \t range_loss: 0.6938664317131042\n",
      "pred_error: 0.07622350752353668 \t range_loss: 0.6938664317131042\n",
      "pred_error: 0.07622340321540833 \t range_loss: 0.6938664317131042\n",
      "pred_error: 0.07689008116722107 \t range_loss: 0.6972097158432007\n",
      "pred_error: 0.07619241625070572 \t range_loss: 0.6972145438194275\n",
      "pred_error: 0.07702084630727768 \t range_loss: 0.6942716240882874\n",
      "pred_error: 0.075550876557827 \t range_loss: 0.7066576480865479\n",
      "pred_error: 0.07672867923974991 \t range_loss: 0.6952783465385437\n",
      "pred_error: 0.07661382853984833 \t range_loss: 0.6948899030685425\n",
      "pred_error: 0.07596573233604431 \t range_loss: 0.6942546963691711\n",
      "pred_error: 0.07555096596479416 \t range_loss: 0.7058200836181641\n",
      "pred_error: 0.07658146321773529 \t range_loss: 0.6977213025093079\n",
      "pred_error: 0.0765816867351532 \t range_loss: 0.6977213025093079\n",
      "pred_error: 0.07598083466291428 \t range_loss: 0.6987788677215576\n",
      "pred_error: 0.07559646666049957 \t range_loss: 0.7067760229110718\n",
      "pred_error: 0.07665272057056427 \t range_loss: 0.6950600147247314\n",
      "pred_error: 0.07587607949972153 \t range_loss: 0.6953478455543518\n",
      "pred_error: 0.07587622851133347 \t range_loss: 0.6953478455543518\n",
      "pred_error: 0.07547871768474579 \t range_loss: 0.7058790326118469\n",
      "pred_error: 0.07605444639921188 \t range_loss: 0.6943410634994507\n",
      "pred_error: 0.07576794177293777 \t range_loss: 0.7051584124565125\n",
      "pred_error: 0.07618206739425659 \t range_loss: 0.6937885284423828\n",
      "pred_error: 0.07618219405412674 \t range_loss: 0.6937885284423828\n",
      "Step: 506 \t Loss: 1.4521278142929077\n",
      "pred_error: 0.07561908662319183 \t range_loss: 0.7013633251190186\n",
      "pred_error: 0.07561901211738586 \t range_loss: 0.7013633251190186\n",
      "pred_error: 0.07706478983163834 \t range_loss: 0.6948118805885315\n",
      "pred_error: 0.0770648866891861 \t range_loss: 0.6948118805885315\n",
      "pred_error: 0.07683461159467697 \t range_loss: 0.6947998404502869\n",
      "pred_error: 0.07683446258306503 \t range_loss: 0.6947998404502869\n",
      "pred_error: 0.07662473618984222 \t range_loss: 0.6936559677124023\n",
      "pred_error: 0.07588283717632294 \t range_loss: 0.7058631181716919\n",
      "pred_error: 0.07588271051645279 \t range_loss: 0.7058631181716919\n",
      "pred_error: 0.07689232379198074 \t range_loss: 0.6965630650520325\n",
      "pred_error: 0.0769285187125206 \t range_loss: 0.6927583813667297\n",
      "Step: 530 \t Loss: 1.4520411491394043\n",
      "pred_error: 0.07595624774694443 \t range_loss: 0.6924780011177063\n",
      "pred_error: 0.07628906518220901 \t range_loss: 0.6925604939460754\n",
      "pred_error: 0.07709814608097076 \t range_loss: 0.6913893222808838\n",
      "pred_error: 0.07613340020179749 \t range_loss: 0.6915412545204163\n",
      "pred_error: 0.07613366842269897 \t range_loss: 0.6915412545204163\n",
      "pred_error: 0.0761336013674736 \t range_loss: 0.6915412545204163\n",
      "pred_error: 0.0766986832022667 \t range_loss: 0.6897464990615845\n",
      "pred_error: 0.07621467858552933 \t range_loss: 0.6999621987342834\n",
      "pred_error: 0.07609402388334274 \t range_loss: 0.6980730891227722\n",
      "pred_error: 0.07636778801679611 \t range_loss: 0.6901718974113464\n",
      "pred_error: 0.07636769860982895 \t range_loss: 0.6901718974113464\n",
      "pred_error: 0.07636699080467224 \t range_loss: 0.6901718974113464\n",
      "pred_error: 0.07639371603727341 \t range_loss: 0.6922215223312378\n",
      "pred_error: 0.07713223993778229 \t range_loss: 0.6950963735580444\n",
      "pred_error: 0.07643962651491165 \t range_loss: 0.6917739510536194\n",
      "pred_error: 0.07634947448968887 \t range_loss: 0.6931282877922058\n",
      "pred_error: 0.07718078792095184 \t range_loss: 0.6919364929199219\n",
      "pred_error: 0.07595059275627136 \t range_loss: 0.7033941149711609\n",
      "pred_error: 0.07703817635774612 \t range_loss: 0.6915919780731201\n",
      "pred_error: 0.07578559219837189 \t range_loss: 0.7023277878761292\n",
      "pred_error: 0.07684475928544998 \t range_loss: 0.6926042437553406\n",
      "pred_error: 0.07715176045894623 \t range_loss: 0.6913996338844299\n",
      "pred_error: 0.07613204419612885 \t range_loss: 0.702974796295166\n",
      "pred_error: 0.07635219395160675 \t range_loss: 0.6916491389274597\n",
      "pred_error: 0.07635212689638138 \t range_loss: 0.6916491389274597\n",
      "pred_error: 0.07706601917743683 \t range_loss: 0.6916134357452393\n",
      "pred_error: 0.07653787732124329 \t range_loss: 0.6921059489250183\n",
      "pred_error: 0.07653792202472687 \t range_loss: 0.6921059489250183\n",
      "pred_error: 0.07723136991262436 \t range_loss: 0.6936674118041992\n",
      "pred_error: 0.07723156362771988 \t range_loss: 0.6936674118041992\n",
      "pred_error: 0.0759287104010582 \t range_loss: 0.7008843421936035\n",
      "pred_error: 0.07592872530221939 \t range_loss: 0.7008843421936035\n",
      "pred_error: 0.07624532282352448 \t range_loss: 0.6924307942390442\n",
      "pred_error: 0.07595862448215485 \t range_loss: 0.702741801738739\n",
      "pred_error: 0.07622653990983963 \t range_loss: 0.701994001865387\n",
      "pred_error: 0.07719153165817261 \t range_loss: 0.6914963126182556\n",
      "pred_error: 0.07719152420759201 \t range_loss: 0.6914962530136108\n",
      "pred_error: 0.07719147205352783 \t range_loss: 0.6914962530136108\n",
      "pred_error: 0.07718867808580399 \t range_loss: 0.6914962530136108\n",
      "pred_error: 0.07719159126281738 \t range_loss: 0.6914962530136108\n",
      "Step: 608 \t Loss: 1.4513025283813477\n",
      "pred_error: 0.07605129480361938 \t range_loss: 0.690791666507721\n",
      "pred_error: 0.07604470103979111 \t range_loss: 0.6946976184844971\n",
      "pred_error: 0.07604440301656723 \t range_loss: 0.6946976184844971\n",
      "pred_error: 0.07625414431095123 \t range_loss: 0.6924988627433777\n",
      "pred_error: 0.07638874650001526 \t range_loss: 0.6916923522949219\n",
      "pred_error: 0.0770142450928688 \t range_loss: 0.6916865110397339\n",
      "pred_error: 0.07701423019170761 \t range_loss: 0.6916865110397339\n",
      "pred_error: 0.07576698064804077 \t range_loss: 0.7030072808265686\n",
      "Step: 620 \t Loss: 1.4511401653289795\n",
      "pred_error: 0.07606340944766998 \t range_loss: 0.6905059218406677\n",
      "pred_error: 0.07623124867677689 \t range_loss: 0.69086092710495\n",
      "pred_error: 0.07597522437572479 \t range_loss: 0.7035406231880188\n",
      "pred_error: 0.07597529888153076 \t range_loss: 0.7035406231880188\n",
      "pred_error: 0.0771200880408287 \t range_loss: 0.6928514242172241\n",
      "pred_error: 0.07712011784315109 \t range_loss: 0.6928514242172241\n",
      "pred_error: 0.07592672854661942 \t range_loss: 0.7017858624458313\n",
      "pred_error: 0.07703649252653122 \t range_loss: 0.6932352185249329\n",
      "pred_error: 0.07703625410795212 \t range_loss: 0.6932352185249329\n",
      "pred_error: 0.07586855441331863 \t range_loss: 0.701267659664154\n",
      "pred_error: 0.07695381343364716 \t range_loss: 0.6943327188491821\n",
      "pred_error: 0.07695359736680984 \t range_loss: 0.6943327188491821\n",
      "pred_error: 0.07600033283233643 \t range_loss: 0.7027234435081482\n",
      "pred_error: 0.07600032538175583 \t range_loss: 0.7027234435081482\n",
      "pred_error: 0.07600022852420807 \t range_loss: 0.7027234435081482\n",
      "pred_error: 0.07703690230846405 \t range_loss: 0.6919680237770081\n",
      "pred_error: 0.07607873529195786 \t range_loss: 0.7023125886917114\n",
      "pred_error: 0.07710519433021545 \t range_loss: 0.6911547780036926\n",
      "pred_error: 0.07587028294801712 \t range_loss: 0.7026448845863342\n",
      "pred_error: 0.07634763419628143 \t range_loss: 0.6933683753013611\n",
      "pred_error: 0.07607322931289673 \t range_loss: 0.7025025486946106\n",
      "pred_error: 0.07605819404125214 \t range_loss: 0.701602578163147\n",
      "pred_error: 0.07706034928560257 \t range_loss: 0.6932231783866882\n",
      "pred_error: 0.07644645124673843 \t range_loss: 0.691382646560669\n",
      "pred_error: 0.07575634866952896 \t range_loss: 0.7023736834526062\n",
      "pred_error: 0.07575618475675583 \t range_loss: 0.7023736834526062\n",
      "pred_error: 0.07592600584030151 \t range_loss: 0.7018892168998718\n",
      "pred_error: 0.07699307054281235 \t range_loss: 0.6918814182281494\n",
      "Step: 665 \t Loss: 1.4509973526000977\n",
      "pred_error: 0.07607907801866531 \t range_loss: 0.6902081370353699\n",
      "pred_error: 0.07584621012210846 \t range_loss: 0.701107919216156\n",
      "pred_error: 0.07625776529312134 \t range_loss: 0.6905465722084045\n",
      "pred_error: 0.07605677098035812 \t range_loss: 0.7013324499130249\n",
      "pred_error: 0.076056569814682 \t range_loss: 0.7013324499130249\n",
      "pred_error: 0.07728300243616104 \t range_loss: 0.693145751953125\n",
      "pred_error: 0.07629776746034622 \t range_loss: 0.6996715664863586\n",
      "pred_error: 0.07605310529470444 \t range_loss: 0.699714720249176\n",
      "pred_error: 0.0760607123374939 \t range_loss: 0.7000727653503418\n",
      "pred_error: 0.0771416500210762 \t range_loss: 0.6916537284851074\n",
      "pred_error: 0.07708372920751572 \t range_loss: 0.6947251558303833\n",
      "pred_error: 0.0770840272307396 \t range_loss: 0.6947251558303833\n",
      "pred_error: 0.07620420306921005 \t range_loss: 0.7008640766143799\n",
      "pred_error: 0.07700248062610626 \t range_loss: 0.6894670128822327\n",
      "pred_error: 0.07604736089706421 \t range_loss: 0.7005115151405334\n",
      "pred_error: 0.07589104771614075 \t range_loss: 0.7012909054756165\n",
      "pred_error: 0.07589077204465866 \t range_loss: 0.7012909054756165\n",
      "pred_error: 0.07589114457368851 \t range_loss: 0.7012909054756165\n",
      "pred_error: 0.0769568383693695 \t range_loss: 0.69376140832901\n",
      "pred_error: 0.07695671916007996 \t range_loss: 0.69376140832901\n",
      "pred_error: 0.07603868097066879 \t range_loss: 0.7005952000617981\n",
      "pred_error: 0.0760386660695076 \t range_loss: 0.7005952000617981\n",
      "pred_error: 0.07629228383302689 \t range_loss: 0.6887948513031006\n",
      "pred_error: 0.07590829581022263 \t range_loss: 0.701107382774353\n",
      "pred_error: 0.07590816915035248 \t range_loss: 0.701107382774353\n",
      "pred_error: 0.07707066833972931 \t range_loss: 0.6899906396865845\n",
      "pred_error: 0.07707063853740692 \t range_loss: 0.6899906396865845\n",
      "pred_error: 0.076373390853405 \t range_loss: 0.690670371055603\n",
      "pred_error: 0.07605872303247452 \t range_loss: 0.7018740773200989\n",
      "pred_error: 0.0765957236289978 \t range_loss: 0.6895941495895386\n",
      "pred_error: 0.07633860409259796 \t range_loss: 0.6960711479187012\n",
      "pred_error: 0.07745066285133362 \t range_loss: 0.691227912902832\n",
      "pred_error: 0.07730760425329208 \t range_loss: 0.6903648376464844\n",
      "pred_error: 0.07594329863786697 \t range_loss: 0.7013216614723206\n",
      "pred_error: 0.07611510902643204 \t range_loss: 0.7013660669326782\n",
      "pred_error: 0.07614437490701675 \t range_loss: 0.7012125253677368\n",
      "pred_error: 0.07614433020353317 \t range_loss: 0.7012125253677368\n",
      "pred_error: 0.07613549381494522 \t range_loss: 0.7009921669960022\n",
      "Step: 751 \t Loss: 1.4495313167572021\n",
      "pred_error: 0.07734853774309158 \t range_loss: 0.6873546838760376\n",
      "pred_error: 0.0767626017332077 \t range_loss: 0.688890814781189\n",
      "pred_error: 0.07657553255558014 \t range_loss: 0.6885210871696472\n",
      "pred_error: 0.07626812160015106 \t range_loss: 0.6873990297317505\n",
      "pred_error: 0.07647185772657394 \t range_loss: 0.6976988911628723\n",
      "pred_error: 0.07647231221199036 \t range_loss: 0.6976988911628723\n",
      "pred_error: 0.07673078775405884 \t range_loss: 0.6872747540473938\n",
      "pred_error: 0.07673090696334839 \t range_loss: 0.6872747540473938\n",
      "pred_error: 0.07615023106336594 \t range_loss: 0.6997636556625366\n",
      "pred_error: 0.07720306515693665 \t range_loss: 0.6880616545677185\n",
      "pred_error: 0.07646051794290543 \t range_loss: 0.6873292922973633\n",
      "pred_error: 0.07640586048364639 \t range_loss: 0.6887444853782654\n",
      "pred_error: 0.07758414000272751 \t range_loss: 0.687460720539093\n",
      "pred_error: 0.07758402079343796 \t range_loss: 0.687460720539093\n",
      "pred_error: 0.07764125615358353 \t range_loss: 0.6875305771827698\n",
      "pred_error: 0.07764077186584473 \t range_loss: 0.6875305771827698\n",
      "pred_error: 0.07656364142894745 \t range_loss: 0.6872267127037048\n",
      "pred_error: 0.07656364142894745 \t range_loss: 0.6872267127037048\n",
      "pred_error: 0.07656389474868774 \t range_loss: 0.6872267127037048\n",
      "pred_error: 0.0765291154384613 \t range_loss: 0.6879952549934387\n",
      "pred_error: 0.07652906328439713 \t range_loss: 0.6879952549934387\n",
      "pred_error: 0.07652906328439713 \t range_loss: 0.6879952549934387\n",
      "pred_error: 0.07745858281850815 \t range_loss: 0.6887557506561279\n",
      "pred_error: 0.07663296163082123 \t range_loss: 0.6881728172302246\n",
      "pred_error: 0.07639345526695251 \t range_loss: 0.6983270645141602\n",
      "pred_error: 0.0761149451136589 \t range_loss: 0.6983851194381714\n",
      "BEST LOSS: 1.4495313\n",
      "==== Model: block4_cob_activation_norm  in Layer: 4 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 15:55:39,975 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 15:57:38,245 model.rs:1246 value (354112) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 15:57:38,251 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 15:57:57,164 model.rs:1246 value (354112) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 15:57:57,167 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 15:57:57,190 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 15:57:57,209 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 15:57:57,225 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.00000028271268 | -0.000027433038 | 0.00030887127 | -0.00034918636 | 0.00003924855  | 0.000027433038   | 0.00034918636 | 0             | 0.0000000026895994 | -0.0000058611404   | 0.001038025            |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 623541 64 [-546704, 260716] 1 [16]\n",
      "===============================\n",
      "==== Model: block4_cob_activation_norm_teleported  in Layer: 4 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-15 15:58:17,117 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-15 16:00:13,158 model.rs:1246 value (-142528) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 16:00:13,164 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-15 16:00:34,422 model.rs:1246 value (-142528) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-15 16:00:34,425 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-15 16:00:34,443 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-15 16:00:34,463 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-15 16:00:34,477 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+-----------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error    | max_error     | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+-----------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000009074068 | -0.000029921532 | 0.00029349327 | -0.0003067255 | 0.000040070565 | 0.000029921532   | 0.0003067255  | 0             | 0.0000000027790503 | -0.00006125927     | 0.0009031908           |\n",
      "+-----------------+-----------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 623551 64 [-382072, 182758] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 5 , \t  activation_stats: {'relu_1': {'norm': tensor(619.9756), 'max': tensor(2.9422), 'min': tensor(-6.9142), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(9.8564)\n",
      "pred_error: 0.09495171159505844 \t range_loss: 1.0013583898544312\n",
      "Step: 0 \t Loss: 1.8888273239135742\n",
      "pred_error: 0.09454551339149475 \t range_loss: 0.943378210067749\n",
      "Step: 1 \t Loss: 1.839162826538086\n",
      "Step: 2 \t Loss: 1.822648525238037\n",
      "Step: 3 \t Loss: 1.810874581336975\n",
      "pred_error: 0.09383119642734528 \t range_loss: 0.8725627064704895\n",
      "Step: 4 \t Loss: 1.8028297424316406\n",
      "Step: 5 \t Loss: 1.792088508605957\n",
      "Step: 6 \t Loss: 1.7778489589691162\n",
      "Step: 7 \t Loss: 1.767829179763794\n",
      "Step: 8 \t Loss: 1.762483835220337\n",
      "Step: 9 \t Loss: 1.7534878253936768\n",
      "pred_error: 0.09302030503749847 \t range_loss: 0.8232869505882263\n",
      "Step: 10 \t Loss: 1.745476245880127\n",
      "Step: 11 \t Loss: 1.73582923412323\n",
      "Step: 12 \t Loss: 1.7345225811004639\n",
      "pred_error: 0.09280692040920258 \t range_loss: 0.8064534068107605\n",
      "pred_error: 0.09280606359243393 \t range_loss: 0.8064534068107605\n",
      "Step: 13 \t Loss: 1.7283236980438232\n",
      "Step: 14 \t Loss: 1.7273857593536377\n",
      "Step: 15 \t Loss: 1.7212162017822266\n",
      "pred_error: 0.09279471635818481 \t range_loss: 0.7932696342468262\n",
      "Step: 16 \t Loss: 1.717786431312561\n",
      "Step: 17 \t Loss: 1.7131178379058838\n",
      "Step: 18 \t Loss: 1.7100868225097656\n",
      "Step: 19 \t Loss: 1.707816243171692\n",
      "pred_error: 0.09252910315990448 \t range_loss: 0.7825253009796143\n",
      "pred_error: 0.09252902865409851 \t range_loss: 0.7825253009796143\n",
      "pred_error: 0.09252901375293732 \t range_loss: 0.7825253009796143\n",
      "Step: 20 \t Loss: 1.7061920166015625\n",
      "pred_error: 0.09233613312244415 \t range_loss: 0.7828353047370911\n",
      "pred_error: 0.09233526885509491 \t range_loss: 0.7828353047370911\n",
      "Step: 22 \t Loss: 1.7041594982147217\n",
      "pred_error: 0.0927681252360344 \t range_loss: 0.7764781713485718\n",
      "pred_error: 0.0927681252360344 \t range_loss: 0.7764781713485718\n",
      "Step: 23 \t Loss: 1.6939330101013184\n",
      "Step: 24 \t Loss: 1.6885572671890259\n",
      "pred_error: 0.09246189147233963 \t range_loss: 0.7639383673667908\n",
      "pred_error: 0.09255120158195496 \t range_loss: 0.7638729214668274\n",
      "pred_error: 0.0931018516421318 \t range_loss: 0.7599923610687256\n",
      "Step: 28 \t Loss: 1.6872968673706055\n",
      "Step: 29 \t Loss: 1.685034990310669\n",
      "pred_error: 0.09282460808753967 \t range_loss: 0.7567927837371826\n",
      "Step: 30 \t Loss: 1.682342290878296\n",
      "Step: 31 \t Loss: 1.6783509254455566\n",
      "Step: 32 \t Loss: 1.6751593351364136\n",
      "pred_error: 0.09235912561416626 \t range_loss: 0.7515774965286255\n",
      "Step: 34 \t Loss: 1.6751142740249634\n",
      "pred_error: 0.09247376024723053 \t range_loss: 0.7503767013549805\n",
      "Step: 35 \t Loss: 1.67405104637146\n",
      "Step: 36 \t Loss: 1.670559287071228\n",
      "pred_error: 0.09273774176836014 \t range_loss: 0.7465834021568298\n",
      "pred_error: 0.0925518125295639 \t range_loss: 0.7458090782165527\n",
      "pred_error: 0.09255184233188629 \t range_loss: 0.7458090782165527\n",
      "pred_error: 0.09255111962556839 \t range_loss: 0.7458090782165527\n",
      "pred_error: 0.09255191683769226 \t range_loss: 0.7458090782165527\n",
      "pred_error: 0.0925518125295639 \t range_loss: 0.7458090782165527\n",
      "pred_error: 0.09236117452383041 \t range_loss: 0.7492233514785767\n",
      "Step: 41 \t Loss: 1.6682300567626953\n",
      "Step: 42 \t Loss: 1.6645489931106567\n",
      "Step: 43 \t Loss: 1.6632099151611328\n",
      "Step: 44 \t Loss: 1.6596074104309082\n",
      "pred_error: 0.09213598072528839 \t range_loss: 0.7382481098175049\n",
      "pred_error: 0.09269433468580246 \t range_loss: 0.7369441390037537\n",
      "pred_error: 0.09269408881664276 \t range_loss: 0.7369441390037537\n",
      "Step: 50 \t Loss: 1.6581547260284424\n",
      "Step: 51 \t Loss: 1.65793776512146\n",
      "pred_error: 0.092234767973423 \t range_loss: 0.7355901002883911\n",
      "pred_error: 0.09279350936412811 \t range_loss: 0.7323840856552124\n",
      "Step: 53 \t Loss: 1.656648874282837\n",
      "pred_error: 0.09242400527000427 \t range_loss: 0.7324137091636658\n",
      "Step: 54 \t Loss: 1.6527059078216553\n",
      "Step: 56 \t Loss: 1.6519262790679932\n",
      "pred_error: 0.09233930706977844 \t range_loss: 0.7285286784172058\n",
      "pred_error: 0.09212770313024521 \t range_loss: 0.7326257228851318\n",
      "Step: 59 \t Loss: 1.6514637470245361\n",
      "pred_error: 0.09241533279418945 \t range_loss: 0.7273096442222595\n",
      "Step: 60 \t Loss: 1.650852918624878\n",
      "pred_error: 0.09235987067222595 \t range_loss: 0.7272541522979736\n",
      "Step: 61 \t Loss: 1.6495966911315918\n",
      "pred_error: 0.09227292239665985 \t range_loss: 0.7268710732460022\n",
      "Step: 62 \t Loss: 1.6471948623657227\n",
      "Step: 67 \t Loss: 1.6447898149490356\n",
      "pred_error: 0.0922689288854599 \t range_loss: 0.7221018671989441\n",
      "Step: 68 \t Loss: 1.6435506343841553\n",
      "pred_error: 0.09218188375234604 \t range_loss: 0.7217317819595337\n",
      "pred_error: 0.09251032024621964 \t range_loss: 0.721224844455719\n",
      "pred_error: 0.09245478361845016 \t range_loss: 0.7217200398445129\n",
      "pred_error: 0.09245990216732025 \t range_loss: 0.721104621887207\n",
      "pred_error: 0.09268268942832947 \t range_loss: 0.7202408909797668\n",
      "pred_error: 0.09268298000097275 \t range_loss: 0.7202408909797668\n",
      "pred_error: 0.0926826074719429 \t range_loss: 0.7202408909797668\n",
      "pred_error: 0.09268263727426529 \t range_loss: 0.7202408909797668\n",
      "pred_error: 0.092385433614254 \t range_loss: 0.719813883304596\n",
      "Step: 74 \t Loss: 1.6422232389450073\n",
      "pred_error: 0.09219080209732056 \t range_loss: 0.7203169465065002\n",
      "Step: 78 \t Loss: 1.6412389278411865\n",
      "Step: 79 \t Loss: 1.6401524543762207\n",
      "pred_error: 0.09223444759845734 \t range_loss: 0.7178246974945068\n",
      "pred_error: 0.09223435074090958 \t range_loss: 0.7178246974945068\n",
      "Step: 81 \t Loss: 1.6378295421600342\n",
      "pred_error: 0.09234626591205597 \t range_loss: 0.7162596583366394\n",
      "Step: 84 \t Loss: 1.6364974975585938\n",
      "pred_error: 0.09217043220996857 \t range_loss: 0.7147930860519409\n",
      "Step: 85 \t Loss: 1.6348209381103516\n",
      "pred_error: 0.0920734778046608 \t range_loss: 0.7140856385231018\n",
      "Step: 86 \t Loss: 1.633407473564148\n",
      "pred_error: 0.09242603182792664 \t range_loss: 0.7149999737739563\n",
      "pred_error: 0.09203439205884933 \t range_loss: 0.7135753631591797\n",
      "Step: 93 \t Loss: 1.6326851844787598\n",
      "pred_error: 0.09188670665025711 \t range_loss: 0.7138194441795349\n",
      "pred_error: 0.09246248751878738 \t range_loss: 0.7117961645126343\n",
      "Step: 97 \t Loss: 1.632607102394104\n",
      "Step: 98 \t Loss: 1.6293185949325562\n",
      "pred_error: 0.09200198203325272 \t range_loss: 0.712151288986206\n",
      "pred_error: 0.09200187772512436 \t range_loss: 0.712151288986206\n",
      "pred_error: 0.09252352267503738 \t range_loss: 0.7088072896003723\n",
      "pred_error: 0.09252335876226425 \t range_loss: 0.7088072896003723\n",
      "Step: 105 \t Loss: 1.6287842988967896\n",
      "Step: 108 \t Loss: 1.6278791427612305\n",
      "pred_error: 0.0920739471912384 \t range_loss: 0.7071337103843689\n",
      "Step: 109 \t Loss: 1.6253323554992676\n",
      "pred_error: 0.09199514985084534 \t range_loss: 0.7066177129745483\n",
      "pred_error: 0.09199568629264832 \t range_loss: 0.7066177129745483\n",
      "Step: 120 \t Loss: 1.6224534511566162\n",
      "pred_error: 0.09191732108592987 \t range_loss: 0.7032803297042847\n",
      "Step: 121 \t Loss: 1.6215684413909912\n",
      "Step: 122 \t Loss: 1.6202499866485596\n",
      "pred_error: 0.09248322993516922 \t range_loss: 0.7066960334777832\n",
      "pred_error: 0.09215376526117325 \t range_loss: 0.7040675282478333\n",
      "pred_error: 0.0921536237001419 \t range_loss: 0.7040675282478333\n",
      "pred_error: 0.09194939583539963 \t range_loss: 0.704445481300354\n",
      "pred_error: 0.0919041559100151 \t range_loss: 0.701448380947113\n",
      "pred_error: 0.09190394729375839 \t range_loss: 0.701448380947113\n",
      "Step: 134 \t Loss: 1.6179211139678955\n",
      "pred_error: 0.09183727949857712 \t range_loss: 0.6995491981506348\n",
      "pred_error: 0.09157594293355942 \t range_loss: 0.7030972242355347\n",
      "pred_error: 0.09188512712717056 \t range_loss: 0.700351357460022\n",
      "pred_error: 0.09208641201257706 \t range_loss: 0.6990824937820435\n",
      "pred_error: 0.09210997074842453 \t range_loss: 0.6997430324554443\n",
      "pred_error: 0.09217868745326996 \t range_loss: 0.6998103260993958\n",
      "pred_error: 0.0919884592294693 \t range_loss: 0.6982460618019104\n",
      "pred_error: 0.09198866039514542 \t range_loss: 0.6982460618019104\n",
      "pred_error: 0.09176362305879593 \t range_loss: 0.7004431486129761\n",
      "pred_error: 0.0921555683016777 \t range_loss: 0.6978220343589783\n",
      "pred_error: 0.0921555608510971 \t range_loss: 0.6978220343589783\n",
      "Step: 151 \t Loss: 1.6173367500305176\n",
      "pred_error: 0.09192477911710739 \t range_loss: 0.6980889439582825\n",
      "Step: 152 \t Loss: 1.615241289138794\n",
      "pred_error: 0.09188341349363327 \t range_loss: 0.6989865899085999\n",
      "pred_error: 0.09237390756607056 \t range_loss: 0.6980182528495789\n",
      "pred_error: 0.09237386286258698 \t range_loss: 0.6980182528495789\n",
      "pred_error: 0.09237415343523026 \t range_loss: 0.6980182528495789\n",
      "pred_error: 0.0921231210231781 \t range_loss: 0.6946713924407959\n",
      "Step: 164 \t Loss: 1.611464262008667\n",
      "pred_error: 0.09236972033977509 \t range_loss: 0.6957288980484009\n",
      "pred_error: 0.09194567799568176 \t range_loss: 0.6937105059623718\n",
      "pred_error: 0.09223797172307968 \t range_loss: 0.6928138136863708\n",
      "pred_error: 0.09241694957017899 \t range_loss: 0.6931502819061279\n",
      "Step: 176 \t Loss: 1.6110749244689941\n",
      "pred_error: 0.09180063009262085 \t range_loss: 0.6943007707595825\n",
      "pred_error: 0.09232238680124283 \t range_loss: 0.6910377740859985\n",
      "Step: 179 \t Loss: 1.6109819412231445\n",
      "Step: 180 \t Loss: 1.6099001169204712\n",
      "pred_error: 0.09195733070373535 \t range_loss: 0.6903232932090759\n",
      "pred_error: 0.09195820987224579 \t range_loss: 0.6903232932090759\n",
      "pred_error: 0.09225016832351685 \t range_loss: 0.6908840537071228\n",
      "pred_error: 0.09290581196546555 \t range_loss: 0.6912227869033813\n",
      "pred_error: 0.09243910759687424 \t range_loss: 0.6907615065574646\n",
      "Step: 187 \t Loss: 1.6088645458221436\n",
      "pred_error: 0.09191729128360748 \t range_loss: 0.6896909475326538\n",
      "pred_error: 0.09209518134593964 \t range_loss: 0.688732922077179\n",
      "pred_error: 0.0921977087855339 \t range_loss: 0.6897730827331543\n",
      "pred_error: 0.09213943034410477 \t range_loss: 0.6902898550033569\n",
      "pred_error: 0.09270531684160233 \t range_loss: 0.6896440982818604\n",
      "pred_error: 0.09207333624362946 \t range_loss: 0.6891536116600037\n",
      "Step: 200 \t Loss: 1.6081093549728394\n",
      "pred_error: 0.09257747232913971 \t range_loss: 0.6889773011207581\n",
      "Step: 206 \t Loss: 1.6050198078155518\n",
      "Step: 207 \t Loss: 1.6049703359603882\n",
      "pred_error: 0.09154306352138519 \t range_loss: 0.6895402073860168\n",
      "pred_error: 0.09215231984853745 \t range_loss: 0.6910422444343567\n",
      "pred_error: 0.0921524241566658 \t range_loss: 0.6910422444343567\n",
      "pred_error: 0.0919361338019371 \t range_loss: 0.6887817978858948\n",
      "pred_error: 0.09240905195474625 \t range_loss: 0.6877337694168091\n",
      "pred_error: 0.09240920096635818 \t range_loss: 0.6877337694168091\n",
      "Step: 219 \t Loss: 1.6035375595092773\n",
      "pred_error: 0.09178457409143448 \t range_loss: 0.685688853263855\n",
      "pred_error: 0.09202103316783905 \t range_loss: 0.6875617504119873\n",
      "pred_error: 0.09249670058488846 \t range_loss: 0.6866295337677002\n",
      "pred_error: 0.09249662607908249 \t range_loss: 0.6866295337677002\n",
      "pred_error: 0.09207548201084137 \t range_loss: 0.6857951879501343\n",
      "pred_error: 0.09221573919057846 \t range_loss: 0.6851539611816406\n",
      "pred_error: 0.09237173199653625 \t range_loss: 0.6832165122032166\n",
      "Step: 237 \t Loss: 1.6030497550964355\n",
      "pred_error: 0.09192308783531189 \t range_loss: 0.6863208413124084\n",
      "pred_error: 0.09239295870065689 \t range_loss: 0.6855008602142334\n",
      "pred_error: 0.09248404204845428 \t range_loss: 0.6811104416847229\n",
      "Step: 247 \t Loss: 1.6027374267578125\n",
      "pred_error: 0.09203267097473145 \t range_loss: 0.682601273059845\n",
      "pred_error: 0.0920325219631195 \t range_loss: 0.682601273059845\n",
      "pred_error: 0.09243377298116684 \t range_loss: 0.683256208896637\n",
      "pred_error: 0.09243364632129669 \t range_loss: 0.683256208896637\n",
      "Step: 255 \t Loss: 1.6010360717773438\n",
      "pred_error: 0.09238065034151077 \t range_loss: 0.6799797415733337\n",
      "pred_error: 0.09224874526262283 \t range_loss: 0.6789929866790771\n",
      "Step: 262 \t Loss: 1.6010162830352783\n",
      "pred_error: 0.09196227043867111 \t range_loss: 0.6813935041427612\n",
      "pred_error: 0.09237650781869888 \t range_loss: 0.6806432604789734\n",
      "pred_error: 0.0927039235830307 \t range_loss: 0.6788895130157471\n",
      "pred_error: 0.09223739802837372 \t range_loss: 0.6804007291793823\n",
      "pred_error: 0.09279536455869675 \t range_loss: 0.6798377633094788\n",
      "pred_error: 0.09256099164485931 \t range_loss: 0.6800141930580139\n",
      "pred_error: 0.09214711934328079 \t range_loss: 0.6797738075256348\n",
      "pred_error: 0.0921473428606987 \t range_loss: 0.6797738075256348\n",
      "Step: 285 \t Loss: 1.5989906787872314\n",
      "pred_error: 0.0922684594988823 \t range_loss: 0.6814668774604797\n",
      "pred_error: 0.09270726889371872 \t range_loss: 0.682898223400116\n",
      "pred_error: 0.09226544946432114 \t range_loss: 0.6777166128158569\n",
      "pred_error: 0.0922655239701271 \t range_loss: 0.6777166128158569\n",
      "Step: 290 \t Loss: 1.598801851272583\n",
      "pred_error: 0.09208410978317261 \t range_loss: 0.6810253262519836\n",
      "pred_error: 0.0921926498413086 \t range_loss: 0.6797844767570496\n",
      "pred_error: 0.09247574210166931 \t range_loss: 0.6778071522712708\n",
      "pred_error: 0.09228941798210144 \t range_loss: 0.6794276237487793\n",
      "pred_error: 0.092719666659832 \t range_loss: 0.6791017651557922\n",
      "pred_error: 0.09224408864974976 \t range_loss: 0.6780797243118286\n",
      "pred_error: 0.09200350940227509 \t range_loss: 0.6799272894859314\n",
      "pred_error: 0.09246933460235596 \t range_loss: 0.6754829287528992\n",
      "Step: 313 \t Loss: 1.596983551979065\n",
      "pred_error: 0.09224052727222443 \t range_loss: 0.6745777726173401\n",
      "Step: 314 \t Loss: 1.5957822799682617\n",
      "pred_error: 0.09291449934244156 \t range_loss: 0.674183189868927\n",
      "pred_error: 0.09246502816677094 \t range_loss: 0.6750783920288086\n",
      "pred_error: 0.09231694787740707 \t range_loss: 0.675899088382721\n",
      "pred_error: 0.09231694787740707 \t range_loss: 0.675899088382721\n",
      "pred_error: 0.09231703728437424 \t range_loss: 0.675899088382721\n",
      "pred_error: 0.09268250316381454 \t range_loss: 0.6783995628356934\n",
      "pred_error: 0.09321887046098709 \t range_loss: 0.6758425831794739\n",
      "pred_error: 0.09277431666851044 \t range_loss: 0.6755356788635254\n",
      "pred_error: 0.09239049255847931 \t range_loss: 0.6744569540023804\n",
      "pred_error: 0.09239043295383453 \t range_loss: 0.6744569540023804\n",
      "pred_error: 0.09280414879322052 \t range_loss: 0.6774910688400269\n",
      "pred_error: 0.09230269491672516 \t range_loss: 0.6746611595153809\n",
      "pred_error: 0.09262184053659439 \t range_loss: 0.678326427936554\n",
      "Step: 344 \t Loss: 1.5931158065795898\n",
      "pred_error: 0.09208396077156067 \t range_loss: 0.672275960445404\n",
      "pred_error: 0.09279432892799377 \t range_loss: 0.677331805229187\n",
      "pred_error: 0.09244347363710403 \t range_loss: 0.6722584962844849\n",
      "pred_error: 0.09229766577482224 \t range_loss: 0.6717835664749146\n",
      "pred_error: 0.09220262616872787 \t range_loss: 0.6748631000518799\n",
      "pred_error: 0.0922025591135025 \t range_loss: 0.6748631000518799\n",
      "pred_error: 0.09220286458730698 \t range_loss: 0.6748631000518799\n",
      "pred_error: 0.09284432977437973 \t range_loss: 0.6738223433494568\n",
      "pred_error: 0.09284637123346329 \t range_loss: 0.6767245531082153\n",
      "pred_error: 0.09284618496894836 \t range_loss: 0.6767245531082153\n",
      "pred_error: 0.09242884069681168 \t range_loss: 0.6754616498947144\n",
      "pred_error: 0.09241517633199692 \t range_loss: 0.6743651032447815\n",
      "pred_error: 0.09243319183588028 \t range_loss: 0.6749289035797119\n",
      "pred_error: 0.0924328938126564 \t range_loss: 0.6749289035797119\n",
      "pred_error: 0.09284450113773346 \t range_loss: 0.6738181114196777\n",
      "pred_error: 0.09240666776895523 \t range_loss: 0.673682689666748\n",
      "pred_error: 0.09240656346082687 \t range_loss: 0.673682689666748\n",
      "pred_error: 0.09242622554302216 \t range_loss: 0.6749796867370605\n",
      "pred_error: 0.09261398762464523 \t range_loss: 0.6722261905670166\n",
      "pred_error: 0.09281658381223679 \t range_loss: 0.6729575991630554\n",
      "pred_error: 0.09281650930643082 \t range_loss: 0.6729575991630554\n",
      "pred_error: 0.09235750138759613 \t range_loss: 0.6723350882530212\n",
      "pred_error: 0.09222220629453659 \t range_loss: 0.6726412177085876\n",
      "pred_error: 0.09230859577655792 \t range_loss: 0.6727758646011353\n",
      "pred_error: 0.09244801104068756 \t range_loss: 0.6727287173271179\n",
      "pred_error: 0.09251155704259872 \t range_loss: 0.6723810434341431\n",
      "pred_error: 0.09249816089868546 \t range_loss: 0.6739650964736938\n",
      "pred_error: 0.09303522855043411 \t range_loss: 0.6735193729400635\n",
      "pred_error: 0.09265188127756119 \t range_loss: 0.6717050671577454\n",
      "pred_error: 0.09266503900289536 \t range_loss: 0.6717050671577454\n",
      "pred_error: 0.0927291065454483 \t range_loss: 0.6738479137420654\n",
      "pred_error: 0.09268785268068314 \t range_loss: 0.672126293182373\n",
      "pred_error: 0.09255209565162659 \t range_loss: 0.6692570447921753\n",
      "pred_error: 0.09255196899175644 \t range_loss: 0.6692570447921753\n",
      "pred_error: 0.09304171055555344 \t range_loss: 0.6745633482933044\n",
      "pred_error: 0.09280776232481003 \t range_loss: 0.6728839874267578\n",
      "pred_error: 0.092559315264225 \t range_loss: 0.6685802340507507\n",
      "pred_error: 0.09299734234809875 \t range_loss: 0.6695108413696289\n",
      "pred_error: 0.09294470399618149 \t range_loss: 0.6696598529815674\n",
      "pred_error: 0.09269805997610092 \t range_loss: 0.6717930436134338\n",
      "pred_error: 0.09269804507493973 \t range_loss: 0.6717930436134338\n",
      "pred_error: 0.09269795566797256 \t range_loss: 0.6717930436134338\n",
      "pred_error: 0.09296967089176178 \t range_loss: 0.669414758682251\n",
      "pred_error: 0.09271641075611115 \t range_loss: 0.6708428859710693\n",
      "pred_error: 0.0932486355304718 \t range_loss: 0.6663448810577393\n",
      "pred_error: 0.09270884841680527 \t range_loss: 0.6664614081382751\n",
      "pred_error: 0.09284733980894089 \t range_loss: 0.6690759658813477\n",
      "Step: 453 \t Loss: 1.5925085544586182\n",
      "pred_error: 0.09264860302209854 \t range_loss: 0.6660240888595581\n",
      "Step: 454 \t Loss: 1.5924980640411377\n",
      "pred_error: 0.09235560894012451 \t range_loss: 0.6689431667327881\n",
      "pred_error: 0.09321077167987823 \t range_loss: 0.6707509160041809\n",
      "pred_error: 0.09263434261083603 \t range_loss: 0.6681528091430664\n",
      "Step: 460 \t Loss: 1.5920863151550293\n",
      "pred_error: 0.09226783365011215 \t range_loss: 0.6715821623802185\n",
      "pred_error: 0.09282714128494263 \t range_loss: 0.6723705530166626\n",
      "pred_error: 0.09282689541578293 \t range_loss: 0.6723705530166626\n",
      "pred_error: 0.09280889481306076 \t range_loss: 0.6723705530166626\n",
      "pred_error: 0.09250403940677643 \t range_loss: 0.6719262599945068\n",
      "pred_error: 0.09262710809707642 \t range_loss: 0.6710127592086792\n",
      "pred_error: 0.09283719211816788 \t range_loss: 0.6712994575500488\n",
      "pred_error: 0.09265363216400146 \t range_loss: 0.6709285378456116\n",
      "pred_error: 0.09248499572277069 \t range_loss: 0.6711828112602234\n",
      "pred_error: 0.09237493574619293 \t range_loss: 0.6715156435966492\n",
      "pred_error: 0.09237492084503174 \t range_loss: 0.6715156435966492\n",
      "pred_error: 0.09249041974544525 \t range_loss: 0.6683497428894043\n",
      "pred_error: 0.09248217940330505 \t range_loss: 0.6710132956504822\n",
      "pred_error: 0.0924823209643364 \t range_loss: 0.6710132956504822\n",
      "pred_error: 0.09300388395786285 \t range_loss: 0.6728439331054688\n",
      "pred_error: 0.09287690371274948 \t range_loss: 0.6697279810905457\n",
      "pred_error: 0.09287691116333008 \t range_loss: 0.6697279810905457\n",
      "pred_error: 0.09262043237686157 \t range_loss: 0.670050323009491\n",
      "pred_error: 0.09217575937509537 \t range_loss: 0.6712861061096191\n",
      "pred_error: 0.09256860613822937 \t range_loss: 0.6704816818237305\n",
      "Step: 495 \t Loss: 1.5897387266159058\n",
      "pred_error: 0.09217599034309387 \t range_loss: 0.66797935962677\n",
      "pred_error: 0.09278248995542526 \t range_loss: 0.6677583456039429\n",
      "pred_error: 0.09278221428394318 \t range_loss: 0.6677583456039429\n",
      "pred_error: 0.09246543049812317 \t range_loss: 0.6700462698936462\n",
      "pred_error: 0.09277863055467606 \t range_loss: 0.6663376092910767\n",
      "pred_error: 0.09302285313606262 \t range_loss: 0.6659529209136963\n",
      "pred_error: 0.09257245808839798 \t range_loss: 0.6659730076789856\n",
      "pred_error: 0.09312538057565689 \t range_loss: 0.6695864200592041\n",
      "pred_error: 0.0931609496474266 \t range_loss: 0.6669846177101135\n",
      "pred_error: 0.09253111481666565 \t range_loss: 0.6672938466072083\n",
      "pred_error: 0.09247298538684845 \t range_loss: 0.6671996712684631\n",
      "pred_error: 0.09291346371173859 \t range_loss: 0.6716050505638123\n",
      "pred_error: 0.09251216799020767 \t range_loss: 0.6667865514755249\n",
      "pred_error: 0.09235431253910065 \t range_loss: 0.6702473163604736\n",
      "pred_error: 0.09235449880361557 \t range_loss: 0.6702473163604736\n",
      "pred_error: 0.09274807572364807 \t range_loss: 0.6676071882247925\n",
      "pred_error: 0.09308803826570511 \t range_loss: 0.6677412986755371\n",
      "pred_error: 0.09267570823431015 \t range_loss: 0.6670833230018616\n",
      "pred_error: 0.09225202351808548 \t range_loss: 0.6695029139518738\n",
      "pred_error: 0.09225225448608398 \t range_loss: 0.6695029139518738\n",
      "pred_error: 0.09265048056840897 \t range_loss: 0.6688690185546875\n",
      "pred_error: 0.09259910136461258 \t range_loss: 0.6682201027870178\n",
      "pred_error: 0.09285331517457962 \t range_loss: 0.6700390577316284\n",
      "pred_error: 0.0931088924407959 \t range_loss: 0.668752133846283\n",
      "pred_error: 0.0926312506198883 \t range_loss: 0.6661716103553772\n",
      "pred_error: 0.0923459529876709 \t range_loss: 0.6707272529602051\n",
      "pred_error: 0.09310412406921387 \t range_loss: 0.6691986918449402\n",
      "pred_error: 0.09310387074947357 \t range_loss: 0.6691986918449402\n",
      "pred_error: 0.09285801649093628 \t range_loss: 0.6659772992134094\n",
      "pred_error: 0.09269125014543533 \t range_loss: 0.6657955646514893\n",
      "pred_error: 0.09269125759601593 \t range_loss: 0.6657955646514893\n",
      "pred_error: 0.0933336615562439 \t range_loss: 0.6659380197525024\n",
      "pred_error: 0.0929143950343132 \t range_loss: 0.6663925051689148\n",
      "pred_error: 0.09271982312202454 \t range_loss: 0.6679989695549011\n",
      "pred_error: 0.0929650217294693 \t range_loss: 0.6690689325332642\n",
      "pred_error: 0.09296445548534393 \t range_loss: 0.6640854477882385\n",
      "pred_error: 0.09296125918626785 \t range_loss: 0.6693600416183472\n",
      "pred_error: 0.09296131879091263 \t range_loss: 0.6693600416183472\n",
      "pred_error: 0.09311211854219437 \t range_loss: 0.6650556921958923\n",
      "pred_error: 0.09306290745735168 \t range_loss: 0.6648468375205994\n",
      "Step: 571 \t Loss: 1.5894336700439453\n",
      "pred_error: 0.09244722127914429 \t range_loss: 0.6649614572525024\n",
      "pred_error: 0.09244737029075623 \t range_loss: 0.6683638095855713\n",
      "pred_error: 0.09280692785978317 \t range_loss: 0.6651596426963806\n",
      "pred_error: 0.0926118791103363 \t range_loss: 0.6646436452865601\n",
      "Step: 577 \t Loss: 1.5886417627334595\n",
      "pred_error: 0.09294148534536362 \t range_loss: 0.6690642833709717\n",
      "pred_error: 0.09277933835983276 \t range_loss: 0.6654661297798157\n",
      "pred_error: 0.09277916699647903 \t range_loss: 0.6654661297798157\n",
      "pred_error: 0.09267588704824448 \t range_loss: 0.6695481538772583\n",
      "pred_error: 0.09267573058605194 \t range_loss: 0.6695481538772583\n",
      "pred_error: 0.09325318038463593 \t range_loss: 0.666724443435669\n",
      "pred_error: 0.09325230121612549 \t range_loss: 0.666724443435669\n",
      "pred_error: 0.09276001900434494 \t range_loss: 0.6658297181129456\n",
      "pred_error: 0.09287011623382568 \t range_loss: 0.6639211773872375\n",
      "pred_error: 0.09287012368440628 \t range_loss: 0.6639211773872375\n",
      "pred_error: 0.09311481565237045 \t range_loss: 0.6631144285202026\n",
      "pred_error: 0.09302205592393875 \t range_loss: 0.663033664226532\n",
      "pred_error: 0.0928366482257843 \t range_loss: 0.6635697484016418\n",
      "pred_error: 0.09270259737968445 \t range_loss: 0.6636082530021667\n",
      "pred_error: 0.09312253445386887 \t range_loss: 0.6626126766204834\n",
      "pred_error: 0.09312260895967484 \t range_loss: 0.6626126766204834\n",
      "pred_error: 0.09277688711881638 \t range_loss: 0.6621615886688232\n",
      "pred_error: 0.09257238358259201 \t range_loss: 0.6629402041435242\n",
      "pred_error: 0.09314567595720291 \t range_loss: 0.6633667945861816\n",
      "pred_error: 0.09354019165039062 \t range_loss: 0.664787769317627\n",
      "pred_error: 0.09328222274780273 \t range_loss: 0.6618027091026306\n",
      "pred_error: 0.09315654635429382 \t range_loss: 0.6644747853279114\n",
      "pred_error: 0.09315656125545502 \t range_loss: 0.6644747853279114\n",
      "pred_error: 0.09315640479326248 \t range_loss: 0.6644747853279114\n",
      "pred_error: 0.09315662086009979 \t range_loss: 0.6644747853279114\n",
      "pred_error: 0.09336630254983902 \t range_loss: 0.662885308265686\n",
      "pred_error: 0.09337252378463745 \t range_loss: 0.6632330417633057\n",
      "pred_error: 0.09363190084695816 \t range_loss: 0.6627254486083984\n",
      "pred_error: 0.09363190084695816 \t range_loss: 0.6627254486083984\n",
      "pred_error: 0.09322521835565567 \t range_loss: 0.661470353603363\n",
      "pred_error: 0.09322529286146164 \t range_loss: 0.661470353603363\n",
      "pred_error: 0.09322526305913925 \t range_loss: 0.661470353603363\n",
      "pred_error: 0.09302196651697159 \t range_loss: 0.6610842943191528\n",
      "pred_error: 0.09302195906639099 \t range_loss: 0.6610842943191528\n",
      "pred_error: 0.0930180773139 \t range_loss: 0.661791980266571\n",
      "pred_error: 0.09301808476448059 \t range_loss: 0.661791980266571\n",
      "pred_error: 0.09310276061296463 \t range_loss: 0.6626317501068115\n",
      "pred_error: 0.09310252219438553 \t range_loss: 0.6626317501068115\n",
      "pred_error: 0.09288666397333145 \t range_loss: 0.666620671749115\n",
      "pred_error: 0.0928880125284195 \t range_loss: 0.666620671749115\n",
      "pred_error: 0.09340852499008179 \t range_loss: 0.6641780138015747\n",
      "pred_error: 0.09330739080905914 \t range_loss: 0.6642359495162964\n",
      "pred_error: 0.0933075025677681 \t range_loss: 0.6642359495162964\n",
      "pred_error: 0.09324552863836288 \t range_loss: 0.6636630892753601\n",
      "pred_error: 0.09301293641328812 \t range_loss: 0.6658956408500671\n",
      "pred_error: 0.0932876393198967 \t range_loss: 0.6645360589027405\n",
      "pred_error: 0.09304048120975494 \t range_loss: 0.6631211042404175\n",
      "pred_error: 0.09346050024032593 \t range_loss: 0.6627650260925293\n",
      "pred_error: 0.09321233630180359 \t range_loss: 0.662610650062561\n",
      "pred_error: 0.09321235865354538 \t range_loss: 0.662610650062561\n",
      "pred_error: 0.09281940758228302 \t range_loss: 0.6620360612869263\n",
      "pred_error: 0.0931878536939621 \t range_loss: 0.6661345362663269\n",
      "pred_error: 0.09299711883068085 \t range_loss: 0.6623379588127136\n",
      "pred_error: 0.09299708902835846 \t range_loss: 0.6623379588127136\n",
      "pred_error: 0.09332612156867981 \t range_loss: 0.6616520285606384\n",
      "pred_error: 0.0933259129524231 \t range_loss: 0.6616520285606384\n",
      "pred_error: 0.09298714995384216 \t range_loss: 0.6611331701278687\n",
      "pred_error: 0.09325673431158066 \t range_loss: 0.6608919501304626\n",
      "pred_error: 0.09315027296543121 \t range_loss: 0.6621557474136353\n",
      "pred_error: 0.09309282898902893 \t range_loss: 0.6625111103057861\n",
      "pred_error: 0.09288616478443146 \t range_loss: 0.6637127995491028\n",
      "pred_error: 0.09288616478443146 \t range_loss: 0.6637127995491028\n",
      "pred_error: 0.09281077235937119 \t range_loss: 0.6636734008789062\n",
      "pred_error: 0.09281051158905029 \t range_loss: 0.6636734008789062\n",
      "pred_error: 0.09261726588010788 \t range_loss: 0.6633915305137634\n",
      "pred_error: 0.09288588911294937 \t range_loss: 0.6632017493247986\n",
      "pred_error: 0.09284591674804688 \t range_loss: 0.6627986431121826\n",
      "pred_error: 0.09345198422670364 \t range_loss: 0.6655498147010803\n",
      "pred_error: 0.09321021288633347 \t range_loss: 0.6632311940193176\n",
      "pred_error: 0.09272991865873337 \t range_loss: 0.6618279814720154\n",
      "Step: 728 \t Loss: 1.586434006690979\n",
      "pred_error: 0.09253440052270889 \t range_loss: 0.6610900163650513\n",
      "pred_error: 0.09253471344709396 \t range_loss: 0.6610900163650513\n",
      "pred_error: 0.0927467793226242 \t range_loss: 0.6661435961723328\n",
      "pred_error: 0.0933518186211586 \t range_loss: 0.662761390209198\n",
      "pred_error: 0.0933518186211586 \t range_loss: 0.662761390209198\n",
      "pred_error: 0.09335178881883621 \t range_loss: 0.662761390209198\n",
      "pred_error: 0.09278230369091034 \t range_loss: 0.6644361615180969\n",
      "pred_error: 0.09265072643756866 \t range_loss: 0.6656739711761475\n",
      "pred_error: 0.09303082525730133 \t range_loss: 0.6625189185142517\n",
      "pred_error: 0.09303086251020432 \t range_loss: 0.6625189185142517\n",
      "pred_error: 0.09292987734079361 \t range_loss: 0.6674489378929138\n",
      "pred_error: 0.0934787392616272 \t range_loss: 0.6670613288879395\n",
      "pred_error: 0.09302102029323578 \t range_loss: 0.6634570360183716\n",
      "pred_error: 0.09330345690250397 \t range_loss: 0.66175776720047\n",
      "pred_error: 0.09347829222679138 \t range_loss: 0.6622209548950195\n",
      "pred_error: 0.09347822517156601 \t range_loss: 0.6622209548950195\n",
      "pred_error: 0.0928339883685112 \t range_loss: 0.6610111594200134\n",
      "pred_error: 0.09328261762857437 \t range_loss: 0.6612579822540283\n",
      "pred_error: 0.09313546121120453 \t range_loss: 0.6637391448020935\n",
      "pred_error: 0.09319904446601868 \t range_loss: 0.6644823551177979\n",
      "pred_error: 0.09291401505470276 \t range_loss: 0.6630971431732178\n",
      "pred_error: 0.09330438822507858 \t range_loss: 0.6635749340057373\n",
      "pred_error: 0.09330423176288605 \t range_loss: 0.6635749340057373\n",
      "pred_error: 0.09307151287794113 \t range_loss: 0.6620213389396667\n",
      "pred_error: 0.09247564524412155 \t range_loss: 0.6618407964706421\n",
      "BEST LOSS: 1.586434\n",
      "==== Model: block5_cob_activation_norm  in Layer: 5 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 04:21:19,138 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 04:23:31,748 model.rs:1246 value (315392) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 04:23:31,755 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 04:23:51,742 model.rs:1246 value (315392) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 04:23:51,749 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 04:23:51,768 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 04:23:51,788 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 04:23:51,803 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000002367632 | 0.00008819997 | 0.0003267564 | -0.00036628544 | 0.000045873196 | 0.00008819997    | 0.00036628544 | 0             | 0.0000000036281333 | 0.00029594338      | 0.0011922804           |\n",
      "+-----------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 634379 64 [-640796, 272718] 1 [16]\n",
      "===============================\n",
      "==== Model: block5_cob_activation_norm_teleported  in Layer: 5 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 04:24:12,315 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 04:26:22,146 model.rs:1246 value (135488) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 04:26:22,153 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 04:26:42,219 model.rs:1246 value (135488) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 04:26:42,227 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 04:26:42,287 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 04:26:42,332 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 04:26:42,357 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error  | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000016930313 | 0.00006182492 | 0.0004247427 | -0.00037395954 | 0.000045706838 | 0.00006182492    | 0.0004247427  | 0             | 0.0000000036138637 | 0.0001272583       | 0.0011056521           |\n",
      "+------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 634378 64 [-424114, 184242] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 6 , \t  activation_stats: {'relu_1': {'norm': tensor(712.5450), 'max': tensor(4.4912), 'min': tensor(-7.4060), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(11.8972)\n",
      "pred_error: 0.12392259389162064 \t range_loss: 0.9924841523170471\n",
      "Step: 0 \t Loss: 2.188109874725342\n",
      "Step: 1 \t Loss: 2.1527414321899414\n",
      "Step: 2 \t Loss: 2.12804913520813\n",
      "pred_error: 0.12204756587743759 \t range_loss: 0.9075735211372375\n",
      "Step: 3 \t Loss: 2.096567392349243\n",
      "pred_error: 0.12147915363311768 \t range_loss: 0.8817883133888245\n",
      "Step: 4 \t Loss: 2.0748579502105713\n",
      "Step: 5 \t Loss: 2.051121234893799\n",
      "pred_error: 0.1206107810139656 \t range_loss: 0.845018744468689\n",
      "Step: 6 \t Loss: 2.032914638519287\n",
      "Step: 7 \t Loss: 2.0214433670043945\n",
      "pred_error: 0.12005343288183212 \t range_loss: 0.8209089040756226\n",
      "Step: 8 \t Loss: 2.008849620819092\n",
      "Step: 9 \t Loss: 1.998255729675293\n",
      "pred_error: 0.11956613510847092 \t range_loss: 0.8025946021080017\n",
      "Step: 10 \t Loss: 1.9870479106903076\n",
      "Step: 11 \t Loss: 1.9733898639678955\n",
      "Step: 12 \t Loss: 1.968000054359436\n",
      "pred_error: 0.11855950951576233 \t range_loss: 0.7824022769927979\n",
      "Step: 13 \t Loss: 1.9593794345855713\n",
      "Step: 14 \t Loss: 1.9566996097564697\n",
      "pred_error: 0.11834988743066788 \t range_loss: 0.7732055187225342\n",
      "Step: 15 \t Loss: 1.9493447542190552\n",
      "pred_error: 0.11842872202396393 \t range_loss: 0.7650535106658936\n",
      "Step: 16 \t Loss: 1.9448835849761963\n",
      "Step: 17 \t Loss: 1.9415309429168701\n",
      "pred_error: 0.11810973286628723 \t range_loss: 0.760432779788971\n",
      "Step: 18 \t Loss: 1.923396110534668\n",
      "Step: 19 \t Loss: 1.9216731786727905\n",
      "pred_error: 0.11781024932861328 \t range_loss: 0.7435706853866577\n",
      "Step: 20 \t Loss: 1.9179736375808716\n",
      "Step: 21 \t Loss: 1.9158250093460083\n",
      "pred_error: 0.11738098412752151 \t range_loss: 0.7420096397399902\n",
      "Step: 22 \t Loss: 1.9094018936157227\n",
      "Step: 23 \t Loss: 1.9064291715621948\n",
      "pred_error: 0.11711785197257996 \t range_loss: 0.7352505922317505\n",
      "Step: 24 \t Loss: 1.9030823707580566\n",
      "pred_error: 0.11726528406143188 \t range_loss: 0.7304307818412781\n",
      "Step: 25 \t Loss: 1.8985720872879028\n",
      "pred_error: 0.11691556870937347 \t range_loss: 0.729422926902771\n",
      "Step: 26 \t Loss: 1.898301601409912\n",
      "pred_error: 0.1171863004565239 \t range_loss: 0.7264384627342224\n",
      "Step: 27 \t Loss: 1.8911149501800537\n",
      "Step: 28 \t Loss: 1.8903999328613281\n",
      "Step: 29 \t Loss: 1.8886489868164062\n",
      "pred_error: 0.11706037074327469 \t range_loss: 0.7183801531791687\n",
      "pred_error: 0.11706038564443588 \t range_loss: 0.7183801531791687\n",
      "pred_error: 0.11706024408340454 \t range_loss: 0.7183801531791687\n",
      "Step: 31 \t Loss: 1.8806812763214111\n",
      "Step: 32 \t Loss: 1.8799328804016113\n",
      "Step: 33 \t Loss: 1.8767001628875732\n",
      "Step: 34 \t Loss: 1.8740451335906982\n",
      "Step: 36 \t Loss: 1.8724602460861206\n",
      "pred_error: 0.11624899506568909 \t range_loss: 0.709968090057373\n",
      "Step: 38 \t Loss: 1.8668314218521118\n",
      "pred_error: 0.11607275158166885 \t range_loss: 0.7061014175415039\n",
      "Step: 40 \t Loss: 1.864786148071289\n",
      "Step: 41 \t Loss: 1.8632731437683105\n",
      "pred_error: 0.11570543795824051 \t range_loss: 0.706218421459198\n",
      "pred_error: 0.11570517718791962 \t range_loss: 0.706218421459198\n",
      "pred_error: 0.11605539172887802 \t range_loss: 0.70291668176651\n",
      "Step: 43 \t Loss: 1.858403205871582\n",
      "pred_error: 0.11586808413267136 \t range_loss: 0.6997225880622864\n",
      "pred_error: 0.11586803197860718 \t range_loss: 0.6997225880622864\n",
      "Step: 44 \t Loss: 1.8571518659591675\n",
      "pred_error: 0.11602570861577988 \t range_loss: 0.6989745497703552\n",
      "Step: 46 \t Loss: 1.8544578552246094\n",
      "pred_error: 0.11624718457460403 \t range_loss: 0.6919788718223572\n",
      "Step: 47 \t Loss: 1.8500466346740723\n",
      "pred_error: 0.11586195230484009 \t range_loss: 0.6914271712303162\n",
      "Step: 48 \t Loss: 1.8499152660369873\n",
      "pred_error: 0.11610355973243713 \t range_loss: 0.6888819336891174\n",
      "Step: 49 \t Loss: 1.8454495668411255\n",
      "Step: 53 \t Loss: 1.8432557582855225\n",
      "pred_error: 0.11579647660255432 \t range_loss: 0.6852883696556091\n",
      "Step: 54 \t Loss: 1.8411821126937866\n",
      "pred_error: 0.11585474014282227 \t range_loss: 0.6839492321014404\n",
      "pred_error: 0.11585405468940735 \t range_loss: 0.6839492321014404\n",
      "Step: 56 \t Loss: 1.839375615119934\n",
      "Step: 58 \t Loss: 1.8388147354125977\n",
      "pred_error: 0.11579568684101105 \t range_loss: 0.6821292638778687\n",
      "Step: 60 \t Loss: 1.8373064994812012\n",
      "pred_error: 0.11595863848924637 \t range_loss: 0.6777234077453613\n",
      "pred_error: 0.11595872044563293 \t range_loss: 0.6777234077453613\n",
      "Step: 61 \t Loss: 1.8350292444229126\n",
      "pred_error: 0.11589784175157547 \t range_loss: 0.6780298948287964\n",
      "Step: 63 \t Loss: 1.832179307937622\n",
      "Step: 64 \t Loss: 1.8317785263061523\n",
      "Step: 65 \t Loss: 1.8293983936309814\n",
      "pred_error: 0.11546330899000168 \t range_loss: 0.674763560295105\n",
      "Step: 66 \t Loss: 1.828919768333435\n",
      "pred_error: 0.11523924022912979 \t range_loss: 0.6765272617340088\n",
      "Step: 68 \t Loss: 1.8271759748458862\n",
      "pred_error: 0.11547527462244034 \t range_loss: 0.672418475151062\n",
      "pred_error: 0.11547549068927765 \t range_loss: 0.672418475151062\n",
      "Step: 70 \t Loss: 1.8254618644714355\n",
      "Step: 71 \t Loss: 1.8246228694915771\n",
      "Step: 73 \t Loss: 1.8202053308486938\n",
      "pred_error: 0.1151718720793724 \t range_loss: 0.6684812307357788\n",
      "pred_error: 0.11554708331823349 \t range_loss: 0.6687974333763123\n",
      "Step: 78 \t Loss: 1.8185629844665527\n",
      "Step: 80 \t Loss: 1.814558744430542\n",
      "pred_error: 0.11492647975683212 \t range_loss: 0.6678932309150696\n",
      "Step: 85 \t Loss: 1.8117220401763916\n",
      "pred_error: 0.11518166959285736 \t range_loss: 0.6598958373069763\n",
      "pred_error: 0.11500634253025055 \t range_loss: 0.6647788882255554\n",
      "Step: 88 \t Loss: 1.8115313053131104\n",
      "pred_error: 0.11543042957782745 \t range_loss: 0.6589047312736511\n",
      "pred_error: 0.11542795598506927 \t range_loss: 0.6583342552185059\n",
      "Step: 93 \t Loss: 1.8105567693710327\n",
      "Step: 94 \t Loss: 1.8104926347732544\n",
      "Step: 95 \t Loss: 1.80558180809021\n",
      "pred_error: 0.11505638062953949 \t range_loss: 0.6550222039222717\n",
      "Step: 100 \t Loss: 1.803925633430481\n",
      "Step: 102 \t Loss: 1.8032015562057495\n",
      "pred_error: 0.11508326977491379 \t range_loss: 0.652366042137146\n",
      "Step: 103 \t Loss: 1.7999217510223389\n",
      "Step: 105 \t Loss: 1.7997982501983643\n",
      "Step: 112 \t Loss: 1.7994670867919922\n",
      "Step: 113 \t Loss: 1.797838807106018\n",
      "pred_error: 0.11484028398990631 \t range_loss: 0.6494359970092773\n",
      "pred_error: 0.1151965856552124 \t range_loss: 0.6475638747215271\n",
      "Step: 118 \t Loss: 1.7962961196899414\n",
      "pred_error: 0.11490773409605026 \t range_loss: 0.6472179293632507\n",
      "Step: 120 \t Loss: 1.7935876846313477\n",
      "pred_error: 0.11491131782531738 \t range_loss: 0.6444769501686096\n",
      "pred_error: 0.1148071214556694 \t range_loss: 0.6478104591369629\n",
      "pred_error: 0.11501536518335342 \t range_loss: 0.6439152359962463\n",
      "pred_error: 0.11501564830541611 \t range_loss: 0.6439152359962463\n",
      "Step: 128 \t Loss: 1.7916176319122314\n",
      "pred_error: 0.11516974121332169 \t range_loss: 0.6440425515174866\n",
      "pred_error: 0.11500111222267151 \t range_loss: 0.6423150300979614\n",
      "Step: 133 \t Loss: 1.7908987998962402\n",
      "Step: 135 \t Loss: 1.7906421422958374\n",
      "Step: 136 \t Loss: 1.787801742553711\n",
      "pred_error: 0.11487965285778046 \t range_loss: 0.6420602798461914\n",
      "pred_error: 0.11455042660236359 \t range_loss: 0.644478440284729\n",
      "pred_error: 0.11493177711963654 \t range_loss: 0.6411246657371521\n",
      "Step: 141 \t Loss: 1.787455439567566\n",
      "pred_error: 0.1149320974946022 \t range_loss: 0.6397166848182678\n",
      "pred_error: 0.11468353867530823 \t range_loss: 0.642922043800354\n",
      "pred_error: 0.1150532066822052 \t range_loss: 0.6399787664413452\n",
      "pred_error: 0.11505291610956192 \t range_loss: 0.6399787664413452\n",
      "Step: 146 \t Loss: 1.7862581014633179\n",
      "Step: 148 \t Loss: 1.7845673561096191\n",
      "pred_error: 0.11463788896799088 \t range_loss: 0.6381908059120178\n",
      "pred_error: 0.1153099462389946 \t range_loss: 0.6370758414268494\n",
      "pred_error: 0.11527587473392487 \t range_loss: 0.6356822848320007\n",
      "Step: 159 \t Loss: 1.7843916416168213\n",
      "Step: 161 \t Loss: 1.7843339443206787\n",
      "Step: 162 \t Loss: 1.7832837104797363\n",
      "Step: 164 \t Loss: 1.7791359424591064\n",
      "pred_error: 0.1146116629242897 \t range_loss: 0.633018970489502\n",
      "pred_error: 0.11503367125988007 \t range_loss: 0.6352184414863586\n",
      "pred_error: 0.11503365635871887 \t range_loss: 0.6352184414863586\n",
      "Step: 172 \t Loss: 1.778924822807312\n",
      "pred_error: 0.11454595625400543 \t range_loss: 0.6334656476974487\n",
      "pred_error: 0.1150778979063034 \t range_loss: 0.6316027641296387\n",
      "Step: 177 \t Loss: 1.7782608270645142\n",
      "pred_error: 0.11474733054637909 \t range_loss: 0.6307878494262695\n",
      "Step: 180 \t Loss: 1.7770248651504517\n",
      "pred_error: 0.11431604623794556 \t range_loss: 0.63646399974823\n",
      "pred_error: 0.11498040705919266 \t range_loss: 0.6305480599403381\n",
      "pred_error: 0.11474445462226868 \t range_loss: 0.6306983828544617\n",
      "pred_error: 0.11497144401073456 \t range_loss: 0.6304079294204712\n",
      "Step: 187 \t Loss: 1.77688467502594\n",
      "pred_error: 0.11485102027654648 \t range_loss: 0.6309066414833069\n",
      "pred_error: 0.11484892666339874 \t range_loss: 0.6309066414833069\n",
      "Step: 192 \t Loss: 1.7762699127197266\n",
      "Step: 195 \t Loss: 1.7731786966323853\n",
      "pred_error: 0.11459065228700638 \t range_loss: 0.6272757053375244\n",
      "pred_error: 0.11436916142702103 \t range_loss: 0.6318686008453369\n",
      "Step: 197 \t Loss: 1.7729390859603882\n",
      "pred_error: 0.11498695611953735 \t range_loss: 0.6290168762207031\n",
      "pred_error: 0.11494804173707962 \t range_loss: 0.6293045282363892\n",
      "pred_error: 0.11517339944839478 \t range_loss: 0.6268837451934814\n",
      "pred_error: 0.11471737921237946 \t range_loss: 0.6304528713226318\n",
      "pred_error: 0.11506666988134384 \t range_loss: 0.6260390281677246\n",
      "pred_error: 0.11506639420986176 \t range_loss: 0.6260390281677246\n",
      "pred_error: 0.11509992927312851 \t range_loss: 0.6259588003158569\n",
      "Step: 210 \t Loss: 1.7724977731704712\n",
      "Step: 213 \t Loss: 1.7718825340270996\n",
      "pred_error: 0.11512239277362823 \t range_loss: 0.6245052814483643\n",
      "pred_error: 0.11512245982885361 \t range_loss: 0.6245052814483643\n",
      "pred_error: 0.11478985846042633 \t range_loss: 0.6290411353111267\n",
      "pred_error: 0.11503096669912338 \t range_loss: 0.6229532361030579\n",
      "Step: 221 \t Loss: 1.7715437412261963\n",
      "pred_error: 0.11477259546518326 \t range_loss: 0.6251606345176697\n",
      "Step: 224 \t Loss: 1.7696294784545898\n",
      "pred_error: 0.11471986025571823 \t range_loss: 0.6224294304847717\n",
      "pred_error: 0.11495126038789749 \t range_loss: 0.6218430995941162\n",
      "Step: 227 \t Loss: 1.7679064273834229\n",
      "pred_error: 0.11473848670721054 \t range_loss: 0.6219439506530762\n",
      "pred_error: 0.11510613560676575 \t range_loss: 0.6240218877792358\n",
      "Step: 237 \t Loss: 1.767754077911377\n",
      "pred_error: 0.11485626548528671 \t range_loss: 0.6215874552726746\n",
      "pred_error: 0.11490553617477417 \t range_loss: 0.6195692420005798\n",
      "pred_error: 0.11472954601049423 \t range_loss: 0.6223758459091187\n",
      "Step: 250 \t Loss: 1.7675471305847168\n",
      "Step: 252 \t Loss: 1.7671772241592407\n",
      "pred_error: 0.11473327875137329 \t range_loss: 0.6198432445526123\n",
      "Step: 253 \t Loss: 1.766829013824463\n",
      "Step: 255 \t Loss: 1.7652416229248047\n",
      "pred_error: 0.11464842408895493 \t range_loss: 0.6239691376686096\n",
      "pred_error: 0.11476419121026993 \t range_loss: 0.6227383017539978\n",
      "pred_error: 0.11476664245128632 \t range_loss: 0.6178474426269531\n",
      "pred_error: 0.11454243957996368 \t range_loss: 0.6218581199645996\n",
      "pred_error: 0.1151936948299408 \t range_loss: 0.620629072189331\n",
      "Step: 269 \t Loss: 1.7633118629455566\n",
      "pred_error: 0.11451712995767593 \t range_loss: 0.6181395053863525\n",
      "pred_error: 0.11451730132102966 \t range_loss: 0.6181395053863525\n",
      "pred_error: 0.11438867449760437 \t range_loss: 0.6219466924667358\n",
      "pred_error: 0.11469930410385132 \t range_loss: 0.6183859705924988\n",
      "Step: 272 \t Loss: 1.7632226943969727\n",
      "pred_error: 0.11462568491697311 \t range_loss: 0.6230933666229248\n",
      "pred_error: 0.11462613940238953 \t range_loss: 0.6230933666229248\n",
      "pred_error: 0.11494624614715576 \t range_loss: 0.6174712181091309\n",
      "pred_error: 0.11494036018848419 \t range_loss: 0.6168309450149536\n",
      "pred_error: 0.1145503968000412 \t range_loss: 0.6195014715194702\n",
      "pred_error: 0.11455026268959045 \t range_loss: 0.6195014715194702\n",
      "pred_error: 0.11503604054450989 \t range_loss: 0.6172977089881897\n",
      "Step: 285 \t Loss: 1.762742280960083\n",
      "pred_error: 0.11469125002622604 \t range_loss: 0.615830659866333\n",
      "Step: 286 \t Loss: 1.7603942155838013\n",
      "pred_error: 0.11446845531463623 \t range_loss: 0.6169636845588684\n",
      "pred_error: 0.11466296017169952 \t range_loss: 0.6177926659584045\n",
      "pred_error: 0.11494003981351852 \t range_loss: 0.6157328486442566\n",
      "pred_error: 0.1148536279797554 \t range_loss: 0.6157405972480774\n",
      "pred_error: 0.11467970907688141 \t range_loss: 0.6152440905570984\n",
      "pred_error: 0.11467969417572021 \t range_loss: 0.6152440905570984\n",
      "pred_error: 0.11449263989925385 \t range_loss: 0.6158217191696167\n",
      "pred_error: 0.11505357176065445 \t range_loss: 0.6126843094825745\n",
      "pred_error: 0.11499769985675812 \t range_loss: 0.6167905330657959\n",
      "pred_error: 0.11499842256307602 \t range_loss: 0.6167905330657959\n",
      "pred_error: 0.11521722376346588 \t range_loss: 0.6136231422424316\n",
      "pred_error: 0.11521739512681961 \t range_loss: 0.6136231422424316\n",
      "pred_error: 0.11459062248468399 \t range_loss: 0.617084801197052\n",
      "pred_error: 0.11459167301654816 \t range_loss: 0.617084801197052\n",
      "Step: 316 \t Loss: 1.760131597518921\n",
      "pred_error: 0.11478535085916519 \t range_loss: 0.6122763752937317\n",
      "Step: 317 \t Loss: 1.7582740783691406\n",
      "pred_error: 0.11478547006845474 \t range_loss: 0.6136049628257751\n",
      "pred_error: 0.11484043300151825 \t range_loss: 0.6137709021568298\n",
      "pred_error: 0.1148415133357048 \t range_loss: 0.6137709021568298\n",
      "pred_error: 0.11515994369983673 \t range_loss: 0.6127120852470398\n",
      "pred_error: 0.11515980213880539 \t range_loss: 0.6127120852470398\n",
      "pred_error: 0.11496982723474503 \t range_loss: 0.6124666929244995\n",
      "pred_error: 0.11451943218708038 \t range_loss: 0.613743782043457\n",
      "Step: 334 \t Loss: 1.7576367855072021\n",
      "pred_error: 0.11461149156093597 \t range_loss: 0.6123051047325134\n",
      "pred_error: 0.11483827978372574 \t range_loss: 0.612663984298706\n",
      "pred_error: 0.11483809351921082 \t range_loss: 0.612663984298706\n",
      "pred_error: 0.11469172686338425 \t range_loss: 0.6108044385910034\n",
      "pred_error: 0.114888995885849 \t range_loss: 0.6101346015930176\n",
      "Step: 354 \t Loss: 1.7543399333953857\n",
      "pred_error: 0.11448865383863449 \t range_loss: 0.6099799275398254\n",
      "pred_error: 0.11490005999803543 \t range_loss: 0.6113855242729187\n",
      "pred_error: 0.11457254737615585 \t range_loss: 0.6152157783508301\n",
      "pred_error: 0.11457248032093048 \t range_loss: 0.6152157783508301\n",
      "pred_error: 0.11518612504005432 \t range_loss: 0.6097642779350281\n",
      "pred_error: 0.11518626660108566 \t range_loss: 0.6097642779350281\n",
      "pred_error: 0.11487065255641937 \t range_loss: 0.6093201637268066\n",
      "pred_error: 0.11505401134490967 \t range_loss: 0.6080764532089233\n",
      "pred_error: 0.11465029418468475 \t range_loss: 0.6084001660346985\n",
      "pred_error: 0.11515405774116516 \t range_loss: 0.6083696484565735\n",
      "pred_error: 0.11482009291648865 \t range_loss: 0.6082117557525635\n",
      "pred_error: 0.11479320377111435 \t range_loss: 0.6109879612922668\n",
      "pred_error: 0.11479348689317703 \t range_loss: 0.6109879612922668\n",
      "pred_error: 0.11484584212303162 \t range_loss: 0.6112417578697205\n",
      "pred_error: 0.11507038027048111 \t range_loss: 0.6079023480415344\n",
      "pred_error: 0.11507038027048111 \t range_loss: 0.6079023480415344\n",
      "pred_error: 0.11485309153795242 \t range_loss: 0.6067407727241516\n",
      "pred_error: 0.11516574025154114 \t range_loss: 0.6063850522041321\n",
      "pred_error: 0.11545006185770035 \t range_loss: 0.6086260676383972\n",
      "pred_error: 0.11545006185770035 \t range_loss: 0.6086260676383972\n",
      "pred_error: 0.11525743454694748 \t range_loss: 0.6069061756134033\n",
      "pred_error: 0.11482024192810059 \t range_loss: 0.6064819693565369\n",
      "pred_error: 0.1149575412273407 \t range_loss: 0.6063486337661743\n",
      "pred_error: 0.11518728733062744 \t range_loss: 0.6073971390724182\n",
      "pred_error: 0.1148085966706276 \t range_loss: 0.6085554361343384\n",
      "pred_error: 0.11495396494865417 \t range_loss: 0.6080284118652344\n",
      "pred_error: 0.11495421081781387 \t range_loss: 0.6080284118652344\n",
      "pred_error: 0.11495404690504074 \t range_loss: 0.6080284118652344\n",
      "pred_error: 0.11486047506332397 \t range_loss: 0.6069190502166748\n",
      "pred_error: 0.11486046761274338 \t range_loss: 0.6069190502166748\n",
      "pred_error: 0.11466819047927856 \t range_loss: 0.6098587512969971\n",
      "pred_error: 0.11466802656650543 \t range_loss: 0.6098587512969971\n",
      "Step: 419 \t Loss: 1.752834439277649\n",
      "pred_error: 0.1146300882101059 \t range_loss: 0.6065335273742676\n",
      "pred_error: 0.11462986469268799 \t range_loss: 0.6065335273742676\n",
      "pred_error: 0.11444327235221863 \t range_loss: 0.6117399334907532\n",
      "pred_error: 0.11510057002305984 \t range_loss: 0.6088188886642456\n",
      "pred_error: 0.1148299127817154 \t range_loss: 0.6088137626647949\n",
      "pred_error: 0.11510591953992844 \t range_loss: 0.6065315008163452\n",
      "pred_error: 0.1148730143904686 \t range_loss: 0.6067346930503845\n",
      "pred_error: 0.1150752529501915 \t range_loss: 0.6072334051132202\n",
      "pred_error: 0.11493804305791855 \t range_loss: 0.6071367859840393\n",
      "pred_error: 0.11478514969348907 \t range_loss: 0.6082927584648132\n",
      "pred_error: 0.11503059417009354 \t range_loss: 0.6088612079620361\n",
      "pred_error: 0.11556980758905411 \t range_loss: 0.6059350967407227\n",
      "pred_error: 0.11556769162416458 \t range_loss: 0.6059350967407227\n",
      "pred_error: 0.1151953712105751 \t range_loss: 0.605597972869873\n",
      "pred_error: 0.11518122255802155 \t range_loss: 0.6053415536880493\n",
      "pred_error: 0.11496623605489731 \t range_loss: 0.6043117642402649\n",
      "pred_error: 0.11520152539014816 \t range_loss: 0.6058495044708252\n",
      "pred_error: 0.11532319337129593 \t range_loss: 0.6056587100028992\n",
      "pred_error: 0.1150374710559845 \t range_loss: 0.6053409576416016\n",
      "pred_error: 0.11525524407625198 \t range_loss: 0.6045624017715454\n",
      "pred_error: 0.11492308229207993 \t range_loss: 0.6045446991920471\n",
      "pred_error: 0.11492325365543365 \t range_loss: 0.6045446991920471\n",
      "pred_error: 0.11550062149763107 \t range_loss: 0.6040699481964111\n",
      "Step: 464 \t Loss: 1.7520581483840942\n",
      "pred_error: 0.11537334322929382 \t range_loss: 0.6042188405990601\n",
      "Step: 469 \t Loss: 1.750357747077942\n",
      "pred_error: 0.1153068020939827 \t range_loss: 0.6049184203147888\n",
      "pred_error: 0.11502081155776978 \t range_loss: 0.6096031665802002\n",
      "pred_error: 0.11531849950551987 \t range_loss: 0.6044828295707703\n",
      "pred_error: 0.11531839519739151 \t range_loss: 0.6044828295707703\n",
      "pred_error: 0.11510929465293884 \t range_loss: 0.6049262881278992\n",
      "pred_error: 0.11507463455200195 \t range_loss: 0.6037558317184448\n",
      "pred_error: 0.11483947187662125 \t range_loss: 0.6035460233688354\n",
      "pred_error: 0.11526349931955338 \t range_loss: 0.6039319038391113\n",
      "pred_error: 0.11494070291519165 \t range_loss: 0.6031677722930908\n",
      "pred_error: 0.11467432975769043 \t range_loss: 0.6060732007026672\n",
      "pred_error: 0.11467432975769043 \t range_loss: 0.6060732007026672\n",
      "pred_error: 0.11495927721261978 \t range_loss: 0.6089576482772827\n",
      "pred_error: 0.11515143513679504 \t range_loss: 0.6040470004081726\n",
      "pred_error: 0.11515499651432037 \t range_loss: 0.6040732264518738\n",
      "pred_error: 0.11515508592128754 \t range_loss: 0.6040732264518738\n",
      "pred_error: 0.11495475471019745 \t range_loss: 0.6037740707397461\n",
      "pred_error: 0.1149769052863121 \t range_loss: 0.604482114315033\n",
      "pred_error: 0.11501377075910568 \t range_loss: 0.6047717332839966\n",
      "pred_error: 0.11522964388132095 \t range_loss: 0.6035383939743042\n",
      "pred_error: 0.11525872349739075 \t range_loss: 0.6027710437774658\n",
      "pred_error: 0.11481286585330963 \t range_loss: 0.6060833930969238\n",
      "pred_error: 0.11518900096416473 \t range_loss: 0.6041864156723022\n",
      "pred_error: 0.11501216888427734 \t range_loss: 0.607482373714447\n",
      "pred_error: 0.11495950818061829 \t range_loss: 0.6033523678779602\n",
      "pred_error: 0.11479844897985458 \t range_loss: 0.6069009900093079\n",
      "pred_error: 0.11484614759683609 \t range_loss: 0.605708658695221\n",
      "pred_error: 0.11510369926691055 \t range_loss: 0.6043232083320618\n",
      "pred_error: 0.11500583589076996 \t range_loss: 0.6034358739852905\n",
      "pred_error: 0.11482390761375427 \t range_loss: 0.6065253019332886\n",
      "pred_error: 0.11502790451049805 \t range_loss: 0.6030987501144409\n",
      "pred_error: 0.11475763469934464 \t range_loss: 0.6043394804000854\n",
      "pred_error: 0.11490951478481293 \t range_loss: 0.6090788245201111\n",
      "pred_error: 0.11490928381681442 \t range_loss: 0.6090788245201111\n",
      "pred_error: 0.11486338078975677 \t range_loss: 0.6040724515914917\n",
      "pred_error: 0.1151675209403038 \t range_loss: 0.6036279797554016\n",
      "pred_error: 0.11473877727985382 \t range_loss: 0.6036213636398315\n",
      "pred_error: 0.11448962986469269 \t range_loss: 0.6058083176612854\n",
      "pred_error: 0.11487075686454773 \t range_loss: 0.6035858392715454\n",
      "Step: 537 \t Loss: 1.7503235340118408\n",
      "pred_error: 0.11499986797571182 \t range_loss: 0.6034626960754395\n",
      "pred_error: 0.11487462371587753 \t range_loss: 0.6039090752601624\n",
      "pred_error: 0.11488304287195206 \t range_loss: 0.6036272048950195\n",
      "pred_error: 0.11466967314481735 \t range_loss: 0.6079466938972473\n",
      "pred_error: 0.1145511046051979 \t range_loss: 0.6088298559188843\n",
      "pred_error: 0.11455117166042328 \t range_loss: 0.6088298559188843\n",
      "Step: 564 \t Loss: 1.7499127388000488\n",
      "pred_error: 0.11460304260253906 \t range_loss: 0.6038823127746582\n",
      "pred_error: 0.11502043902873993 \t range_loss: 0.6037716269493103\n",
      "pred_error: 0.1150398775935173 \t range_loss: 0.60255366563797\n",
      "pred_error: 0.11503985524177551 \t range_loss: 0.60255366563797\n",
      "pred_error: 0.11473508924245834 \t range_loss: 0.6062417030334473\n",
      "pred_error: 0.11510384827852249 \t range_loss: 0.6026991009712219\n",
      "pred_error: 0.11502640694379807 \t range_loss: 0.6022454500198364\n",
      "pred_error: 0.11478886753320694 \t range_loss: 0.6064263582229614\n",
      "Step: 582 \t Loss: 1.749767541885376\n",
      "pred_error: 0.11515743285417557 \t range_loss: 0.6033984422683716\n",
      "pred_error: 0.11515733599662781 \t range_loss: 0.6033984422683716\n",
      "pred_error: 0.11492975801229477 \t range_loss: 0.6020668148994446\n",
      "pred_error: 0.11492978781461716 \t range_loss: 0.6020668148994446\n",
      "pred_error: 0.11499299108982086 \t range_loss: 0.6043890714645386\n",
      "pred_error: 0.11489170044660568 \t range_loss: 0.601909339427948\n",
      "pred_error: 0.11465299874544144 \t range_loss: 0.603918194770813\n",
      "pred_error: 0.11487425118684769 \t range_loss: 0.6018618941307068\n",
      "Step: 599 \t Loss: 1.7485909461975098\n",
      "pred_error: 0.11459468305110931 \t range_loss: 0.6026433706283569\n",
      "pred_error: 0.114896759390831 \t range_loss: 0.6062361001968384\n",
      "pred_error: 0.11485204845666885 \t range_loss: 0.6019412875175476\n",
      "pred_error: 0.11508374661207199 \t range_loss: 0.601997435092926\n",
      "pred_error: 0.115083709359169 \t range_loss: 0.601997435092926\n",
      "pred_error: 0.11462776362895966 \t range_loss: 0.6043906211853027\n",
      "pred_error: 0.11502116918563843 \t range_loss: 0.6030704975128174\n",
      "pred_error: 0.11501049250364304 \t range_loss: 0.6017071604728699\n",
      "Step: 613 \t Loss: 1.747387409210205\n",
      "pred_error: 0.11464858055114746 \t range_loss: 0.6009017825126648\n",
      "pred_error: 0.11491585522890091 \t range_loss: 0.6043791174888611\n",
      "pred_error: 0.11521053314208984 \t range_loss: 0.6023504137992859\n",
      "pred_error: 0.11477269232273102 \t range_loss: 0.6063122153282166\n",
      "pred_error: 0.11465783417224884 \t range_loss: 0.6017422676086426\n",
      "pred_error: 0.11465785652399063 \t range_loss: 0.6017422676086426\n",
      "pred_error: 0.11505794525146484 \t range_loss: 0.6013525128364563\n",
      "pred_error: 0.11518672108650208 \t range_loss: 0.6017184853553772\n",
      "pred_error: 0.11505642533302307 \t range_loss: 0.60243159532547\n",
      "pred_error: 0.11505638062953949 \t range_loss: 0.60243159532547\n",
      "pred_error: 0.11468567699193954 \t range_loss: 0.6052750945091248\n",
      "pred_error: 0.11491269618272781 \t range_loss: 0.6007516384124756\n",
      "Step: 641 \t Loss: 1.7468297481536865\n",
      "pred_error: 0.11462259292602539 \t range_loss: 0.6006036400794983\n",
      "pred_error: 0.11462243646383286 \t range_loss: 0.6006036400794983\n",
      "pred_error: 0.11468720436096191 \t range_loss: 0.6045764684677124\n",
      "pred_error: 0.11507399380207062 \t range_loss: 0.6007428765296936\n",
      "pred_error: 0.11507339030504227 \t range_loss: 0.6007428765296936\n",
      "pred_error: 0.11507419496774673 \t range_loss: 0.6007428765296936\n",
      "pred_error: 0.1149538978934288 \t range_loss: 0.60468590259552\n",
      "pred_error: 0.11495387554168701 \t range_loss: 0.60468590259552\n",
      "pred_error: 0.11502264440059662 \t range_loss: 0.6012317538261414\n",
      "pred_error: 0.11547845602035522 \t range_loss: 0.600558340549469\n",
      "pred_error: 0.11492481827735901 \t range_loss: 0.6039258241653442\n",
      "pred_error: 0.11540864408016205 \t range_loss: 0.5988328456878662\n",
      "pred_error: 0.11542721837759018 \t range_loss: 0.6010011434555054\n",
      "pred_error: 0.11552025377750397 \t range_loss: 0.6003397703170776\n",
      "pred_error: 0.11521658301353455 \t range_loss: 0.6034989953041077\n",
      "pred_error: 0.11521671712398529 \t range_loss: 0.6034989953041077\n",
      "pred_error: 0.11562921106815338 \t range_loss: 0.5998295545578003\n",
      "pred_error: 0.11518716812133789 \t range_loss: 0.6003208756446838\n",
      "pred_error: 0.11518725752830505 \t range_loss: 0.6003208756446838\n",
      "pred_error: 0.11518718302249908 \t range_loss: 0.6003208756446838\n",
      "pred_error: 0.11550245434045792 \t range_loss: 0.5994589328765869\n",
      "pred_error: 0.1151057779788971 \t range_loss: 0.6003182530403137\n",
      "pred_error: 0.11490850895643234 \t range_loss: 0.5986947417259216\n",
      "pred_error: 0.11512055993080139 \t range_loss: 0.6026301383972168\n",
      "pred_error: 0.11501257121562958 \t range_loss: 0.6051419377326965\n",
      "pred_error: 0.11489903181791306 \t range_loss: 0.6008068919181824\n",
      "pred_error: 0.11495637893676758 \t range_loss: 0.6015692353248596\n",
      "pred_error: 0.11495613306760788 \t range_loss: 0.6015692353248596\n",
      "pred_error: 0.11483974754810333 \t range_loss: 0.6034100651741028\n",
      "pred_error: 0.11483969539403915 \t range_loss: 0.6034100651741028\n",
      "pred_error: 0.11483969539403915 \t range_loss: 0.6034100651741028\n",
      "pred_error: 0.11514901369810104 \t range_loss: 0.6003691554069519\n",
      "pred_error: 0.11486002057790756 \t range_loss: 0.5998908877372742\n",
      "pred_error: 0.11504649370908737 \t range_loss: 0.5987157821655273\n",
      "pred_error: 0.11516062915325165 \t range_loss: 0.5999570488929749\n",
      "pred_error: 0.11516086012125015 \t range_loss: 0.5999570488929749\n",
      "pred_error: 0.11516064405441284 \t range_loss: 0.5999570488929749\n",
      "pred_error: 0.1155184879899025 \t range_loss: 0.6033509969711304\n",
      "pred_error: 0.11551844328641891 \t range_loss: 0.6033509969711304\n",
      "pred_error: 0.11511196941137314 \t range_loss: 0.599490761756897\n",
      "pred_error: 0.1151660904288292 \t range_loss: 0.602314829826355\n",
      "pred_error: 0.1151660904288292 \t range_loss: 0.602314829826355\n",
      "pred_error: 0.11546242237091064 \t range_loss: 0.5991829633712769\n",
      "pred_error: 0.11518613249063492 \t range_loss: 0.598681628704071\n",
      "pred_error: 0.11518609523773193 \t range_loss: 0.598681628704071\n",
      "pred_error: 0.11557609587907791 \t range_loss: 0.6032604575157166\n",
      "pred_error: 0.11526709794998169 \t range_loss: 0.6020311117172241\n",
      "pred_error: 0.11490838974714279 \t range_loss: 0.6002073884010315\n",
      "pred_error: 0.11495677381753922 \t range_loss: 0.599277138710022\n",
      "pred_error: 0.11503827571868896 \t range_loss: 0.5998357534408569\n",
      "pred_error: 0.11469583958387375 \t range_loss: 0.6009148359298706\n",
      "pred_error: 0.11469580978155136 \t range_loss: 0.6009148359298706\n",
      "pred_error: 0.11498185247182846 \t range_loss: 0.5995745062828064\n",
      "pred_error: 0.1146857887506485 \t range_loss: 0.6009699106216431\n",
      "pred_error: 0.11481799185276031 \t range_loss: 0.6033515334129333\n",
      "pred_error: 0.11531076580286026 \t range_loss: 0.6015151143074036\n",
      "pred_error: 0.11506155878305435 \t range_loss: 0.6022054553031921\n",
      "pred_error: 0.11506184190511703 \t range_loss: 0.6022054553031921\n",
      "pred_error: 0.11506170779466629 \t range_loss: 0.6022054553031921\n",
      "pred_error: 0.11529960483312607 \t range_loss: 0.6013617515563965\n",
      "pred_error: 0.1145782321691513 \t range_loss: 0.602959930896759\n",
      "pred_error: 0.1147058978676796 \t range_loss: 0.6015730500221252\n",
      "pred_error: 0.11470600962638855 \t range_loss: 0.6015730500221252\n",
      "pred_error: 0.11485752463340759 \t range_loss: 0.6051064729690552\n",
      "pred_error: 0.11485752463340759 \t range_loss: 0.6051064729690552\n",
      "pred_error: 0.11485745757818222 \t range_loss: 0.6051064729690552\n",
      "pred_error: 0.11497075110673904 \t range_loss: 0.6029065251350403\n",
      "pred_error: 0.11480323225259781 \t range_loss: 0.6003046035766602\n",
      "pred_error: 0.11517398804426193 \t range_loss: 0.5995754599571228\n",
      "pred_error: 0.11517417430877686 \t range_loss: 0.5995754599571228\n",
      "pred_error: 0.11517390608787537 \t range_loss: 0.5995754599571228\n",
      "pred_error: 0.11476484686136246 \t range_loss: 0.6004362106323242\n",
      "pred_error: 0.11494172364473343 \t range_loss: 0.5998367667198181\n",
      "pred_error: 0.11477349698543549 \t range_loss: 0.6041837930679321\n",
      "pred_error: 0.11477383226156235 \t range_loss: 0.6041837930679321\n",
      "pred_error: 0.11477390676736832 \t range_loss: 0.6041837930679321\n",
      "pred_error: 0.11477357149124146 \t range_loss: 0.6041837930679321\n",
      "pred_error: 0.1149873435497284 \t range_loss: 0.6013989448547363\n",
      "pred_error: 0.11498710513114929 \t range_loss: 0.6013989448547363\n",
      "pred_error: 0.11512657254934311 \t range_loss: 0.6024800539016724\n",
      "pred_error: 0.11493291705846786 \t range_loss: 0.6046463251113892\n",
      "pred_error: 0.11489357054233551 \t range_loss: 0.6007450819015503\n",
      "pred_error: 0.11495067179203033 \t range_loss: 0.6003073453903198\n",
      "pred_error: 0.11521580070257187 \t range_loss: 0.5980634689331055\n",
      "pred_error: 0.11552367359399796 \t range_loss: 0.6000903248786926\n",
      "pred_error: 0.11524230241775513 \t range_loss: 0.6047523617744446\n",
      "pred_error: 0.11538050323724747 \t range_loss: 0.5985347032546997\n",
      "pred_error: 0.11503969132900238 \t range_loss: 0.598291277885437\n",
      "pred_error: 0.11497598141431808 \t range_loss: 0.6004112362861633\n",
      "pred_error: 0.11516871303319931 \t range_loss: 0.6003806591033936\n",
      "pred_error: 0.11516868323087692 \t range_loss: 0.6003806591033936\n",
      "pred_error: 0.11527252197265625 \t range_loss: 0.5999647378921509\n",
      "pred_error: 0.11501023918390274 \t range_loss: 0.6032240986824036\n",
      "pred_error: 0.11501020938158035 \t range_loss: 0.6032240986824036\n",
      "BEST LOSS: 1.7468297\n",
      "==== Model: block6_cob_activation_norm  in Layer: 6 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 05:23:00,676 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 05:25:10,475 model.rs:1246 value (-18624) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 05:25:10,482 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 05:25:32,499 model.rs:1246 value (-18624) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 05:25:32,503 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 05:25:32,529 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 05:25:32,549 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 05:25:32,565 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+--------------------+------------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error         | median_error     | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+--------------------+------------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000000008999839 | -0.0000021457672 | 0.00035792217 | -0.00048039854 | 0.000051266914 | 0.0000021457672  | 0.00048039854 | 0             | 0.0000000046482844 | -0.0001920712      | 0.0012473243           |\n",
      "+--------------------+------------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 656040 64 [-686376, 416250] 1 [16]\n",
      "===============================\n",
      "==== Model: block6_cob_activation_norm_teleported  in Layer: 6 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 05:25:54,103 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 05:28:05,899 model.rs:1246 value (-199680) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 05:28:05,907 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 05:28:26,507 model.rs:1246 value (-199680) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 05:28:26,512 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 05:28:26,531 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 05:28:26,557 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 05:28:26,571 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error   | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000000640198 | 0.000021070242 | 0.00041724741 | -0.00049191713 | 0.000051276227 | 0.000021070242   | 0.00049191713 | 0             | 0.000000004651996  | -0.0003189166      | 0.0014374803           |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 656046 64 [-459538, 202078] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 7 , \t  activation_stats: {'relu_1': {'norm': tensor(780.0437), 'max': tensor(7.7553), 'min': tensor(-7.5588), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(15.3141)\n",
      "Step: 0 \t Loss: 2.4000091552734375\n",
      "Step: 1 \t Loss: 2.355992555618286\n",
      "pred_error: 0.142011821269989 \t range_loss: 0.9358773827552795\n",
      "pred_error: 0.14201141893863678 \t range_loss: 0.9358773827552795\n",
      "Step: 2 \t Loss: 2.3272876739501953\n",
      "Step: 3 \t Loss: 2.2944388389587402\n",
      "pred_error: 0.1416870802640915 \t range_loss: 0.8775700330734253\n",
      "Step: 4 \t Loss: 2.2675065994262695\n",
      "Step: 5 \t Loss: 2.2425472736358643\n",
      "Step: 6 \t Loss: 2.216999053955078\n",
      "pred_error: 0.14176827669143677 \t range_loss: 0.799316942691803\n",
      "Step: 7 \t Loss: 2.1931138038635254\n",
      "Step: 8 \t Loss: 2.185520887374878\n",
      "pred_error: 0.14220556616783142 \t range_loss: 0.7634617686271667\n",
      "Step: 9 \t Loss: 2.175858974456787\n",
      "Step: 10 \t Loss: 2.1717400550842285\n",
      "Step: 11 \t Loss: 2.1601157188415527\n",
      "Step: 12 \t Loss: 2.1600584983825684\n",
      "pred_error: 0.14230825006961823 \t range_loss: 0.7369756102561951\n",
      "Step: 13 \t Loss: 2.14733624458313\n",
      "Step: 15 \t Loss: 2.142334461212158\n",
      "Step: 16 \t Loss: 2.1374218463897705\n",
      "pred_error: 0.14170441031455994 \t range_loss: 0.7203798294067383\n",
      "pred_error: 0.1421300172805786 \t range_loss: 0.718513548374176\n",
      "Step: 18 \t Loss: 2.1294164657592773\n",
      "pred_error: 0.14149890840053558 \t range_loss: 0.7155340909957886\n",
      "Step: 20 \t Loss: 2.127581834793091\n",
      "Step: 21 \t Loss: 2.116872549057007\n",
      "pred_error: 0.14160634577274323 \t range_loss: 0.7008092403411865\n",
      "Step: 22 \t Loss: 2.114086151123047\n",
      "Step: 23 \t Loss: 2.108875274658203\n",
      "Step: 25 \t Loss: 2.1038949489593506\n",
      "pred_error: 0.14175841212272644 \t range_loss: 0.6863057613372803\n",
      "pred_error: 0.14175891876220703 \t range_loss: 0.6863057613372803\n",
      "Step: 26 \t Loss: 2.1037161350250244\n",
      "Step: 27 \t Loss: 2.1022651195526123\n",
      "Step: 28 \t Loss: 2.0956900119781494\n",
      "pred_error: 0.1416078507900238 \t range_loss: 0.6796111464500427\n",
      "pred_error: 0.1416078805923462 \t range_loss: 0.6796111464500427\n",
      "Step: 30 \t Loss: 2.090895891189575\n",
      "Step: 32 \t Loss: 2.0893993377685547\n",
      "Step: 33 \t Loss: 2.0863609313964844\n",
      "Step: 34 \t Loss: 2.0831973552703857\n",
      "Step: 35 \t Loss: 2.078455924987793\n",
      "pred_error: 0.14146174490451813 \t range_loss: 0.6638368964195251\n",
      "Step: 37 \t Loss: 2.074491500854492\n",
      "pred_error: 0.14176972210407257 \t range_loss: 0.6567944288253784\n",
      "pred_error: 0.14177045226097107 \t range_loss: 0.6567944288253784\n",
      "Step: 39 \t Loss: 2.0674147605895996\n",
      "pred_error: 0.14182604849338531 \t range_loss: 0.6491543054580688\n",
      "Step: 41 \t Loss: 2.067035675048828\n",
      "Step: 43 \t Loss: 2.0612130165100098\n",
      "Step: 48 \t Loss: 2.058805465698242\n",
      "Step: 49 \t Loss: 2.058224678039551\n",
      "Step: 50 \t Loss: 2.05509090423584\n",
      "pred_error: 0.14185474812984467 \t range_loss: 0.6365378499031067\n",
      "pred_error: 0.14238445460796356 \t range_loss: 0.6356706619262695\n",
      "pred_error: 0.1423845738172531 \t range_loss: 0.6356706619262695\n",
      "pred_error: 0.1423838585615158 \t range_loss: 0.6356706619262695\n",
      "Step: 52 \t Loss: 2.0521597862243652\n",
      "Step: 54 \t Loss: 2.0488839149475098\n",
      "pred_error: 0.1413973867893219 \t range_loss: 0.6388401389122009\n",
      "pred_error: 0.14195065200328827 \t range_loss: 0.6303341388702393\n",
      "Step: 58 \t Loss: 2.048348903656006\n",
      "pred_error: 0.14199881255626678 \t range_loss: 0.6283608078956604\n",
      "Step: 59 \t Loss: 2.0458784103393555\n",
      "Step: 61 \t Loss: 2.0424611568450928\n",
      "pred_error: 0.14166566729545593 \t range_loss: 0.6258000135421753\n",
      "pred_error: 0.142117440700531 \t range_loss: 0.6248654127120972\n",
      "Step: 63 \t Loss: 2.04034423828125\n",
      "pred_error: 0.14107145369052887 \t range_loss: 0.633041262626648\n",
      "Step: 65 \t Loss: 2.0400679111480713\n",
      "pred_error: 0.14162500202655792 \t range_loss: 0.6238162517547607\n",
      "pred_error: 0.14184199273586273 \t range_loss: 0.6240026950836182\n",
      "Step: 68 \t Loss: 2.0375070571899414\n",
      "pred_error: 0.14204911887645721 \t range_loss: 0.6197736263275146\n",
      "Step: 70 \t Loss: 2.0335803031921387\n",
      "Step: 72 \t Loss: 2.0329949855804443\n",
      "Step: 74 \t Loss: 2.032414436340332\n",
      "Step: 75 \t Loss: 2.0315029621124268\n",
      "Step: 76 \t Loss: 2.0314648151397705\n",
      "Step: 77 \t Loss: 2.027872085571289\n",
      "pred_error: 0.14113116264343262 \t range_loss: 0.6165614128112793\n",
      "Step: 79 \t Loss: 2.0269041061401367\n",
      "Step: 81 \t Loss: 2.0248935222625732\n",
      "pred_error: 0.14152978360652924 \t range_loss: 0.6121808290481567\n",
      "pred_error: 0.14163745939731598 \t range_loss: 0.6113784909248352\n",
      "Step: 86 \t Loss: 2.023043632507324\n",
      "pred_error: 0.14073315262794495 \t range_loss: 0.6236099600791931\n",
      "pred_error: 0.14144013822078705 \t range_loss: 0.6091801524162292\n",
      "pred_error: 0.14098088443279266 \t range_loss: 0.6178991198539734\n",
      "Step: 91 \t Loss: 2.0228521823883057\n",
      "Step: 93 \t Loss: 2.0199778079986572\n",
      "pred_error: 0.1417469084262848 \t range_loss: 0.6088765263557434\n",
      "Step: 95 \t Loss: 2.0189619064331055\n",
      "pred_error: 0.14088499546051025 \t range_loss: 0.6121641993522644\n",
      "Step: 100 \t Loss: 2.0171706676483154\n",
      "Step: 102 \t Loss: 2.012298107147217\n",
      "pred_error: 0.1408737450838089 \t range_loss: 0.6035605669021606\n",
      "pred_error: 0.14087370038032532 \t range_loss: 0.6035605669021606\n",
      "pred_error: 0.14129367470741272 \t range_loss: 0.6036674380302429\n",
      "pred_error: 0.14148256182670593 \t range_loss: 0.6025208830833435\n",
      "pred_error: 0.14150893688201904 \t range_loss: 0.6041781902313232\n",
      "Step: 108 \t Loss: 2.011212110519409\n",
      "pred_error: 0.14154917001724243 \t range_loss: 0.6007615327835083\n",
      "pred_error: 0.14106732606887817 \t range_loss: 0.6007912158966064\n",
      "pred_error: 0.14074400067329407 \t range_loss: 0.6097221374511719\n",
      "pred_error: 0.14074434340000153 \t range_loss: 0.6097221374511719\n",
      "pred_error: 0.14079076051712036 \t range_loss: 0.6050223112106323\n",
      "pred_error: 0.14079079031944275 \t range_loss: 0.6050223112106323\n",
      "Step: 117 \t Loss: 2.008732557296753\n",
      "pred_error: 0.14098826050758362 \t range_loss: 0.5988500714302063\n",
      "pred_error: 0.14056487381458282 \t range_loss: 0.6088956594467163\n",
      "pred_error: 0.14056484401226044 \t range_loss: 0.6088956594467163\n",
      "pred_error: 0.1411445438861847 \t range_loss: 0.5984001159667969\n",
      "pred_error: 0.14114490151405334 \t range_loss: 0.5984001159667969\n",
      "pred_error: 0.1408473253250122 \t range_loss: 0.6011620163917542\n",
      "pred_error: 0.14084678888320923 \t range_loss: 0.6011620163917542\n",
      "pred_error: 0.14134620130062103 \t range_loss: 0.5989375710487366\n",
      "pred_error: 0.14134620130062103 \t range_loss: 0.5989375710487366\n",
      "Step: 124 \t Loss: 2.0047831535339355\n",
      "pred_error: 0.14126457273960114 \t range_loss: 0.5985497832298279\n",
      "pred_error: 0.1406085044145584 \t range_loss: 0.6054859757423401\n",
      "pred_error: 0.14060857892036438 \t range_loss: 0.6054859757423401\n",
      "pred_error: 0.14110654592514038 \t range_loss: 0.5951528549194336\n",
      "pred_error: 0.1410565823316574 \t range_loss: 0.5982059240341187\n",
      "pred_error: 0.14105646312236786 \t range_loss: 0.5982059240341187\n",
      "Step: 133 \t Loss: 2.003429889678955\n",
      "pred_error: 0.14047253131866455 \t range_loss: 0.6036514639854431\n",
      "pred_error: 0.1404729038476944 \t range_loss: 0.6036514639854431\n",
      "pred_error: 0.14118395745754242 \t range_loss: 0.5952062606811523\n",
      "Step: 140 \t Loss: 1.9988857507705688\n",
      "pred_error: 0.1402566283941269 \t range_loss: 0.6012381911277771\n",
      "pred_error: 0.14025674760341644 \t range_loss: 0.6012381911277771\n",
      "pred_error: 0.14085669815540314 \t range_loss: 0.5925645232200623\n",
      "pred_error: 0.1403353065252304 \t range_loss: 0.5958907008171082\n",
      "pred_error: 0.1409478336572647 \t range_loss: 0.5951352119445801\n",
      "Step: 147 \t Loss: 1.9986634254455566\n",
      "Step: 149 \t Loss: 1.9984303712844849\n",
      "pred_error: 0.14028504490852356 \t range_loss: 0.5955979824066162\n",
      "pred_error: 0.14028504490852356 \t range_loss: 0.5955979824066162\n",
      "Step: 154 \t Loss: 1.9965155124664307\n",
      "pred_error: 0.14055387675762177 \t range_loss: 0.5909767746925354\n",
      "pred_error: 0.1403040587902069 \t range_loss: 0.5960492491722107\n",
      "pred_error: 0.14092448353767395 \t range_loss: 0.5905189514160156\n",
      "pred_error: 0.14064592123031616 \t range_loss: 0.5919204354286194\n",
      "pred_error: 0.14064598083496094 \t range_loss: 0.5919204354286194\n",
      "Step: 160 \t Loss: 1.9959230422973633\n",
      "pred_error: 0.14014072716236115 \t range_loss: 0.5985116958618164\n",
      "pred_error: 0.1401403397321701 \t range_loss: 0.5985116958618164\n",
      "Step: 163 \t Loss: 1.9956108331680298\n",
      "Step: 164 \t Loss: 1.9953181743621826\n",
      "Step: 165 \t Loss: 1.9894874095916748\n",
      "pred_error: 0.14046268165111542 \t range_loss: 0.5886760950088501\n",
      "pred_error: 0.14014936983585358 \t range_loss: 0.596062183380127\n",
      "pred_error: 0.1401493400335312 \t range_loss: 0.596062183380127\n",
      "pred_error: 0.14014947414398193 \t range_loss: 0.596062183380127\n",
      "pred_error: 0.1408366709947586 \t range_loss: 0.5894672870635986\n",
      "pred_error: 0.1411457657814026 \t range_loss: 0.5862725377082825\n",
      "pred_error: 0.14019399881362915 \t range_loss: 0.5893394351005554\n",
      "pred_error: 0.14055490493774414 \t range_loss: 0.5871225595474243\n",
      "pred_error: 0.1405535787343979 \t range_loss: 0.5871225595474243\n",
      "pred_error: 0.14114567637443542 \t range_loss: 0.5864225625991821\n",
      "Step: 181 \t Loss: 1.987797737121582\n",
      "pred_error: 0.14052896201610565 \t range_loss: 0.5825081467628479\n",
      "pred_error: 0.14079299569129944 \t range_loss: 0.5835510492324829\n",
      "pred_error: 0.1405325084924698 \t range_loss: 0.5827134847640991\n",
      "pred_error: 0.14130087196826935 \t range_loss: 0.5820348858833313\n",
      "pred_error: 0.14060764014720917 \t range_loss: 0.5826059579849243\n",
      "pred_error: 0.14101681113243103 \t range_loss: 0.5813860297203064\n",
      "Step: 195 \t Loss: 1.9849693775177002\n",
      "pred_error: 0.14046330749988556 \t range_loss: 0.5805295705795288\n",
      "pred_error: 0.14046332240104675 \t range_loss: 0.5805295705795288\n",
      "pred_error: 0.14046339690685272 \t range_loss: 0.5805295705795288\n",
      "pred_error: 0.14046332240104675 \t range_loss: 0.5805295705795288\n",
      "pred_error: 0.14046332240104675 \t range_loss: 0.5805295705795288\n",
      "pred_error: 0.14002016186714172 \t range_loss: 0.5897248387336731\n",
      "pred_error: 0.1407385915517807 \t range_loss: 0.5825040936470032\n",
      "pred_error: 0.1407386064529419 \t range_loss: 0.5825040936470032\n",
      "pred_error: 0.1408473253250122 \t range_loss: 0.5841984152793884\n",
      "Step: 202 \t Loss: 1.984905481338501\n",
      "pred_error: 0.14039316773414612 \t range_loss: 0.5809671878814697\n",
      "pred_error: 0.14020225405693054 \t range_loss: 0.5853853225708008\n",
      "pred_error: 0.14041364192962646 \t range_loss: 0.5813037157058716\n",
      "pred_error: 0.14041349291801453 \t range_loss: 0.5813037157058716\n",
      "pred_error: 0.1405995786190033 \t range_loss: 0.5789814591407776\n",
      "pred_error: 0.14010530710220337 \t range_loss: 0.5891252160072327\n",
      "pred_error: 0.14028701186180115 \t range_loss: 0.5839242339134216\n",
      "Step: 214 \t Loss: 1.983016848564148\n",
      "pred_error: 0.14088819921016693 \t range_loss: 0.5801243782043457\n",
      "pred_error: 0.14035198092460632 \t range_loss: 0.5796480774879456\n",
      "pred_error: 0.1407049000263214 \t range_loss: 0.5790920853614807\n",
      "pred_error: 0.1403394341468811 \t range_loss: 0.5820080041885376\n",
      "pred_error: 0.14033950865268707 \t range_loss: 0.5820080041885376\n",
      "pred_error: 0.1403396725654602 \t range_loss: 0.5820080041885376\n",
      "Step: 221 \t Loss: 1.9828600883483887\n",
      "Step: 223 \t Loss: 1.981917142868042\n",
      "pred_error: 0.14054253697395325 \t range_loss: 0.5764917731285095\n",
      "pred_error: 0.14054584503173828 \t range_loss: 0.5781649947166443\n",
      "Step: 226 \t Loss: 1.9814984798431396\n",
      "Step: 228 \t Loss: 1.9789252281188965\n",
      "pred_error: 0.1404409110546112 \t range_loss: 0.5798216462135315\n",
      "pred_error: 0.1404581367969513 \t range_loss: 0.5761380791664124\n",
      "pred_error: 0.14086127281188965 \t range_loss: 0.577553927898407\n",
      "pred_error: 0.1408623605966568 \t range_loss: 0.5781174302101135\n",
      "pred_error: 0.1405191272497177 \t range_loss: 0.5753034949302673\n",
      "pred_error: 0.1405189484357834 \t range_loss: 0.5753034949302673\n",
      "pred_error: 0.14123256504535675 \t range_loss: 0.5749391913414001\n",
      "pred_error: 0.14052170515060425 \t range_loss: 0.5748946070671082\n",
      "pred_error: 0.14052172005176544 \t range_loss: 0.5748946070671082\n",
      "pred_error: 0.14075304567813873 \t range_loss: 0.5757867693901062\n",
      "pred_error: 0.14069180190563202 \t range_loss: 0.5740715861320496\n",
      "pred_error: 0.14112873375415802 \t range_loss: 0.5735761523246765\n",
      "Step: 249 \t Loss: 1.977736473083496\n",
      "pred_error: 0.13992537558078766 \t range_loss: 0.5817925930023193\n",
      "pred_error: 0.14033345878124237 \t range_loss: 0.5772308707237244\n",
      "pred_error: 0.14042550325393677 \t range_loss: 0.5734958648681641\n",
      "pred_error: 0.14042550325393677 \t range_loss: 0.5734958648681641\n",
      "pred_error: 0.14030209183692932 \t range_loss: 0.5807310938835144\n",
      "pred_error: 0.1401899755001068 \t range_loss: 0.576627790927887\n",
      "pred_error: 0.14086897671222687 \t range_loss: 0.5742486119270325\n",
      "pred_error: 0.14086897671222687 \t range_loss: 0.5742486119270325\n",
      "Step: 261 \t Loss: 1.9756819009780884\n",
      "pred_error: 0.1403166502714157 \t range_loss: 0.5773295164108276\n",
      "pred_error: 0.14035660028457642 \t range_loss: 0.5801458358764648\n",
      "pred_error: 0.14035671949386597 \t range_loss: 0.5801458358764648\n",
      "pred_error: 0.14099332690238953 \t range_loss: 0.5702435374259949\n",
      "Step: 272 \t Loss: 1.9742040634155273\n",
      "pred_error: 0.1403413861989975 \t range_loss: 0.5707902312278748\n",
      "pred_error: 0.14084208011627197 \t range_loss: 0.5706108212471008\n",
      "pred_error: 0.14081498980522156 \t range_loss: 0.5733630061149597\n",
      "Step: 279 \t Loss: 1.9741461277008057\n",
      "pred_error: 0.14079724252223969 \t range_loss: 0.5703909993171692\n",
      "pred_error: 0.14003843069076538 \t range_loss: 0.579853892326355\n",
      "pred_error: 0.14024721086025238 \t range_loss: 0.5746471285820007\n",
      "Step: 291 \t Loss: 1.9729866981506348\n",
      "pred_error: 0.1408969908952713 \t range_loss: 0.5689120292663574\n",
      "pred_error: 0.14047761261463165 \t range_loss: 0.569010317325592\n",
      "pred_error: 0.14047759771347046 \t range_loss: 0.569010317325592\n",
      "pred_error: 0.13998201489448547 \t range_loss: 0.5782712697982788\n",
      "pred_error: 0.14023080468177795 \t range_loss: 0.5726612210273743\n",
      "pred_error: 0.14076772332191467 \t range_loss: 0.5683112740516663\n",
      "Step: 298 \t Loss: 1.9725703001022339\n",
      "pred_error: 0.1408705711364746 \t range_loss: 0.5716586112976074\n",
      "pred_error: 0.14051218330860138 \t range_loss: 0.5685675144195557\n",
      "pred_error: 0.14050647616386414 \t range_loss: 0.5696370601654053\n",
      "pred_error: 0.1406995803117752 \t range_loss: 0.5720484256744385\n",
      "Step: 307 \t Loss: 1.9720686674118042\n",
      "Step: 312 \t Loss: 1.9717791080474854\n",
      "pred_error: 0.1402481645345688 \t range_loss: 0.569296658039093\n",
      "pred_error: 0.1399402916431427 \t range_loss: 0.5783306360244751\n",
      "pred_error: 0.14047157764434814 \t range_loss: 0.5677090287208557\n",
      "pred_error: 0.14066900312900543 \t range_loss: 0.5683337450027466\n",
      "Step: 317 \t Loss: 1.971325159072876\n",
      "pred_error: 0.14095143973827362 \t range_loss: 0.5678116679191589\n",
      "Step: 319 \t Loss: 1.968796968460083\n",
      "pred_error: 0.1403343677520752 \t range_loss: 0.5705707669258118\n",
      "pred_error: 0.14094632863998413 \t range_loss: 0.5675579905509949\n",
      "pred_error: 0.14034703373908997 \t range_loss: 0.5754344463348389\n",
      "pred_error: 0.1401553750038147 \t range_loss: 0.5701059103012085\n",
      "pred_error: 0.14062249660491943 \t range_loss: 0.5683913230895996\n",
      "Step: 331 \t Loss: 1.9675074815750122\n",
      "pred_error: 0.14078621566295624 \t range_loss: 0.5669644474983215\n",
      "pred_error: 0.14034616947174072 \t range_loss: 0.5650226473808289\n",
      "pred_error: 0.14069105684757233 \t range_loss: 0.5657274723052979\n",
      "pred_error: 0.14042960107326508 \t range_loss: 0.5674237012863159\n",
      "pred_error: 0.14057953655719757 \t range_loss: 0.5666471123695374\n",
      "pred_error: 0.14057955145835876 \t range_loss: 0.5666471123695374\n",
      "pred_error: 0.1400279551744461 \t range_loss: 0.5744934678077698\n",
      "pred_error: 0.1405472457408905 \t range_loss: 0.5654141902923584\n",
      "pred_error: 0.14035364985466003 \t range_loss: 0.5668182969093323\n",
      "pred_error: 0.14091776311397552 \t range_loss: 0.5649511218070984\n",
      "pred_error: 0.13999377191066742 \t range_loss: 0.5742180347442627\n",
      "pred_error: 0.1403253674507141 \t range_loss: 0.5655457377433777\n",
      "Step: 364 \t Loss: 1.9674348831176758\n",
      "pred_error: 0.14081795513629913 \t range_loss: 0.5645655393600464\n",
      "pred_error: 0.14037637412548065 \t range_loss: 0.5641865134239197\n",
      "Step: 372 \t Loss: 1.967075228691101\n",
      "pred_error: 0.14019052684307098 \t range_loss: 0.5651665925979614\n",
      "pred_error: 0.140190988779068 \t range_loss: 0.5651665925979614\n",
      "pred_error: 0.14018937945365906 \t range_loss: 0.5651665925979614\n",
      "Step: 386 \t Loss: 1.963752269744873\n",
      "pred_error: 0.13992245495319366 \t range_loss: 0.5717102289199829\n",
      "pred_error: 0.14038819074630737 \t range_loss: 0.5616867542266846\n",
      "pred_error: 0.140666201710701 \t range_loss: 0.5647809505462646\n",
      "pred_error: 0.14029660820960999 \t range_loss: 0.5641621351242065\n",
      "pred_error: 0.14091624319553375 \t range_loss: 0.5623634457588196\n",
      "pred_error: 0.14049869775772095 \t range_loss: 0.5618219375610352\n",
      "pred_error: 0.14049874246120453 \t range_loss: 0.5618219375610352\n",
      "pred_error: 0.14019817113876343 \t range_loss: 0.5652689933776855\n",
      "pred_error: 0.1409313827753067 \t range_loss: 0.5622514486312866\n",
      "pred_error: 0.14093118906021118 \t range_loss: 0.5622514486312866\n",
      "pred_error: 0.1403101086616516 \t range_loss: 0.5651487112045288\n",
      "pred_error: 0.1407930552959442 \t range_loss: 0.5591849088668823\n",
      "Step: 420 \t Loss: 1.9627861976623535\n",
      "pred_error: 0.14024725556373596 \t range_loss: 0.5603134632110596\n",
      "pred_error: 0.14087064564228058 \t range_loss: 0.5595354437828064\n",
      "pred_error: 0.1408703476190567 \t range_loss: 0.5595354437828064\n",
      "pred_error: 0.14057506620883942 \t range_loss: 0.5619575381278992\n",
      "pred_error: 0.14057473838329315 \t range_loss: 0.5619575381278992\n",
      "pred_error: 0.1400436908006668 \t range_loss: 0.569909393787384\n",
      "pred_error: 0.14092595875263214 \t range_loss: 0.5591709613800049\n",
      "pred_error: 0.14088588953018188 \t range_loss: 0.5584850907325745\n",
      "Step: 436 \t Loss: 1.9617221355438232\n",
      "pred_error: 0.1400471180677414 \t range_loss: 0.5672566294670105\n",
      "pred_error: 0.14056862890720367 \t range_loss: 0.5611910820007324\n",
      "pred_error: 0.1405685693025589 \t range_loss: 0.5611910820007324\n",
      "pred_error: 0.1405169814825058 \t range_loss: 0.5601896047592163\n",
      "pred_error: 0.1405676156282425 \t range_loss: 0.5581981539726257\n",
      "pred_error: 0.14007668197155 \t range_loss: 0.566775381565094\n",
      "pred_error: 0.1403738409280777 \t range_loss: 0.5591303110122681\n",
      "pred_error: 0.14056020975112915 \t range_loss: 0.5581532716751099\n",
      "pred_error: 0.14088398218154907 \t range_loss: 0.5598240494728088\n",
      "pred_error: 0.14088375866413116 \t range_loss: 0.5598240494728088\n",
      "pred_error: 0.14046849310398102 \t range_loss: 0.5595995187759399\n",
      "pred_error: 0.1411689668893814 \t range_loss: 0.5584782361984253\n",
      "pred_error: 0.14116910099983215 \t range_loss: 0.5584782361984253\n",
      "pred_error: 0.14047777652740479 \t range_loss: 0.5582512617111206\n",
      "pred_error: 0.1407448649406433 \t range_loss: 0.5603331923484802\n",
      "pred_error: 0.1407451033592224 \t range_loss: 0.5603331923484802\n",
      "pred_error: 0.14040525257587433 \t range_loss: 0.5593479871749878\n",
      "pred_error: 0.1409604251384735 \t range_loss: 0.5605189204216003\n",
      "pred_error: 0.13988249003887177 \t range_loss: 0.5660784244537354\n",
      "pred_error: 0.13988159596920013 \t range_loss: 0.5660784244537354\n",
      "pred_error: 0.14049111306667328 \t range_loss: 0.558583676815033\n",
      "pred_error: 0.14071187376976013 \t range_loss: 0.5609357953071594\n",
      "pred_error: 0.1407119184732437 \t range_loss: 0.5609357953071594\n",
      "pred_error: 0.14050796627998352 \t range_loss: 0.5576729774475098\n",
      "pred_error: 0.14113107323646545 \t range_loss: 0.557201087474823\n",
      "pred_error: 0.140396386384964 \t range_loss: 0.5578276515007019\n",
      "pred_error: 0.1403963714838028 \t range_loss: 0.5578276515007019\n",
      "pred_error: 0.14028878509998322 \t range_loss: 0.5656597018241882\n",
      "pred_error: 0.14028818905353546 \t range_loss: 0.5656597018241882\n",
      "pred_error: 0.14084656536579132 \t range_loss: 0.5590248107910156\n",
      "pred_error: 0.1403799206018448 \t range_loss: 0.560620903968811\n",
      "pred_error: 0.1409946233034134 \t range_loss: 0.5563305020332336\n",
      "pred_error: 0.14099526405334473 \t range_loss: 0.5563305020332336\n",
      "Step: 488 \t Loss: 1.9612038135528564\n",
      "pred_error: 0.1409570872783661 \t range_loss: 0.5555686950683594\n",
      "pred_error: 0.14061909914016724 \t range_loss: 0.5552607774734497\n",
      "pred_error: 0.14061905443668365 \t range_loss: 0.5552607774734497\n",
      "pred_error: 0.1407780945301056 \t range_loss: 0.5572114586830139\n",
      "pred_error: 0.140435129404068 \t range_loss: 0.5603110790252686\n",
      "pred_error: 0.14100849628448486 \t range_loss: 0.5567529797554016\n",
      "pred_error: 0.14095959067344666 \t range_loss: 0.5569044351577759\n",
      "pred_error: 0.14060606062412262 \t range_loss: 0.5552147030830383\n",
      "pred_error: 0.14076146483421326 \t range_loss: 0.5604049563407898\n",
      "pred_error: 0.14088398218154907 \t range_loss: 0.5605664849281311\n",
      "Step: 505 \t Loss: 1.9604461193084717\n",
      "Step: 517 \t Loss: 1.958423137664795\n",
      "pred_error: 0.1399504840373993 \t range_loss: 0.5658450722694397\n",
      "pred_error: 0.140513613820076 \t range_loss: 0.5566831827163696\n",
      "pred_error: 0.1401546746492386 \t range_loss: 0.5619326829910278\n",
      "pred_error: 0.14048010110855103 \t range_loss: 0.5561963319778442\n",
      "pred_error: 0.14100635051727295 \t range_loss: 0.5568598508834839\n",
      "pred_error: 0.14100642502307892 \t range_loss: 0.5568598508834839\n",
      "pred_error: 0.14100632071495056 \t range_loss: 0.5568598508834839\n",
      "pred_error: 0.140737384557724 \t range_loss: 0.556571900844574\n",
      "pred_error: 0.1405107080936432 \t range_loss: 0.5568751096725464\n",
      "pred_error: 0.14014863967895508 \t range_loss: 0.5672703385353088\n",
      "pred_error: 0.14014846086502075 \t range_loss: 0.5672703385353088\n",
      "pred_error: 0.14042577147483826 \t range_loss: 0.5560400485992432\n",
      "pred_error: 0.13982214033603668 \t range_loss: 0.5638464689254761\n",
      "pred_error: 0.14009809494018555 \t range_loss: 0.5590224266052246\n",
      "Step: 548 \t Loss: 1.9578630924224854\n",
      "pred_error: 0.14083048701286316 \t range_loss: 0.5572214722633362\n",
      "pred_error: 0.1399758905172348 \t range_loss: 0.567828357219696\n",
      "pred_error: 0.14025749266147614 \t range_loss: 0.5560806393623352\n",
      "pred_error: 0.14038227498531342 \t range_loss: 0.5574475526809692\n",
      "pred_error: 0.14038239419460297 \t range_loss: 0.5574475526809692\n",
      "pred_error: 0.1400471180677414 \t range_loss: 0.559745192527771\n",
      "pred_error: 0.14044184982776642 \t range_loss: 0.5567196011543274\n",
      "pred_error: 0.14009100198745728 \t range_loss: 0.5647699236869812\n",
      "pred_error: 0.14066942036151886 \t range_loss: 0.5583202838897705\n",
      "pred_error: 0.14066942036151886 \t range_loss: 0.5583202838897705\n",
      "pred_error: 0.1402071714401245 \t range_loss: 0.5599719285964966\n",
      "Step: 584 \t Loss: 1.9574368000030518\n",
      "pred_error: 0.14063259959220886 \t range_loss: 0.5562165379524231\n",
      "pred_error: 0.14021682739257812 \t range_loss: 0.5553489923477173\n",
      "pred_error: 0.14064903557300568 \t range_loss: 0.5590682625770569\n",
      "pred_error: 0.14064903557300568 \t range_loss: 0.5590682625770569\n",
      "pred_error: 0.14064927399158478 \t range_loss: 0.5590682625770569\n",
      "Step: 591 \t Loss: 1.9568244218826294\n",
      "pred_error: 0.1401127427816391 \t range_loss: 0.5556968450546265\n",
      "pred_error: 0.1397889107465744 \t range_loss: 0.5653671026229858\n",
      "pred_error: 0.1400362104177475 \t range_loss: 0.5604225993156433\n",
      "pred_error: 0.14068332314491272 \t range_loss: 0.556083619594574\n",
      "pred_error: 0.14013555645942688 \t range_loss: 0.5562163591384888\n",
      "pred_error: 0.14013560116291046 \t range_loss: 0.5562163591384888\n",
      "pred_error: 0.14073379337787628 \t range_loss: 0.5567101240158081\n",
      "pred_error: 0.13995787501335144 \t range_loss: 0.5639970898628235\n",
      "pred_error: 0.14074572920799255 \t range_loss: 0.5581598877906799\n",
      "pred_error: 0.1402602642774582 \t range_loss: 0.5547574758529663\n",
      "pred_error: 0.14026041328907013 \t range_loss: 0.5547574758529663\n",
      "pred_error: 0.14026054739952087 \t range_loss: 0.5547574758529663\n",
      "pred_error: 0.14068059623241425 \t range_loss: 0.5550029277801514\n",
      "pred_error: 0.1406802088022232 \t range_loss: 0.5550029277801514\n",
      "pred_error: 0.1402541548013687 \t range_loss: 0.5547969937324524\n",
      "pred_error: 0.14074359834194183 \t range_loss: 0.5571224689483643\n",
      "pred_error: 0.14029328525066376 \t range_loss: 0.5571697950363159\n",
      "pred_error: 0.1405794322490692 \t range_loss: 0.555126965045929\n",
      "pred_error: 0.14076273143291473 \t range_loss: 0.5580288171768188\n",
      "Step: 624 \t Loss: 1.9560606479644775\n",
      "pred_error: 0.14015993475914001 \t range_loss: 0.5544623732566833\n",
      "pred_error: 0.14015977084636688 \t range_loss: 0.5544623732566833\n",
      "pred_error: 0.1400579810142517 \t range_loss: 0.5570284724235535\n",
      "Step: 629 \t Loss: 1.955909252166748\n",
      "pred_error: 0.1404687613248825 \t range_loss: 0.5582002401351929\n",
      "pred_error: 0.1404685378074646 \t range_loss: 0.5582002401351929\n",
      "pred_error: 0.14023491740226746 \t range_loss: 0.5633172988891602\n",
      "pred_error: 0.14074887335300446 \t range_loss: 0.5533064603805542\n",
      "pred_error: 0.1407487690448761 \t range_loss: 0.5533064603805542\n",
      "pred_error: 0.1407482922077179 \t range_loss: 0.5533064603805542\n",
      "pred_error: 0.14073196053504944 \t range_loss: 0.5562933087348938\n",
      "pred_error: 0.14073222875595093 \t range_loss: 0.5562933087348938\n",
      "pred_error: 0.14102178812026978 \t range_loss: 0.5553661584854126\n",
      "pred_error: 0.14044815301895142 \t range_loss: 0.5536560416221619\n",
      "pred_error: 0.14044815301895142 \t range_loss: 0.5536560416221619\n",
      "pred_error: 0.14044806361198425 \t range_loss: 0.5536560416221619\n",
      "pred_error: 0.14018815755844116 \t range_loss: 0.5571359992027283\n",
      "pred_error: 0.14090071618556976 \t range_loss: 0.5541196465492249\n",
      "pred_error: 0.14042213559150696 \t range_loss: 0.5545069575309753\n",
      "Step: 652 \t Loss: 1.9556323289871216\n",
      "pred_error: 0.14041964709758759 \t range_loss: 0.5532187819480896\n",
      "pred_error: 0.1399116963148117 \t range_loss: 0.5617818236351013\n",
      "pred_error: 0.1399116963148117 \t range_loss: 0.5617818236351013\n",
      "pred_error: 0.14060601592063904 \t range_loss: 0.5560004711151123\n",
      "pred_error: 0.1406879872083664 \t range_loss: 0.5522573590278625\n",
      "pred_error: 0.140687957406044 \t range_loss: 0.5522573590278625\n",
      "pred_error: 0.1402895599603653 \t range_loss: 0.5551321506500244\n",
      "pred_error: 0.1399705410003662 \t range_loss: 0.5633264780044556\n",
      "pred_error: 0.139970600605011 \t range_loss: 0.5633264780044556\n",
      "pred_error: 0.14015305042266846 \t range_loss: 0.5586655139923096\n",
      "pred_error: 0.1401529461145401 \t range_loss: 0.5586655139923096\n",
      "pred_error: 0.1407405287027359 \t range_loss: 0.5545154809951782\n",
      "pred_error: 0.14074073731899261 \t range_loss: 0.5545154809951782\n",
      "pred_error: 0.14075134694576263 \t range_loss: 0.5553162693977356\n",
      "pred_error: 0.1403324007987976 \t range_loss: 0.5564464926719666\n",
      "Step: 678 \t Loss: 1.9545785188674927\n",
      "pred_error: 0.14014849066734314 \t range_loss: 0.5530933141708374\n",
      "pred_error: 0.14044028520584106 \t range_loss: 0.5531067252159119\n",
      "pred_error: 0.14044038951396942 \t range_loss: 0.5531067252159119\n",
      "pred_error: 0.14013035595417023 \t range_loss: 0.5582735538482666\n",
      "pred_error: 0.14013029634952545 \t range_loss: 0.5582735538482666\n",
      "pred_error: 0.1406880021095276 \t range_loss: 0.5573843121528625\n",
      "pred_error: 0.14031288027763367 \t range_loss: 0.5541999340057373\n",
      "pred_error: 0.14011776447296143 \t range_loss: 0.5616378784179688\n",
      "pred_error: 0.14011751115322113 \t range_loss: 0.5616378784179688\n",
      "pred_error: 0.14062662422657013 \t range_loss: 0.5538389682769775\n",
      "pred_error: 0.14062662422657013 \t range_loss: 0.5538389682769775\n",
      "pred_error: 0.13995587825775146 \t range_loss: 0.5636326670646667\n",
      "pred_error: 0.1408059000968933 \t range_loss: 0.5524194836616516\n",
      "pred_error: 0.14022846519947052 \t range_loss: 0.5602660179138184\n",
      "pred_error: 0.1407848596572876 \t range_loss: 0.5523515343666077\n",
      "pred_error: 0.1407848596572876 \t range_loss: 0.5523515343666077\n",
      "pred_error: 0.14027124643325806 \t range_loss: 0.5563820600509644\n",
      "pred_error: 0.14027121663093567 \t range_loss: 0.5563820600509644\n",
      "pred_error: 0.1401091367006302 \t range_loss: 0.5622034668922424\n",
      "pred_error: 0.1406886726617813 \t range_loss: 0.5545219779014587\n",
      "pred_error: 0.1408868134021759 \t range_loss: 0.5529664754867554\n",
      "pred_error: 0.14032776653766632 \t range_loss: 0.5527936816215515\n",
      "pred_error: 0.14064230024814606 \t range_loss: 0.5566533803939819\n",
      "pred_error: 0.14074358344078064 \t range_loss: 0.5537799000740051\n",
      "pred_error: 0.14094719290733337 \t range_loss: 0.5529243350028992\n",
      "pred_error: 0.14094699919223785 \t range_loss: 0.5529243350028992\n",
      "pred_error: 0.140305757522583 \t range_loss: 0.5532814264297485\n",
      "pred_error: 0.1408144235610962 \t range_loss: 0.5522240996360779\n",
      "pred_error: 0.1399320662021637 \t range_loss: 0.5613453984260559\n",
      "pred_error: 0.14024904370307922 \t range_loss: 0.5564448237419128\n",
      "pred_error: 0.140249103307724 \t range_loss: 0.5564448237419128\n",
      "pred_error: 0.1407824009656906 \t range_loss: 0.5525550246238708\n",
      "pred_error: 0.13990896940231323 \t range_loss: 0.5634498000144958\n",
      "pred_error: 0.13990885019302368 \t range_loss: 0.5634498000144958\n",
      "pred_error: 0.13990893959999084 \t range_loss: 0.5634498000144958\n",
      "pred_error: 0.140031099319458 \t range_loss: 0.557244062423706\n",
      "pred_error: 0.1405218243598938 \t range_loss: 0.5529429316520691\n",
      "pred_error: 0.14052176475524902 \t range_loss: 0.5529429316520691\n",
      "pred_error: 0.140684112906456 \t range_loss: 0.5546415448188782\n",
      "pred_error: 0.1404888927936554 \t range_loss: 0.554203450679779\n",
      "pred_error: 0.14065638184547424 \t range_loss: 0.5530076622962952\n",
      "pred_error: 0.14065638184547424 \t range_loss: 0.5530076622962952\n",
      "pred_error: 0.14048582315444946 \t range_loss: 0.5548288226127625\n",
      "Step: 735 \t Loss: 1.9543761014938354\n",
      "pred_error: 0.14017468690872192 \t range_loss: 0.552628755569458\n",
      "pred_error: 0.14006304740905762 \t range_loss: 0.5574753880500793\n",
      "pred_error: 0.14018450677394867 \t range_loss: 0.5534732937812805\n",
      "pred_error: 0.14090120792388916 \t range_loss: 0.5519800782203674\n",
      "pred_error: 0.140912726521492 \t range_loss: 0.5518347024917603\n",
      "pred_error: 0.14028269052505493 \t range_loss: 0.5515936017036438\n",
      "pred_error: 0.14085961878299713 \t range_loss: 0.5529573559761047\n",
      "pred_error: 0.1399393081665039 \t range_loss: 0.5592482686042786\n",
      "pred_error: 0.13993939757347107 \t range_loss: 0.5592482686042786\n",
      "pred_error: 0.14070481061935425 \t range_loss: 0.553977370262146\n",
      "pred_error: 0.14039869606494904 \t range_loss: 0.5543404817581177\n",
      "pred_error: 0.14039859175682068 \t range_loss: 0.5543404817581177\n",
      "pred_error: 0.14084964990615845 \t range_loss: 0.551750123500824\n",
      "pred_error: 0.14084982872009277 \t range_loss: 0.551750123500824\n",
      "pred_error: 0.14028044044971466 \t range_loss: 0.5523263812065125\n",
      "pred_error: 0.14028048515319824 \t range_loss: 0.5523263812065125\n",
      "pred_error: 0.14052000641822815 \t range_loss: 0.5508321523666382\n",
      "pred_error: 0.1405198872089386 \t range_loss: 0.5508321523666382\n",
      "pred_error: 0.14069005846977234 \t range_loss: 0.5512521862983704\n",
      "pred_error: 0.14069008827209473 \t range_loss: 0.5512521862983704\n",
      "pred_error: 0.1403011530637741 \t range_loss: 0.5518323183059692\n",
      "pred_error: 0.14030078053474426 \t range_loss: 0.5518323183059692\n",
      "pred_error: 0.1402546912431717 \t range_loss: 0.552164614200592\n",
      "pred_error: 0.14082071185112 \t range_loss: 0.5528063774108887\n",
      "pred_error: 0.1401427537202835 \t range_loss: 0.5557119846343994\n",
      "pred_error: 0.1407213807106018 \t range_loss: 0.5543825626373291\n",
      "Step: 780 \t Loss: 1.9538679122924805\n",
      "Step: 785 \t Loss: 1.9536508321762085\n",
      "pred_error: 0.14030490815639496 \t range_loss: 0.5505883693695068\n",
      "pred_error: 0.14057859778404236 \t range_loss: 0.5525960922241211\n",
      "pred_error: 0.14057879149913788 \t range_loss: 0.5525960922241211\n",
      "pred_error: 0.14033502340316772 \t range_loss: 0.5583056807518005\n",
      "pred_error: 0.14058350026607513 \t range_loss: 0.551723301410675\n",
      "pred_error: 0.14012758433818817 \t range_loss: 0.560935914516449\n",
      "pred_error: 0.1401846557855606 \t range_loss: 0.5551177263259888\n",
      "pred_error: 0.14033377170562744 \t range_loss: 0.5509242415428162\n",
      "pred_error: 0.14033369719982147 \t range_loss: 0.5509242415428162\n",
      "pred_error: 0.14033357799053192 \t range_loss: 0.5509242415428162\n",
      "BEST LOSS: 1.9536508\n",
      "==== Model: block7_cob_activation_norm  in Layer: 7 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 06:18:22,543 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 06:22:11,785 model.rs:1246 value (-196608) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 06:22:11,793 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 06:22:43,170 model.rs:1246 value (-196608) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 06:22:43,179 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 06:22:43,206 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 06:22:43,231 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 06:22:43,418 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error   | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000007581326 | 0.00006842613 | 0.000642363 | -0.00049084425 | 0.00006235357  | 0.00006842613    | 0.000642363   | 0             | 0.0000000069747355 | -0.0009197648      | 0.0023834805           |\n",
      "+-----------------+---------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 584817 64 [-700576, 718768] 1 [16]\n",
      "===============================\n",
      "==== Model: block7_cob_activation_norm_teleported  in Layer: 7 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 06:23:15,250 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 06:27:11,203 model.rs:1246 value (-195328) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 06:27:11,209 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 06:27:42,179 model.rs:1246 value (-195328) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 06:27:42,185 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 06:27:42,203 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 06:27:42,223 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 06:27:42,236 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+--------------+--------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error    | min_error    | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+--------------+--------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000004070636 | -0.000069737434 | 0.0005187001 | -0.000513494 | 0.00006585663  | 0.000069737434   | 0.0005187001  | 0             | 0.000000007672296  | -0.0013587776      | 0.002706852            |\n",
      "+------------------+-----------------+--------------+--------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 584825 64 [-529318, 345658] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 8 , \t  activation_stats: {'relu_1': {'norm': tensor(1188.2712), 'max': tensor(6.0490), 'min': tensor(-11.4612), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(17.5103)\n",
      "pred_error: 0.16138382256031036 \t range_loss: 0.9883725643157959\n",
      "Step: 0 \t Loss: 2.585822582244873\n",
      "Step: 1 \t Loss: 2.564241886138916\n",
      "Step: 2 \t Loss: 2.5389745235443115\n",
      "Step: 3 \t Loss: 2.522891044616699\n",
      "pred_error: 0.16115903854370117 \t range_loss: 0.9113008975982666\n",
      "Step: 4 \t Loss: 2.5083346366882324\n",
      "pred_error: 0.16109329462051392 \t range_loss: 0.8974074125289917\n",
      "Step: 5 \t Loss: 2.503572463989258\n",
      "pred_error: 0.16094842553138733 \t range_loss: 0.8940881490707397\n",
      "Step: 6 \t Loss: 2.4996590614318848\n",
      "Step: 7 \t Loss: 2.4927926063537598\n",
      "Step: 8 \t Loss: 2.488091230392456\n",
      "pred_error: 0.16084060072898865 \t range_loss: 0.8796855211257935\n",
      "Step: 9 \t Loss: 2.479769229888916\n",
      "Step: 10 \t Loss: 2.4774391651153564\n",
      "pred_error: 0.1606878638267517 \t range_loss: 0.8705564737319946\n",
      "pred_error: 0.16068826615810394 \t range_loss: 0.8705564737319946\n",
      "pred_error: 0.16068889200687408 \t range_loss: 0.8705564737319946\n",
      "Step: 11 \t Loss: 2.4732508659362793\n",
      "Step: 12 \t Loss: 2.469284772872925\n",
      "Step: 13 \t Loss: 2.460040807723999\n",
      "Step: 14 \t Loss: 2.4531712532043457\n",
      "Step: 15 \t Loss: 2.4504480361938477\n",
      "Step: 16 \t Loss: 2.4487743377685547\n",
      "Step: 17 \t Loss: 2.445136070251465\n",
      "Step: 18 \t Loss: 2.439377546310425\n",
      "pred_error: 0.1604728251695633 \t range_loss: 0.8346493244171143\n",
      "pred_error: 0.16047273576259613 \t range_loss: 0.8346493244171143\n",
      "Step: 19 \t Loss: 2.436391830444336\n",
      "pred_error: 0.16040867567062378 \t range_loss: 0.8323051929473877\n",
      "pred_error: 0.1604083925485611 \t range_loss: 0.8323051929473877\n",
      "Step: 20 \t Loss: 2.434157609939575\n",
      "pred_error: 0.16036197543144226 \t range_loss: 0.8305378556251526\n",
      "pred_error: 0.16036194562911987 \t range_loss: 0.8305378556251526\n",
      "Step: 21 \t Loss: 2.4311654567718506\n",
      "Step: 22 \t Loss: 2.4274914264678955\n",
      "Step: 23 \t Loss: 2.42586612701416\n",
      "pred_error: 0.16020749509334564 \t range_loss: 0.8237911462783813\n",
      "Step: 24 \t Loss: 2.424194574356079\n",
      "Step: 25 \t Loss: 2.4223170280456543\n",
      "Step: 26 \t Loss: 2.420579671859741\n",
      "pred_error: 0.1600840985774994 \t range_loss: 0.8197386264801025\n",
      "Step: 27 \t Loss: 2.4188594818115234\n",
      "pred_error: 0.16009172797203064 \t range_loss: 0.8179422616958618\n",
      "pred_error: 0.16009141504764557 \t range_loss: 0.8179422616958618\n",
      "Step: 28 \t Loss: 2.4155704975128174\n",
      "pred_error: 0.16002249717712402 \t range_loss: 0.8153454661369324\n",
      "Step: 29 \t Loss: 2.413374900817871\n",
      "pred_error: 0.15998180210590363 \t range_loss: 0.8135530352592468\n",
      "Step: 30 \t Loss: 2.4097070693969727\n",
      "pred_error: 0.15990251302719116 \t range_loss: 0.8106818199157715\n",
      "Step: 31 \t Loss: 2.4093172550201416\n",
      "Step: 32 \t Loss: 2.408074378967285\n",
      "Step: 33 \t Loss: 2.4059486389160156\n",
      "Step: 34 \t Loss: 2.4045231342315674\n",
      "pred_error: 0.1600155085325241 \t range_loss: 0.8043680787086487\n",
      "pred_error: 0.16001538932323456 \t range_loss: 0.8043680787086487\n",
      "Step: 35 \t Loss: 2.402402400970459\n",
      "Step: 36 \t Loss: 2.3994150161743164\n",
      "Step: 37 \t Loss: 2.397653102874756\n",
      "Step: 38 \t Loss: 2.395749568939209\n",
      "pred_error: 0.15987801551818848 \t range_loss: 0.7969695329666138\n",
      "pred_error: 0.159878209233284 \t range_loss: 0.7969695329666138\n",
      "Step: 39 \t Loss: 2.389958143234253\n",
      "Step: 40 \t Loss: 2.3875651359558105\n",
      "pred_error: 0.15978188812732697 \t range_loss: 0.789746880531311\n",
      "Step: 41 \t Loss: 2.3861536979675293\n",
      "pred_error: 0.15975601971149445 \t range_loss: 0.7885936498641968\n",
      "pred_error: 0.15975552797317505 \t range_loss: 0.7885936498641968\n",
      "pred_error: 0.15975601971149445 \t range_loss: 0.7885936498641968\n",
      "Step: 42 \t Loss: 2.385338068008423\n",
      "pred_error: 0.1597418189048767 \t range_loss: 0.7879198789596558\n",
      "Step: 43 \t Loss: 2.383970260620117\n",
      "Step: 44 \t Loss: 2.382105827331543\n",
      "Step: 45 \t Loss: 2.379554510116577\n",
      "Step: 46 \t Loss: 2.377678871154785\n",
      "pred_error: 0.1596832275390625 \t range_loss: 0.7808465361595154\n",
      "Step: 47 \t Loss: 2.377315044403076\n",
      "pred_error: 0.1597367525100708 \t range_loss: 0.7799437046051025\n",
      "Step: 48 \t Loss: 2.376655101776123\n",
      "Step: 49 \t Loss: 2.376464366912842\n",
      "pred_error: 0.1597450226545334 \t range_loss: 0.7790096998214722\n",
      "pred_error: 0.1597454696893692 \t range_loss: 0.7790096998214722\n",
      "pred_error: 0.1599373072385788 \t range_loss: 0.7780627012252808\n",
      "pred_error: 0.1599370688199997 \t range_loss: 0.7780627012252808\n",
      "pred_error: 0.15993720293045044 \t range_loss: 0.7780627012252808\n",
      "Step: 52 \t Loss: 2.374067544937134\n",
      "pred_error: 0.15986621379852295 \t range_loss: 0.7754059433937073\n",
      "Step: 53 \t Loss: 2.3724961280822754\n",
      "Step: 54 \t Loss: 2.3705506324768066\n",
      "Step: 55 \t Loss: 2.3698792457580566\n",
      "Step: 56 \t Loss: 2.3685178756713867\n",
      "pred_error: 0.15976741909980774 \t range_loss: 0.7708437442779541\n",
      "pred_error: 0.15976758301258087 \t range_loss: 0.7708437442779541\n",
      "Step: 57 \t Loss: 2.367330551147461\n",
      "pred_error: 0.15970037877559662 \t range_loss: 0.770329475402832\n",
      "Step: 58 \t Loss: 2.3669352531433105\n",
      "pred_error: 0.15975886583328247 \t range_loss: 0.7693465352058411\n",
      "Step: 59 \t Loss: 2.3645806312561035\n",
      "pred_error: 0.15972159802913666 \t range_loss: 0.7673677206039429\n",
      "Step: 60 \t Loss: 2.363028049468994\n",
      "pred_error: 0.15967829525470734 \t range_loss: 0.7662452459335327\n",
      "Step: 61 \t Loss: 2.3617358207702637\n",
      "pred_error: 0.1596357375383377 \t range_loss: 0.7653785943984985\n",
      "pred_error: 0.1596352756023407 \t range_loss: 0.7653785943984985\n",
      "Step: 62 \t Loss: 2.360154628753662\n",
      "pred_error: 0.15961715579032898 \t range_loss: 0.7639808654785156\n",
      "Step: 63 \t Loss: 2.3589484691619873\n",
      "pred_error: 0.1595730185508728 \t range_loss: 0.7632182836532593\n",
      "Step: 64 \t Loss: 2.358933925628662\n",
      "Step: 65 \t Loss: 2.3570618629455566\n",
      "Step: 66 \t Loss: 2.3563661575317383\n",
      "Step: 68 \t Loss: 2.3551604747772217\n",
      "pred_error: 0.15980061888694763 \t range_loss: 0.7571496963500977\n",
      "Step: 69 \t Loss: 2.3535966873168945\n",
      "Step: 70 \t Loss: 2.35231876373291\n",
      "Step: 71 \t Loss: 2.3507189750671387\n",
      "Step: 72 \t Loss: 2.3490538597106934\n",
      "pred_error: 0.15958018600940704 \t range_loss: 0.7534663081169128\n",
      "Step: 75 \t Loss: 2.3474740982055664\n",
      "pred_error: 0.15968747437000275 \t range_loss: 0.7505994439125061\n",
      "Step: 76 \t Loss: 2.3469207286834717\n",
      "Step: 77 \t Loss: 2.3458690643310547\n",
      "Step: 78 \t Loss: 2.345712184906006\n",
      "Step: 79 \t Loss: 2.3456602096557617\n",
      "Step: 80 \t Loss: 2.343740940093994\n",
      "Step: 81 \t Loss: 2.3427114486694336\n",
      "Step: 82 \t Loss: 2.3418755531311035\n",
      "Step: 84 \t Loss: 2.341433048248291\n",
      "pred_error: 0.15989670157432556 \t range_loss: 0.7424658536911011\n",
      "Step: 85 \t Loss: 2.3405261039733887\n",
      "Step: 86 \t Loss: 2.3403398990631104\n",
      "Step: 87 \t Loss: 2.3397324085235596\n",
      "pred_error: 0.15997610986232758 \t range_loss: 0.7399731278419495\n",
      "Step: 88 \t Loss: 2.338449239730835\n",
      "pred_error: 0.15992651879787445 \t range_loss: 0.7391856908798218\n",
      "pred_error: 0.15992632508277893 \t range_loss: 0.7391856908798218\n",
      "Step: 89 \t Loss: 2.3372716903686523\n",
      "Step: 90 \t Loss: 2.335829496383667\n",
      "Step: 91 \t Loss: 2.334174633026123\n",
      "pred_error: 0.159835085272789 \t range_loss: 0.7358773946762085\n",
      "Step: 95 \t Loss: 2.332259178161621\n",
      "pred_error: 0.16000616550445557 \t range_loss: 0.7321935296058655\n",
      "pred_error: 0.16000667214393616 \t range_loss: 0.7321935296058655\n",
      "Step: 96 \t Loss: 2.3305892944335938\n",
      "Step: 97 \t Loss: 2.329897880554199\n",
      "pred_error: 0.15995432436466217 \t range_loss: 0.7303547859191895\n",
      "Step: 98 \t Loss: 2.3289682865142822\n",
      "pred_error: 0.15992477536201477 \t range_loss: 0.7297174334526062\n",
      "Step: 101 \t Loss: 2.328904867172241\n",
      "pred_error: 0.16014550626277924 \t range_loss: 0.7274498343467712\n",
      "Step: 102 \t Loss: 2.328118324279785\n",
      "Step: 103 \t Loss: 2.3264482021331787\n",
      "Step: 104 \t Loss: 2.3249077796936035\n",
      "pred_error: 0.16004467010498047 \t range_loss: 0.7244656085968018\n",
      "Step: 105 \t Loss: 2.3246448040008545\n",
      "pred_error: 0.15999366343021393 \t range_loss: 0.7247074246406555\n",
      "Step: 106 \t Loss: 2.3238816261291504\n",
      "pred_error: 0.16034862399101257 \t range_loss: 0.7217926383018494\n",
      "pred_error: 0.16033411026000977 \t range_loss: 0.7211773991584778\n",
      "Step: 112 \t Loss: 2.323509454727173\n",
      "pred_error: 0.16032491624355316 \t range_loss: 0.7202579975128174\n",
      "pred_error: 0.16032510995864868 \t range_loss: 0.7202579975128174\n",
      "Step: 113 \t Loss: 2.322368860244751\n",
      "pred_error: 0.16026826202869415 \t range_loss: 0.7196862101554871\n",
      "Step: 115 \t Loss: 2.3216135501861572\n",
      "Step: 116 \t Loss: 2.320343017578125\n",
      "Step: 117 \t Loss: 2.3197073936462402\n",
      "pred_error: 0.16051670908927917 \t range_loss: 0.7151684761047363\n",
      "pred_error: 0.16046561300754547 \t range_loss: 0.7157865762710571\n",
      "Step: 123 \t Loss: 2.3182878494262695\n",
      "pred_error: 0.16050195693969727 \t range_loss: 0.7132694125175476\n",
      "Step: 124 \t Loss: 2.3165578842163086\n",
      "pred_error: 0.1605456918478012 \t range_loss: 0.711841881275177\n",
      "pred_error: 0.1605457365512848 \t range_loss: 0.711841881275177\n",
      "Step: 127 \t Loss: 2.3139243125915527\n",
      "pred_error: 0.16047930717468262 \t range_loss: 0.7091303467750549\n",
      "pred_error: 0.16070429980754852 \t range_loss: 0.7082934975624084\n",
      "pred_error: 0.16068650782108307 \t range_loss: 0.7081301808357239\n",
      "pred_error: 0.16084285080432892 \t range_loss: 0.7068856358528137\n",
      "Step: 132 \t Loss: 2.3134384155273438\n",
      "Step: 133 \t Loss: 2.313357353210449\n",
      "Step: 134 \t Loss: 2.3127408027648926\n",
      "pred_error: 0.16071665287017822 \t range_loss: 0.7055732011795044\n",
      "pred_error: 0.16099680960178375 \t range_loss: 0.7045559883117676\n",
      "Step: 138 \t Loss: 2.3125500679016113\n",
      "pred_error: 0.16096532344818115 \t range_loss: 0.7028967142105103\n",
      "pred_error: 0.16126655042171478 \t range_loss: 0.7013040781021118\n",
      "Step: 143 \t Loss: 2.311408519744873\n",
      "Step: 144 \t Loss: 2.310741901397705\n",
      "Step: 145 \t Loss: 2.3106210231781006\n",
      "Step: 147 \t Loss: 2.309844493865967\n",
      "pred_error: 0.1611647754907608 \t range_loss: 0.6981960535049438\n",
      "Step: 148 \t Loss: 2.309497594833374\n",
      "pred_error: 0.1613207310438156 \t range_loss: 0.697472095489502\n",
      "pred_error: 0.16125088930130005 \t range_loss: 0.6969892978668213\n",
      "Step: 152 \t Loss: 2.3082330226898193\n",
      "pred_error: 0.16125309467315674 \t range_loss: 0.6961166262626648\n",
      "pred_error: 0.16136562824249268 \t range_loss: 0.6954344511032104\n",
      "Step: 156 \t Loss: 2.3074231147766113\n",
      "pred_error: 0.1612810492515564 \t range_loss: 0.6946138143539429\n",
      "pred_error: 0.16142432391643524 \t range_loss: 0.6942875981330872\n",
      "pred_error: 0.16156518459320068 \t range_loss: 0.6944639682769775\n",
      "Step: 165 \t Loss: 2.30674409866333\n",
      "Step: 166 \t Loss: 2.305267333984375\n",
      "pred_error: 0.1615072786808014 \t range_loss: 0.6914142966270447\n",
      "pred_error: 0.16165734827518463 \t range_loss: 0.6907671689987183\n",
      "pred_error: 0.16155456006526947 \t range_loss: 0.6897659301757812\n",
      "Step: 170 \t Loss: 2.304917335510254\n",
      "pred_error: 0.1615668386220932 \t range_loss: 0.6892310380935669\n",
      "Step: 171 \t Loss: 2.3039262294769287\n",
      "Step: 172 \t Loss: 2.3037915229797363\n",
      "pred_error: 0.16151510179042816 \t range_loss: 0.6886395812034607\n",
      "pred_error: 0.16151544451713562 \t range_loss: 0.6886395812034607\n",
      "pred_error: 0.1615152508020401 \t range_loss: 0.6886395812034607\n",
      "pred_error: 0.16166074573993683 \t range_loss: 0.6876654624938965\n",
      "pred_error: 0.16159050166606903 \t range_loss: 0.6879996061325073\n",
      "Step: 176 \t Loss: 2.303453207015991\n",
      "pred_error: 0.1617305725812912 \t range_loss: 0.6865418553352356\n",
      "pred_error: 0.16192661225795746 \t range_loss: 0.6843692064285278\n",
      "Step: 182 \t Loss: 2.3024463653564453\n",
      "Step: 187 \t Loss: 2.3024258613586426\n",
      "Step: 188 \t Loss: 2.3016293048858643\n",
      "pred_error: 0.16197842359542847 \t range_loss: 0.6818450689315796\n",
      "pred_error: 0.16197814047336578 \t range_loss: 0.6818450689315796\n",
      "pred_error: 0.16206245124340057 \t range_loss: 0.6810535788536072\n",
      "Step: 190 \t Loss: 2.3009893894195557\n",
      "Step: 191 \t Loss: 2.30009126663208\n",
      "Step: 192 \t Loss: 2.299325466156006\n",
      "pred_error: 0.16199830174446106 \t range_loss: 0.6793426275253296\n",
      "pred_error: 0.16217894852161407 \t range_loss: 0.6794629096984863\n",
      "pred_error: 0.16236746311187744 \t range_loss: 0.6806895136833191\n",
      "pred_error: 0.16252189874649048 \t range_loss: 0.6790904402732849\n",
      "pred_error: 0.16244754195213318 \t range_loss: 0.6784817576408386\n",
      "pred_error: 0.1624479442834854 \t range_loss: 0.6784817576408386\n",
      "pred_error: 0.16244754195213318 \t range_loss: 0.6784817576408386\n",
      "pred_error: 0.16268780827522278 \t range_loss: 0.6781767010688782\n",
      "pred_error: 0.16268721222877502 \t range_loss: 0.6781767010688782\n",
      "pred_error: 0.16254067420959473 \t range_loss: 0.6775058507919312\n",
      "Step: 206 \t Loss: 2.2990827560424805\n",
      "pred_error: 0.16227486729621887 \t range_loss: 0.6763368248939514\n",
      "pred_error: 0.16227395832538605 \t range_loss: 0.6763368248939514\n",
      "Step: 207 \t Loss: 2.297865152359009\n",
      "pred_error: 0.16220681369304657 \t range_loss: 0.6757949590682983\n",
      "pred_error: 0.16264557838439941 \t range_loss: 0.6760038137435913\n",
      "pred_error: 0.1625417023897171 \t range_loss: 0.6760868430137634\n",
      "pred_error: 0.16253769397735596 \t range_loss: 0.674964189529419\n",
      "pred_error: 0.16265448927879333 \t range_loss: 0.6743888258934021\n",
      "pred_error: 0.16248507797718048 \t range_loss: 0.6745457649230957\n",
      "pred_error: 0.16270779073238373 \t range_loss: 0.6745755672454834\n",
      "pred_error: 0.16270779073238373 \t range_loss: 0.6745755672454834\n",
      "pred_error: 0.1625596433877945 \t range_loss: 0.6734829545021057\n",
      "pred_error: 0.16255956888198853 \t range_loss: 0.6734829545021057\n",
      "Step: 226 \t Loss: 2.2973132133483887\n",
      "pred_error: 0.16251178085803986 \t range_loss: 0.674701452255249\n",
      "pred_error: 0.16269156336784363 \t range_loss: 0.6733155250549316\n",
      "pred_error: 0.16271370649337769 \t range_loss: 0.6720972061157227\n",
      "pred_error: 0.16271363198757172 \t range_loss: 0.6720972061157227\n",
      "Step: 231 \t Loss: 2.2971367835998535\n",
      "pred_error: 0.1625256985425949 \t range_loss: 0.6718826293945312\n",
      "Step: 232 \t Loss: 2.2968626022338867\n",
      "pred_error: 0.16250042617321014 \t range_loss: 0.671862781047821\n",
      "pred_error: 0.1626228243112564 \t range_loss: 0.6720530390739441\n",
      "pred_error: 0.16293826699256897 \t range_loss: 0.6708529591560364\n",
      "Step: 243 \t Loss: 2.2959070205688477\n",
      "pred_error: 0.16265518963336945 \t range_loss: 0.6693552136421204\n",
      "Step: 244 \t Loss: 2.2955217361450195\n",
      "pred_error: 0.16313768923282623 \t range_loss: 0.6685379147529602\n",
      "pred_error: 0.1630743443965912 \t range_loss: 0.6675714254379272\n",
      "pred_error: 0.16307391226291656 \t range_loss: 0.6675714254379272\n",
      "Step: 255 \t Loss: 2.294935464859009\n",
      "Step: 256 \t Loss: 2.294700860977173\n",
      "pred_error: 0.1627592295408249 \t range_loss: 0.6684685349464417\n",
      "Step: 266 \t Loss: 2.2938427925109863\n",
      "Step: 267 \t Loss: 2.2931137084960938\n",
      "pred_error: 0.16268981993198395 \t range_loss: 0.6678377985954285\n",
      "pred_error: 0.16268962621688843 \t range_loss: 0.6678377985954285\n",
      "pred_error: 0.16295550763607025 \t range_loss: 0.6665021777153015\n",
      "pred_error: 0.16281631588935852 \t range_loss: 0.6675495505332947\n",
      "pred_error: 0.1628592312335968 \t range_loss: 0.6667247414588928\n",
      "pred_error: 0.1629115641117096 \t range_loss: 0.6658928990364075\n",
      "pred_error: 0.16292476654052734 \t range_loss: 0.6651385426521301\n",
      "Step: 287 \t Loss: 2.2925162315368652\n",
      "pred_error: 0.16268035769462585 \t range_loss: 0.6657111048698425\n",
      "pred_error: 0.1630629599094391 \t range_loss: 0.6652732491493225\n",
      "pred_error: 0.16306264698505402 \t range_loss: 0.6652732491493225\n",
      "pred_error: 0.1630900800228119 \t range_loss: 0.6647315621376038\n",
      "pred_error: 0.1630900800228119 \t range_loss: 0.6647315621376038\n",
      "pred_error: 0.16295985877513885 \t range_loss: 0.6647442579269409\n",
      "pred_error: 0.16313019394874573 \t range_loss: 0.6642571091651917\n",
      "pred_error: 0.16313019394874573 \t range_loss: 0.6642571091651917\n",
      "pred_error: 0.1633051186800003 \t range_loss: 0.6645106673240662\n",
      "pred_error: 0.16286413371562958 \t range_loss: 0.6640445590019226\n",
      "Step: 302 \t Loss: 2.2920267581939697\n",
      "pred_error: 0.16291065514087677 \t range_loss: 0.6630129218101501\n",
      "Step: 305 \t Loss: 2.2915422916412354\n",
      "pred_error: 0.1631048321723938 \t range_loss: 0.6631123423576355\n",
      "pred_error: 0.1631048321723938 \t range_loss: 0.6631123423576355\n",
      "Step: 313 \t Loss: 2.2912063598632812\n",
      "pred_error: 0.1627652645111084 \t range_loss: 0.6636653542518616\n",
      "Step: 317 \t Loss: 2.291077136993408\n",
      "pred_error: 0.16278311610221863 \t range_loss: 0.6645915508270264\n",
      "Step: 322 \t Loss: 2.290926456451416\n",
      "pred_error: 0.16275948286056519 \t range_loss: 0.6633396744728088\n",
      "pred_error: 0.16320879757404327 \t range_loss: 0.6632915139198303\n",
      "Step: 329 \t Loss: 2.290877342224121\n",
      "pred_error: 0.16298933327198029 \t range_loss: 0.6609843373298645\n",
      "pred_error: 0.16342079639434814 \t range_loss: 0.6609688401222229\n",
      "pred_error: 0.16344258189201355 \t range_loss: 0.6619971394538879\n",
      "pred_error: 0.16317418217658997 \t range_loss: 0.6603825092315674\n",
      "pred_error: 0.1631830483675003 \t range_loss: 0.6593939065933228\n",
      "pred_error: 0.16316133737564087 \t range_loss: 0.659525454044342\n",
      "Step: 339 \t Loss: 2.2902443408966064\n",
      "pred_error: 0.1637365072965622 \t range_loss: 0.6581661105155945\n",
      "pred_error: 0.16373668611049652 \t range_loss: 0.6581661105155945\n",
      "pred_error: 0.16375309228897095 \t range_loss: 0.6579544544219971\n",
      "pred_error: 0.16358564794063568 \t range_loss: 0.6571637392044067\n",
      "pred_error: 0.16365757584571838 \t range_loss: 0.6568912863731384\n",
      "pred_error: 0.16354544460773468 \t range_loss: 0.65654057264328\n",
      "pred_error: 0.16342414915561676 \t range_loss: 0.6562659740447998\n",
      "pred_error: 0.1633329838514328 \t range_loss: 0.6571321487426758\n",
      "pred_error: 0.16354134678840637 \t range_loss: 0.6568583846092224\n",
      "pred_error: 0.1637931615114212 \t range_loss: 0.6565338969230652\n",
      "pred_error: 0.16356155276298523 \t range_loss: 0.6556232571601868\n",
      "Step: 370 \t Loss: 2.289357900619507\n",
      "pred_error: 0.16342692077159882 \t range_loss: 0.6550881862640381\n",
      "pred_error: 0.1634269505739212 \t range_loss: 0.6550881862640381\n",
      "Step: 371 \t Loss: 2.287309408187866\n",
      "pred_error: 0.16325116157531738 \t range_loss: 0.6547959446907043\n",
      "pred_error: 0.1634701043367386 \t range_loss: 0.6553412675857544\n",
      "pred_error: 0.16367028653621674 \t range_loss: 0.6559495329856873\n",
      "pred_error: 0.16354359686374664 \t range_loss: 0.654664158821106\n",
      "pred_error: 0.16341745853424072 \t range_loss: 0.6554854512214661\n",
      "pred_error: 0.16357603669166565 \t range_loss: 0.655513346195221\n",
      "pred_error: 0.16339705884456635 \t range_loss: 0.6556314826011658\n",
      "pred_error: 0.1633344441652298 \t range_loss: 0.6563507914543152\n",
      "pred_error: 0.16333428025245667 \t range_loss: 0.6563507914543152\n",
      "pred_error: 0.16349710524082184 \t range_loss: 0.6551127433776855\n",
      "pred_error: 0.16349956393241882 \t range_loss: 0.6551127433776855\n",
      "pred_error: 0.1634467989206314 \t range_loss: 0.6554193496704102\n",
      "pred_error: 0.16344889998435974 \t range_loss: 0.6560307741165161\n",
      "pred_error: 0.16325025260448456 \t range_loss: 0.6557080149650574\n",
      "pred_error: 0.1634262204170227 \t range_loss: 0.6571314930915833\n",
      "pred_error: 0.16374436020851135 \t range_loss: 0.6557574272155762\n",
      "pred_error: 0.1635981947183609 \t range_loss: 0.6555606126785278\n",
      "pred_error: 0.16356831789016724 \t range_loss: 0.6562293767929077\n",
      "pred_error: 0.1635090857744217 \t range_loss: 0.6547695398330688\n",
      "pred_error: 0.16338323056697845 \t range_loss: 0.6536815166473389\n",
      "pred_error: 0.16374309360980988 \t range_loss: 0.6547108888626099\n",
      "pred_error: 0.16376686096191406 \t range_loss: 0.6548073887825012\n",
      "pred_error: 0.16376490890979767 \t range_loss: 0.6563137769699097\n",
      "pred_error: 0.1636526882648468 \t range_loss: 0.6543066501617432\n",
      "Step: 421 \t Loss: 2.287017583847046\n",
      "pred_error: 0.1634092926979065 \t range_loss: 0.6529257893562317\n",
      "pred_error: 0.16365453600883484 \t range_loss: 0.6519553065299988\n",
      "pred_error: 0.16401341557502747 \t range_loss: 0.6517768502235413\n",
      "pred_error: 0.16401255130767822 \t range_loss: 0.6517768502235413\n",
      "pred_error: 0.16382113099098206 \t range_loss: 0.6516055464744568\n",
      "pred_error: 0.16379037499427795 \t range_loss: 0.6531892418861389\n",
      "pred_error: 0.16381321847438812 \t range_loss: 0.6512098908424377\n",
      "pred_error: 0.16416169703006744 \t range_loss: 0.6515219807624817\n",
      "pred_error: 0.16416212916374207 \t range_loss: 0.6515219807624817\n",
      "pred_error: 0.16405430436134338 \t range_loss: 0.6505540013313293\n",
      "pred_error: 0.16423028707504272 \t range_loss: 0.6501522064208984\n",
      "pred_error: 0.1639316976070404 \t range_loss: 0.6512434482574463\n",
      "pred_error: 0.16406911611557007 \t range_loss: 0.6506655812263489\n",
      "pred_error: 0.1642085760831833 \t range_loss: 0.650252103805542\n",
      "pred_error: 0.1642231047153473 \t range_loss: 0.6493476629257202\n",
      "pred_error: 0.16411051154136658 \t range_loss: 0.6493479013442993\n",
      "pred_error: 0.16411051154136658 \t range_loss: 0.6493479013442993\n",
      "pred_error: 0.1641598492860794 \t range_loss: 0.6499210596084595\n",
      "pred_error: 0.16389955580234528 \t range_loss: 0.6489018201828003\n",
      "pred_error: 0.16433043777942657 \t range_loss: 0.6482264399528503\n",
      "pred_error: 0.1643289476633072 \t range_loss: 0.6482264399528503\n",
      "pred_error: 0.16411088407039642 \t range_loss: 0.648570716381073\n",
      "pred_error: 0.1641118824481964 \t range_loss: 0.648570716381073\n",
      "pred_error: 0.1641942262649536 \t range_loss: 0.648236095905304\n",
      "pred_error: 0.16419406235218048 \t range_loss: 0.648236095905304\n",
      "pred_error: 0.16391247510910034 \t range_loss: 0.6496993899345398\n",
      "pred_error: 0.1639125943183899 \t range_loss: 0.6496993899345398\n",
      "pred_error: 0.16411185264587402 \t range_loss: 0.6492024660110474\n",
      "pred_error: 0.16411152482032776 \t range_loss: 0.6492024660110474\n",
      "pred_error: 0.16411162912845612 \t range_loss: 0.6492024660110474\n",
      "pred_error: 0.164174422621727 \t range_loss: 0.6490203142166138\n",
      "pred_error: 0.16400213539600372 \t range_loss: 0.64976567029953\n",
      "pred_error: 0.1639874428510666 \t range_loss: 0.6479864120483398\n",
      "pred_error: 0.16394281387329102 \t range_loss: 0.6483510732650757\n",
      "pred_error: 0.16394281387329102 \t range_loss: 0.6483510732650757\n",
      "pred_error: 0.163995623588562 \t range_loss: 0.64827561378479\n",
      "pred_error: 0.16395270824432373 \t range_loss: 0.6496787071228027\n",
      "pred_error: 0.16439662873744965 \t range_loss: 0.6482564210891724\n",
      "pred_error: 0.16411817073822021 \t range_loss: 0.6480812430381775\n",
      "pred_error: 0.16403713822364807 \t range_loss: 0.648044228553772\n",
      "pred_error: 0.1638481765985489 \t range_loss: 0.6496080756187439\n",
      "pred_error: 0.1638481467962265 \t range_loss: 0.6496080756187439\n",
      "Step: 503 \t Loss: 2.286943197250366\n",
      "pred_error: 0.16422662138938904 \t range_loss: 0.6488526463508606\n",
      "pred_error: 0.16422662138938904 \t range_loss: 0.6488526463508606\n",
      "pred_error: 0.16411438584327698 \t range_loss: 0.6473396420478821\n",
      "pred_error: 0.1642005443572998 \t range_loss: 0.6471459865570068\n",
      "pred_error: 0.16400878131389618 \t range_loss: 0.6491233706474304\n",
      "pred_error: 0.16381296515464783 \t range_loss: 0.6495469212532043\n",
      "pred_error: 0.1640518456697464 \t range_loss: 0.6481285691261292\n",
      "pred_error: 0.16405175626277924 \t range_loss: 0.6481285691261292\n",
      "pred_error: 0.1639745682477951 \t range_loss: 0.648760199546814\n",
      "pred_error: 0.16397447884082794 \t range_loss: 0.648760199546814\n",
      "pred_error: 0.16397444903850555 \t range_loss: 0.648760199546814\n",
      "Step: 527 \t Loss: 2.286832332611084\n",
      "pred_error: 0.16397862136363983 \t range_loss: 0.6470447778701782\n",
      "pred_error: 0.16397874057292938 \t range_loss: 0.6470447778701782\n",
      "Step: 528 \t Loss: 2.286311149597168\n",
      "pred_error: 0.16413021087646484 \t range_loss: 0.6469250321388245\n",
      "pred_error: 0.16413021087646484 \t range_loss: 0.6469250321388245\n",
      "pred_error: 0.16413022577762604 \t range_loss: 0.6469250321388245\n",
      "pred_error: 0.1643957793712616 \t range_loss: 0.6484841108322144\n",
      "pred_error: 0.1643126755952835 \t range_loss: 0.6470189094543457\n",
      "Step: 536 \t Loss: 2.2862548828125\n",
      "pred_error: 0.16406235098838806 \t range_loss: 0.6456311941146851\n",
      "pred_error: 0.16406238079071045 \t range_loss: 0.6456311941146851\n",
      "pred_error: 0.16406238079071045 \t range_loss: 0.6456311941146851\n",
      "pred_error: 0.16417092084884644 \t range_loss: 0.6451210379600525\n",
      "pred_error: 0.16417084634304047 \t range_loss: 0.6451210379600525\n",
      "pred_error: 0.16417066752910614 \t range_loss: 0.6451210379600525\n",
      "pred_error: 0.1645023673772812 \t range_loss: 0.647054135799408\n",
      "pred_error: 0.1646444946527481 \t range_loss: 0.6462371349334717\n",
      "pred_error: 0.16426457464694977 \t range_loss: 0.6461962461471558\n",
      "pred_error: 0.16426464915275574 \t range_loss: 0.6461962461471558\n",
      "pred_error: 0.16418395936489105 \t range_loss: 0.6463105082511902\n",
      "pred_error: 0.16411493718624115 \t range_loss: 0.6474738717079163\n",
      "pred_error: 0.1643316149711609 \t range_loss: 0.6456422805786133\n",
      "pred_error: 0.16415801644325256 \t range_loss: 0.6457595825195312\n",
      "pred_error: 0.16413339972496033 \t range_loss: 0.6465874314308167\n",
      "pred_error: 0.16413980722427368 \t range_loss: 0.6477799415588379\n",
      "Step: 568 \t Loss: 2.2856764793395996\n",
      "Step: 569 \t Loss: 2.2842466831207275\n",
      "pred_error: 0.16397279500961304 \t range_loss: 0.6445186734199524\n",
      "Step: 570 \t Loss: 2.2837791442871094\n",
      "pred_error: 0.16389133036136627 \t range_loss: 0.6448682546615601\n",
      "pred_error: 0.16402091085910797 \t range_loss: 0.6498557925224304\n",
      "pred_error: 0.1640208512544632 \t range_loss: 0.6498557925224304\n",
      "pred_error: 0.16429904103279114 \t range_loss: 0.6475350856781006\n",
      "pred_error: 0.16404438018798828 \t range_loss: 0.6462743282318115\n",
      "pred_error: 0.16425326466560364 \t range_loss: 0.6460137367248535\n",
      "pred_error: 0.1642533242702484 \t range_loss: 0.6460137367248535\n",
      "pred_error: 0.16426323354244232 \t range_loss: 0.6463494300842285\n",
      "pred_error: 0.16393087804317474 \t range_loss: 0.6471138596534729\n",
      "pred_error: 0.16437090933322906 \t range_loss: 0.6459956169128418\n",
      "pred_error: 0.16437120735645294 \t range_loss: 0.6459956169128418\n",
      "pred_error: 0.164323091506958 \t range_loss: 0.6452702879905701\n",
      "pred_error: 0.16393044590950012 \t range_loss: 0.6465457677841187\n",
      "pred_error: 0.1642119288444519 \t range_loss: 0.6461566686630249\n",
      "pred_error: 0.1643649935722351 \t range_loss: 0.6460389494895935\n",
      "pred_error: 0.16436882317066193 \t range_loss: 0.6453783512115479\n",
      "pred_error: 0.16436879336833954 \t range_loss: 0.6453783512115479\n",
      "pred_error: 0.1642969399690628 \t range_loss: 0.6444089412689209\n",
      "pred_error: 0.1641467809677124 \t range_loss: 0.6449888944625854\n",
      "pred_error: 0.16411736607551575 \t range_loss: 0.6454864740371704\n",
      "pred_error: 0.16420483589172363 \t range_loss: 0.6437627673149109\n",
      "pred_error: 0.16412368416786194 \t range_loss: 0.6457830667495728\n",
      "pred_error: 0.16426829993724823 \t range_loss: 0.6434183716773987\n",
      "pred_error: 0.16426843404769897 \t range_loss: 0.6434183716773987\n",
      "pred_error: 0.16423843801021576 \t range_loss: 0.643269956111908\n",
      "pred_error: 0.16408832371234894 \t range_loss: 0.6432439684867859\n",
      "pred_error: 0.16428442299365997 \t range_loss: 0.6437604427337646\n",
      "pred_error: 0.16428442299365997 \t range_loss: 0.6437604427337646\n",
      "pred_error: 0.16427209973335266 \t range_loss: 0.6445547938346863\n",
      "pred_error: 0.1644655019044876 \t range_loss: 0.6435281038284302\n",
      "pred_error: 0.1642301380634308 \t range_loss: 0.6428861021995544\n",
      "Step: 635 \t Loss: 2.283653736114502\n",
      "pred_error: 0.1640157550573349 \t range_loss: 0.6443309187889099\n",
      "pred_error: 0.1640157550573349 \t range_loss: 0.6443309187889099\n",
      "pred_error: 0.16423524916172028 \t range_loss: 0.6438348889350891\n",
      "pred_error: 0.16423529386520386 \t range_loss: 0.6438348889350891\n",
      "pred_error: 0.16426192224025726 \t range_loss: 0.6440688371658325\n",
      "pred_error: 0.16402344405651093 \t range_loss: 0.6458480954170227\n",
      "pred_error: 0.16402298212051392 \t range_loss: 0.6458480954170227\n",
      "pred_error: 0.16440148651599884 \t range_loss: 0.6446039080619812\n",
      "pred_error: 0.16433629393577576 \t range_loss: 0.6446236968040466\n",
      "pred_error: 0.1643315553665161 \t range_loss: 0.6432631611824036\n",
      "pred_error: 0.16420580446720123 \t range_loss: 0.6426827907562256\n",
      "pred_error: 0.16420580446720123 \t range_loss: 0.6426827907562256\n",
      "pred_error: 0.16404224932193756 \t range_loss: 0.644565224647522\n",
      "pred_error: 0.1640421748161316 \t range_loss: 0.644565224647522\n",
      "pred_error: 0.16404218971729279 \t range_loss: 0.644565224647522\n",
      "pred_error: 0.16436885297298431 \t range_loss: 0.6444405913352966\n",
      "pred_error: 0.1643686592578888 \t range_loss: 0.6444405913352966\n",
      "pred_error: 0.16434769332408905 \t range_loss: 0.6436285376548767\n",
      "pred_error: 0.16434769332408905 \t range_loss: 0.6436285376548767\n",
      "pred_error: 0.163999542593956 \t range_loss: 0.643735408782959\n",
      "Step: 666 \t Loss: 2.2831130027770996\n",
      "pred_error: 0.16411645710468292 \t range_loss: 0.6455831527709961\n",
      "pred_error: 0.16411645710468292 \t range_loss: 0.6455831527709961\n",
      "pred_error: 0.16433119773864746 \t range_loss: 0.6451647281646729\n",
      "pred_error: 0.1642848551273346 \t range_loss: 0.644234836101532\n",
      "pred_error: 0.16428492963314056 \t range_loss: 0.644234836101532\n",
      "pred_error: 0.16412563621997833 \t range_loss: 0.6443997621536255\n",
      "pred_error: 0.16450245678424835 \t range_loss: 0.6458752751350403\n",
      "pred_error: 0.16451267898082733 \t range_loss: 0.6444790959358215\n",
      "pred_error: 0.16435271501541138 \t range_loss: 0.6438859701156616\n",
      "pred_error: 0.16448326408863068 \t range_loss: 0.6431583762168884\n",
      "pred_error: 0.1641218066215515 \t range_loss: 0.6434197425842285\n",
      "pred_error: 0.1640479564666748 \t range_loss: 0.6436318159103394\n",
      "pred_error: 0.16403768956661224 \t range_loss: 0.6435308456420898\n",
      "pred_error: 0.16410599648952484 \t range_loss: 0.6441585421562195\n",
      "pred_error: 0.1643802374601364 \t range_loss: 0.6444919109344482\n",
      "pred_error: 0.16412511467933655 \t range_loss: 0.6423388719558716\n",
      "Step: 700 \t Loss: 2.282223701477051\n",
      "pred_error: 0.16397304832935333 \t range_loss: 0.6424905061721802\n",
      "pred_error: 0.16397325694561005 \t range_loss: 0.6424905061721802\n",
      "Step: 701 \t Loss: 2.2816126346588135\n",
      "pred_error: 0.1639377921819687 \t range_loss: 0.6422350406646729\n",
      "pred_error: 0.16445371508598328 \t range_loss: 0.6436577439308167\n",
      "pred_error: 0.16444233059883118 \t range_loss: 0.6440044641494751\n",
      "pred_error: 0.1643664538860321 \t range_loss: 0.6438941955566406\n",
      "pred_error: 0.1643650084733963 \t range_loss: 0.6421564221382141\n",
      "pred_error: 0.16405177116394043 \t range_loss: 0.6426453590393066\n",
      "pred_error: 0.1644844114780426 \t range_loss: 0.6429492831230164\n",
      "pred_error: 0.16416862607002258 \t range_loss: 0.6427813172340393\n",
      "pred_error: 0.1641939878463745 \t range_loss: 0.6427369713783264\n",
      "pred_error: 0.1641215831041336 \t range_loss: 0.6422670483589172\n",
      "pred_error: 0.16399002075195312 \t range_loss: 0.6419715285301208\n",
      "pred_error: 0.1639213114976883 \t range_loss: 0.6434342265129089\n",
      "pred_error: 0.16392068564891815 \t range_loss: 0.6434342265129089\n",
      "pred_error: 0.16421015560626984 \t range_loss: 0.6454575657844543\n",
      "pred_error: 0.16435222327709198 \t range_loss: 0.6435582637786865\n",
      "pred_error: 0.16435222327709198 \t range_loss: 0.6435582637786865\n",
      "pred_error: 0.16411802172660828 \t range_loss: 0.6429297924041748\n",
      "Step: 736 \t Loss: 2.280855894088745\n",
      "pred_error: 0.16380062699317932 \t range_loss: 0.6428496837615967\n",
      "pred_error: 0.1642703413963318 \t range_loss: 0.6431129574775696\n",
      "pred_error: 0.16434890031814575 \t range_loss: 0.644352376461029\n",
      "pred_error: 0.16444534063339233 \t range_loss: 0.6426348686218262\n",
      "pred_error: 0.16444537043571472 \t range_loss: 0.6426348686218262\n",
      "pred_error: 0.1641615927219391 \t range_loss: 0.6429002285003662\n",
      "pred_error: 0.16448022425174713 \t range_loss: 0.6433115601539612\n",
      "pred_error: 0.16432887315750122 \t range_loss: 0.6428492069244385\n",
      "pred_error: 0.1643795520067215 \t range_loss: 0.6417980790138245\n",
      "pred_error: 0.16436448693275452 \t range_loss: 0.6450565457344055\n",
      "pred_error: 0.16396667063236237 \t range_loss: 0.6424441337585449\n",
      "pred_error: 0.16394375264644623 \t range_loss: 0.6417409181594849\n",
      "pred_error: 0.16394369304180145 \t range_loss: 0.6417409181594849\n",
      "pred_error: 0.16439814865589142 \t range_loss: 0.6429251432418823\n",
      "pred_error: 0.1642737090587616 \t range_loss: 0.6437310576438904\n",
      "pred_error: 0.16424492001533508 \t range_loss: 0.6445987224578857\n",
      "pred_error: 0.1642109453678131 \t range_loss: 0.6441763043403625\n",
      "pred_error: 0.16402293741703033 \t range_loss: 0.6440226435661316\n",
      "pred_error: 0.16402293741703033 \t range_loss: 0.6440226435661316\n",
      "pred_error: 0.16392673552036285 \t range_loss: 0.6434350609779358\n",
      "pred_error: 0.16382645070552826 \t range_loss: 0.6434469819068909\n",
      "Step: 786 \t Loss: 2.280294895172119\n",
      "pred_error: 0.16402487456798553 \t range_loss: 0.6428261399269104\n",
      "pred_error: 0.1639135479927063 \t range_loss: 0.6438952684402466\n",
      "pred_error: 0.16409826278686523 \t range_loss: 0.643617570400238\n",
      "pred_error: 0.16403169929981232 \t range_loss: 0.6445326209068298\n",
      "pred_error: 0.16414760053157806 \t range_loss: 0.6428636908531189\n",
      "BEST LOSS: 2.280295\n",
      "==== Model: block8_cob_activation_norm  in Layer: 8 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 07:20:33,222 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 07:24:48,542 model.rs:1246 value (-97024) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 07:24:48,551 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 07:25:22,717 model.rs:1246 value (-97024) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 07:25:22,723 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 07:25:22,769 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 07:25:22,847 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 07:25:22,864 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error   | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000012372174 | -0.00004041195 | 0.0006183386 | -0.00071543455 | 0.00008166607  | 0.00004041195    | 0.00071543455 | 0             | 0.000000011993131  | 0.00006229167      | 0.0015281404           |\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 547124 64 [-1062178, 613306] 1 [16]\n",
      "===============================\n",
      "==== Model: block8_cob_activation_norm_teleported  in Layer: 8 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 07:26:00,188 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 07:30:25,144 model.rs:1246 value (34048) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 07:30:25,150 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 07:30:59,399 model.rs:1246 value (34048) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 07:30:59,404 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 07:30:59,429 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 07:30:59,452 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 07:30:59,466 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+---------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error  | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+---------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.00000054211625 | 0.00004684925 | 0.0006971359 | -0.0007066671 | 0.00008518935  | 0.00004684925    | 0.0007066671  | 0             | 0.000000013168083  | -0.0004856663      | 0.0019394831           |\n",
      "+------------------+---------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 547130 64 [-734172, 597804] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 9 , \t  activation_stats: {'relu_1': {'norm': tensor(939.6653), 'max': tensor(5.8807), 'min': tensor(-11.0276), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(16.9083)\n",
      "Step: 0 \t Loss: 2.7631068229675293\n",
      "Step: 1 \t Loss: 2.7355902194976807\n",
      "Step: 2 \t Loss: 2.714576482772827\n",
      "Step: 3 \t Loss: 2.7048869132995605\n",
      "pred_error: 0.18013383448123932 \t range_loss: 0.9035484790802002\n",
      "Step: 4 \t Loss: 2.691004991531372\n",
      "Step: 5 \t Loss: 2.6723194122314453\n",
      "Step: 6 \t Loss: 2.668616533279419\n",
      "Step: 7 \t Loss: 2.662386655807495\n",
      "Step: 8 \t Loss: 2.657371997833252\n",
      "Step: 9 \t Loss: 2.6474733352661133\n",
      "Step: 10 \t Loss: 2.6421279907226562\n",
      "pred_error: 0.17998036742210388 \t range_loss: 0.8423241972923279\n",
      "Step: 11 \t Loss: 2.6409249305725098\n",
      "Step: 12 \t Loss: 2.6244683265686035\n",
      "Step: 13 \t Loss: 2.6186842918395996\n",
      "pred_error: 0.17994405329227448 \t range_loss: 0.8192436695098877\n",
      "Step: 14 \t Loss: 2.612316131591797\n",
      "Step: 15 \t Loss: 2.6107003688812256\n",
      "Step: 16 \t Loss: 2.6032028198242188\n",
      "Step: 17 \t Loss: 2.602137327194214\n",
      "pred_error: 0.17997848987579346 \t range_loss: 0.8023493885993958\n",
      "Step: 18 \t Loss: 2.600665807723999\n",
      "pred_error: 0.1799241602420807 \t range_loss: 0.8014243245124817\n",
      "pred_error: 0.17992419004440308 \t range_loss: 0.8014243245124817\n",
      "Step: 19 \t Loss: 2.5960464477539062\n",
      "pred_error: 0.17994694411754608 \t range_loss: 0.7965768575668335\n",
      "pred_error: 0.17994670569896698 \t range_loss: 0.7965768575668335\n",
      "Step: 20 \t Loss: 2.585845947265625\n",
      "pred_error: 0.1800009161233902 \t range_loss: 0.7858413457870483\n",
      "Step: 21 \t Loss: 2.5827059745788574\n",
      "Step: 22 \t Loss: 2.579606056213379\n",
      "Step: 23 \t Loss: 2.5795140266418457\n",
      "Step: 24 \t Loss: 2.577328681945801\n",
      "Step: 25 \t Loss: 2.5730702877044678\n",
      "Step: 26 \t Loss: 2.567352056503296\n",
      "Step: 27 \t Loss: 2.566443920135498\n",
      "pred_error: 0.17972764372825623 \t range_loss: 0.7691675424575806\n",
      "pred_error: 0.17972762882709503 \t range_loss: 0.7691675424575806\n",
      "pred_error: 0.17972764372825623 \t range_loss: 0.7691675424575806\n",
      "Step: 28 \t Loss: 2.5661392211914062\n",
      "Step: 29 \t Loss: 2.563063859939575\n",
      "Step: 30 \t Loss: 2.561189651489258\n",
      "Step: 31 \t Loss: 2.560762405395508\n",
      "pred_error: 0.17982468008995056 \t range_loss: 0.7625153064727783\n",
      "pred_error: 0.17982441186904907 \t range_loss: 0.7625153064727783\n",
      "Step: 32 \t Loss: 2.5587754249572754\n",
      "pred_error: 0.1799110472202301 \t range_loss: 0.7596684098243713\n",
      "Step: 33 \t Loss: 2.5571277141571045\n",
      "Step: 34 \t Loss: 2.5514230728149414\n",
      "Step: 35 \t Loss: 2.549065113067627\n",
      "Step: 36 \t Loss: 2.547457695007324\n",
      "pred_error: 0.17965254187583923 \t range_loss: 0.7509320974349976\n",
      "pred_error: 0.17965254187583923 \t range_loss: 0.7509320974349976\n",
      "Step: 37 \t Loss: 2.5446839332580566\n",
      "pred_error: 0.17975300550460815 \t range_loss: 0.7471538782119751\n",
      "Step: 38 \t Loss: 2.541165351867676\n",
      "pred_error: 0.17965146899223328 \t range_loss: 0.7446501851081848\n",
      "Step: 39 \t Loss: 2.538569450378418\n",
      "Step: 40 \t Loss: 2.538191556930542\n",
      "Step: 41 \t Loss: 2.5370006561279297\n",
      "pred_error: 0.17968927323818207 \t range_loss: 0.7401080131530762\n",
      "pred_error: 0.17968939244747162 \t range_loss: 0.7401080131530762\n",
      "pred_error: 0.17968930304050446 \t range_loss: 0.7401080131530762\n",
      "pred_error: 0.1796894073486328 \t range_loss: 0.7401080131530762\n",
      "Step: 42 \t Loss: 2.534048557281494\n",
      "Step: 43 \t Loss: 2.5322723388671875\n",
      "Step: 44 \t Loss: 2.5315980911254883\n",
      "Step: 46 \t Loss: 2.531517505645752\n",
      "Step: 47 \t Loss: 2.528550386428833\n",
      "Step: 48 \t Loss: 2.526899576187134\n",
      "pred_error: 0.17976653575897217 \t range_loss: 0.7292354702949524\n",
      "Step: 50 \t Loss: 2.523505449295044\n",
      "Step: 51 \t Loss: 2.5204503536224365\n",
      "pred_error: 0.1798042356967926 \t range_loss: 0.7224081158638\n",
      "pred_error: 0.1798042356967926 \t range_loss: 0.7224081158638\n",
      "Step: 52 \t Loss: 2.5183119773864746\n",
      "pred_error: 0.17973051965236664 \t range_loss: 0.7210081219673157\n",
      "pred_error: 0.17988772690296173 \t range_loss: 0.7205774784088135\n",
      "pred_error: 0.17986810207366943 \t range_loss: 0.7197601795196533\n",
      "Step: 55 \t Loss: 2.5161080360412598\n",
      "Step: 56 \t Loss: 2.513414144515991\n",
      "pred_error: 0.17971281707286835 \t range_loss: 0.7162877917289734\n",
      "pred_error: 0.1797124296426773 \t range_loss: 0.7162877917289734\n",
      "pred_error: 0.17974530160427094 \t range_loss: 0.7169239521026611\n",
      "Step: 59 \t Loss: 2.513094425201416\n",
      "Step: 60 \t Loss: 2.5121641159057617\n",
      "Step: 61 \t Loss: 2.5121216773986816\n",
      "pred_error: 0.18001055717468262 \t range_loss: 0.712016224861145\n",
      "pred_error: 0.18001055717468262 \t range_loss: 0.712016224861145\n",
      "Step: 62 \t Loss: 2.511869192123413\n",
      "pred_error: 0.18013723194599152 \t range_loss: 0.7104988098144531\n",
      "Step: 63 \t Loss: 2.509657859802246\n",
      "Step: 64 \t Loss: 2.507319450378418\n",
      "Step: 65 \t Loss: 2.5058865547180176\n",
      "pred_error: 0.17989832162857056 \t range_loss: 0.7074075937271118\n",
      "pred_error: 0.18009015917778015 \t range_loss: 0.7051947712898254\n",
      "Step: 69 \t Loss: 2.503795623779297\n",
      "pred_error: 0.17999285459518433 \t range_loss: 0.7038639783859253\n",
      "Step: 70 \t Loss: 2.5009608268737793\n",
      "Step: 71 \t Loss: 2.49989652633667\n",
      "pred_error: 0.18012280762195587 \t range_loss: 0.7004826068878174\n",
      "Step: 73 \t Loss: 2.4990391731262207\n",
      "Step: 74 \t Loss: 2.49763560295105\n",
      "Step: 75 \t Loss: 2.495547294616699\n",
      "pred_error: 0.17986419796943665 \t range_loss: 0.696905255317688\n",
      "pred_error: 0.17986419796943665 \t range_loss: 0.696905255317688\n",
      "pred_error: 0.17986419796943665 \t range_loss: 0.696905255317688\n",
      "Step: 77 \t Loss: 2.4954304695129395\n",
      "Step: 78 \t Loss: 2.493295192718506\n",
      "pred_error: 0.17995585501194 \t range_loss: 0.6937363147735596\n",
      "Step: 79 \t Loss: 2.4930460453033447\n",
      "pred_error: 0.18029434978961945 \t range_loss: 0.6919299960136414\n",
      "pred_error: 0.18029382824897766 \t range_loss: 0.6919299960136414\n",
      "Step: 81 \t Loss: 2.492318630218506\n",
      "pred_error: 0.18019017577171326 \t range_loss: 0.6904178857803345\n",
      "pred_error: 0.1801900863647461 \t range_loss: 0.6904178857803345\n",
      "Step: 82 \t Loss: 2.492159605026245\n",
      "pred_error: 0.18010154366493225 \t range_loss: 0.6911463141441345\n",
      "Step: 83 \t Loss: 2.491961717605591\n",
      "Step: 84 \t Loss: 2.491525650024414\n",
      "pred_error: 0.18016719818115234 \t range_loss: 0.6898535490036011\n",
      "pred_error: 0.18016694486141205 \t range_loss: 0.6898535490036011\n",
      "Step: 85 \t Loss: 2.4902656078338623\n",
      "pred_error: 0.1802893877029419 \t range_loss: 0.6873695254325867\n",
      "Step: 86 \t Loss: 2.487858295440674\n",
      "Step: 87 \t Loss: 2.4856653213500977\n",
      "Step: 88 \t Loss: 2.4853627681732178\n",
      "Step: 90 \t Loss: 2.484670877456665\n",
      "Step: 91 \t Loss: 2.482697010040283\n",
      "pred_error: 0.18006445467472076 \t range_loss: 0.6820525527000427\n",
      "pred_error: 0.17995336651802063 \t range_loss: 0.683704137802124\n",
      "pred_error: 0.18027015030384064 \t range_loss: 0.6808220148086548\n",
      "Step: 100 \t Loss: 2.4816768169403076\n",
      "pred_error: 0.18068675696849823 \t range_loss: 0.6760799884796143\n",
      "Step: 102 \t Loss: 2.4809274673461914\n",
      "pred_error: 0.18058395385742188 \t range_loss: 0.6750820875167847\n",
      "pred_error: 0.18058481812477112 \t range_loss: 0.6750820875167847\n",
      "Step: 103 \t Loss: 2.4788897037506104\n",
      "Step: 104 \t Loss: 2.4777212142944336\n",
      "pred_error: 0.1803397536277771 \t range_loss: 0.6743231415748596\n",
      "pred_error: 0.18045854568481445 \t range_loss: 0.6746100783348083\n",
      "pred_error: 0.18045854568481445 \t range_loss: 0.6746100783348083\n",
      "Step: 107 \t Loss: 2.47670841217041\n",
      "pred_error: 0.18047019839286804 \t range_loss: 0.6720021963119507\n",
      "Step: 110 \t Loss: 2.474757432937622\n",
      "pred_error: 0.18058882653713226 \t range_loss: 0.6688668727874756\n",
      "Step: 111 \t Loss: 2.473144292831421\n",
      "pred_error: 0.1804826557636261 \t range_loss: 0.6683178544044495\n",
      "pred_error: 0.1804826557636261 \t range_loss: 0.6683178544044495\n",
      "pred_error: 0.18057432770729065 \t range_loss: 0.6691071391105652\n",
      "pred_error: 0.18069930374622345 \t range_loss: 0.6672706604003906\n",
      "Step: 114 \t Loss: 2.472337245941162\n",
      "pred_error: 0.18046315014362335 \t range_loss: 0.6678727269172668\n",
      "pred_error: 0.18046294152736664 \t range_loss: 0.6678727269172668\n",
      "pred_error: 0.18062493205070496 \t range_loss: 0.6672118902206421\n",
      "pred_error: 0.18097315728664398 \t range_loss: 0.6645563840866089\n",
      "Step: 122 \t Loss: 2.4705944061279297\n",
      "Step: 123 \t Loss: 2.4690096378326416\n",
      "pred_error: 0.1808442771434784 \t range_loss: 0.6608065962791443\n",
      "Step: 126 \t Loss: 2.468205213546753\n",
      "pred_error: 0.18079537153244019 \t range_loss: 0.6602514982223511\n",
      "pred_error: 0.18111462891101837 \t range_loss: 0.6598308086395264\n",
      "pred_error: 0.18091197311878204 \t range_loss: 0.6619190573692322\n",
      "Step: 135 \t Loss: 2.467273712158203\n",
      "pred_error: 0.18107329308986664 \t range_loss: 0.6565406322479248\n",
      "pred_error: 0.18107326328754425 \t range_loss: 0.6565406322479248\n",
      "Step: 137 \t Loss: 2.4656224250793457\n",
      "Step: 138 \t Loss: 2.4640305042266846\n",
      "Step: 141 \t Loss: 2.4634852409362793\n",
      "Step: 142 \t Loss: 2.4632740020751953\n",
      "pred_error: 0.18126846849918365 \t range_loss: 0.6512826085090637\n",
      "pred_error: 0.1811881959438324 \t range_loss: 0.6514679789543152\n",
      "pred_error: 0.18131764233112335 \t range_loss: 0.6507593989372253\n",
      "Step: 147 \t Loss: 2.4630281925201416\n",
      "pred_error: 0.18144354224205017 \t range_loss: 0.6485925316810608\n",
      "pred_error: 0.18128837645053864 \t range_loss: 0.6508335471153259\n",
      "pred_error: 0.1816176176071167 \t range_loss: 0.6497533917427063\n",
      "pred_error: 0.18167254328727722 \t range_loss: 0.6480861902236938\n",
      "pred_error: 0.18167288601398468 \t range_loss: 0.6480861902236938\n",
      "pred_error: 0.1818077117204666 \t range_loss: 0.6472857594490051\n",
      "Step: 153 \t Loss: 2.4627606868743896\n",
      "pred_error: 0.18162906169891357 \t range_loss: 0.6464700698852539\n",
      "Step: 154 \t Loss: 2.460184097290039\n",
      "Step: 156 \t Loss: 2.458933115005493\n",
      "pred_error: 0.1817920058965683 \t range_loss: 0.6442049741744995\n",
      "pred_error: 0.18178211152553558 \t range_loss: 0.6444496512413025\n",
      "pred_error: 0.18190684914588928 \t range_loss: 0.6424112915992737\n",
      "pred_error: 0.181758850812912 \t range_loss: 0.6416253447532654\n",
      "Step: 165 \t Loss: 2.457573890686035\n",
      "pred_error: 0.1817365437746048 \t range_loss: 0.6425647735595703\n",
      "pred_error: 0.1819588840007782 \t range_loss: 0.6433412432670593\n",
      "pred_error: 0.18221381306648254 \t range_loss: 0.6411711573600769\n",
      "pred_error: 0.18188586831092834 \t range_loss: 0.6393477916717529\n",
      "Step: 174 \t Loss: 2.4570701122283936\n",
      "Step: 175 \t Loss: 2.4556939601898193\n",
      "pred_error: 0.18171438574790955 \t range_loss: 0.638552188873291\n",
      "pred_error: 0.18181052803993225 \t range_loss: 0.6377196311950684\n",
      "pred_error: 0.18183234333992004 \t range_loss: 0.6407884955406189\n",
      "pred_error: 0.1822427660226822 \t range_loss: 0.6378253102302551\n",
      "pred_error: 0.18220816552639008 \t range_loss: 0.6356273293495178\n",
      "pred_error: 0.18201053142547607 \t range_loss: 0.6372962594032288\n",
      "pred_error: 0.18206270039081573 \t range_loss: 0.6355326771736145\n",
      "pred_error: 0.18199455738067627 \t range_loss: 0.6374630928039551\n",
      "Step: 193 \t Loss: 2.454908847808838\n",
      "pred_error: 0.18207168579101562 \t range_loss: 0.6341925263404846\n",
      "Step: 195 \t Loss: 2.454878568649292\n",
      "Step: 196 \t Loss: 2.4544167518615723\n",
      "pred_error: 0.18244454264640808 \t range_loss: 0.6327794194221497\n",
      "pred_error: 0.1824428290128708 \t range_loss: 0.6327794194221497\n",
      "Step: 204 \t Loss: 2.452540874481201\n",
      "pred_error: 0.18203043937683105 \t range_loss: 0.6322013139724731\n",
      "pred_error: 0.18215833604335785 \t range_loss: 0.631664514541626\n",
      "Step: 206 \t Loss: 2.4521303176879883\n",
      "pred_error: 0.18201616406440735 \t range_loss: 0.6319723129272461\n",
      "pred_error: 0.1821761429309845 \t range_loss: 0.6321990489959717\n",
      "pred_error: 0.18217548727989197 \t range_loss: 0.6321990489959717\n",
      "pred_error: 0.18229004740715027 \t range_loss: 0.630996584892273\n",
      "pred_error: 0.18218043446540833 \t range_loss: 0.6324847936630249\n",
      "pred_error: 0.1823541671037674 \t range_loss: 0.6308108568191528\n",
      "Step: 216 \t Loss: 2.450727939605713\n",
      "pred_error: 0.18231408298015594 \t range_loss: 0.6277438402175903\n",
      "pred_error: 0.18231438100337982 \t range_loss: 0.6277438402175903\n",
      "Step: 219 \t Loss: 2.4486026763916016\n",
      "pred_error: 0.18202368915081024 \t range_loss: 0.62851881980896\n",
      "pred_error: 0.18237535655498505 \t range_loss: 0.6291382312774658\n",
      "pred_error: 0.18237517774105072 \t range_loss: 0.6291382312774658\n",
      "pred_error: 0.1826632171869278 \t range_loss: 0.627894401550293\n",
      "pred_error: 0.18266341090202332 \t range_loss: 0.627894401550293\n",
      "pred_error: 0.18266341090202332 \t range_loss: 0.627894401550293\n",
      "pred_error: 0.18224823474884033 \t range_loss: 0.6294947862625122\n",
      "pred_error: 0.18245697021484375 \t range_loss: 0.6274934411048889\n",
      "pred_error: 0.18253223598003387 \t range_loss: 0.6273178458213806\n",
      "pred_error: 0.18241123855113983 \t range_loss: 0.6274972558021545\n",
      "pred_error: 0.18266873061656952 \t range_loss: 0.6266195178031921\n",
      "pred_error: 0.18232442438602448 \t range_loss: 0.6275769472122192\n",
      "pred_error: 0.18273597955703735 \t range_loss: 0.6258370876312256\n",
      "pred_error: 0.1825820803642273 \t range_loss: 0.6254484057426453\n",
      "pred_error: 0.18244247138500214 \t range_loss: 0.6278908252716064\n",
      "Step: 240 \t Loss: 2.4481663703918457\n",
      "pred_error: 0.1824313998222351 \t range_loss: 0.6238522529602051\n",
      "pred_error: 0.18243125081062317 \t range_loss: 0.6238522529602051\n",
      "Step: 241 \t Loss: 2.4477314949035645\n",
      "pred_error: 0.18268364667892456 \t range_loss: 0.6234908699989319\n",
      "pred_error: 0.1825065314769745 \t range_loss: 0.6235461235046387\n",
      "pred_error: 0.18242011964321136 \t range_loss: 0.6262806057929993\n",
      "pred_error: 0.18279701471328735 \t range_loss: 0.6231773495674133\n",
      "pred_error: 0.18279701471328735 \t range_loss: 0.6231773495674133\n",
      "pred_error: 0.1827428638935089 \t range_loss: 0.623704731464386\n",
      "pred_error: 0.18258924782276154 \t range_loss: 0.6238887310028076\n",
      "pred_error: 0.18258947134017944 \t range_loss: 0.6238887310028076\n",
      "pred_error: 0.18275286257266998 \t range_loss: 0.6222062706947327\n",
      "pred_error: 0.1829567551612854 \t range_loss: 0.6205729246139526\n",
      "pred_error: 0.1827685534954071 \t range_loss: 0.620262086391449\n",
      "Step: 264 \t Loss: 2.4475696086883545\n",
      "pred_error: 0.18265452980995178 \t range_loss: 0.6210243701934814\n",
      "Step: 265 \t Loss: 2.4475653171539307\n",
      "pred_error: 0.18281595408916473 \t range_loss: 0.6194058060646057\n",
      "pred_error: 0.1828157603740692 \t range_loss: 0.6194058060646057\n",
      "Step: 267 \t Loss: 2.446798324584961\n",
      "pred_error: 0.18287070095539093 \t range_loss: 0.6180892586708069\n",
      "pred_error: 0.18287131190299988 \t range_loss: 0.6180892586708069\n",
      "Step: 268 \t Loss: 2.444916248321533\n",
      "pred_error: 0.1830071061849594 \t range_loss: 0.6199450492858887\n",
      "pred_error: 0.18296195566654205 \t range_loss: 0.6184775233268738\n",
      "pred_error: 0.1828545182943344 \t range_loss: 0.6194708943367004\n",
      "pred_error: 0.18285465240478516 \t range_loss: 0.6194708943367004\n",
      "pred_error: 0.18305230140686035 \t range_loss: 0.6199556589126587\n",
      "pred_error: 0.18315857648849487 \t range_loss: 0.6177201867103577\n",
      "pred_error: 0.18296028673648834 \t range_loss: 0.6173387169837952\n",
      "pred_error: 0.18301089107990265 \t range_loss: 0.6178019642829895\n",
      "pred_error: 0.18295611441135406 \t range_loss: 0.6192652583122253\n",
      "Step: 284 \t Loss: 2.4438109397888184\n",
      "pred_error: 0.18280892074108124 \t range_loss: 0.6157217025756836\n",
      "Step: 285 \t Loss: 2.4437336921691895\n",
      "pred_error: 0.1830180585384369 \t range_loss: 0.6158758997917175\n",
      "pred_error: 0.18282675743103027 \t range_loss: 0.6165950298309326\n",
      "pred_error: 0.18282631039619446 \t range_loss: 0.6165950298309326\n",
      "pred_error: 0.18285152316093445 \t range_loss: 0.6179717183113098\n",
      "pred_error: 0.18285158276557922 \t range_loss: 0.6179717183113098\n",
      "pred_error: 0.18314515054225922 \t range_loss: 0.6165697574615479\n",
      "pred_error: 0.18300217390060425 \t range_loss: 0.6178158521652222\n",
      "pred_error: 0.18313518166542053 \t range_loss: 0.6179543137550354\n",
      "pred_error: 0.18319009244441986 \t range_loss: 0.6169179677963257\n",
      "pred_error: 0.18308596312999725 \t range_loss: 0.6160863041877747\n",
      "pred_error: 0.18308527767658234 \t range_loss: 0.6160863041877747\n",
      "pred_error: 0.18289874494075775 \t range_loss: 0.6161508560180664\n",
      "pred_error: 0.18306440114974976 \t range_loss: 0.617315948009491\n",
      "pred_error: 0.18297338485717773 \t range_loss: 0.616260826587677\n",
      "pred_error: 0.18297359347343445 \t range_loss: 0.616260826587677\n",
      "pred_error: 0.18311014771461487 \t range_loss: 0.617519736289978\n",
      "pred_error: 0.18311014771461487 \t range_loss: 0.617519736289978\n",
      "pred_error: 0.1830156296491623 \t range_loss: 0.6147655844688416\n",
      "Step: 311 \t Loss: 2.4429919719696045\n",
      "pred_error: 0.18286728858947754 \t range_loss: 0.6143178343772888\n",
      "pred_error: 0.18314294517040253 \t range_loss: 0.6132490038871765\n",
      "pred_error: 0.18314294517040253 \t range_loss: 0.6132490038871765\n",
      "pred_error: 0.18299618363380432 \t range_loss: 0.6137798428535461\n",
      "pred_error: 0.18299318850040436 \t range_loss: 0.6137798428535461\n",
      "pred_error: 0.18299613893032074 \t range_loss: 0.6137798428535461\n",
      "pred_error: 0.18309983611106873 \t range_loss: 0.6129047274589539\n",
      "pred_error: 0.18356385827064514 \t range_loss: 0.6128368377685547\n",
      "pred_error: 0.18356332182884216 \t range_loss: 0.6128368377685547\n",
      "pred_error: 0.18333053588867188 \t range_loss: 0.6124299168586731\n",
      "pred_error: 0.18333058059215546 \t range_loss: 0.6124299168586731\n",
      "pred_error: 0.18318915367126465 \t range_loss: 0.612154483795166\n",
      "pred_error: 0.18319573998451233 \t range_loss: 0.6116594672203064\n",
      "pred_error: 0.18319584429264069 \t range_loss: 0.6116594672203064\n",
      "pred_error: 0.18335746228694916 \t range_loss: 0.6119170188903809\n",
      "pred_error: 0.1832793802022934 \t range_loss: 0.6140140295028687\n",
      "pred_error: 0.18347960710525513 \t range_loss: 0.6123420596122742\n",
      "pred_error: 0.1834263950586319 \t range_loss: 0.6117308735847473\n",
      "pred_error: 0.18326139450073242 \t range_loss: 0.6138104796409607\n",
      "pred_error: 0.18318046629428864 \t range_loss: 0.6122013926506042\n",
      "pred_error: 0.18324077129364014 \t range_loss: 0.6113783121109009\n",
      "Step: 343 \t Loss: 2.4412779808044434\n",
      "pred_error: 0.1834515631198883 \t range_loss: 0.6108829975128174\n",
      "pred_error: 0.18353471159934998 \t range_loss: 0.6106172800064087\n",
      "pred_error: 0.18353506922721863 \t range_loss: 0.6106172800064087\n",
      "pred_error: 0.18353404104709625 \t range_loss: 0.6106172800064087\n",
      "pred_error: 0.1833915114402771 \t range_loss: 0.6093536615371704\n",
      "Step: 350 \t Loss: 2.4406540393829346\n",
      "pred_error: 0.18320101499557495 \t range_loss: 0.6086439490318298\n",
      "pred_error: 0.18320101499557495 \t range_loss: 0.6086439490318298\n",
      "pred_error: 0.1833799183368683 \t range_loss: 0.6098653674125671\n",
      "pred_error: 0.18342827260494232 \t range_loss: 0.6098565459251404\n",
      "pred_error: 0.1835649013519287 \t range_loss: 0.6110379099845886\n",
      "pred_error: 0.18360672891139984 \t range_loss: 0.6116337180137634\n",
      "pred_error: 0.1836213618516922 \t range_loss: 0.6136071681976318\n",
      "pred_error: 0.18336918950080872 \t range_loss: 0.6096723079681396\n",
      "pred_error: 0.18350917100906372 \t range_loss: 0.6098451018333435\n",
      "pred_error: 0.18310561776161194 \t range_loss: 0.6101774573326111\n",
      "pred_error: 0.18349801003932953 \t range_loss: 0.6098103523254395\n",
      "pred_error: 0.18357279896736145 \t range_loss: 0.6115320920944214\n",
      "pred_error: 0.1836172342300415 \t range_loss: 0.6085137724876404\n",
      "pred_error: 0.18361714482307434 \t range_loss: 0.6085137724876404\n",
      "pred_error: 0.18334628641605377 \t range_loss: 0.6080291271209717\n",
      "pred_error: 0.18339897692203522 \t range_loss: 0.6079185009002686\n",
      "pred_error: 0.18328315019607544 \t range_loss: 0.6080684661865234\n",
      "pred_error: 0.18373702466487885 \t range_loss: 0.606013298034668\n",
      "pred_error: 0.18396402895450592 \t range_loss: 0.6055976152420044\n",
      "pred_error: 0.18367330729961395 \t range_loss: 0.6069815158843994\n",
      "pred_error: 0.18367604911327362 \t range_loss: 0.6063037514686584\n",
      "pred_error: 0.18367600440979004 \t range_loss: 0.6063037514686584\n",
      "pred_error: 0.1839505434036255 \t range_loss: 0.6062019467353821\n",
      "pred_error: 0.18395014107227325 \t range_loss: 0.6062019467353821\n",
      "pred_error: 0.1839505434036255 \t range_loss: 0.6062019467353821\n",
      "pred_error: 0.18379807472229004 \t range_loss: 0.6052486300468445\n",
      "pred_error: 0.1836085468530655 \t range_loss: 0.605741560459137\n",
      "pred_error: 0.18388082087039948 \t range_loss: 0.6059595346450806\n",
      "pred_error: 0.18371401727199554 \t range_loss: 0.6069201827049255\n",
      "pred_error: 0.1838037222623825 \t range_loss: 0.6066728830337524\n",
      "pred_error: 0.18380318582057953 \t range_loss: 0.6066728830337524\n",
      "Step: 404 \t Loss: 2.439591884613037\n",
      "pred_error: 0.1837662011384964 \t range_loss: 0.6055909395217896\n",
      "pred_error: 0.18408413231372833 \t range_loss: 0.6054116487503052\n",
      "pred_error: 0.1840394139289856 \t range_loss: 0.6040706038475037\n",
      "pred_error: 0.18370968103408813 \t range_loss: 0.6034271717071533\n",
      "pred_error: 0.18393062055110931 \t range_loss: 0.6040746569633484\n",
      "pred_error: 0.18372300267219543 \t range_loss: 0.6039254665374756\n",
      "pred_error: 0.1837230920791626 \t range_loss: 0.6039254665374756\n",
      "pred_error: 0.18415410816669464 \t range_loss: 0.6025317311286926\n",
      "pred_error: 0.1838994175195694 \t range_loss: 0.6059075593948364\n",
      "pred_error: 0.18375414609909058 \t range_loss: 0.604979932308197\n",
      "Step: 437 \t Loss: 2.4386544227600098\n",
      "pred_error: 0.18356288969516754 \t range_loss: 0.603027880191803\n",
      "pred_error: 0.1834651082754135 \t range_loss: 0.6059454679489136\n",
      "pred_error: 0.1834651380777359 \t range_loss: 0.6059454679489136\n",
      "pred_error: 0.18387256562709808 \t range_loss: 0.6035643815994263\n",
      "pred_error: 0.18372109532356262 \t range_loss: 0.6053128242492676\n",
      "pred_error: 0.1837482750415802 \t range_loss: 0.6026666164398193\n",
      "Step: 454 \t Loss: 2.4384050369262695\n",
      "Step: 455 \t Loss: 2.4374873638153076\n",
      "pred_error: 0.1835525780916214 \t range_loss: 0.6042298078536987\n",
      "pred_error: 0.1836928278207779 \t range_loss: 0.6053489446640015\n",
      "pred_error: 0.1839267611503601 \t range_loss: 0.6039806008338928\n",
      "pred_error: 0.18384888768196106 \t range_loss: 0.6032349467277527\n",
      "pred_error: 0.1835908591747284 \t range_loss: 0.6043344140052795\n",
      "pred_error: 0.18383339047431946 \t range_loss: 0.602479100227356\n",
      "pred_error: 0.18370653688907623 \t range_loss: 0.6012267470359802\n",
      "pred_error: 0.18370597064495087 \t range_loss: 0.6012267470359802\n",
      "pred_error: 0.18415723741054535 \t range_loss: 0.6013703346252441\n",
      "pred_error: 0.1839209496974945 \t range_loss: 0.6009606122970581\n",
      "pred_error: 0.18402764201164246 \t range_loss: 0.6024910807609558\n",
      "pred_error: 0.1842087209224701 \t range_loss: 0.6000458598136902\n",
      "pred_error: 0.18399052321910858 \t range_loss: 0.600362241268158\n",
      "pred_error: 0.18408706784248352 \t range_loss: 0.6004021167755127\n",
      "pred_error: 0.1841086894273758 \t range_loss: 0.5994310975074768\n",
      "pred_error: 0.1841486543416977 \t range_loss: 0.5995275974273682\n",
      "pred_error: 0.18414857983589172 \t range_loss: 0.5995275974273682\n",
      "pred_error: 0.1841995120048523 \t range_loss: 0.5993137955665588\n",
      "pred_error: 0.1842322200536728 \t range_loss: 0.5997061729431152\n",
      "pred_error: 0.18402236700057983 \t range_loss: 0.5973928570747375\n",
      "pred_error: 0.1842184215784073 \t range_loss: 0.598067581653595\n",
      "pred_error: 0.1842181384563446 \t range_loss: 0.598067581653595\n",
      "pred_error: 0.18415707349777222 \t range_loss: 0.5979284048080444\n",
      "pred_error: 0.18415707349777222 \t range_loss: 0.5979284048080444\n",
      "pred_error: 0.18408548831939697 \t range_loss: 0.5989665389060974\n",
      "pred_error: 0.18419583141803741 \t range_loss: 0.5974141359329224\n",
      "pred_error: 0.18441016972064972 \t range_loss: 0.5987976789474487\n",
      "pred_error: 0.18441016972064972 \t range_loss: 0.5987976789474487\n",
      "pred_error: 0.18447817862033844 \t range_loss: 0.5976086854934692\n",
      "pred_error: 0.18447817862033844 \t range_loss: 0.5976086854934692\n",
      "pred_error: 0.18447817862033844 \t range_loss: 0.5976086854934692\n",
      "pred_error: 0.18426869809627533 \t range_loss: 0.5974559783935547\n",
      "pred_error: 0.1843954473733902 \t range_loss: 0.5964196920394897\n",
      "pred_error: 0.18447937071323395 \t range_loss: 0.5963234305381775\n",
      "pred_error: 0.18427734076976776 \t range_loss: 0.5993884205818176\n",
      "pred_error: 0.18429061770439148 \t range_loss: 0.5982509851455688\n",
      "pred_error: 0.18450476229190826 \t range_loss: 0.5975288152694702\n",
      "pred_error: 0.18434324860572815 \t range_loss: 0.5986618399620056\n",
      "pred_error: 0.18440628051757812 \t range_loss: 0.5970611572265625\n",
      "pred_error: 0.18427887558937073 \t range_loss: 0.5985285043716431\n",
      "pred_error: 0.18433140218257904 \t range_loss: 0.5972489714622498\n",
      "pred_error: 0.18433135747909546 \t range_loss: 0.5972489714622498\n",
      "pred_error: 0.18437016010284424 \t range_loss: 0.5965234637260437\n",
      "pred_error: 0.18453478813171387 \t range_loss: 0.5966365337371826\n",
      "pred_error: 0.18446335196495056 \t range_loss: 0.5963094234466553\n",
      "pred_error: 0.18444503843784332 \t range_loss: 0.5981574654579163\n",
      "pred_error: 0.18443013727664948 \t range_loss: 0.5984086990356445\n",
      "pred_error: 0.18442977964878082 \t range_loss: 0.5984086990356445\n",
      "pred_error: 0.18418224155902863 \t range_loss: 0.597209095954895\n",
      "pred_error: 0.1841823160648346 \t range_loss: 0.597209095954895\n",
      "pred_error: 0.18438252806663513 \t range_loss: 0.5971725583076477\n",
      "pred_error: 0.18443521857261658 \t range_loss: 0.5972228050231934\n",
      "pred_error: 0.1842585802078247 \t range_loss: 0.5970239639282227\n",
      "pred_error: 0.18424612283706665 \t range_loss: 0.5967122316360474\n",
      "pred_error: 0.18418800830841064 \t range_loss: 0.5966140627861023\n",
      "pred_error: 0.18428456783294678 \t range_loss: 0.5958660840988159\n",
      "Step: 591 \t Loss: 2.437065601348877\n",
      "Step: 592 \t Loss: 2.4370057582855225\n",
      "pred_error: 0.18424491584300995 \t range_loss: 0.597012460231781\n",
      "pred_error: 0.18437618017196655 \t range_loss: 0.5966407060623169\n",
      "pred_error: 0.18437618017196655 \t range_loss: 0.5966407060623169\n",
      "pred_error: 0.1843762844800949 \t range_loss: 0.5966407060623169\n",
      "pred_error: 0.18442147970199585 \t range_loss: 0.59674072265625\n",
      "pred_error: 0.1841995120048523 \t range_loss: 0.5978956818580627\n",
      "pred_error: 0.18435049057006836 \t range_loss: 0.5973100066184998\n",
      "pred_error: 0.18457727134227753 \t range_loss: 0.5974622964859009\n",
      "pred_error: 0.1842787265777588 \t range_loss: 0.5962725281715393\n",
      "pred_error: 0.18437783420085907 \t range_loss: 0.5969302654266357\n",
      "pred_error: 0.18445706367492676 \t range_loss: 0.5958069562911987\n",
      "pred_error: 0.1844073235988617 \t range_loss: 0.5982598066329956\n",
      "pred_error: 0.18424226343631744 \t range_loss: 0.5960651636123657\n",
      "pred_error: 0.18423685431480408 \t range_loss: 0.5960651636123657\n",
      "pred_error: 0.18438628315925598 \t range_loss: 0.5952816009521484\n",
      "pred_error: 0.18438667058944702 \t range_loss: 0.5948842167854309\n",
      "pred_error: 0.18441098928451538 \t range_loss: 0.5949336886405945\n",
      "pred_error: 0.18429747223854065 \t range_loss: 0.5948421955108643\n",
      "pred_error: 0.18440763652324677 \t range_loss: 0.5941194891929626\n",
      "pred_error: 0.18429872393608093 \t range_loss: 0.5953967571258545\n",
      "pred_error: 0.18429864943027496 \t range_loss: 0.5953967571258545\n",
      "pred_error: 0.18434996902942657 \t range_loss: 0.5961288213729858\n",
      "pred_error: 0.1841384917497635 \t range_loss: 0.5971131920814514\n",
      "pred_error: 0.18468251824378967 \t range_loss: 0.5953044295310974\n",
      "pred_error: 0.18449664115905762 \t range_loss: 0.5943617224693298\n",
      "pred_error: 0.1843651533126831 \t range_loss: 0.5946735143661499\n",
      "pred_error: 0.1846131831407547 \t range_loss: 0.5952059626579285\n",
      "pred_error: 0.18428125977516174 \t range_loss: 0.595107913017273\n",
      "pred_error: 0.18428125977516174 \t range_loss: 0.595107913017273\n",
      "pred_error: 0.18456721305847168 \t range_loss: 0.5944663882255554\n",
      "pred_error: 0.18456721305847168 \t range_loss: 0.5944663882255554\n",
      "pred_error: 0.18450643122196198 \t range_loss: 0.5944089889526367\n",
      "pred_error: 0.1844530552625656 \t range_loss: 0.5940645933151245\n",
      "pred_error: 0.1844935566186905 \t range_loss: 0.593719482421875\n",
      "Step: 666 \t Loss: 2.436793804168701\n",
      "pred_error: 0.18457470834255219 \t range_loss: 0.5933789014816284\n",
      "pred_error: 0.1846165806055069 \t range_loss: 0.5932546257972717\n",
      "pred_error: 0.18458449840545654 \t range_loss: 0.5939159393310547\n",
      "pred_error: 0.18458455801010132 \t range_loss: 0.5939159393310547\n",
      "pred_error: 0.18469703197479248 \t range_loss: 0.5942553281784058\n",
      "pred_error: 0.1847493052482605 \t range_loss: 0.5943596363067627\n",
      "pred_error: 0.18472498655319214 \t range_loss: 0.593524694442749\n",
      "pred_error: 0.18468426167964935 \t range_loss: 0.5930393934249878\n",
      "pred_error: 0.18435794115066528 \t range_loss: 0.5945888757705688\n",
      "pred_error: 0.18484698235988617 \t range_loss: 0.593144953250885\n",
      "pred_error: 0.18490059673786163 \t range_loss: 0.5924110412597656\n",
      "pred_error: 0.18501123785972595 \t range_loss: 0.5925653576850891\n",
      "pred_error: 0.18478548526763916 \t range_loss: 0.5923264026641846\n",
      "pred_error: 0.18457664549350739 \t range_loss: 0.5918177366256714\n",
      "pred_error: 0.1844009906053543 \t range_loss: 0.5947138667106628\n",
      "Step: 712 \t Loss: 2.436368227005005\n",
      "pred_error: 0.1843683272600174 \t range_loss: 0.5950111746788025\n",
      "pred_error: 0.18459798395633698 \t range_loss: 0.5948291420936584\n",
      "pred_error: 0.1845979392528534 \t range_loss: 0.5948291420936584\n",
      "pred_error: 0.18468692898750305 \t range_loss: 0.5953346490859985\n",
      "pred_error: 0.18459434807300568 \t range_loss: 0.5936339497566223\n",
      "pred_error: 0.18459419906139374 \t range_loss: 0.5936339497566223\n",
      "pred_error: 0.18443550169467926 \t range_loss: 0.5936968326568604\n",
      "pred_error: 0.18443448841571808 \t range_loss: 0.5936968326568604\n",
      "pred_error: 0.18454323709011078 \t range_loss: 0.5934948325157166\n",
      "pred_error: 0.1843566596508026 \t range_loss: 0.5951992273330688\n",
      "pred_error: 0.1845950484275818 \t range_loss: 0.593061625957489\n",
      "pred_error: 0.1843886375427246 \t range_loss: 0.5929960608482361\n",
      "pred_error: 0.18465402722358704 \t range_loss: 0.5930389165878296\n",
      "Step: 732 \t Loss: 2.4359476566314697\n",
      "pred_error: 0.18471090495586395 \t range_loss: 0.5929964780807495\n",
      "pred_error: 0.18448413908481598 \t range_loss: 0.5927971005439758\n",
      "pred_error: 0.18468131124973297 \t range_loss: 0.5932155251502991\n",
      "pred_error: 0.18477195501327515 \t range_loss: 0.5925319790840149\n",
      "pred_error: 0.18456242978572845 \t range_loss: 0.5914661884307861\n",
      "pred_error: 0.18493004143238068 \t range_loss: 0.5921115875244141\n",
      "pred_error: 0.18498829007148743 \t range_loss: 0.5921547412872314\n",
      "pred_error: 0.18507668375968933 \t range_loss: 0.5915088057518005\n",
      "pred_error: 0.18507665395736694 \t range_loss: 0.5915088057518005\n",
      "pred_error: 0.1848672330379486 \t range_loss: 0.5921415686607361\n",
      "pred_error: 0.18510092794895172 \t range_loss: 0.5909498929977417\n",
      "pred_error: 0.18488657474517822 \t range_loss: 0.5919477939605713\n",
      "pred_error: 0.184886634349823 \t range_loss: 0.5919477939605713\n",
      "pred_error: 0.1852361112833023 \t range_loss: 0.5924564003944397\n",
      "pred_error: 0.18485915660858154 \t range_loss: 0.5905970335006714\n",
      "pred_error: 0.1848105788230896 \t range_loss: 0.5943943858146667\n",
      "pred_error: 0.18481038510799408 \t range_loss: 0.5943943858146667\n",
      "pred_error: 0.18467825651168823 \t range_loss: 0.591812789440155\n",
      "pred_error: 0.18467828631401062 \t range_loss: 0.591812789440155\n",
      "pred_error: 0.1846817135810852 \t range_loss: 0.5932496786117554\n",
      "pred_error: 0.184681698679924 \t range_loss: 0.5932496786117554\n",
      "pred_error: 0.18455398082733154 \t range_loss: 0.5946379899978638\n",
      "pred_error: 0.18455393612384796 \t range_loss: 0.5946379899978638\n",
      "pred_error: 0.18472813069820404 \t range_loss: 0.5919826626777649\n",
      "pred_error: 0.18472857773303986 \t range_loss: 0.5919826626777649\n",
      "pred_error: 0.18469493091106415 \t range_loss: 0.5942057967185974\n",
      "pred_error: 0.18473391234874725 \t range_loss: 0.5919126868247986\n",
      "pred_error: 0.18478839099407196 \t range_loss: 0.5925190448760986\n",
      "pred_error: 0.18457664549350739 \t range_loss: 0.5920295715332031\n",
      "pred_error: 0.18475309014320374 \t range_loss: 0.5921184420585632\n",
      "pred_error: 0.1845983862876892 \t range_loss: 0.5925933718681335\n",
      "BEST LOSS: 2.4359477\n",
      "==== Model: block9_cob_activation_norm  in Layer: 9 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 08:25:36,021 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 08:32:16,866 model.rs:1246 value (393216) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 08:32:16,892 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 08:32:54,707 model.rs:1246 value (393216) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 08:32:54,714 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 08:32:54,745 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 08:32:54,766 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 08:32:54,793 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error     | median_error   | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.000002296632 | 0.000037111342 | 0.0006939769 | -0.00067067146 | 0.00008371327  | 0.000037111342   | 0.0006939769  | 0             | 0.000000013730766  | 0.0030206195       | 0.0054899002           |\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 480310 64 [-1022058, 994510] 1 [16]\n",
      "===============================\n",
      "==== Model: block9_cob_activation_norm_teleported  in Layer: 9 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 08:33:36,396 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 08:43:17,872 model.rs:1246 value (-45056) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 08:43:17,883 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 08:52:02,275 model.rs:1246 value (-45056) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 08:52:02,281 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 08:53:20,274 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 08:53:20,359 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 08:53:20,371 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000013023744 | 0.00018252432 | 0.0007787943 | -0.0006825924 | 0.00008721519  | 0.00018252432    | 0.0007787943  | 0             | 0.000000015009517  | -0.00025274057     | 0.0013703674           |\n",
      "+-----------------+---------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 480306 64 [-617688, 976596] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 10 , \t  activation_stats: {'relu_1': {'norm': tensor(907.2410), 'max': tensor(4.6855), 'min': tensor(-8.9226), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(13.6081)\n",
      "Step: 0 \t Loss: 2.7448906898498535\n",
      "Step: 1 \t Loss: 2.724097728729248\n",
      "pred_error: 0.17995814979076385 \t range_loss: 0.9245136380195618\n",
      "pred_error: 0.17995798587799072 \t range_loss: 0.9245136380195618\n",
      "Step: 2 \t Loss: 2.709272861480713\n",
      "Step: 3 \t Loss: 2.6965317726135254\n",
      "Step: 4 \t Loss: 2.681473731994629\n",
      "Step: 5 \t Loss: 2.6570727825164795\n",
      "pred_error: 0.1798810362815857 \t range_loss: 0.8582672476768494\n",
      "Step: 6 \t Loss: 2.6451754570007324\n",
      "Step: 7 \t Loss: 2.643073081970215\n",
      "Step: 8 \t Loss: 2.640688896179199\n",
      "pred_error: 0.1798955500125885 \t range_loss: 0.8417323231697083\n",
      "pred_error: 0.17989566922187805 \t range_loss: 0.8417323231697083\n",
      "Step: 9 \t Loss: 2.62632155418396\n",
      "Step: 10 \t Loss: 2.6219592094421387\n",
      "Step: 11 \t Loss: 2.615091323852539\n",
      "Step: 12 \t Loss: 2.6076488494873047\n",
      "Step: 13 \t Loss: 2.606814384460449\n",
      "Step: 14 \t Loss: 2.6036429405212402\n",
      "pred_error: 0.17962391674518585 \t range_loss: 0.8074087500572205\n",
      "Step: 15 \t Loss: 2.5999879837036133\n",
      "Step: 16 \t Loss: 2.5994873046875\n",
      "Step: 17 \t Loss: 2.595120429992676\n",
      "pred_error: 0.17975786328315735 \t range_loss: 0.7975448369979858\n",
      "Step: 18 \t Loss: 2.5888924598693848\n",
      "pred_error: 0.17968681454658508 \t range_loss: 0.7920241951942444\n",
      "pred_error: 0.17968684434890747 \t range_loss: 0.7920241951942444\n",
      "pred_error: 0.17968720197677612 \t range_loss: 0.7920241951942444\n",
      "Step: 19 \t Loss: 2.5869390964508057\n",
      "Step: 20 \t Loss: 2.5845518112182617\n",
      "Step: 21 \t Loss: 2.5791444778442383\n",
      "pred_error: 0.17957882583141327 \t range_loss: 0.7833561897277832\n",
      "Step: 22 \t Loss: 2.5767345428466797\n",
      "Step: 24 \t Loss: 2.573678970336914\n",
      "pred_error: 0.17965112626552582 \t range_loss: 0.7771676778793335\n",
      "pred_error: 0.17965129017829895 \t range_loss: 0.7771676778793335\n",
      "Step: 25 \t Loss: 2.5690102577209473\n",
      "Step: 26 \t Loss: 2.568981647491455\n",
      "pred_error: 0.17938393354415894 \t range_loss: 0.7751376628875732\n",
      "Step: 27 \t Loss: 2.5620675086975098\n",
      "Step: 28 \t Loss: 2.559469699859619\n",
      "Step: 31 \t Loss: 2.5561470985412598\n",
      "pred_error: 0.17973186075687408 \t range_loss: 0.7588256597518921\n",
      "pred_error: 0.17973197996616364 \t range_loss: 0.7588256597518921\n",
      "Step: 32 \t Loss: 2.5513410568237305\n",
      "Step: 34 \t Loss: 2.5482900142669678\n",
      "pred_error: 0.17971494793891907 \t range_loss: 0.751141369342804\n",
      "Step: 35 \t Loss: 2.5471487045288086\n",
      "pred_error: 0.17975588142871857 \t range_loss: 0.7495872974395752\n",
      "pred_error: 0.1801106035709381 \t range_loss: 0.7474811673164368\n",
      "pred_error: 0.180111825466156 \t range_loss: 0.7474811673164368\n",
      "Step: 37 \t Loss: 2.5467610359191895\n",
      "pred_error: 0.18005380034446716 \t range_loss: 0.7462204098701477\n",
      "pred_error: 0.18005399405956268 \t range_loss: 0.7462204098701477\n",
      "Step: 38 \t Loss: 2.5443246364593506\n",
      "Step: 39 \t Loss: 2.5428144931793213\n",
      "pred_error: 0.1800924390554428 \t range_loss: 0.741886556148529\n",
      "Step: 40 \t Loss: 2.542384147644043\n",
      "Step: 41 \t Loss: 2.5406486988067627\n",
      "Step: 42 \t Loss: 2.5375638008117676\n",
      "pred_error: 0.18011514842510223 \t range_loss: 0.7364104390144348\n",
      "Step: 43 \t Loss: 2.5350725650787354\n",
      "pred_error: 0.17989787459373474 \t range_loss: 0.7360959649085999\n",
      "pred_error: 0.17989632487297058 \t range_loss: 0.736367404460907\n",
      "pred_error: 0.17985139787197113 \t range_loss: 0.7366613149642944\n",
      "Step: 45 \t Loss: 2.533958673477173\n",
      "Step: 46 \t Loss: 2.5308680534362793\n",
      "pred_error: 0.18000619113445282 \t range_loss: 0.7322267889976501\n",
      "pred_error: 0.1800065040588379 \t range_loss: 0.7322267889976501\n",
      "Step: 48 \t Loss: 2.528172016143799\n",
      "Step: 49 \t Loss: 2.5268657207489014\n",
      "pred_error: 0.18003042042255402 \t range_loss: 0.7265608906745911\n",
      "pred_error: 0.17999377846717834 \t range_loss: 0.7280105948448181\n",
      "pred_error: 0.17999406158924103 \t range_loss: 0.7280105948448181\n",
      "pred_error: 0.17999374866485596 \t range_loss: 0.7280105948448181\n",
      "pred_error: 0.17999359965324402 \t range_loss: 0.7280105948448181\n",
      "Step: 52 \t Loss: 2.525871753692627\n",
      "pred_error: 0.18043635785579681 \t range_loss: 0.723005473613739\n",
      "Step: 55 \t Loss: 2.524022102355957\n",
      "Step: 56 \t Loss: 2.522677183151245\n",
      "pred_error: 0.18023750185966492 \t range_loss: 0.7203023433685303\n",
      "pred_error: 0.18038031458854675 \t range_loss: 0.7195752263069153\n",
      "Step: 58 \t Loss: 2.5212554931640625\n",
      "pred_error: 0.18040147423744202 \t range_loss: 0.7172412276268005\n",
      "Step: 60 \t Loss: 2.519122362136841\n",
      "Step: 61 \t Loss: 2.516542673110962\n",
      "Step: 63 \t Loss: 2.5154757499694824\n",
      "pred_error: 0.18023884296417236 \t range_loss: 0.7130878567695618\n",
      "Step: 66 \t Loss: 2.514692783355713\n",
      "Step: 67 \t Loss: 2.514179229736328\n",
      "pred_error: 0.18048924207687378 \t range_loss: 0.7110601663589478\n",
      "Step: 71 \t Loss: 2.513009548187256\n",
      "Step: 72 \t Loss: 2.509707450866699\n",
      "Step: 78 \t Loss: 2.5073139667510986\n",
      "pred_error: 0.1803703010082245 \t range_loss: 0.7036096453666687\n",
      "Step: 84 \t Loss: 2.5042152404785156\n",
      "pred_error: 0.18031638860702515 \t range_loss: 0.7037658095359802\n",
      "pred_error: 0.1805972158908844 \t range_loss: 0.7000434994697571\n",
      "Step: 87 \t Loss: 2.503046751022339\n",
      "pred_error: 0.1810171753168106 \t range_loss: 0.6950094103813171\n",
      "pred_error: 0.18075668811798096 \t range_loss: 0.6971402764320374\n",
      "pred_error: 0.18091870844364166 \t range_loss: 0.6965105533599854\n",
      "pred_error: 0.18092289566993713 \t range_loss: 0.6953878998756409\n",
      "Step: 97 \t Loss: 2.5012118816375732\n",
      "Step: 98 \t Loss: 2.4997096061706543\n",
      "pred_error: 0.18071572482585907 \t range_loss: 0.6925493478775024\n",
      "Step: 100 \t Loss: 2.498143196105957\n",
      "Step: 101 \t Loss: 2.497922420501709\n",
      "pred_error: 0.18078190088272095 \t range_loss: 0.6922515630722046\n",
      "pred_error: 0.1809198409318924 \t range_loss: 0.6909662485122681\n",
      "pred_error: 0.18127354979515076 \t range_loss: 0.687092125415802\n",
      "Step: 108 \t Loss: 2.4959495067596436\n",
      "pred_error: 0.18121038377285004 \t range_loss: 0.6871579885482788\n",
      "pred_error: 0.18121029436588287 \t range_loss: 0.6871579885482788\n",
      "pred_error: 0.18108251690864563 \t range_loss: 0.6898723244667053\n",
      "Step: 114 \t Loss: 2.4959120750427246\n",
      "pred_error: 0.1814187914133072 \t range_loss: 0.6836206316947937\n",
      "pred_error: 0.18141886591911316 \t range_loss: 0.6836206316947937\n",
      "Step: 119 \t Loss: 2.4939584732055664\n",
      "pred_error: 0.18089421093463898 \t range_loss: 0.6863343715667725\n",
      "pred_error: 0.1812535673379898 \t range_loss: 0.6825771331787109\n",
      "Step: 129 \t Loss: 2.493687868118286\n",
      "pred_error: 0.18143543601036072 \t range_loss: 0.6793321967124939\n",
      "Step: 130 \t Loss: 2.492067813873291\n",
      "pred_error: 0.18133464455604553 \t range_loss: 0.682732880115509\n",
      "pred_error: 0.18176543712615967 \t range_loss: 0.6781485676765442\n",
      "pred_error: 0.18144591152668 \t range_loss: 0.6777162551879883\n",
      "pred_error: 0.1814461499452591 \t range_loss: 0.6777162551879883\n",
      "Step: 140 \t Loss: 2.491955280303955\n",
      "pred_error: 0.18160410225391388 \t range_loss: 0.67591392993927\n",
      "Step: 141 \t Loss: 2.4917750358581543\n",
      "pred_error: 0.18167516589164734 \t range_loss: 0.6750224828720093\n",
      "pred_error: 0.1818806529045105 \t range_loss: 0.6760236620903015\n",
      "pred_error: 0.1816999614238739 \t range_loss: 0.6767193078994751\n",
      "pred_error: 0.18176813423633575 \t range_loss: 0.6754329204559326\n",
      "pred_error: 0.18176697194576263 \t range_loss: 0.6755668520927429\n",
      "Step: 150 \t Loss: 2.489762544631958\n",
      "pred_error: 0.1815810203552246 \t range_loss: 0.6739469766616821\n",
      "pred_error: 0.1814499944448471 \t range_loss: 0.6758111715316772\n",
      "pred_error: 0.18145014345645905 \t range_loss: 0.6758111715316772\n",
      "Step: 153 \t Loss: 2.488755702972412\n",
      "pred_error: 0.18203620612621307 \t range_loss: 0.6713887453079224\n",
      "pred_error: 0.1820361465215683 \t range_loss: 0.6713887453079224\n",
      "pred_error: 0.18182335793972015 \t range_loss: 0.6712496876716614\n",
      "pred_error: 0.18218731880187988 \t range_loss: 0.6714205741882324\n",
      "pred_error: 0.18201595544815063 \t range_loss: 0.6705226302146912\n",
      "pred_error: 0.18201585114002228 \t range_loss: 0.6705226302146912\n",
      "pred_error: 0.1821344494819641 \t range_loss: 0.6719496250152588\n",
      "pred_error: 0.1821344941854477 \t range_loss: 0.6719496250152588\n",
      "pred_error: 0.18215762078762054 \t range_loss: 0.671230673789978\n",
      "pred_error: 0.18215757608413696 \t range_loss: 0.671230673789978\n",
      "pred_error: 0.18211428821086884 \t range_loss: 0.6697996258735657\n",
      "Step: 167 \t Loss: 2.487844467163086\n",
      "pred_error: 0.18185186386108398 \t range_loss: 0.6693998575210571\n",
      "Step: 170 \t Loss: 2.4874720573425293\n",
      "pred_error: 0.1818600296974182 \t range_loss: 0.6688717007637024\n",
      "pred_error: 0.18174277245998383 \t range_loss: 0.6716244220733643\n",
      "pred_error: 0.18211275339126587 \t range_loss: 0.6674493551254272\n",
      "Step: 179 \t Loss: 2.487454414367676\n",
      "pred_error: 0.18189801275730133 \t range_loss: 0.6684730648994446\n",
      "pred_error: 0.18205900490283966 \t range_loss: 0.6687180399894714\n",
      "Step: 181 \t Loss: 2.486985921859741\n",
      "pred_error: 0.18188408017158508 \t range_loss: 0.6681434512138367\n",
      "pred_error: 0.18211740255355835 \t range_loss: 0.6669527292251587\n",
      "pred_error: 0.1821175366640091 \t range_loss: 0.6669527292251587\n",
      "Step: 190 \t Loss: 2.486680269241333\n",
      "pred_error: 0.1822020262479782 \t range_loss: 0.6646609902381897\n",
      "Step: 192 \t Loss: 2.483753204345703\n",
      "Step: 193 \t Loss: 2.4836878776550293\n",
      "pred_error: 0.18195420503616333 \t range_loss: 0.6660531163215637\n",
      "pred_error: 0.1820266991853714 \t range_loss: 0.665830671787262\n",
      "pred_error: 0.18202531337738037 \t range_loss: 0.665830671787262\n",
      "pred_error: 0.18206056952476501 \t range_loss: 0.6688119769096375\n",
      "pred_error: 0.18213042616844177 \t range_loss: 0.6641371250152588\n",
      "pred_error: 0.1823415905237198 \t range_loss: 0.6651420593261719\n",
      "pred_error: 0.1821662038564682 \t range_loss: 0.6630591154098511\n",
      "Step: 204 \t Loss: 2.482369899749756\n",
      "pred_error: 0.18220806121826172 \t range_loss: 0.666935384273529\n",
      "pred_error: 0.18242120742797852 \t range_loss: 0.662242591381073\n",
      "pred_error: 0.18263433873653412 \t range_loss: 0.6602708697319031\n",
      "pred_error: 0.18265824019908905 \t range_loss: 0.6604732871055603\n",
      "pred_error: 0.18242423236370087 \t range_loss: 0.6593284010887146\n",
      "pred_error: 0.18254892528057098 \t range_loss: 0.6584585309028625\n",
      "pred_error: 0.18259550631046295 \t range_loss: 0.6635562181472778\n",
      "pred_error: 0.18259617686271667 \t range_loss: 0.6639565229415894\n",
      "pred_error: 0.18231315910816193 \t range_loss: 0.6617707014083862\n",
      "pred_error: 0.18289482593536377 \t range_loss: 0.6596124768257141\n",
      "pred_error: 0.18263350427150726 \t range_loss: 0.6584027409553528\n",
      "pred_error: 0.1826370358467102 \t range_loss: 0.6584027409553528\n",
      "pred_error: 0.18254072964191437 \t range_loss: 0.6606098413467407\n",
      "pred_error: 0.18253116309642792 \t range_loss: 0.65761798620224\n",
      "Step: 242 \t Loss: 2.481696128845215\n",
      "Step: 243 \t Loss: 2.4801383018493652\n",
      "pred_error: 0.1829240918159485 \t range_loss: 0.6559519171714783\n",
      "pred_error: 0.18301193416118622 \t range_loss: 0.6543675065040588\n",
      "pred_error: 0.18281397223472595 \t range_loss: 0.6549977660179138\n",
      "pred_error: 0.18303413689136505 \t range_loss: 0.6548888087272644\n",
      "pred_error: 0.1830838918685913 \t range_loss: 0.6531016826629639\n",
      "pred_error: 0.18311575055122375 \t range_loss: 0.656494677066803\n",
      "pred_error: 0.18287108838558197 \t range_loss: 0.6544539928436279\n",
      "Step: 276 \t Loss: 2.4778871536254883\n",
      "pred_error: 0.18259744346141815 \t range_loss: 0.6519126892089844\n",
      "pred_error: 0.18259741365909576 \t range_loss: 0.6519126892089844\n",
      "pred_error: 0.182783305644989 \t range_loss: 0.6559773683547974\n",
      "pred_error: 0.18314065039157867 \t range_loss: 0.6546835899353027\n",
      "pred_error: 0.18297326564788818 \t range_loss: 0.6529491543769836\n",
      "pred_error: 0.18282805383205414 \t range_loss: 0.6536901593208313\n",
      "pred_error: 0.18316853046417236 \t range_loss: 0.6549113392829895\n",
      "pred_error: 0.1828625202178955 \t range_loss: 0.6565941572189331\n",
      "pred_error: 0.183052197098732 \t range_loss: 0.6516594290733337\n",
      "pred_error: 0.18295906484127045 \t range_loss: 0.6515901684761047\n",
      "pred_error: 0.18269246816635132 \t range_loss: 0.6530210971832275\n",
      "pred_error: 0.1830926090478897 \t range_loss: 0.6516605615615845\n",
      "pred_error: 0.18290899693965912 \t range_loss: 0.6511380076408386\n",
      "pred_error: 0.18317709863185883 \t range_loss: 0.6519074440002441\n",
      "pred_error: 0.18309125304222107 \t range_loss: 0.6546543836593628\n",
      "pred_error: 0.18323571979999542 \t range_loss: 0.6523765325546265\n",
      "pred_error: 0.18266989290714264 \t range_loss: 0.6542885303497314\n",
      "pred_error: 0.18320332467556 \t range_loss: 0.6540282368659973\n",
      "pred_error: 0.18323881924152374 \t range_loss: 0.6523343920707703\n",
      "pred_error: 0.1832481026649475 \t range_loss: 0.6522425413131714\n",
      "pred_error: 0.1832936853170395 \t range_loss: 0.651364266872406\n",
      "pred_error: 0.18293319642543793 \t range_loss: 0.6515448093414307\n",
      "pred_error: 0.18293549120426178 \t range_loss: 0.6502016186714172\n",
      "pred_error: 0.18285267055034637 \t range_loss: 0.6501447558403015\n",
      "pred_error: 0.18280966579914093 \t range_loss: 0.6509192585945129\n",
      "pred_error: 0.18273736536502838 \t range_loss: 0.6509998440742493\n",
      "pred_error: 0.18273723125457764 \t range_loss: 0.6509998440742493\n",
      "pred_error: 0.1828516125679016 \t range_loss: 0.6551772356033325\n",
      "pred_error: 0.18318293988704681 \t range_loss: 0.650400698184967\n",
      "pred_error: 0.18317152559757233 \t range_loss: 0.6495924592018127\n",
      "pred_error: 0.1829933226108551 \t range_loss: 0.6484264135360718\n",
      "Step: 332 \t Loss: 2.4778518676757812\n",
      "pred_error: 0.18297213315963745 \t range_loss: 0.6481299996376038\n",
      "Step: 335 \t Loss: 2.475949287414551\n",
      "pred_error: 0.1828756481409073 \t range_loss: 0.6471883058547974\n",
      "Step: 337 \t Loss: 2.4756946563720703\n",
      "pred_error: 0.1833200752735138 \t range_loss: 0.6488605737686157\n",
      "pred_error: 0.18314263224601746 \t range_loss: 0.6486077904701233\n",
      "pred_error: 0.18314270675182343 \t range_loss: 0.6486077904701233\n",
      "pred_error: 0.18335475027561188 \t range_loss: 0.6494194865226746\n",
      "pred_error: 0.18335789442062378 \t range_loss: 0.6489189267158508\n",
      "pred_error: 0.18311074376106262 \t range_loss: 0.6490018367767334\n",
      "pred_error: 0.1834644377231598 \t range_loss: 0.6483715176582336\n",
      "pred_error: 0.1832198202610016 \t range_loss: 0.6475785374641418\n",
      "pred_error: 0.1828613579273224 \t range_loss: 0.6495402455329895\n",
      "pred_error: 0.18286126852035522 \t range_loss: 0.6495402455329895\n",
      "pred_error: 0.18286766111850739 \t range_loss: 0.6511936783790588\n",
      "pred_error: 0.18286791443824768 \t range_loss: 0.6511936783790588\n",
      "pred_error: 0.1835470199584961 \t range_loss: 0.6472539901733398\n",
      "pred_error: 0.1835470199584961 \t range_loss: 0.6472539901733398\n",
      "pred_error: 0.18323484063148499 \t range_loss: 0.6478216648101807\n",
      "pred_error: 0.18323484063148499 \t range_loss: 0.6478216648101807\n",
      "pred_error: 0.1831604540348053 \t range_loss: 0.6482878923416138\n",
      "pred_error: 0.18316058814525604 \t range_loss: 0.6482878923416138\n",
      "pred_error: 0.18328534066677094 \t range_loss: 0.6468267440795898\n",
      "Step: 368 \t Loss: 2.4755051136016846\n",
      "pred_error: 0.18298521637916565 \t range_loss: 0.6485261917114258\n",
      "pred_error: 0.18335025012493134 \t range_loss: 0.6468684673309326\n",
      "pred_error: 0.18352030217647552 \t range_loss: 0.6452797651290894\n",
      "pred_error: 0.18348342180252075 \t range_loss: 0.6458609104156494\n",
      "pred_error: 0.18327820301055908 \t range_loss: 0.6458750367164612\n",
      "pred_error: 0.18327823281288147 \t range_loss: 0.6458750367164612\n",
      "pred_error: 0.18340152502059937 \t range_loss: 0.648419201374054\n",
      "pred_error: 0.18337999284267426 \t range_loss: 0.6469354033470154\n",
      "pred_error: 0.18337872624397278 \t range_loss: 0.6469354033470154\n",
      "pred_error: 0.1832388937473297 \t range_loss: 0.6459928750991821\n",
      "pred_error: 0.18298101425170898 \t range_loss: 0.6489173769950867\n",
      "pred_error: 0.18298101425170898 \t range_loss: 0.6489173769950867\n",
      "pred_error: 0.18364526331424713 \t range_loss: 0.6459678411483765\n",
      "pred_error: 0.18371357023715973 \t range_loss: 0.644577145576477\n",
      "pred_error: 0.18367010354995728 \t range_loss: 0.64776211977005\n",
      "pred_error: 0.1837509423494339 \t range_loss: 0.6456249356269836\n",
      "pred_error: 0.18376648426055908 \t range_loss: 0.6446250081062317\n",
      "pred_error: 0.18376652896404266 \t range_loss: 0.6446250081062317\n",
      "pred_error: 0.1836353987455368 \t range_loss: 0.6435027122497559\n",
      "pred_error: 0.18387410044670105 \t range_loss: 0.6470524072647095\n",
      "pred_error: 0.18412788212299347 \t range_loss: 0.6443657875061035\n",
      "pred_error: 0.18348737061023712 \t range_loss: 0.6455321311950684\n",
      "pred_error: 0.18363255262374878 \t range_loss: 0.6430779099464417\n",
      "pred_error: 0.18347050249576569 \t range_loss: 0.6445538997650146\n",
      "pred_error: 0.18366968631744385 \t range_loss: 0.6451869010925293\n",
      "pred_error: 0.18368598818778992 \t range_loss: 0.6459007859230042\n",
      "pred_error: 0.18369801342487335 \t range_loss: 0.6468480229377747\n",
      "pred_error: 0.18344944715499878 \t range_loss: 0.6457538604736328\n",
      "pred_error: 0.18366162478923798 \t range_loss: 0.6429481506347656\n",
      "pred_error: 0.18366138637065887 \t range_loss: 0.6429481506347656\n",
      "pred_error: 0.18358929455280304 \t range_loss: 0.6433354616165161\n",
      "pred_error: 0.18397140502929688 \t range_loss: 0.6413232684135437\n",
      "pred_error: 0.18361163139343262 \t range_loss: 0.6400496959686279\n",
      "pred_error: 0.18376748263835907 \t range_loss: 0.6410815119743347\n",
      "pred_error: 0.18381716310977936 \t range_loss: 0.6448577642440796\n",
      "pred_error: 0.183725044131279 \t range_loss: 0.6416201591491699\n",
      "pred_error: 0.18359488248825073 \t range_loss: 0.6453866362571716\n",
      "pred_error: 0.18394479155540466 \t range_loss: 0.6436848044395447\n",
      "pred_error: 0.18394236266613007 \t range_loss: 0.6424616575241089\n",
      "pred_error: 0.18382593989372253 \t range_loss: 0.6425778865814209\n",
      "pred_error: 0.18369963765144348 \t range_loss: 0.6432650685310364\n",
      "pred_error: 0.18329481780529022 \t range_loss: 0.644146740436554\n",
      "pred_error: 0.18348391354084015 \t range_loss: 0.6441848874092102\n",
      "pred_error: 0.18348413705825806 \t range_loss: 0.6441848874092102\n",
      "pred_error: 0.18348780274391174 \t range_loss: 0.6461571455001831\n",
      "pred_error: 0.18370813131332397 \t range_loss: 0.6418170928955078\n",
      "pred_error: 0.1838953047990799 \t range_loss: 0.6425715684890747\n",
      "pred_error: 0.183818519115448 \t range_loss: 0.6431944370269775\n",
      "pred_error: 0.18388499319553375 \t range_loss: 0.6428482532501221\n",
      "pred_error: 0.18341904878616333 \t range_loss: 0.6437941789627075\n",
      "pred_error: 0.18368858098983765 \t range_loss: 0.6405066251754761\n",
      "pred_error: 0.18346290290355682 \t range_loss: 0.6446551084518433\n",
      "pred_error: 0.18373557925224304 \t range_loss: 0.6448842883110046\n",
      "pred_error: 0.18373553454875946 \t range_loss: 0.6448842883110046\n",
      "pred_error: 0.18371373414993286 \t range_loss: 0.6451180577278137\n",
      "pred_error: 0.18371371924877167 \t range_loss: 0.6451180577278137\n",
      "pred_error: 0.18350015580654144 \t range_loss: 0.6426748037338257\n",
      "Step: 482 \t Loss: 2.474789619445801\n",
      "pred_error: 0.18341685831546783 \t range_loss: 0.6406210660934448\n",
      "pred_error: 0.1837306171655655 \t range_loss: 0.64056396484375\n",
      "pred_error: 0.1837305724620819 \t range_loss: 0.64056396484375\n",
      "pred_error: 0.18373051285743713 \t range_loss: 0.64056396484375\n",
      "pred_error: 0.1837952435016632 \t range_loss: 0.6434091925621033\n",
      "pred_error: 0.18348433077335358 \t range_loss: 0.6405825018882751\n",
      "pred_error: 0.18348437547683716 \t range_loss: 0.6405825018882751\n",
      "pred_error: 0.1833195984363556 \t range_loss: 0.6428879499435425\n",
      "pred_error: 0.18365633487701416 \t range_loss: 0.6424499154090881\n",
      "pred_error: 0.1836223006248474 \t range_loss: 0.6432204246520996\n",
      "pred_error: 0.18347246944904327 \t range_loss: 0.6451791524887085\n",
      "pred_error: 0.1837480366230011 \t range_loss: 0.6406111121177673\n",
      "pred_error: 0.1836811751127243 \t range_loss: 0.6435405611991882\n",
      "pred_error: 0.18374761939048767 \t range_loss: 0.6410601139068604\n",
      "pred_error: 0.1837475746870041 \t range_loss: 0.6410601139068604\n",
      "pred_error: 0.1837773621082306 \t range_loss: 0.6442410349845886\n",
      "pred_error: 0.18383926153182983 \t range_loss: 0.640652060508728\n",
      "pred_error: 0.1838391274213791 \t range_loss: 0.640652060508728\n",
      "pred_error: 0.18355709314346313 \t range_loss: 0.6423168778419495\n",
      "pred_error: 0.1835571825504303 \t range_loss: 0.6423168778419495\n",
      "pred_error: 0.1839393973350525 \t range_loss: 0.6408312320709229\n",
      "pred_error: 0.18347741663455963 \t range_loss: 0.6430417895317078\n",
      "pred_error: 0.1836080104112625 \t range_loss: 0.642245888710022\n",
      "pred_error: 0.18360809981822968 \t range_loss: 0.642245888710022\n",
      "pred_error: 0.18381547927856445 \t range_loss: 0.642971932888031\n",
      "pred_error: 0.18374931812286377 \t range_loss: 0.642018735408783\n",
      "pred_error: 0.18374933302402496 \t range_loss: 0.642018735408783\n",
      "pred_error: 0.18374931812286377 \t range_loss: 0.642018735408783\n",
      "pred_error: 0.18350303173065186 \t range_loss: 0.6410001516342163\n",
      "pred_error: 0.18377462029457092 \t range_loss: 0.6442933678627014\n",
      "pred_error: 0.18378277122974396 \t range_loss: 0.6411064863204956\n",
      "pred_error: 0.18354615569114685 \t range_loss: 0.6439390182495117\n",
      "pred_error: 0.18384994566440582 \t range_loss: 0.6446266174316406\n",
      "pred_error: 0.18407826125621796 \t range_loss: 0.6402568221092224\n",
      "pred_error: 0.18377365171909332 \t range_loss: 0.6409591436386108\n",
      "pred_error: 0.18388737738132477 \t range_loss: 0.6410939693450928\n",
      "pred_error: 0.18347392976284027 \t range_loss: 0.6407758593559265\n",
      "pred_error: 0.18347398936748505 \t range_loss: 0.6407758593559265\n",
      "Step: 546 \t Loss: 2.473921298980713\n",
      "pred_error: 0.18346785008907318 \t range_loss: 0.6406546235084534\n",
      "pred_error: 0.18356291949748993 \t range_loss: 0.6438360810279846\n",
      "pred_error: 0.1833779364824295 \t range_loss: 0.6445726752281189\n",
      "pred_error: 0.1835688203573227 \t range_loss: 0.6424495577812195\n",
      "pred_error: 0.1836118996143341 \t range_loss: 0.6416146755218506\n",
      "pred_error: 0.18367978930473328 \t range_loss: 0.6404786705970764\n",
      "pred_error: 0.18377023935317993 \t range_loss: 0.642244815826416\n",
      "pred_error: 0.18376843631267548 \t range_loss: 0.6414982080459595\n",
      "pred_error: 0.18377165496349335 \t range_loss: 0.6408883333206177\n",
      "pred_error: 0.18377171456813812 \t range_loss: 0.6408883333206177\n",
      "pred_error: 0.18381550908088684 \t range_loss: 0.6375518441200256\n",
      "pred_error: 0.18381543457508087 \t range_loss: 0.6375518441200256\n",
      "pred_error: 0.18397419154644012 \t range_loss: 0.6372379660606384\n",
      "pred_error: 0.18421635031700134 \t range_loss: 0.6372881531715393\n",
      "pred_error: 0.18421636521816254 \t range_loss: 0.6372881531715393\n",
      "pred_error: 0.18398675322532654 \t range_loss: 0.6402376294136047\n",
      "pred_error: 0.1836899071931839 \t range_loss: 0.6407045722007751\n",
      "pred_error: 0.184075266122818 \t range_loss: 0.6401143670082092\n",
      "pred_error: 0.18407534062862396 \t range_loss: 0.6401143670082092\n",
      "pred_error: 0.18400977551937103 \t range_loss: 0.6402703523635864\n",
      "pred_error: 0.1837761253118515 \t range_loss: 0.6409779787063599\n",
      "pred_error: 0.1838054656982422 \t range_loss: 0.6382132172584534\n",
      "pred_error: 0.18380606174468994 \t range_loss: 0.6382132172584534\n",
      "pred_error: 0.1839870661497116 \t range_loss: 0.6422352194786072\n",
      "pred_error: 0.18386149406433105 \t range_loss: 0.6422777771949768\n",
      "pred_error: 0.18398040533065796 \t range_loss: 0.6393623948097229\n",
      "pred_error: 0.18369346857070923 \t range_loss: 0.6387113332748413\n",
      "pred_error: 0.18349938094615936 \t range_loss: 0.6419415473937988\n",
      "pred_error: 0.1836552917957306 \t range_loss: 0.6415248513221741\n",
      "pred_error: 0.1840444654226303 \t range_loss: 0.6395072340965271\n",
      "pred_error: 0.18436411023139954 \t range_loss: 0.6395784020423889\n",
      "pred_error: 0.18401044607162476 \t range_loss: 0.6383673548698425\n",
      "pred_error: 0.18401175737380981 \t range_loss: 0.6396337747573853\n",
      "pred_error: 0.1843893826007843 \t range_loss: 0.639042317867279\n",
      "pred_error: 0.18413031101226807 \t range_loss: 0.63966965675354\n",
      "pred_error: 0.18383380770683289 \t range_loss: 0.6385414600372314\n",
      "pred_error: 0.18384835124015808 \t range_loss: 0.639451801776886\n",
      "pred_error: 0.1839146912097931 \t range_loss: 0.6414689421653748\n",
      "pred_error: 0.18411482870578766 \t range_loss: 0.6386823058128357\n",
      "pred_error: 0.18401864171028137 \t range_loss: 0.6432705521583557\n",
      "pred_error: 0.18367189168930054 \t range_loss: 0.6377520561218262\n",
      "pred_error: 0.18373273313045502 \t range_loss: 0.64141446352005\n",
      "pred_error: 0.18411894142627716 \t range_loss: 0.6406348347663879\n",
      "pred_error: 0.18411915004253387 \t range_loss: 0.6406348347663879\n",
      "pred_error: 0.18403662741184235 \t range_loss: 0.6390565037727356\n",
      "pred_error: 0.1837732195854187 \t range_loss: 0.6402638554573059\n",
      "pred_error: 0.18392424285411835 \t range_loss: 0.6396626830101013\n",
      "pred_error: 0.18382371962070465 \t range_loss: 0.6408416628837585\n",
      "pred_error: 0.18390332162380219 \t range_loss: 0.6394871473312378\n",
      "pred_error: 0.18384549021720886 \t range_loss: 0.6385763883590698\n",
      "pred_error: 0.18370328843593597 \t range_loss: 0.6409310698509216\n",
      "pred_error: 0.18374532461166382 \t range_loss: 0.6383236050605774\n",
      "pred_error: 0.18356354534626007 \t range_loss: 0.6403935551643372\n",
      "pred_error: 0.18378989398479462 \t range_loss: 0.6385354995727539\n",
      "pred_error: 0.18369945883750916 \t range_loss: 0.6397334337234497\n",
      "pred_error: 0.18419787287712097 \t range_loss: 0.6401581168174744\n",
      "pred_error: 0.1841977834701538 \t range_loss: 0.6401581168174744\n",
      "pred_error: 0.184164360165596 \t range_loss: 0.6395688056945801\n",
      "pred_error: 0.18416470289230347 \t range_loss: 0.6395688056945801\n",
      "pred_error: 0.1839027851819992 \t range_loss: 0.6372623443603516\n",
      "Step: 674 \t Loss: 2.4732513427734375\n",
      "pred_error: 0.1836102157831192 \t range_loss: 0.6371486186981201\n",
      "pred_error: 0.18361029028892517 \t range_loss: 0.6371486186981201\n",
      "pred_error: 0.1839192658662796 \t range_loss: 0.6379226446151733\n",
      "pred_error: 0.1837260127067566 \t range_loss: 0.6380259990692139\n",
      "pred_error: 0.18379341065883636 \t range_loss: 0.6410053372383118\n",
      "pred_error: 0.18377447128295898 \t range_loss: 0.6403536200523376\n",
      "pred_error: 0.1839969903230667 \t range_loss: 0.6400997638702393\n",
      "pred_error: 0.18399690091609955 \t range_loss: 0.6400997638702393\n",
      "pred_error: 0.18412470817565918 \t range_loss: 0.6379591822624207\n",
      "pred_error: 0.18389654159545898 \t range_loss: 0.6379360556602478\n",
      "pred_error: 0.1837230920791626 \t range_loss: 0.6401858925819397\n",
      "pred_error: 0.18372300267219543 \t range_loss: 0.6401858925819397\n",
      "pred_error: 0.18367719650268555 \t range_loss: 0.6416597366333008\n",
      "pred_error: 0.18365007638931274 \t range_loss: 0.6375075578689575\n",
      "pred_error: 0.18390630185604095 \t range_loss: 0.6387172341346741\n",
      "pred_error: 0.18390630185604095 \t range_loss: 0.6387172341346741\n",
      "pred_error: 0.18390625715255737 \t range_loss: 0.6387172341346741\n",
      "pred_error: 0.18385599553585052 \t range_loss: 0.6394752264022827\n",
      "pred_error: 0.18385587632656097 \t range_loss: 0.6394752264022827\n",
      "pred_error: 0.18421436846256256 \t range_loss: 0.6385406851768494\n",
      "pred_error: 0.18367791175842285 \t range_loss: 0.6395748257637024\n",
      "pred_error: 0.18378670513629913 \t range_loss: 0.6396058797836304\n",
      "pred_error: 0.18378667533397675 \t range_loss: 0.6396058797836304\n",
      "pred_error: 0.18377208709716797 \t range_loss: 0.6415427923202515\n",
      "pred_error: 0.1839972287416458 \t range_loss: 0.6393194198608398\n",
      "pred_error: 0.1840423047542572 \t range_loss: 0.6416556239128113\n",
      "pred_error: 0.18438686430454254 \t range_loss: 0.6370367407798767\n",
      "pred_error: 0.18438683450222015 \t range_loss: 0.6370367407798767\n",
      "pred_error: 0.1839868575334549 \t range_loss: 0.6367654800415039\n",
      "pred_error: 0.183755561709404 \t range_loss: 0.6389285326004028\n",
      "pred_error: 0.18397273123264313 \t range_loss: 0.6373652815818787\n",
      "pred_error: 0.1839727759361267 \t range_loss: 0.6373652815818787\n",
      "pred_error: 0.18397271633148193 \t range_loss: 0.6373652815818787\n",
      "pred_error: 0.18383470177650452 \t range_loss: 0.6395949721336365\n",
      "pred_error: 0.18389122188091278 \t range_loss: 0.6383762955665588\n",
      "pred_error: 0.18389135599136353 \t range_loss: 0.6383762955665588\n",
      "pred_error: 0.1838601678609848 \t range_loss: 0.6399086117744446\n",
      "pred_error: 0.1838933527469635 \t range_loss: 0.640139639377594\n",
      "pred_error: 0.18389327824115753 \t range_loss: 0.640139639377594\n",
      "pred_error: 0.1839979588985443 \t range_loss: 0.637313187122345\n",
      "pred_error: 0.183818057179451 \t range_loss: 0.6366755366325378\n",
      "pred_error: 0.18381808698177338 \t range_loss: 0.6366755366325378\n",
      "pred_error: 0.18381808698177338 \t range_loss: 0.6366755366325378\n",
      "pred_error: 0.18390432000160217 \t range_loss: 0.6383281946182251\n",
      "pred_error: 0.18414990603923798 \t range_loss: 0.6395436525344849\n",
      "pred_error: 0.18414995074272156 \t range_loss: 0.6395436525344849\n",
      "pred_error: 0.18381907045841217 \t range_loss: 0.640453577041626\n",
      "pred_error: 0.18401750922203064 \t range_loss: 0.6397792100906372\n",
      "pred_error: 0.1840173304080963 \t range_loss: 0.6397792100906372\n",
      "pred_error: 0.18401199579238892 \t range_loss: 0.6374999284744263\n",
      "pred_error: 0.1840120404958725 \t range_loss: 0.6374999284744263\n",
      "pred_error: 0.18399864435195923 \t range_loss: 0.6382529139518738\n",
      "pred_error: 0.1837204545736313 \t range_loss: 0.6387977600097656\n",
      "pred_error: 0.1837204247713089 \t range_loss: 0.6387977600097656\n",
      "pred_error: 0.18411685526371002 \t range_loss: 0.6383257508277893\n",
      "pred_error: 0.18417462706565857 \t range_loss: 0.6389368772506714\n",
      "pred_error: 0.1841743141412735 \t range_loss: 0.6389368772506714\n",
      "pred_error: 0.18412071466445923 \t range_loss: 0.6373488903045654\n",
      "pred_error: 0.1843538135290146 \t range_loss: 0.6352794766426086\n",
      "pred_error: 0.18436448276042938 \t range_loss: 0.6351544260978699\n",
      "pred_error: 0.18392635881900787 \t range_loss: 0.6345798373222351\n",
      "pred_error: 0.1839257776737213 \t range_loss: 0.6345798373222351\n",
      "pred_error: 0.1845516413450241 \t range_loss: 0.6369335651397705\n",
      "pred_error: 0.18455053865909576 \t range_loss: 0.6369335651397705\n",
      "pred_error: 0.1843968778848648 \t range_loss: 0.6370194554328918\n",
      "pred_error: 0.18446172773838043 \t range_loss: 0.6383222341537476\n",
      "pred_error: 0.18452687561511993 \t range_loss: 0.6411891579627991\n",
      "pred_error: 0.1840916872024536 \t range_loss: 0.6388164758682251\n",
      "pred_error: 0.18409176170825958 \t range_loss: 0.6388164758682251\n",
      "pred_error: 0.1846170425415039 \t range_loss: 0.6342204809188843\n",
      "pred_error: 0.1845359057188034 \t range_loss: 0.6343566179275513\n",
      "BEST LOSS: 2.4732513\n",
      "==== Model: block10_cob_activation_norm  in Layer: 10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 09:56:15,311 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 09:59:11,733 model.rs:1246 value (24384) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 09:59:11,741 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 09:59:43,510 model.rs:1246 value (24384) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 09:59:43,529 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 09:59:43,563 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 09:59:43,591 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 09:59:43,606 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error   | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.00000025132047 | -0.00004208088 | 0.0006685257 | -0.0006670952 | 0.0000545973   | 0.00004208088    | 0.0006685257  | 0             | 0.000000006134236  | 0.0000973302       | 0.0013384807           |\n",
      "+------------------+----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 539648 64 [-826962, 1168916] 1 [16]\n",
      "===============================\n",
      "==== Model: block10_cob_activation_norm_teleported  in Layer: 10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 10:00:10,155 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 10:03:21,848 model.rs:1246 value (181504) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 10:03:21,874 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 10:03:46,462 model.rs:1246 value (181504) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 10:03:46,468 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 10:03:46,487 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 10:03:46,507 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 10:03:46,522 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+----------------+----------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error     | median_error   | max_error     | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+----------------+----------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.000001674582 | 0.000011652708 | 0.00060617924 | -0.0007240176 | 0.000057442117 | 0.000011652708   | 0.0007240176  | 0             | 0.000000006675087  | 0.00004674946      | 0.0012179666           |\n",
      "+----------------+----------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 539647 64 [-559636, 1155260] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 11 , \t  activation_stats: {'relu_1': {'norm': tensor(872.8521), 'max': tensor(4.7341), 'min': tensor(-9.7593), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(14.4935)\n",
      "pred_error: 0.15531477332115173 \t range_loss: 0.9880701303482056\n",
      "pred_error: 0.15531526505947113 \t range_loss: 0.9880701303482056\n",
      "Step: 0 \t Loss: 2.525686502456665\n",
      "Step: 1 \t Loss: 2.474626064300537\n",
      "Step: 2 \t Loss: 2.4679782390594482\n",
      "Step: 3 \t Loss: 2.444410562515259\n",
      "pred_error: 0.1549733728170395 \t range_loss: 0.894676148891449\n",
      "Step: 4 \t Loss: 2.4367077350616455\n",
      "Step: 5 \t Loss: 2.4249472618103027\n",
      "pred_error: 0.15481989085674286 \t range_loss: 0.8767462372779846\n",
      "Step: 6 \t Loss: 2.416201114654541\n",
      "Step: 7 \t Loss: 2.4106078147888184\n",
      "Step: 8 \t Loss: 2.4066359996795654\n",
      "pred_error: 0.15465199947357178 \t range_loss: 0.8601159453392029\n",
      "Step: 9 \t Loss: 2.4011476039886475\n",
      "Step: 10 \t Loss: 2.395451545715332\n",
      "pred_error: 0.15453170239925385 \t range_loss: 0.8501340746879578\n",
      "Step: 11 \t Loss: 2.3877065181732178\n",
      "Step: 12 \t Loss: 2.3836984634399414\n",
      "Step: 13 \t Loss: 2.378416061401367\n",
      "Step: 14 \t Loss: 2.375037431716919\n",
      "pred_error: 0.15427950024604797 \t range_loss: 0.8322439789772034\n",
      "Step: 15 \t Loss: 2.370741367340088\n",
      "Step: 16 \t Loss: 2.3659780025482178\n",
      "Step: 17 \t Loss: 2.3628358840942383\n",
      "pred_error: 0.15416084229946136 \t range_loss: 0.8212299346923828\n",
      "pred_error: 0.1541607826948166 \t range_loss: 0.8212299346923828\n",
      "Step: 18 \t Loss: 2.3562092781066895\n",
      "Step: 19 \t Loss: 2.3541347980499268\n",
      "Step: 20 \t Loss: 2.3490843772888184\n",
      "Step: 21 \t Loss: 2.348015785217285\n",
      "Step: 22 \t Loss: 2.341093063354492\n",
      "Step: 23 \t Loss: 2.3360941410064697\n",
      "pred_error: 0.1540834903717041 \t range_loss: 0.7952591776847839\n",
      "Step: 24 \t Loss: 2.3321681022644043\n",
      "Step: 25 \t Loss: 2.327312707901001\n",
      "pred_error: 0.15401436388492584 \t range_loss: 0.7871691584587097\n",
      "Step: 26 \t Loss: 2.324148416519165\n",
      "Step: 27 \t Loss: 2.323272228240967\n",
      "Step: 28 \t Loss: 2.3195016384124756\n",
      "pred_error: 0.15401510894298553 \t range_loss: 0.7793504595756531\n",
      "pred_error: 0.1540149301290512 \t range_loss: 0.7793504595756531\n",
      "pred_error: 0.15401507914066315 \t range_loss: 0.7793504595756531\n",
      "Step: 29 \t Loss: 2.316931962966919\n",
      "Step: 30 \t Loss: 2.314906358718872\n",
      "pred_error: 0.1539706289768219 \t range_loss: 0.7752012610435486\n",
      "Step: 31 \t Loss: 2.3124916553497314\n",
      "Step: 32 \t Loss: 2.30985951423645\n",
      "pred_error: 0.15393540263175964 \t range_loss: 0.7704997658729553\n",
      "Step: 33 \t Loss: 2.307495594024658\n",
      "pred_error: 0.15393388271331787 \t range_loss: 0.7681532502174377\n",
      "Step: 34 \t Loss: 2.306882858276367\n",
      "pred_error: 0.1538834422826767 \t range_loss: 0.7680482864379883\n",
      "pred_error: 0.15388324856758118 \t range_loss: 0.7680482864379883\n",
      "Step: 35 \t Loss: 2.3025875091552734\n",
      "pred_error: 0.15403473377227783 \t range_loss: 0.7622388005256653\n",
      "Step: 36 \t Loss: 2.3012869358062744\n",
      "Step: 37 \t Loss: 2.300384283065796\n",
      "Step: 38 \t Loss: 2.2988507747650146\n",
      "Step: 39 \t Loss: 2.296294689178467\n",
      "pred_error: 0.15398231148719788 \t range_loss: 0.7564719319343567\n",
      "Step: 40 \t Loss: 2.2946977615356445\n",
      "Step: 41 \t Loss: 2.291234016418457\n",
      "pred_error: 0.15395021438598633 \t range_loss: 0.751727283000946\n",
      "Step: 42 \t Loss: 2.2897744178771973\n",
      "Step: 43 \t Loss: 2.285428047180176\n",
      "Step: 44 \t Loss: 2.2820703983306885\n",
      "Step: 45 \t Loss: 2.2812883853912354\n",
      "pred_error: 0.1539851725101471 \t range_loss: 0.7414541244506836\n",
      "pred_error: 0.1539834439754486 \t range_loss: 0.7414541244506836\n",
      "pred_error: 0.15398363769054413 \t range_loss: 0.7414541244506836\n",
      "Step: 46 \t Loss: 2.279719114303589\n",
      "Step: 47 \t Loss: 2.2789247035980225\n",
      "pred_error: 0.15398725867271423 \t range_loss: 0.7390495538711548\n",
      "pred_error: 0.1541052609682083 \t range_loss: 0.7382670640945435\n",
      "Step: 50 \t Loss: 2.2767701148986816\n",
      "Step: 51 \t Loss: 2.273446559906006\n",
      "pred_error: 0.15410669147968292 \t range_loss: 0.7323763370513916\n",
      "Step: 52 \t Loss: 2.2699294090270996\n",
      "Step: 54 \t Loss: 2.2686197757720947\n",
      "pred_error: 0.1540706306695938 \t range_loss: 0.7279154658317566\n",
      "pred_error: 0.15407021343708038 \t range_loss: 0.7279154658317566\n",
      "Step: 55 \t Loss: 2.2677268981933594\n",
      "Step: 56 \t Loss: 2.266541004180908\n",
      "pred_error: 0.1540462076663971 \t range_loss: 0.7260732650756836\n",
      "Step: 57 \t Loss: 2.2658567428588867\n",
      "pred_error: 0.15413089096546173 \t range_loss: 0.7246941924095154\n",
      "Step: 59 \t Loss: 2.2646431922912598\n",
      "Step: 60 \t Loss: 2.2633676528930664\n",
      "pred_error: 0.1541164070367813 \t range_loss: 0.722203254699707\n",
      "Step: 61 \t Loss: 2.2627995014190674\n",
      "Step: 62 \t Loss: 2.260488748550415\n",
      "pred_error: 0.15415896475315094 \t range_loss: 0.7188990712165833\n",
      "pred_error: 0.15415896475315094 \t range_loss: 0.7188990712165833\n",
      "Step: 63 \t Loss: 2.2589101791381836\n",
      "pred_error: 0.15412335097789764 \t range_loss: 0.7176777124404907\n",
      "Step: 64 \t Loss: 2.2584269046783447\n",
      "pred_error: 0.1541690081357956 \t range_loss: 0.7167349457740784\n",
      "Step: 65 \t Loss: 2.255998373031616\n",
      "Step: 66 \t Loss: 2.254014015197754\n",
      "Step: 67 \t Loss: 2.253082275390625\n",
      "Step: 68 \t Loss: 2.2517025470733643\n",
      "Step: 70 \t Loss: 2.2504749298095703\n",
      "pred_error: 0.15410937368869781 \t range_loss: 0.7093760371208191\n",
      "pred_error: 0.15430404245853424 \t range_loss: 0.7074801325798035\n",
      "pred_error: 0.15430404245853424 \t range_loss: 0.7074801325798035\n",
      "pred_error: 0.15430381894111633 \t range_loss: 0.7074801325798035\n",
      "Step: 73 \t Loss: 2.249295234680176\n",
      "Step: 74 \t Loss: 2.2483370304107666\n",
      "Step: 75 \t Loss: 2.2474048137664795\n",
      "Step: 76 \t Loss: 2.246579170227051\n",
      "pred_error: 0.1542896330356598 \t range_loss: 0.7036718130111694\n",
      "Step: 77 \t Loss: 2.245373487472534\n",
      "Step: 78 \t Loss: 2.242396593093872\n",
      "Step: 79 \t Loss: 2.2398993968963623\n",
      "pred_error: 0.15415345132350922 \t range_loss: 0.6984216570854187\n",
      "Step: 81 \t Loss: 2.239231586456299\n",
      "Step: 82 \t Loss: 2.238234281539917\n",
      "pred_error: 0.1542481929063797 \t range_loss: 0.6960448622703552\n",
      "Step: 85 \t Loss: 2.2381162643432617\n",
      "pred_error: 0.1543804109096527 \t range_loss: 0.694307267665863\n",
      "Step: 86 \t Loss: 2.2377424240112305\n",
      "pred_error: 0.15442055463790894 \t range_loss: 0.6935380101203918\n",
      "pred_error: 0.15441960096359253 \t range_loss: 0.6935380101203918\n",
      "pred_error: 0.15442056953907013 \t range_loss: 0.6935380101203918\n",
      "Step: 87 \t Loss: 2.236969232559204\n",
      "pred_error: 0.15434780716896057 \t range_loss: 0.6934921741485596\n",
      "Step: 88 \t Loss: 2.236919641494751\n",
      "pred_error: 0.15436269342899323 \t range_loss: 0.6932917237281799\n",
      "pred_error: 0.15436266362667084 \t range_loss: 0.6932917237281799\n",
      "Step: 89 \t Loss: 2.2359912395477295\n",
      "pred_error: 0.15438690781593323 \t range_loss: 0.6921172738075256\n",
      "Step: 90 \t Loss: 2.2345383167266846\n",
      "Step: 91 \t Loss: 2.2332401275634766\n",
      "pred_error: 0.15426264703273773 \t range_loss: 0.6906148195266724\n",
      "Step: 92 \t Loss: 2.232193946838379\n",
      "Step: 94 \t Loss: 2.2306530475616455\n",
      "Step: 95 \t Loss: 2.2285313606262207\n",
      "pred_error: 0.1541629582643509 \t range_loss: 0.6869010329246521\n",
      "pred_error: 0.15407013893127441 \t range_loss: 0.6881912350654602\n",
      "Step: 100 \t Loss: 2.2279000282287598\n",
      "pred_error: 0.15438681840896606 \t range_loss: 0.6840317845344543\n",
      "Step: 102 \t Loss: 2.2260220050811768\n",
      "Step: 103 \t Loss: 2.224118947982788\n",
      "Step: 104 \t Loss: 2.2233009338378906\n",
      "Step: 106 \t Loss: 2.22312068939209\n",
      "Step: 107 \t Loss: 2.221675157546997\n",
      "pred_error: 0.1543787717819214 \t range_loss: 0.6778819561004639\n",
      "pred_error: 0.1545788049697876 \t range_loss: 0.6783429384231567\n",
      "pred_error: 0.15457884967327118 \t range_loss: 0.6783429384231567\n",
      "Step: 112 \t Loss: 2.221493721008301\n",
      "Step: 113 \t Loss: 2.219724178314209\n",
      "Step: 114 \t Loss: 2.2190136909484863\n",
      "pred_error: 0.15426281094551086 \t range_loss: 0.6763888597488403\n",
      "pred_error: 0.15434888005256653 \t range_loss: 0.6770482063293457\n",
      "pred_error: 0.15434879064559937 \t range_loss: 0.6770482063293457\n",
      "pred_error: 0.15434861183166504 \t range_loss: 0.6770482063293457\n",
      "Step: 117 \t Loss: 2.2180123329162598\n",
      "Step: 118 \t Loss: 2.2155981063842773\n",
      "pred_error: 0.15425139665603638 \t range_loss: 0.6730824708938599\n",
      "Step: 119 \t Loss: 2.2154579162597656\n",
      "pred_error: 0.1543310582637787 \t range_loss: 0.6726487874984741\n",
      "pred_error: 0.15433070063591003 \t range_loss: 0.6726487874984741\n",
      "pred_error: 0.1546378880739212 \t range_loss: 0.6710473299026489\n",
      "pred_error: 0.15463775396347046 \t range_loss: 0.6710473299026489\n",
      "Step: 125 \t Loss: 2.214430332183838\n",
      "Step: 129 \t Loss: 2.212852954864502\n",
      "Step: 130 \t Loss: 2.2122247219085693\n",
      "pred_error: 0.1546465903520584 \t range_loss: 0.6670774221420288\n",
      "pred_error: 0.15469975769519806 \t range_loss: 0.6660428643226624\n",
      "Step: 133 \t Loss: 2.211088180541992\n",
      "pred_error: 0.15467199683189392 \t range_loss: 0.6659044027328491\n",
      "pred_error: 0.15466366708278656 \t range_loss: 0.6652919054031372\n",
      "pred_error: 0.15467241406440735 \t range_loss: 0.6659650802612305\n",
      "pred_error: 0.15473902225494385 \t range_loss: 0.6637530326843262\n",
      "Step: 139 \t Loss: 2.2104878425598145\n",
      "Step: 140 \t Loss: 2.2088887691497803\n",
      "Step: 141 \t Loss: 2.207528591156006\n",
      "pred_error: 0.15473607182502747 \t range_loss: 0.6603653430938721\n",
      "Step: 144 \t Loss: 2.2055866718292236\n",
      "pred_error: 0.15482598543167114 \t range_loss: 0.660103440284729\n",
      "pred_error: 0.15472306311130524 \t range_loss: 0.6598556041717529\n",
      "pred_error: 0.1545945703983307 \t range_loss: 0.6606174111366272\n",
      "Step: 151 \t Loss: 2.205350875854492\n",
      "pred_error: 0.15454895794391632 \t range_loss: 0.6598632335662842\n",
      "Step: 152 \t Loss: 2.2047901153564453\n",
      "pred_error: 0.15460950136184692 \t range_loss: 0.6586951613426208\n",
      "Step: 153 \t Loss: 2.203333854675293\n",
      "pred_error: 0.15450158715248108 \t range_loss: 0.6585392951965332\n",
      "Step: 157 \t Loss: 2.2018699645996094\n",
      "pred_error: 0.15466924011707306 \t range_loss: 0.6551799178123474\n",
      "pred_error: 0.15468071401119232 \t range_loss: 0.6562961339950562\n",
      "pred_error: 0.15468041598796844 \t range_loss: 0.6562961339950562\n",
      "Step: 163 \t Loss: 2.2018141746520996\n",
      "pred_error: 0.15486009418964386 \t range_loss: 0.6543360352516174\n",
      "pred_error: 0.15494754910469055 \t range_loss: 0.6536285281181335\n",
      "Step: 166 \t Loss: 2.2003865242004395\n",
      "Step: 167 \t Loss: 2.1991140842437744\n",
      "Step: 168 \t Loss: 2.1987104415893555\n",
      "pred_error: 0.15474757552146912 \t range_loss: 0.6512298583984375\n",
      "Step: 175 \t Loss: 2.198183536529541\n",
      "pred_error: 0.15490718185901642 \t range_loss: 0.6491119265556335\n",
      "pred_error: 0.15506881475448608 \t range_loss: 0.6497806906700134\n",
      "Step: 179 \t Loss: 2.1974916458129883\n",
      "pred_error: 0.15497946739196777 \t range_loss: 0.6476946473121643\n",
      "Step: 180 \t Loss: 2.1957807540893555\n",
      "Step: 181 \t Loss: 2.195758104324341\n",
      "pred_error: 0.1550108641386032 \t range_loss: 0.6498469114303589\n",
      "pred_error: 0.1550111472606659 \t range_loss: 0.6498469114303589\n",
      "pred_error: 0.15498527884483337 \t range_loss: 0.6484485268592834\n",
      "pred_error: 0.15498508512973785 \t range_loss: 0.6484485268592834\n",
      "pred_error: 0.15498502552509308 \t range_loss: 0.6484485268592834\n",
      "pred_error: 0.1549658328294754 \t range_loss: 0.647963285446167\n",
      "pred_error: 0.15495067834854126 \t range_loss: 0.647697925567627\n",
      "Step: 188 \t Loss: 2.1951987743377686\n",
      "Step: 189 \t Loss: 2.194000244140625\n",
      "pred_error: 0.15485599637031555 \t range_loss: 0.6461549401283264\n",
      "pred_error: 0.15485763549804688 \t range_loss: 0.6466653943061829\n",
      "pred_error: 0.15485788881778717 \t range_loss: 0.6466653943061829\n",
      "pred_error: 0.15489670634269714 \t range_loss: 0.6467245817184448\n",
      "Step: 193 \t Loss: 2.193946361541748\n",
      "Step: 194 \t Loss: 2.193847179412842\n",
      "pred_error: 0.1551298052072525 \t range_loss: 0.6437792181968689\n",
      "Step: 201 \t Loss: 2.1926941871643066\n",
      "pred_error: 0.15501223504543304 \t range_loss: 0.6428652405738831\n",
      "Step: 204 \t Loss: 2.191896915435791\n",
      "Step: 206 \t Loss: 2.191744565963745\n",
      "pred_error: 0.15508531033992767 \t range_loss: 0.6408942341804504\n",
      "pred_error: 0.15542544424533844 \t range_loss: 0.6403964161872864\n",
      "pred_error: 0.1553361713886261 \t range_loss: 0.6408395171165466\n",
      "pred_error: 0.1552775502204895 \t range_loss: 0.6395334601402283\n",
      "Step: 215 \t Loss: 2.1894185543060303\n",
      "Step: 216 \t Loss: 2.189349412918091\n",
      "pred_error: 0.15499438345432281 \t range_loss: 0.6394067406654358\n",
      "pred_error: 0.15499453246593475 \t range_loss: 0.6394067406654358\n",
      "pred_error: 0.1552591323852539 \t range_loss: 0.6398083567619324\n",
      "pred_error: 0.15525957942008972 \t range_loss: 0.6398083567619324\n",
      "Step: 224 \t Loss: 2.1891679763793945\n",
      "Step: 226 \t Loss: 2.1890053749084473\n",
      "pred_error: 0.15510571002960205 \t range_loss: 0.6392242312431335\n",
      "pred_error: 0.15510642528533936 \t range_loss: 0.6392242312431335\n",
      "pred_error: 0.15510578453540802 \t range_loss: 0.6392242312431335\n",
      "Step: 228 \t Loss: 2.1884822845458984\n",
      "pred_error: 0.15516486763954163 \t range_loss: 0.6368333101272583\n",
      "Step: 229 \t Loss: 2.1871488094329834\n",
      "pred_error: 0.15505604445934296 \t range_loss: 0.6365876197814941\n",
      "pred_error: 0.1552540361881256 \t range_loss: 0.637041449546814\n",
      "pred_error: 0.15542694926261902 \t range_loss: 0.6372559070587158\n",
      "pred_error: 0.15542584657669067 \t range_loss: 0.6372559070587158\n",
      "pred_error: 0.155451700091362 \t range_loss: 0.6368734240531921\n",
      "pred_error: 0.155490905046463 \t range_loss: 0.6366860866546631\n",
      "pred_error: 0.15531441569328308 \t range_loss: 0.6360846161842346\n",
      "pred_error: 0.15531347692012787 \t range_loss: 0.6360846161842346\n",
      "pred_error: 0.15511709451675415 \t range_loss: 0.636655867099762\n",
      "Step: 238 \t Loss: 2.186450719833374\n",
      "Step: 239 \t Loss: 2.1855645179748535\n",
      "pred_error: 0.15539643168449402 \t range_loss: 0.6351578831672668\n",
      "pred_error: 0.1553967297077179 \t range_loss: 0.6351578831672668\n",
      "pred_error: 0.15527093410491943 \t range_loss: 0.6348662376403809\n",
      "pred_error: 0.15520016849040985 \t range_loss: 0.6361435651779175\n",
      "pred_error: 0.1551867425441742 \t range_loss: 0.6347084641456604\n",
      "Step: 253 \t Loss: 2.1845836639404297\n",
      "Step: 254 \t Loss: 2.1839356422424316\n",
      "pred_error: 0.15503181517124176 \t range_loss: 0.6336140632629395\n",
      "pred_error: 0.15503226220607758 \t range_loss: 0.6336140632629395\n",
      "pred_error: 0.15517354011535645 \t range_loss: 0.6333389282226562\n",
      "pred_error: 0.15513591468334198 \t range_loss: 0.6337890625\n",
      "pred_error: 0.1553044319152832 \t range_loss: 0.6325623393058777\n",
      "pred_error: 0.15517288446426392 \t range_loss: 0.6328452229499817\n",
      "pred_error: 0.15517236292362213 \t range_loss: 0.6328452229499817\n",
      "Step: 260 \t Loss: 2.1837916374206543\n",
      "pred_error: 0.15512822568416595 \t range_loss: 0.6328325867652893\n",
      "pred_error: 0.15526676177978516 \t range_loss: 0.6316177845001221\n",
      "Step: 270 \t Loss: 2.1836109161376953\n",
      "pred_error: 0.1553935408592224 \t range_loss: 0.6321885585784912\n",
      "pred_error: 0.15570630133152008 \t range_loss: 0.629533052444458\n",
      "pred_error: 0.1554664820432663 \t range_loss: 0.629285454750061\n",
      "Step: 283 \t Loss: 2.1826767921447754\n",
      "Step: 284 \t Loss: 2.182192802429199\n",
      "Step: 285 \t Loss: 2.1812655925750732\n",
      "pred_error: 0.1552855670452118 \t range_loss: 0.6289119720458984\n",
      "pred_error: 0.15528550744056702 \t range_loss: 0.6289119720458984\n",
      "pred_error: 0.15528538823127747 \t range_loss: 0.6289119720458984\n",
      "pred_error: 0.1552855670452118 \t range_loss: 0.6289119720458984\n",
      "pred_error: 0.1553444117307663 \t range_loss: 0.630035936832428\n",
      "pred_error: 0.15527650713920593 \t range_loss: 0.6285101175308228\n",
      "Step: 290 \t Loss: 2.1793630123138428\n",
      "pred_error: 0.1550692319869995 \t range_loss: 0.6302062273025513\n",
      "pred_error: 0.15530887246131897 \t range_loss: 0.6281331777572632\n",
      "pred_error: 0.15547344088554382 \t range_loss: 0.6284279227256775\n",
      "pred_error: 0.15548351407051086 \t range_loss: 0.6272457242012024\n",
      "pred_error: 0.15548408031463623 \t range_loss: 0.6272457242012024\n",
      "pred_error: 0.1558282971382141 \t range_loss: 0.6270753145217896\n",
      "pred_error: 0.15586140751838684 \t range_loss: 0.6256284117698669\n",
      "pred_error: 0.15563735365867615 \t range_loss: 0.6251497864723206\n",
      "pred_error: 0.1557743102312088 \t range_loss: 0.6262003183364868\n",
      "pred_error: 0.15547645092010498 \t range_loss: 0.6251741647720337\n",
      "Step: 318 \t Loss: 2.178649425506592\n",
      "pred_error: 0.15546801686286926 \t range_loss: 0.6239562630653381\n",
      "pred_error: 0.15568414330482483 \t range_loss: 0.6237461566925049\n",
      "pred_error: 0.1555924415588379 \t range_loss: 0.6241827607154846\n",
      "pred_error: 0.1555914580821991 \t range_loss: 0.6241827607154846\n",
      "pred_error: 0.15559197962284088 \t range_loss: 0.6241827607154846\n",
      "pred_error: 0.1556602418422699 \t range_loss: 0.6247590184211731\n",
      "pred_error: 0.15576912462711334 \t range_loss: 0.6242359280586243\n",
      "pred_error: 0.15555597841739655 \t range_loss: 0.623688817024231\n",
      "pred_error: 0.1555701643228531 \t range_loss: 0.6241387128829956\n",
      "pred_error: 0.15600267052650452 \t range_loss: 0.6230236291885376\n",
      "pred_error: 0.15593726933002472 \t range_loss: 0.622420072555542\n",
      "pred_error: 0.15578119456768036 \t range_loss: 0.6229967474937439\n",
      "pred_error: 0.15578094124794006 \t range_loss: 0.6229967474937439\n",
      "pred_error: 0.1557198315858841 \t range_loss: 0.6225061416625977\n",
      "Step: 338 \t Loss: 2.1770129203796387\n",
      "pred_error: 0.15552382171154022 \t range_loss: 0.6217728853225708\n",
      "pred_error: 0.15548734366893768 \t range_loss: 0.6234117150306702\n",
      "pred_error: 0.15557175874710083 \t range_loss: 0.6244261264801025\n",
      "pred_error: 0.15579912066459656 \t range_loss: 0.6210103034973145\n",
      "pred_error: 0.155799001455307 \t range_loss: 0.6210103034973145\n",
      "pred_error: 0.15582525730133057 \t range_loss: 0.6205484867095947\n",
      "Step: 351 \t Loss: 2.1769113540649414\n",
      "Step: 352 \t Loss: 2.1755776405334473\n",
      "pred_error: 0.15543949604034424 \t range_loss: 0.6218941807746887\n",
      "pred_error: 0.15543948113918304 \t range_loss: 0.6218941807746887\n",
      "Step: 355 \t Loss: 2.1753504276275635\n",
      "pred_error: 0.1557305008172989 \t range_loss: 0.6201544404029846\n",
      "pred_error: 0.15559598803520203 \t range_loss: 0.620135486125946\n",
      "pred_error: 0.1557265669107437 \t range_loss: 0.6202439665794373\n",
      "pred_error: 0.15565399825572968 \t range_loss: 0.6198186874389648\n",
      "Step: 361 \t Loss: 2.1749284267425537\n",
      "pred_error: 0.15557211637496948 \t range_loss: 0.6195673942565918\n",
      "pred_error: 0.15565520524978638 \t range_loss: 0.6193253993988037\n",
      "pred_error: 0.15576323866844177 \t range_loss: 0.6196938157081604\n",
      "pred_error: 0.15575435757637024 \t range_loss: 0.6196043491363525\n",
      "pred_error: 0.1562620848417282 \t range_loss: 0.6196381449699402\n",
      "pred_error: 0.1560545563697815 \t range_loss: 0.6186714172363281\n",
      "pred_error: 0.15605446696281433 \t range_loss: 0.6186714172363281\n",
      "pred_error: 0.15605467557907104 \t range_loss: 0.6186714172363281\n",
      "pred_error: 0.15584124624729156 \t range_loss: 0.6186622977256775\n",
      "pred_error: 0.15584170818328857 \t range_loss: 0.6186622977256775\n",
      "pred_error: 0.15578413009643555 \t range_loss: 0.6195648908615112\n",
      "pred_error: 0.1557852178812027 \t range_loss: 0.6195648908615112\n",
      "pred_error: 0.15578533709049225 \t range_loss: 0.6195648908615112\n",
      "Step: 375 \t Loss: 2.1747772693634033\n",
      "Step: 376 \t Loss: 2.1745378971099854\n",
      "pred_error: 0.15570726990699768 \t range_loss: 0.6174653172492981\n",
      "pred_error: 0.15592241287231445 \t range_loss: 0.6166548132896423\n",
      "pred_error: 0.15573422610759735 \t range_loss: 0.6173258423805237\n",
      "pred_error: 0.15597723424434662 \t range_loss: 0.6156384944915771\n",
      "pred_error: 0.15587803721427917 \t range_loss: 0.6173038482666016\n",
      "pred_error: 0.15613722801208496 \t range_loss: 0.6160853505134583\n",
      "pred_error: 0.15612713992595673 \t range_loss: 0.6162596940994263\n",
      "pred_error: 0.15612706542015076 \t range_loss: 0.6162596940994263\n",
      "pred_error: 0.15601472556591034 \t range_loss: 0.6162683963775635\n",
      "pred_error: 0.1558683067560196 \t range_loss: 0.6182317733764648\n",
      "pred_error: 0.15611526370048523 \t range_loss: 0.6155555844306946\n",
      "pred_error: 0.15600310266017914 \t range_loss: 0.6163020730018616\n",
      "pred_error: 0.15597397089004517 \t range_loss: 0.6149453520774841\n",
      "Step: 410 \t Loss: 2.1743552684783936\n",
      "Step: 412 \t Loss: 2.1743531227111816\n",
      "pred_error: 0.1559043526649475 \t range_loss: 0.6153095960617065\n",
      "Step: 414 \t Loss: 2.174194812774658\n",
      "pred_error: 0.15604063868522644 \t range_loss: 0.6139795780181885\n",
      "Step: 416 \t Loss: 2.1731514930725098\n",
      "pred_error: 0.155847430229187 \t range_loss: 0.6146761775016785\n",
      "pred_error: 0.15616096556186676 \t range_loss: 0.6148602366447449\n",
      "pred_error: 0.15608085691928864 \t range_loss: 0.6144096255302429\n",
      "pred_error: 0.15600448846817017 \t range_loss: 0.6142325401306152\n",
      "Step: 426 \t Loss: 2.1726107597351074\n",
      "pred_error: 0.15597839653491974 \t range_loss: 0.6152754426002502\n",
      "pred_error: 0.1560007631778717 \t range_loss: 0.6148337721824646\n",
      "pred_error: 0.1561015397310257 \t range_loss: 0.6124039888381958\n",
      "Step: 435 \t Loss: 2.171616315841675\n",
      "Step: 436 \t Loss: 2.1713435649871826\n",
      "pred_error: 0.15595124661922455 \t range_loss: 0.613197922706604\n",
      "pred_error: 0.15589085221290588 \t range_loss: 0.6138870716094971\n",
      "Step: 440 \t Loss: 2.170889139175415\n",
      "pred_error: 0.15581291913986206 \t range_loss: 0.6127580404281616\n",
      "pred_error: 0.15618544816970825 \t range_loss: 0.6127119064331055\n",
      "pred_error: 0.156270831823349 \t range_loss: 0.6126572489738464\n",
      "pred_error: 0.15631023049354553 \t range_loss: 0.6129630208015442\n",
      "pred_error: 0.15597505867481232 \t range_loss: 0.6127132773399353\n",
      "pred_error: 0.1560744047164917 \t range_loss: 0.6112940907478333\n",
      "pred_error: 0.15610027313232422 \t range_loss: 0.6121211647987366\n",
      "pred_error: 0.1562378853559494 \t range_loss: 0.6142835021018982\n",
      "pred_error: 0.15623806416988373 \t range_loss: 0.6142835021018982\n",
      "pred_error: 0.1563207060098648 \t range_loss: 0.6137633323669434\n",
      "pred_error: 0.15608260035514832 \t range_loss: 0.6122732162475586\n",
      "Step: 477 \t Loss: 2.170304775238037\n",
      "Step: 478 \t Loss: 2.168045997619629\n",
      "pred_error: 0.15571416914463043 \t range_loss: 0.6109038591384888\n",
      "pred_error: 0.15601450204849243 \t range_loss: 0.6144483685493469\n",
      "pred_error: 0.15603725612163544 \t range_loss: 0.613884687423706\n",
      "pred_error: 0.15602320432662964 \t range_loss: 0.6115656495094299\n",
      "pred_error: 0.15602323412895203 \t range_loss: 0.6115656495094299\n",
      "pred_error: 0.1560218185186386 \t range_loss: 0.6115656495094299\n",
      "pred_error: 0.15584516525268555 \t range_loss: 0.6121718883514404\n",
      "pred_error: 0.1558457314968109 \t range_loss: 0.6121718883514404\n",
      "pred_error: 0.1559581160545349 \t range_loss: 0.610473096370697\n",
      "pred_error: 0.1559017151594162 \t range_loss: 0.6108742356300354\n",
      "pred_error: 0.1558767408132553 \t range_loss: 0.6114445924758911\n",
      "pred_error: 0.15592148900032043 \t range_loss: 0.6118355393409729\n",
      "pred_error: 0.15592165291309357 \t range_loss: 0.6118355393409729\n",
      "pred_error: 0.15612222254276276 \t range_loss: 0.6113242506980896\n",
      "pred_error: 0.1559954285621643 \t range_loss: 0.6112274527549744\n",
      "pred_error: 0.15609556436538696 \t range_loss: 0.6117532849311829\n",
      "pred_error: 0.15609602630138397 \t range_loss: 0.6117532849311829\n",
      "pred_error: 0.15604154765605927 \t range_loss: 0.6132475733757019\n",
      "pred_error: 0.15628401935100555 \t range_loss: 0.6103125214576721\n",
      "pred_error: 0.15603820979595184 \t range_loss: 0.6116060614585876\n",
      "pred_error: 0.15598849952220917 \t range_loss: 0.6098646521568298\n",
      "pred_error: 0.15611372888088226 \t range_loss: 0.6096563935279846\n",
      "Step: 506 \t Loss: 2.1675243377685547\n",
      "pred_error: 0.15607735514640808 \t range_loss: 0.6090065836906433\n",
      "pred_error: 0.1560533195734024 \t range_loss: 0.6079896688461304\n",
      "pred_error: 0.15629683434963226 \t range_loss: 0.6084936261177063\n",
      "pred_error: 0.1563422977924347 \t range_loss: 0.6093074083328247\n",
      "pred_error: 0.15623238682746887 \t range_loss: 0.6083517670631409\n",
      "pred_error: 0.15623246133327484 \t range_loss: 0.6083517670631409\n",
      "pred_error: 0.15634088218212128 \t range_loss: 0.6076241135597229\n",
      "pred_error: 0.1564612090587616 \t range_loss: 0.6063186526298523\n",
      "pred_error: 0.15674573183059692 \t range_loss: 0.6074649691581726\n",
      "pred_error: 0.15663760900497437 \t range_loss: 0.6068075299263\n",
      "pred_error: 0.1564708799123764 \t range_loss: 0.6063233613967896\n",
      "pred_error: 0.15643589198589325 \t range_loss: 0.608141303062439\n",
      "pred_error: 0.1565348356962204 \t range_loss: 0.6077601909637451\n",
      "pred_error: 0.15650996565818787 \t range_loss: 0.6082755923271179\n",
      "pred_error: 0.15658777952194214 \t range_loss: 0.610531210899353\n",
      "pred_error: 0.15635260939598083 \t range_loss: 0.6076627969741821\n",
      "pred_error: 0.15641531348228455 \t range_loss: 0.6081032156944275\n",
      "pred_error: 0.15641529858112335 \t range_loss: 0.6081032156944275\n",
      "pred_error: 0.15645048022270203 \t range_loss: 0.607784628868103\n",
      "pred_error: 0.15647883713245392 \t range_loss: 0.6080604195594788\n",
      "pred_error: 0.156478613615036 \t range_loss: 0.6080604195594788\n",
      "pred_error: 0.15654581785202026 \t range_loss: 0.6088021993637085\n",
      "pred_error: 0.15650543570518494 \t range_loss: 0.6069638133049011\n",
      "pred_error: 0.15624606609344482 \t range_loss: 0.6074728965759277\n",
      "pred_error: 0.15620861947536469 \t range_loss: 0.6064835786819458\n",
      "pred_error: 0.15620861947536469 \t range_loss: 0.6064835786819458\n",
      "pred_error: 0.15621690452098846 \t range_loss: 0.6069047451019287\n",
      "pred_error: 0.1563163697719574 \t range_loss: 0.6069574356079102\n",
      "pred_error: 0.15631631016731262 \t range_loss: 0.6069574356079102\n",
      "pred_error: 0.15650218725204468 \t range_loss: 0.6065618395805359\n",
      "pred_error: 0.15650980174541473 \t range_loss: 0.6070057153701782\n",
      "pred_error: 0.15650959312915802 \t range_loss: 0.6070057153701782\n",
      "pred_error: 0.156509131193161 \t range_loss: 0.6070057153701782\n",
      "pred_error: 0.1564193218946457 \t range_loss: 0.6072457432746887\n",
      "pred_error: 0.15641865134239197 \t range_loss: 0.6072457432746887\n",
      "pred_error: 0.15640097856521606 \t range_loss: 0.6057041883468628\n",
      "pred_error: 0.15640084445476532 \t range_loss: 0.6057041883468628\n",
      "pred_error: 0.1563420593738556 \t range_loss: 0.6068976521492004\n",
      "pred_error: 0.15615199506282806 \t range_loss: 0.6062479615211487\n",
      "pred_error: 0.1562330275774002 \t range_loss: 0.6074406504631042\n",
      "pred_error: 0.15633994340896606 \t range_loss: 0.6080194711685181\n",
      "pred_error: 0.15631753206253052 \t range_loss: 0.6055736541748047\n",
      "Step: 596 \t Loss: 2.167081117630005\n",
      "pred_error: 0.1561800241470337 \t range_loss: 0.6052809357643127\n",
      "Step: 597 \t Loss: 2.166771173477173\n",
      "pred_error: 0.1562870442867279 \t range_loss: 0.6067425608634949\n",
      "pred_error: 0.15633976459503174 \t range_loss: 0.6055224537849426\n",
      "pred_error: 0.15634796023368835 \t range_loss: 0.6047032475471497\n",
      "pred_error: 0.15644724667072296 \t range_loss: 0.6037960648536682\n",
      "pred_error: 0.15648028254508972 \t range_loss: 0.6062746644020081\n",
      "pred_error: 0.1564800888299942 \t range_loss: 0.6052441596984863\n",
      "pred_error: 0.1564430296421051 \t range_loss: 0.6043583154678345\n",
      "pred_error: 0.15644268691539764 \t range_loss: 0.6043583154678345\n",
      "pred_error: 0.1564430296421051 \t range_loss: 0.6043583154678345\n",
      "pred_error: 0.156611368060112 \t range_loss: 0.6030151844024658\n",
      "Step: 629 \t Loss: 2.1663572788238525\n",
      "pred_error: 0.156337708234787 \t range_loss: 0.6062784194946289\n",
      "pred_error: 0.15633784234523773 \t range_loss: 0.6062784194946289\n",
      "pred_error: 0.15656448900699615 \t range_loss: 0.6035038232803345\n",
      "pred_error: 0.15638242661952972 \t range_loss: 0.6036741137504578\n",
      "pred_error: 0.15635254979133606 \t range_loss: 0.6033350825309753\n",
      "pred_error: 0.15635240077972412 \t range_loss: 0.6033350825309753\n",
      "pred_error: 0.15639199316501617 \t range_loss: 0.6026833057403564\n",
      "Step: 638 \t Loss: 2.165585994720459\n",
      "pred_error: 0.15623138844966888 \t range_loss: 0.6032708883285522\n",
      "pred_error: 0.15623153746128082 \t range_loss: 0.6032708883285522\n",
      "pred_error: 0.15625961124897003 \t range_loss: 0.6031063795089722\n",
      "pred_error: 0.1563289314508438 \t range_loss: 0.6026740670204163\n",
      "pred_error: 0.15632814168930054 \t range_loss: 0.6026740670204163\n",
      "pred_error: 0.15618617832660675 \t range_loss: 0.6041876673698425\n",
      "pred_error: 0.15643373131752014 \t range_loss: 0.6033537983894348\n",
      "pred_error: 0.1563478410243988 \t range_loss: 0.6046450734138489\n",
      "Step: 653 \t Loss: 2.1652674674987793\n",
      "pred_error: 0.15619124472141266 \t range_loss: 0.6036948561668396\n",
      "pred_error: 0.15619122982025146 \t range_loss: 0.6036948561668396\n",
      "pred_error: 0.15657316148281097 \t range_loss: 0.6040909290313721\n",
      "pred_error: 0.1566234976053238 \t range_loss: 0.603294849395752\n",
      "pred_error: 0.15653744339942932 \t range_loss: 0.6027209758758545\n",
      "pred_error: 0.15655891597270966 \t range_loss: 0.6037986278533936\n",
      "pred_error: 0.15655885636806488 \t range_loss: 0.6037986278533936\n",
      "pred_error: 0.15655715763568878 \t range_loss: 0.6037986278533936\n",
      "pred_error: 0.15658168494701385 \t range_loss: 0.6027866005897522\n",
      "pred_error: 0.15643079578876495 \t range_loss: 0.6021307110786438\n",
      "pred_error: 0.15660256147384644 \t range_loss: 0.6051909327507019\n",
      "pred_error: 0.15658189356327057 \t range_loss: 0.6032546758651733\n",
      "pred_error: 0.1564464420080185 \t range_loss: 0.6026573777198792\n",
      "pred_error: 0.1564473956823349 \t range_loss: 0.6026573777198792\n",
      "pred_error: 0.15640020370483398 \t range_loss: 0.604134202003479\n",
      "pred_error: 0.1564532220363617 \t range_loss: 0.6025568246841431\n",
      "pred_error: 0.15645304322242737 \t range_loss: 0.6025568246841431\n",
      "Step: 682 \t Loss: 2.1651101112365723\n",
      "pred_error: 0.15643206238746643 \t range_loss: 0.6018173694610596\n",
      "pred_error: 0.15632791817188263 \t range_loss: 0.6023923754692078\n",
      "pred_error: 0.15632790327072144 \t range_loss: 0.6023923754692078\n",
      "pred_error: 0.1563279777765274 \t range_loss: 0.6023923754692078\n",
      "Step: 690 \t Loss: 2.164597511291504\n",
      "Step: 691 \t Loss: 2.1644628047943115\n",
      "pred_error: 0.15642321109771729 \t range_loss: 0.6015811562538147\n",
      "pred_error: 0.1563108116388321 \t range_loss: 0.6023443341255188\n",
      "pred_error: 0.15634682774543762 \t range_loss: 0.6028982400894165\n",
      "pred_error: 0.156523197889328 \t range_loss: 0.6024060249328613\n",
      "pred_error: 0.15652349591255188 \t range_loss: 0.6024060249328613\n",
      "Step: 704 \t Loss: 2.1644506454467773\n",
      "pred_error: 0.1565638929605484 \t range_loss: 0.6011450886726379\n",
      "pred_error: 0.15636561810970306 \t range_loss: 0.6026861667633057\n",
      "pred_error: 0.15636512637138367 \t range_loss: 0.6026861667633057\n",
      "pred_error: 0.15647634863853455 \t range_loss: 0.6028721332550049\n",
      "pred_error: 0.15667520463466644 \t range_loss: 0.6027427911758423\n",
      "pred_error: 0.156601220369339 \t range_loss: 0.6019641160964966\n",
      "pred_error: 0.15666532516479492 \t range_loss: 0.6014172434806824\n",
      "pred_error: 0.15678633749485016 \t range_loss: 0.6000839471817017\n",
      "pred_error: 0.1567869633436203 \t range_loss: 0.6000839471817017\n",
      "pred_error: 0.15676338970661163 \t range_loss: 0.6006202101707458\n",
      "pred_error: 0.15667884051799774 \t range_loss: 0.5991479158401489\n",
      "Step: 732 \t Loss: 2.164208173751831\n",
      "pred_error: 0.15646584331989288 \t range_loss: 0.5995506644248962\n",
      "pred_error: 0.1566695123910904 \t range_loss: 0.6008008122444153\n",
      "pred_error: 0.15678375959396362 \t range_loss: 0.5991846919059753\n",
      "pred_error: 0.15662741661071777 \t range_loss: 0.599113404750824\n",
      "pred_error: 0.15662728250026703 \t range_loss: 0.599113404750824\n",
      "pred_error: 0.15672682225704193 \t range_loss: 0.5988530516624451\n",
      "pred_error: 0.15672682225704193 \t range_loss: 0.5988530516624451\n",
      "pred_error: 0.15675613284111023 \t range_loss: 0.6000986695289612\n",
      "pred_error: 0.15675611793994904 \t range_loss: 0.6000986695289612\n",
      "pred_error: 0.1568695455789566 \t range_loss: 0.5995396375656128\n",
      "pred_error: 0.15681752562522888 \t range_loss: 0.6006883382797241\n",
      "pred_error: 0.15681780874729156 \t range_loss: 0.5998059511184692\n",
      "pred_error: 0.15696550905704498 \t range_loss: 0.6004419922828674\n",
      "pred_error: 0.1569000780582428 \t range_loss: 0.5997060537338257\n",
      "pred_error: 0.15671591460704803 \t range_loss: 0.5992231965065002\n",
      "pred_error: 0.15697284042835236 \t range_loss: 0.5989819765090942\n",
      "pred_error: 0.15686652064323425 \t range_loss: 0.5987254977226257\n",
      "pred_error: 0.15680965781211853 \t range_loss: 0.6007506847381592\n",
      "pred_error: 0.1568010449409485 \t range_loss: 0.5989125370979309\n",
      "pred_error: 0.15680089592933655 \t range_loss: 0.5989125370979309\n",
      "pred_error: 0.15685901045799255 \t range_loss: 0.5993046164512634\n",
      "pred_error: 0.15696944296360016 \t range_loss: 0.5988832116127014\n",
      "pred_error: 0.15697790682315826 \t range_loss: 0.5978904962539673\n",
      "pred_error: 0.15672050416469574 \t range_loss: 0.5979581475257874\n",
      "pred_error: 0.15672053396701813 \t range_loss: 0.5979581475257874\n",
      "pred_error: 0.15671665966510773 \t range_loss: 0.6001728773117065\n",
      "pred_error: 0.15693001449108124 \t range_loss: 0.5993934869766235\n",
      "pred_error: 0.1569383442401886 \t range_loss: 0.598325252532959\n",
      "pred_error: 0.15693825483322144 \t range_loss: 0.598325252532959\n",
      "pred_error: 0.15674474835395813 \t range_loss: 0.5995378494262695\n",
      "pred_error: 0.1567448377609253 \t range_loss: 0.5995378494262695\n",
      "pred_error: 0.1567537635564804 \t range_loss: 0.5980381369590759\n",
      "pred_error: 0.1567537635564804 \t range_loss: 0.5980381369590759\n",
      "pred_error: 0.1566755771636963 \t range_loss: 0.5984824299812317\n",
      "pred_error: 0.15667559206485748 \t range_loss: 0.5984824299812317\n",
      "pred_error: 0.15680287778377533 \t range_loss: 0.598461925983429\n",
      "pred_error: 0.15684263408184052 \t range_loss: 0.5983507037162781\n",
      "pred_error: 0.156972274184227 \t range_loss: 0.597862184047699\n",
      "pred_error: 0.15677402913570404 \t range_loss: 0.6002342700958252\n",
      "pred_error: 0.156768336892128 \t range_loss: 0.5992277264595032\n",
      "pred_error: 0.15661554038524628 \t range_loss: 0.5981160998344421\n",
      "BEST LOSS: 2.1642082\n",
      "==== Model: block11_cob_activation_norm  in Layer: 11 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 10:52:39,258 execute.rs:1044 num calibration batches: 1\n",
      "WARNING ezkl.circuit.table 2024-09-16 10:53:49,694 table.rs:187 Using 2 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-09-16 10:53:49,704 table.rs:187 Using 2 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-09-16 10:53:49,717 table.rs:187 Using 2 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-09-16 10:53:49,724 table.rs:187 Using 2 columns for non-linearity table.\n",
      "ERROR ezkl.graph.model 2024-09-16 10:54:23,232 model.rs:1246 value (130880) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 10:54:23,237 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 10:54:40,058 model.rs:1246 value (130880) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 10:54:40,063 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 10:54:40,111 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 10:54:40,149 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 10:54:40,166 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000023401522 | -0.000017344952 | 0.0020785332 | -0.0051207542 | 0.000053248463 | 0.000017344952   | 0.0051207542  | 0             | 0.000000014794938  | 0.00006478574      | 0.0010136885           |\n",
      "+------------------+-----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 534498 64 [-904508, 1382200] 1 [16]\n",
      "===============================\n",
      "==== Model: block11_cob_activation_norm_teleported  in Layer: 11 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 10:54:56,711 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 10:56:37,801 model.rs:1246 value (107776) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 10:56:37,809 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 10:56:54,288 model.rs:1246 value (107776) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 10:56:54,293 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 10:56:54,308 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 10:56:54,326 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 10:56:54,356 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+----------------+------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error   | max_error  | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+----------------+------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000025380907 | 0.000010147691 | 0.00222826 | -0.0056762695 | 0.000056041823 | 0.000010147691   | 0.0056762695  | 0             | 0.000000017319863  | 0.0000701368       | 0.0012329317           |\n",
      "+------------------+----------------+------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 534499 64 [-552220, 1373298] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import copy\n",
    "import ezkl\n",
    "\n",
    "array_param_visibility = [\"fixed\"]\n",
    "array_input_param_scale = [16]\n",
    "array_num_cols = [64]\n",
    "array_max_log_rows = [-1]\n",
    "array_scale_rebase = [-1]\n",
    "array_lookup_margin = [2]\n",
    "\n",
    "# iterate over all the possible combinations\n",
    "combinations = list(itertools.product(array_param_visibility, array_input_param_scale, array_num_cols, array_max_log_rows, array_scale_rebase, array_lookup_margin))\n",
    "global best_loss\n",
    "# the layer_idx which the logrows are equal to 20 are not gonna be teleported\n",
    "list_of_no_teleportation = [1,3,4,5]\n",
    "\n",
    "# with no gradient pytorch\n",
    "with torch.no_grad():\n",
    "    # iterate over all the possible combinations\n",
    "    for p in combinations:\n",
    "        param_visibility, input_param_scale, num_cols, max_log_rows, scale_rebase, lookup_margin = p\n",
    "        # string experiment_settings as comma separated values\n",
    "        experiment_settings = f\"{param_visibility}/{input_param_scale}/{num_cols}/{max_log_rows}/{scale_rebase}/{lookup_margin}\"\n",
    "        print(\"========= START =========\")\n",
    "        print(f\"input_param_scale: {input_param_scale}, num_cols: {num_cols}, max_log_rows: {max_log_rows}, param_visibility: {param_visibility}, lookup_margin: {lookup_margin}\")\n",
    "\n",
    "        # copy the model\n",
    "        new_model = copy.deepcopy(model)\n",
    "        \n",
    "        # generate compression-model and setting for all vit layers\n",
    "        for layer_idx in range(model.depth):\n",
    "\n",
    "            # if layer_idx >= 4:\n",
    "            #     continue\n",
    "\n",
    "            args.pred_mul = 10\n",
    "            # args.pred_mul = 20\n",
    "            args.steps = 800\n",
    "            # args.steps = 400\n",
    "            args.cob_lr = 0.1\n",
    "            args.zoo_step_size = 0.001 \n",
    "\n",
    "            # check the experiment_settings and layer_idx exists in the csv file\n",
    "            with open(csv_file_path, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                exist_flag = False\n",
    "                for row in reader:\n",
    "                    if row[0] == experiment_settings and int(row[2]) == layer_idx:\n",
    "                        print(f\"Experiment settings: {experiment_settings} and layer_idx: {layer_idx} already exists in the csv file.\")\n",
    "                        exist_flag = True\n",
    "                        break\n",
    "            # if exist_flag:\n",
    "            #     continue\n",
    "\n",
    "            # Hook for the intermediate output of the block\n",
    "            original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "            activation_stats_idx = {}\n",
    "            for i,layer in enumerate(original_mlp_idx.children()):\n",
    "                if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "                    layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "            # run the mlp model to find original_loss\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "            \n",
    "            # print activation stats\n",
    "            print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "            original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "            print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "            # # Load the teleported model\n",
    "            # teleported_model_idx = LinearNet()\n",
    "            # teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "            # load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "\n",
    "            # # get initial weights and cob\n",
    "            # initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "            # initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "            # track best loss\n",
    "            best_loss = 1e9\n",
    "\n",
    "            # define input_teleported_model (used in ng_loss_function)\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "            input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "            # save npy file using in python checking script\n",
    "            np.save(args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\", input_teleported_model.detach().numpy())\n",
    "            # define original_pred (used in ng_loss_function)\n",
    "            input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "            np.save(args.prefix_dir + f\"input_org_{layer_idx}.npy\", input_org.detach().numpy())\n",
    "            original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "            # Apply best COB and save model weights\n",
    "            LN = LinearNet()\n",
    "            LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "            load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "            if layer_idx in list_of_no_teleportation:\n",
    "                print(\"====== NO OPTIMIZATION SINCE NO TELEPORTATION =====\")\n",
    "                best_loss = torch.tensor(best_loss).detach().cpu()\n",
    "                LN = LN.teleport(torch.ones_like(torch.ones(960)), reset_teleportation=True)\n",
    "                torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "            # check whether the teleportation .pth already exists\n",
    "            elif os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "                print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "                LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "                best_loss = torch.tensor(best_loss).detach().cpu()\n",
    "            else:\n",
    "                # zero-order derivative optimization\n",
    "                # if args.teleport_gaussian and layer_idx != 2 and layer_idx != 3:\n",
    "                initial_cob_idx = torch.ones(960)\n",
    "                # add inputs to the function\n",
    "                ackley = functools.partial(\n",
    "                    f_ack,\n",
    "                    input_data=input_teleported_model,\n",
    "                    original_pred=original_pred,\n",
    "                    layer_idx=layer_idx,\n",
    "                    original_loss = original_loss_idx,\\\n",
    "                    tm = LN\n",
    "                )\n",
    "\n",
    "                # training to find best_cob\n",
    "                best_cob = None\n",
    "                for step in range(args.steps):\n",
    "                    # get the gradient of the cob\n",
    "                    grad_cob = cge(ackley, {\"cob\": initial_cob_idx}, None, args.zoo_step_size)\n",
    "                    # update the cob\n",
    "                    initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "                    # calculate the loss\n",
    "                    loss = ackley(initial_cob_idx)\n",
    "                    # update the best loss\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_cob = initial_cob_idx\n",
    "                        print(f\"Step: {step} \\t Loss: {loss}\")\n",
    "\n",
    "                print(\"BEST LOSS:\",best_loss)\n",
    "\n",
    "                # Apply best COB and save model weights\n",
    "                if layer_idx in list_of_no_teleportation:\n",
    "                    print(\"====== NO TELEPORTATION =====\")\n",
    "                else:\n",
    "                    LN = LN.teleport(best_cob, reset_teleportation=True)\n",
    "                # save the .pth of the teleported model\n",
    "                torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "\n",
    "            # Apply the teleportation to the new_model (Using for computing the next layer inputs)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k: v for k, v in sd.items() if 'norm2' not in k}\n",
    "            new_model.blocks[layer_idx].mlp.load_state_dict(sd)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k.replace('norm2.',''): v for k, v in sd.items() if 'norm2' in k}\n",
    "            new_model.blocks[layer_idx].norm2.load_state_dict(sd)\n",
    "\n",
    "            # Export the optimized model to ONNX\n",
    "            torch.onnx.export(LN.network, input_teleported_model, args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.onnx', verbose=False, export_params=True, opset_version=15, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "\n",
    "            # check the validation of the teleportation\n",
    "            \n",
    "            # 1.extract onnx corrosponding to the teleported model (in original onnx)\n",
    "            input_path = args.prefix_dir + f\"network_split_{layer_idx}_False.onnx\"\n",
    "            output_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "            input_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "            output_names = [f\"/blocks.{layer_idx}/mlp/fc2/Add_output_0\"]\n",
    "            onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "            \n",
    "            # # 2. run the python code\n",
    "            # print(\"===== RUNNING PYTHON CODE =====\")\n",
    "            # a = args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\"\n",
    "            # b = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "            # !python onnx_inference.py \\\n",
    "            #     --input {a} --model1 {output_path} --model2 {b}\n",
    "            # print(\"===== PYTHON CODE FINISHED =====\")\n",
    "            # time.sleep(5)\n",
    "\n",
    "            # ezkl to find the resources possible reduction\n",
    "            run_args = ezkl.PyRunArgs()\n",
    "            run_args.input_visibility = \"public\"\n",
    "            run_args.param_visibility = param_visibility\n",
    "            run_args.output_visibility = \"public\"\n",
    "            run_args.input_scale = input_param_scale\n",
    "            run_args.param_scale = input_param_scale\n",
    "\n",
    "            # run_args.logrows = args.log_rows\n",
    "            run_args.num_inner_cols = num_cols\n",
    "            run_args.variables = [('batch_size', BATCHS)]\n",
    "            if max_log_rows != (-1):\n",
    "                run_args.logrows = max_log_rows\n",
    "            if scale_rebase != (-1):\n",
    "                run_args.scale_rebase_multiplier = scale_rebase\n",
    "\n",
    "            # get SRS\n",
    "            # ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)\n",
    "\n",
    "            name0 = f'block{layer_idx}_cob_activation_norm'\n",
    "            name1 = f'block{layer_idx}_cob_activation_norm_teleported'\n",
    "            rng = [name0,name1]\n",
    "\n",
    "            for m in rng:\n",
    "                print(\"==== Model:\",m, \" in Layer:\",layer_idx,\"====\")\n",
    "\n",
    "                if m == name0:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "                    x = input_org.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "                    x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "\n",
    "                # Generate the calibration data\n",
    "                data = dict(input_data=[x])\n",
    "                # cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "                cal_path = args.prefix_dir + f'cal_data_{m}.json'\n",
    "                json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "\n",
    "                settings_path = args.prefix_dir + f'settings_{m}_{layer_idx}.json'\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "\n",
    "                try:\n",
    "                    if scale_rebase != (-1):\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin, scale_rebase_multiplier=[scale_rebase],max_logrows=max_log_rows)\n",
    "                    elif max_log_rows != (-1):\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin, max_logrows=max_log_rows)\n",
    "                    else:\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin)\n",
    "                except:\n",
    "                    print(\"ERROR in calibration: \",m,layer_idx)\n",
    "                    res = False\n",
    "                    # write in the csv file\n",
    "                    with open(csv_file_path, mode='a', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        l = original_loss_idx if m == name0 else best_loss\n",
    "                        writer.writerow([\n",
    "                            experiment_settings,\n",
    "                            m,layer_idx,\n",
    "                            l,\n",
    "                            None, None, None,\n",
    "                            None, None, None, None, None, None,\n",
    "                            None, None, None, None, None, None, None, None\n",
    "                        ])\n",
    "                    continue\n",
    "\n",
    "                # extract the resources\n",
    "                settings = json.load(open(settings_path))\n",
    "                rg = settings.get('run_args', {})\n",
    "                input_scale = rg.get('input_scale', None)\n",
    "                param_scale = rg.get('param_scale', None)\n",
    "                output_scale = settings.get('model_output_scales', None)\n",
    "                scale_rebase_multiplier = rg.get('scale_rebase_multiplier', None)\n",
    "                lookup_range = rg.get('lookup_range', [None, None])\n",
    "                logrows = rg.get('logrows', None)\n",
    "                num_rows = settings.get('num_rows', None)\n",
    "                total_assignments = settings.get('total_assignments', None)\n",
    "                num_cols = rg.get('num_inner_cols', None)\n",
    "                total_constant_size = settings.get('total_const_size', None)\n",
    "                \n",
    "                print(logrows, num_rows, num_cols ,lookup_range, scale_rebase_multiplier, output_scale)\n",
    "                print(\"===============================\")\n",
    "\n",
    "                # activation loss is equal to the original loss if m is equal to name0 else it is the best loss\n",
    "                activation_loss = original_loss_idx if m == name0 else best_loss\n",
    "\n",
    "                # write the results to the csv file\n",
    "                with open(csv_file_path, mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([\n",
    "                        experiment_settings,\n",
    "                        m,layer_idx,\n",
    "                        activation_loss.item(),\n",
    "                        input_scale, param_scale, scale_rebase_multiplier,\n",
    "                        lookup_range[0], lookup_range[1], logrows, num_rows, num_cols, total_assignments,\n",
    "                        total_constant_size, output_scale,\n",
    "                          None, None, None, None, None, None\n",
    "                    ])\n",
    "            \n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract onnx of the teleported model\n",
    "# input_path = args.prefix_dir + \"network_split_0_False.onnx\"\n",
    "# output_path = args.prefix_dir + \"block0_cob_activation_norm.onnx\"\n",
    "# input_names = [\"/blocks.0/Add_2_output_0\"]\n",
    "# output_names = [\"/blocks.0/mlp/fc2/Add_output_0\"]\n",
    "# onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python onnx_inference.py --input {args.prefix_dir + 'input_teleported_model.npy'} \\\n",
    "#     --model1 {args.prefix_dir + 'block0_cob_activation_norm.onnx'} \\\n",
    "#     --model2 {args.prefix_dir + 'block0_cob_activation_norm_teleported.onnx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.input_param_scale: 7\n",
      "args.logrows: 20\n",
      "args.num_cols: 2\n",
      "args.scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "# print important args related to ezkl\n",
    "print(\"args.input_param_scale:\",args.input_param_scale)\n",
    "print(\"args.logrows:\",args.log_rows)\n",
    "print(\"args.num_cols:\",args.num_cols)\n",
    "print(\"args.scale_rebase_multiplier:\",args.scale_rebase_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 10:56:54,674 execute.rs:742 SRS already exists at that path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Future pending cb=[<builtins.PyDoneCallback object at 0x14d976330>()]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ezkl\n",
    "\n",
    "run_args = ezkl.PyRunArgs()\n",
    "run_args.input_visibility = \"public\"\n",
    "# TODO: change that to fixed\n",
    "run_args.param_visibility = \"fixed\"\n",
    "run_args.output_visibility = \"public\"\n",
    "run_args.input_scale = args.input_param_scale\n",
    "run_args.param_scale = args.input_param_scale\n",
    "run_args.logrows = args.log_rows\n",
    "run_args.num_inner_cols = args.num_cols\n",
    "run_args.scale_rebase_multiplier = args.scale_rebase_multiplier\n",
    "run_args.variables = [('batch_size', BATCHS)]\n",
    "\n",
    "ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "001240b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ezkl version: 100.100.101\n"
     ]
    }
   ],
   "source": [
    "# print ezkl version\n",
    "print(\"ezkl version:\",ezkl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source ~/.config/envman/PATH.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !RUST_LOG=trace\n",
    "\n",
    "# rng = ['block0_cob_activation_norm','block0_cob_activation_norm_teleported']\n",
    "\n",
    "# # Generate the calibration data\n",
    "# x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "# data = dict(input_data=[x])\n",
    "# cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "# json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "# for model in rng:\n",
    "\n",
    "#     print(\"==== Model:\",model, \"====\")\n",
    "\n",
    "#     model_path = args.prefix_dir + model + \".onnx\"\n",
    "#     settings_path = f'{args.prefix_dir} settings_{model}.json'\n",
    "\n",
    "#     res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "#     assert res == True\n",
    "\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier],max_logrows=args.log_rows)\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier])\n",
    "#     res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale])\n",
    "#     assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 10:56:54,857 execute.rs:640 read 134217988 bytes from file (vector of len = 134217988)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_69108/2906073395.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# recreate and initialize the model\n",
    "model = build_model(args, pretrained=False)\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# transfer the model to the cpu\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "\n",
    "# op - original_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tout = LN.network(input_teleported_model)\n",
    "# (original_pred - tout).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "926f9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the original predictions in original_pred_array\n",
    "original_pred_array = []\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):    \n",
    "    input_teleported_model = model.split_n(input_convs,layer_idx,half=True)\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_array.append(original_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f90ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 10:56:55,867 execute.rs:647 file hash: 54ef75911da76d7a6b7ea341998aaf66cb06c679c53e0a88a4fe070dd3add963\n",
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_69108/3146665136.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 0.016365107148885727\n",
      "Layer 1 error: 0.013229558244347572\n",
      "Layer 2 error: 0.024285390973091125\n",
      "Layer 3 error: 0.057023655623197556\n",
      "Layer 4 error: 0.05267014354467392\n",
      "Layer 5 error: 0.05369943752884865\n",
      "Layer 6 error: 0.0738224908709526\n",
      "Layer 7 error: 0.07126373797655106\n",
      "Layer 8 error: 0.06540150940418243\n",
      "Layer 9 error: 0.0679381787776947\n",
      "Layer 10 error: 0.0676272064447403\n",
      "Layer 11 error: 0.05973820760846138\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (correct input (not teleported) for each layer)\n",
    "\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    input_layer = model.split_n(input_convs,layer_idx,half=True)\n",
    "    org_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_layer))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "\n",
    "    teleported_pred = teleported_model.network(input_layer)\n",
    "    error = (org_pred - teleported_pred).abs().mean()\n",
    "    error /= org_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82c28ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 0.016365107148885727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_69108/1111111726.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 error: 0.013229558244347572\n",
      "Layer 2 error: 0.03295620530843735\n",
      "Layer 3 error: 0.06952941417694092\n",
      "Layer 4 error: 0.07612324506044388\n",
      "Layer 5 error: 0.09289754182100296\n",
      "Layer 6 error: 0.11527306586503983\n",
      "Layer 7 error: 0.14075452089309692\n",
      "Layer 8 error: 0.16404050588607788\n",
      "Layer 9 error: 0.18459577858448029\n",
      "Layer 10 error: 0.1843269318342209\n",
      "Layer 11 error: 0.15659774839878082\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (teleported input for each layer)\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(new_model.depth):\n",
    "    input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "    input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "    teleported_pred = teleported_model.network(input_teleported_model)\n",
    "    \n",
    "    error = (original_pred - teleported_pred).abs().mean()\n",
    "    error /= original_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25440b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block0_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 0: 0.01636327989399433\n",
      "block1_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 1: 0.013228589668869972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_69108/3984726627.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block2_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 2: 0.032954853028059006\n",
      "block3_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 3: 0.06952785700559616\n",
      "block4_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 4: 0.07612434029579163\n",
      "block5_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 5: 0.09289750456809998\n",
      "block6_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 6: 0.1152738705277443\n",
      "block7_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 7: 0.14075547456741333\n",
      "block8_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 8: 0.16404230892658234\n",
      "block9_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 9: 0.18459667265415192\n",
      "block10_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 10: 0.18432751297950745\n",
      "block11_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 11: 0.1565980613231659\n"
     ]
    }
   ],
   "source": [
    "teleportation_applied_layers = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "# the copy_model is used to consider the previous layer teleportation to the input of the current layer (only layers in the teleportation_applied_layers are gonna be teleported)\n",
    "copy_model = copy.deepcopy(model)\n",
    "copy_model.eval()\n",
    "for param in copy_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for layer_idx in teleportation_applied_layers:\n",
    "    # compute the original_pred_idx - IMPORTANT: PRVIOUS TELEPORTED LAYERS EFFECT THE INPUT OF THE CURRENT LAYER => pred_error WOULD BE DIFFERENT\n",
    "    input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "    input_convs = torch.tensor(input_convs).view(BATCHS,3,224,224)\n",
    "    input_teleported_model = copy_model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    # original_pred_idx = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_idx = original_pred_array[layer_idx]\n",
    "\n",
    "    LN = LinearNet()\n",
    "    LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "    # check whether the teleportation .pth already exists\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "        LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        # raise error\n",
    "        raise Exception(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exists.\")\n",
    "\n",
    "    # subsitude the teleported_model weights in the original model\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k: v for k, v in state_dic.items() if 'norm2' not in k}\n",
    "    copy_model.blocks[layer_idx].mlp.load_state_dict(state_dic)\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k.replace('norm2.',''): v for k, v in state_dic.items() if 'norm2' in k}\n",
    "    copy_model.blocks[layer_idx].norm2.load_state_dict(state_dic)\n",
    "\n",
    "    # compute the prediction error\n",
    "    new_pred = copy_model.blocks[layer_idx].mlp(copy_model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    diff = (original_pred_idx - new_pred).abs().mean()\n",
    "    print(f\"Prediction error in layer {layer_idx}: {diff/original_pred_idx.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the updated model .pth\n",
    "torch.save(copy_model.state_dict(), args.resume.replace(\".pth\",\"_teleported.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# export the updated model to onnx\n",
    "torch.onnx.export(copy_model, x,\\\n",
    "                args.prefix_dir + 'complete_model_teleported.onnx', \\\n",
    "                verbose=False, export_params=True, opset_version=15, do_constant_folding=True, \\\n",
    "                input_names=['input'], output_names=['output'], \\\n",
    "                dynamic_axes={'input' : {0 : 'batch_size'},'output': {0:'batch_size'},},\n",
    ")\n",
    "\n",
    "# define the shape\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "# export the splits of the model\n",
    "\n",
    "input_path = args.prefix_dir + \"complete_model_teleported.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs_teleported.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}_teleported.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "\n",
    "# TODO:\n",
    "# define the data_loader_test an iterator which only returns the img\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "# contain data and label in the dataset\n",
    "# label is shape 1,1000 which is one hot encoded\n",
    "label = torch.tensor(1).unsqueeze(0) \n",
    "data_set_test = torch.utils.data.TensorDataset(data,label)\n",
    "data_loader_test = torch.utils.data.DataLoader(data_set_test, batch_size=BATCHS, shuffle=True)\n",
    "# data_loader_test = torch.utils.data.DataLoader(da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import json\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_ezkl_output(witness_file, settings_file):\n",
    "    # convert the quantized ezkl output to float value\n",
    "    witness_output = json.load(open(witness_file))\n",
    "    outputs = witness_output['outputs']\n",
    "    with open(settings_file) as f:\n",
    "        settings = json.load(f)\n",
    "    ezkl_outputs = [[ezkl.felt_to_float(\n",
    "        outputs[i][j], settings['model_output_scales'][i]) for j in range(len(outputs[i]))] for i in range(len(outputs))]\n",
    "    return ezkl_outputs\n",
    "\n",
    "\n",
    "def get_onnx_output(model_file, input_file):\n",
    "    # generate the ML model output from the ONNX file\n",
    "    onnx_model = onnx.load(model_file)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    with open(input_file) as f:\n",
    "        inputs = json.load(f)\n",
    "    # reshape the input to the model\n",
    "    num_inputs = len(onnx_model.graph.input)\n",
    "\n",
    "    onnx_input = dict()\n",
    "    for i in range(num_inputs):\n",
    "        input_node = onnx_model.graph.input[i]\n",
    "        dims = []\n",
    "        elem_type = input_node.type.tensor_type.elem_type\n",
    "#         print(\"elem_type: \", elem_type)\n",
    "        for dim in input_node.type.tensor_type.shape.dim:\n",
    "            if dim.dim_value == 0:\n",
    "                dims.append(1)\n",
    "            else:\n",
    "                dims.append(dim.dim_value)\n",
    "        if elem_type == 6:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int32).reshape(dims)\n",
    "        elif elem_type == 7:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int64).reshape(dims)\n",
    "        elif elem_type == 9:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                bool).reshape(dims)\n",
    "        else:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.float32).reshape(dims)\n",
    "        onnx_input[input_node.name] = inputs_onnx\n",
    "    try:\n",
    "        onnx_session = onnxruntime.InferenceSession(model_file)\n",
    "        onnx_output = onnx_session.run(None, onnx_input)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "        # onnx_output = inputs['output_data']\n",
    "#     print(\"onnx \", onnx_output)\n",
    "    return onnx_output[0]\n",
    "\n",
    "\n",
    "def compare_outputs(zk_output, onnx_output):\n",
    "    # calculate percentage difference between the 2 outputs (which are lists)\n",
    "    res = []\n",
    "    contains_sublist = any(isinstance(sub, list) for sub in zk_output)\n",
    "    zip_object = zip(np.array(zk_output),\n",
    "                     np.array(onnx_output))\n",
    "    \n",
    "    num_eq_zk_onnx = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for (i, (list1_i, list2_i)) in enumerate(zip_object):\n",
    "        diff = list1_i - list2_i\n",
    "        # iterate and print the diffs  if they are greater than 0.0\n",
    "        res.append(np.linalg.norm(diff,axis=(-1)))\n",
    "        print(\"= index: \",i, \"\\t diff-norm: \",np.linalg.norm(diff,axis=(-1)),\"\\t zk_output: \",list1_i.shape, \"\\t onnx_output: \",list2_i.shape)\n",
    "        \n",
    "        if np.argmax(list1_i) == np.argmax(list2_i):\n",
    "            num_eq_zk_onnx += 1\n",
    "        num_total += 1\n",
    "    \n",
    "    print(\"Accuracy (zk_onnx): \\t\", num_eq_zk_onnx / num_total)\n",
    "    acc_zk_onnx = num_eq_zk_onnx / num_total\n",
    "    return res, acc_zk_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10db441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = img.resize((224,224))\n",
    "# data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "\n",
    "# print(\"data.shape:\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f4a205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_pp max: tensor(8.4160)\n",
      "new_pp argmax: tensor(242)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    new_pp = new_model(img_input)\n",
    "    # print maximum value of the new_pp\n",
    "    print(\"new_pp max:\",torch.max(new_pp))\n",
    "    # print index of the maximum value of the new_pp\n",
    "    print(\"new_pp argmax:\",torch.argmax(new_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17c22f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_pp max: tensor(8.7614)\n",
      "org_pp argmax: tensor(180)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    org_pp = model(img_input)\n",
    "    # print maximum value of the new_pp\n",
    "    print(\"org_pp max:\",torch.max(org_pp))\n",
    "    # print index of the maximum value of the new_pp\n",
    "    print(\"org_pp argmax:\",torch.argmax(org_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3710ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b3924e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy_pp max: tensor(8.4160)\n",
      "copy_pp argmax: tensor(242)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    copy_pp = copy_model(img_input)\n",
    "    print(\"copy_pp max:\",torch.max(copy_pp))\n",
    "    print(\"copy_pp argmax:\",torch.argmax(copy_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d24dd226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top5_org: torch.return_types.topk(\n",
      "values=tensor([[8.7614, 8.3986, 7.9848, 7.1704, 6.1396]]),\n",
      "indices=tensor([[180, 242, 243, 246, 179]]))\n",
      "top5_copy: torch.return_types.topk(\n",
      "values=tensor([[8.4160, 8.2399, 7.6203, 7.4645, 5.8120]]),\n",
      "indices=tensor([[242, 180, 243, 246, 172]]))\n"
     ]
    }
   ],
   "source": [
    "# print top-5 indexes of org_pp and copy_pp and the corrosponding values\n",
    "top5_org = torch.topk(org_pp, 5)\n",
    "top5_copy = torch.topk(copy_pp, 5)\n",
    "print(\"top5_org:\",top5_org)\n",
    "print(\"top5_copy:\",top5_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "856322aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args.input_scale = 16\n",
    "run_args.param_scale = 16\n",
    "run_args.num_inner_cols = 64\n",
    "run_args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t image.shape: torch.Size([1, 3, 224, 224])\n",
      "=====\n",
      "= index:  0 \t diff-norm:  7.032947267152618 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 0.0\n",
      "Accuracy (zk_label): \t 0.0\n",
      "Accuracy (onnx_label): \t 0.0\n",
      "=====\n",
      "\n",
      "\n",
      "= index:  0 \t diff-norm:  7.032947267152618 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 0.0\n",
      "Accuracy (zk_label) 0.0\n",
      "mean norm diff:  7.032947267152618\n",
      "max norm diff:  7.032947267152618\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ezkl_outputs = np.empty((0,1000))\n",
    "onnx_outputs = np.empty((0,1000))\n",
    "labels = np.array([])\n",
    "\n",
    "# Open log file for writing\n",
    "log_file_path = os.path.join(args.prefix_dir, \"log\", \"output_log.txt\")\n",
    "os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "\n",
    "    # Generate dataNum_input_convs.json  \n",
    "    for index, (image, label) in enumerate(data_loader_test):\n",
    "        # image = data\n",
    "        # label = torch.tensor(1).unsqueeze(0)\n",
    "        # index = 0\n",
    "        \n",
    "        print(\"index:\",index,\"\\t image.shape:\",image.shape)\n",
    "        log_file.write(f\"index: {index}\\timage.shape: {image.shape}\\n\")\n",
    "        log_file.flush()\n",
    "        \n",
    "        os.makedirs(args.prefix_dir + \"ezkl_inputs/\"+str(index),exist_ok=True)\n",
    "\n",
    "        # remove batch dimension\n",
    "        output = model(image)\n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        pre_witness_path = None\n",
    "\n",
    "        # computing witness (last witness is important)\n",
    "        for i in [\"convs\"] + [t for t in range(model.depth)]:\n",
    "            for half in [\"True\",\"False\"]:\n",
    "\n",
    "                if i==\"convs\" and half==\"False\":\n",
    "                    continue\n",
    "\n",
    "                # Define paths\n",
    "                if i == \"convs\":\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}.json\"\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_{half}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}_{half}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}_{half}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}_{half}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}_{half}.json\"\n",
    "\n",
    "                # Generating input data\n",
    "                if i == \"convs\":\n",
    "                    data = dict(input_data = [((image).detach().numpy()).reshape([-1]).tolist()])\n",
    "                else:\n",
    "                    inter_i = model.split_n(image,i,half=half)\n",
    "                    data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "                json.dump(data, open(data_path, 'w' ))\n",
    "\n",
    "\n",
    "                # Swapping (output pre_witness -> cur_input of data_path)\n",
    "                if i != \"convs\":\n",
    "                    with open(pre_witness_path, 'r') as prev_witness_file:\n",
    "                        prev_witness_data = json.load(prev_witness_file)\n",
    "                        outputs = prev_witness_data['outputs']\n",
    "                        tmp = {\"input_data\": outputs}\n",
    "                        with open(data_path, 'w') as data_file:\n",
    "                            json.dump(tmp, data_file) \n",
    "\n",
    "                # Generate setting\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "                \n",
    "\n",
    "                # Calibrating setting\n",
    "                # res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\", \\\n",
    "                #                                   scales=[run_args.input_scale],max_logrows=run_args.logrows, scale_rebase_multiplier=[1],lookup_safety_margin=1)\n",
    "                \n",
    "                # Compile circuit\n",
    "                res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "                assert res == True\n",
    "\n",
    "                # Generating witness\n",
    "                res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "                assert os.path.isfile(witness_path)\n",
    "\n",
    "                # Update input_scale\n",
    "                settings = json.load(open(settings_path, 'r'))\n",
    "                run_args.input_scale = settings[\"model_output_scales\"][0]\n",
    "\n",
    "                # Update pre_witness_path\n",
    "                pre_witness_path = witness_path\n",
    "\n",
    "    # check accuracy\n",
    "    model_file = args.prefix_dir + \"network_complete.onnx\" \n",
    "    input_file = args.prefix_dir + f\"ezkl_inputs/{index}/input_convs.json\"\n",
    "    witness_file = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{model.depth-1}_False.json\"\n",
    "    settings_file = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{model.depth-1}_False.json\"\n",
    "\n",
    "    # get the ezkl output\n",
    "    ezkl_output = get_ezkl_output(witness_file, settings_file)\n",
    "    ezkl_output = np.array(ezkl_output).reshape(BATCHS,1000)\n",
    "    # get the onnx output\n",
    "    onnx_output = get_onnx_output(model_file, input_file)\n",
    "\n",
    "    ezkl_outputs = np.concatenate((ezkl_outputs,ezkl_output),axis=0)\n",
    "    onnx_outputs = np.concatenate((onnx_outputs,onnx_output),axis=0)\n",
    "    labels = np.concatenate((labels,label),axis=0)\n",
    "\n",
    "    print(\"=====\")\n",
    "    log_file.write(\"=====\\n\")\n",
    "    _,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "    \n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label): \\t\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label): \\t {acc_zk_label}\\n\")\n",
    "    \n",
    "    acc_onnx_label = np.sum( (labels) == np.argmax(onnx_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (onnx_label): \\t\", acc_onnx_label)\n",
    "    log_file.write(f\"Accuracy (onnx_label): \\t {acc_onnx_label}\\n\")\n",
    "    \n",
    "    print(\"=====\\n\\n\")\n",
    "    log_file.write(\"=====\\n\\n\")\n",
    "    log_file.flush()\n",
    "    \n",
    "    \n",
    "    # compare the outputs\n",
    "    percentage_difference,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "\n",
    "    # compare zk_output and truth_label\n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label)\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label) {acc_zk_label}\\n\")\n",
    "\n",
    "    # print the percentage difference\n",
    "    mean_percentage_difference = np.mean(np.abs(percentage_difference))\n",
    "    max_percentage_difference = np.max(np.abs(percentage_difference))\n",
    "    print(\"mean norm diff: \", mean_percentage_difference)\n",
    "    print(\"max norm diff: \", max_percentage_difference)\n",
    "    log_file.write(f\"mean norm diff: {mean_percentage_difference}\\n\")\n",
    "    log_file.write(f\"max norm diff: {max_percentage_difference}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5389361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac8208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralteleportation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
