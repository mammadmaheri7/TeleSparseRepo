{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# def get_cpu_load():\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = psutil.cpu_percent(interval=1, percpu=True)\n",
    "#     return cpu_loads\n",
    "\n",
    "# def select_k_cpus_with_lowest_load(k):\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = get_cpu_load()\n",
    "    \n",
    "#     # Create a list of tuples (core_id, load)\n",
    "#     cpu_load_tuples = list(enumerate(cpu_loads))\n",
    "    \n",
    "#     # Sort the list based on load\n",
    "#     sorted_cpu_loads = sorted(cpu_load_tuples, key=lambda x: x[1])\n",
    "    \n",
    "#     # Get the IDs of the K cores with lowest load\n",
    "#     selected_cpus = [core[0] for core in sorted_cpu_loads[:k]]\n",
    "    \n",
    "#     return selected_cpus\n",
    "\n",
    "# # Example usage\n",
    "# k = 8  # Number of CPUs with lowest load\n",
    "# selected_cpus = select_k_cpus_with_lowest_load(k)\n",
    "# print(f\"Selected {k} CPUs with lowest load:\", selected_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "\n",
    "# def set_cpu_affinity(pid, cpu_list):\n",
    "#     try:\n",
    "#         p = psutil.Process(pid)\n",
    "#         p.cpu_affinity(cpu_list)\n",
    "#         print(f\"CPU affinity for process {pid} set to: {cpu_list}\")\n",
    "#     except psutil.NoSuchProcess:\n",
    "#         print(f\"Process with PID {pid} does not exist.\")\n",
    "#     except psutil.AccessDenied:\n",
    "#         print(\"Permission denied. You may need sudo privileges to set CPU affinity.\")\n",
    "\n",
    "# # Get the process ID of the current process\n",
    "# pid = os.getpid()\n",
    "\n",
    "# # Set the CPU affinity to only CPU core 0\n",
    "# cpu_list = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "# set_cpu_affinity(pid, cpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as Fhtop\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "# check if notebook is in colab\n",
    "try:\n",
    "    # install ezkl\n",
    "    import google.colab\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
    "\n",
    "# rely on local installation of ezkl if the notebook is not in colab\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# uncomment for more descriptive logging \n",
    "import logging\n",
    "FORMAT = '%(levelname)s %(name)s %(asctime)-15s %(filename)s:%(lineno)d %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EZKL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import argparse\n",
    "\n",
    "def get_args_parser(initial_args=None):\n",
    "    parser = argparse.ArgumentParser('ConvNeXt training and evaluation script for image classification', add_help=False)\n",
    "#     parser.add_argument('--batch_size', default=256, type=int,\n",
    "#                         help='Per GPU batch size')\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--update_freq', default=1, type=int,\n",
    "                        help='gradient accumulation steps')\n",
    "\n",
    "    # Model parameters\n",
    "#     parser.add_argument('--model', default='convnext_tiny', type=str, metavar='MODEL',\n",
    "#                         help='Name of model to train')\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='image input size')\n",
    "    parser.add_argument('--layer_scale_init_value', default=1e-6, type=float,\n",
    "                        help=\"Layer scale initial values\")\n",
    "    \n",
    "    ########################## settings specific to this project ##########################\n",
    "    \n",
    "    # dropout and stochastic depth drop rate; set at most one to non-zero\n",
    "    parser.add_argument('--dropout', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    parser.add_argument('--drop_path', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    \n",
    "    # early / late dropout and stochastic depth settings\n",
    "    parser.add_argument('--drop_mode', type=str, default='standard', choices=['standard', 'early', 'late'], help='drop mode')\n",
    "    parser.add_argument('--drop_schedule', type=str, default='constant', choices=['constant', 'linear'], \n",
    "                        help='drop schedule for early dropout / s.d. only')\n",
    "    parser.add_argument('--cutoff_epoch', type=int, default=0, \n",
    "                        help='if drop_mode is early / late, this is the epoch where dropout ends / starts')\n",
    "    \n",
    "    ####################################################################################### \n",
    "    \n",
    "    # EMA related parameters\n",
    "    parser.add_argument('--model_ema', type=str2bool, default=False)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', type=str2bool, default=False, help='')\n",
    "    parser.add_argument('--model_ema_eval', type=str2bool, default=False, help='Using ema to eval during training.')\n",
    "\n",
    "    # Optimization parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "        weight decay. We use a cosine schedule for WD and using a larger decay by\n",
    "        the end of training improves performance for ViTs.\"\"\")\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=4e-3, metavar='LR',\n",
    "                        help='learning rate (default: 4e-3), with total batch size 4096')\n",
    "    parser.add_argument('--layer_decay', type=float, default=1.0)\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-6)')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=50, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--crop_pct', type=float, default=None)\n",
    "\n",
    "    # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', type=str2bool, default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "    parser.add_argument('--finetune', default='',\n",
    "                        help='finetune from checkpoint')\n",
    "    parser.add_argument('--head_init_scale', default=1.0, type=float,\n",
    "                        help='classifier head initial scale, typically adjusted in fine-tuning')\n",
    "    parser.add_argument('--model_key', default='model|module', type=str,\n",
    "                        help='which key to load from saved state dict, usually model or model_ema')\n",
    "    parser.add_argument('--model_prefix', default='', type=str)\n",
    "\n",
    "    # Dataset parameters\n",
    "#     parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "#                         help='dataset path')\n",
    "    parser.add_argument('--eval_data_path', default=None, type=str,\n",
    "                        help='dataset path for evaluation')\n",
    "    parser.add_argument('--nb_classes', default=1000, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--imagenet_default_mean_and_std', type=str2bool, default=True)\n",
    "    parser.add_argument('--data_set', default='IMNET', choices=['CIFAR', 'IMNET', 'image_folder'],\n",
    "                        type=str, help='ImageNet dataset path')\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "#     parser.add_argument('--resume', default='',\n",
    "#                         help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt_freq', default=1, type=int)\n",
    "    parser.add_argument('--save_ckpt_num', default=3, type=int)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', type=str2bool, default=False,\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', type=str2bool, default=True,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--disable_eval', type=str2bool, default=False,\n",
    "                        help='Disabling evaluation during training')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', type=str2bool, default=True,\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', type=str2bool, default=False)\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--use_amp', type=str2bool, default=False, \n",
    "                        help=\"Use PyTorch's AMP (Automatic Mixed Precision) or not\")\n",
    "\n",
    "    # Weights and Biases arguments\n",
    "    parser.add_argument('--enable_wandb', type=str2bool, default=False,\n",
    "                        help=\"enable logging to Weights and Biases\")\n",
    "    parser.add_argument('--project', default='convnext', type=str,\n",
    "                        help=\"The name of the W&B project where you're sending the new run.\")\n",
    "    parser.add_argument('--wandb_ckpt', type=str2bool, default=False,\n",
    "                        help=\"Save model checkpoints as W&B Artifacts.\")\n",
    "\n",
    "    # arguments for pruning\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=4096)\n",
    "#     parser.add_argument(\"--sparsity\", type=float, default=0.)\n",
    "    parser.add_argument(\"--prune_metric\", type=str, choices=[\"magnitude\", \"wanda\"])\n",
    "    parser.add_argument(\"--prune_granularity\", type=str)\n",
    "    parser.add_argument(\"--blocksize\", type=int, default=1)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"\n",
    "    Converts string to bool type; enables command line \n",
    "    arguments in the format of '--arg1 true --arg2 false'\n",
    "    \"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def get_default_args():\n",
    "    parser = get_args_parser()\n",
    "    default_args = {}\n",
    "    for action in parser._actions:\n",
    "        # Check if action is an argument\n",
    "        if not action.option_strings:\n",
    "            continue\n",
    "        # Use the destination as the key and the default value as the value\n",
    "        default_args[action.dest] = action.default\n",
    "    return DefaultArgs(**default_args)\n",
    "\n",
    "# Example usage:\n",
    "default_args = get_default_args()\n",
    "args = default_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== IMPORTANT: INPUT FLAGS OF PYTHON CODE HERE =====\n",
    "\n",
    "# args.model = \"vit_tiny\"\n",
    "# args.data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "\n",
    "# args.resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# #args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "# # args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/model_weights/deit/deit_tiny_patch16_224-a1311bcf.pth\"\n",
    "\n",
    "\n",
    "# args.sparsity = 0.5\n",
    "# args.batch_size = 32\n",
    "\n",
    "# args.pruning_method = \"CAP\" # DENSE,CAP,WANDA\n",
    "\n",
    "# # args.prune_metric = \"wanda\"\n",
    "# # args.prune_granularity = \"row\"\n",
    "# # pruned_model_dir = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "\n",
    "# # Prefix directory\n",
    "# args.prefix_dir = \"sparse-cap-acc/\" #sparse-cap-acc, dense-acc, sparse-wanda-acc\n",
    "# os.makedirs(args.prefix_dir, exist_ok=True)\n",
    "\n",
    "# # EZKL HPs\n",
    "# args.input_param_scale = 7\n",
    "# args.log_rows = 20\n",
    "# args.num_cols = 4\n",
    "# args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- IN_JUPYTER: True ---- \n",
      "Model: vit_tiny\n",
      "Data Path: /rds/general/user/mm6322/home/imagenet\n",
      "Resume Path: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "Sparsity: 0.5\n",
      "Batch Size: 1\n",
      "Pruning Method: CAP\n",
      "Prefix Directory: sparse-cap-acc-tmp/\n",
      "Input Parameter Scale: 7\n",
      "Log Rows: 20\n",
      "Number of Columns: 2\n",
      "Scale Rebase Multiplier: 1\n",
      "-------- \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# Check if running in Jupyter Notebook\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        IN_JUPYTER = True\n",
    "    else:\n",
    "        IN_JUPYTER = False\n",
    "except:\n",
    "    IN_JUPYTER = False\n",
    "\n",
    "# Define default values\n",
    "default_model = \"vit_tiny\"\n",
    "default_data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "default_resume = \"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# default_resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "default_sparsity = 0.5\n",
    "default_batch_size = 1\n",
    "default_pruning_method = \"CAP\"\n",
    "default_prefix_dir = \"sparse-cap-acc-tmp/\"\n",
    "default_input_param_scale = 7\n",
    "default_log_rows = 20\n",
    "default_num_cols = 2\n",
    "default_scale_rebase_multiplier = 1\n",
    "\n",
    "# Parse command-line arguments if not in Jupyter Notebook\n",
    "if not IN_JUPYTER:\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('--model', default=default_model, type=str, help='Model type')\n",
    "    parser.add_argument('--data_path', default=default_data_path, type=str, help='Data path')\n",
    "    parser.add_argument('--resume', default=default_resume, type=str, help='Resume path')\n",
    "    parser.add_argument('--sparsity', default=default_sparsity, type=float, help='Sparsity value')\n",
    "    parser.add_argument('--batch_size', default=default_batch_size, type=int, help='Batch size')\n",
    "    parser.add_argument('--pruning_method', default=default_pruning_method, type=str, help='Pruning method')\n",
    "    parser.add_argument('--prefix_dir', default=default_prefix_dir, type=str, help='Prefix directory')\n",
    "    parser.add_argument('--input_param_scale', default=default_input_param_scale, type=int, help='Input parameter scale')\n",
    "    parser.add_argument('--log_rows', default=default_log_rows, type=int, help='Log rows')\n",
    "    parser.add_argument('--num_cols', default=default_num_cols, type=int, help='Number of columns')\n",
    "    parser.add_argument('--scale_rebase_multiplier', default=default_scale_rebase_multiplier, type=int, help='Scale rebase multiplier')\n",
    "\n",
    "    args = parser.parse_args(namespace=args)\n",
    "else:\n",
    "    # In Jupyter Notebook, define args with default values\n",
    "    args.model = default_model\n",
    "    args.data_path = default_data_path\n",
    "    args.resume = default_resume\n",
    "    args.sparsity = default_sparsity\n",
    "    args.batch_size = default_batch_size\n",
    "    args.pruning_method = default_pruning_method\n",
    "    args.prefix_dir = default_prefix_dir\n",
    "    args.input_param_scale = default_input_param_scale\n",
    "    args.log_rows = default_log_rows\n",
    "    args.num_cols = default_num_cols\n",
    "    args.scale_rebase_multiplier = default_scale_rebase_multiplier\n",
    "    \n",
    "    \n",
    "print(\"\\n ---- IN_JUPYTER:\",str(IN_JUPYTER),\"---- \")\n",
    "# Print values for verification\n",
    "print(\"Model:\", args.model)\n",
    "print(\"Data Path:\", args.data_path)\n",
    "print(\"Resume Path:\", args.resume)\n",
    "print(\"Sparsity:\", args.sparsity)\n",
    "print(\"Batch Size:\", args.batch_size)\n",
    "print(\"Pruning Method:\", args.pruning_method)\n",
    "print(\"Prefix Directory:\", args.prefix_dir)\n",
    "print(\"Input Parameter Scale:\", args.input_param_scale)\n",
    "print(\"Log Rows:\", args.log_rows)\n",
    "print(\"Number of Columns:\", args.num_cols)\n",
    "print(\"Scale Rebase Multiplier:\", args.scale_rebase_multiplier)\n",
    "print(\"-------- \\n\\n\\n\")\n",
    "\n",
    "os.makedirs(args.prefix_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 300\n",
      "update_freq: 1\n",
      "input_size: 224\n",
      "layer_scale_init_value: 1e-06\n",
      "dropout: 0\n",
      "drop_path: 0\n",
      "drop_mode: standard\n",
      "drop_schedule: constant\n",
      "cutoff_epoch: 0\n",
      "model_ema: False\n",
      "model_ema_decay: 0.9999\n",
      "model_ema_force_cpu: False\n",
      "model_ema_eval: False\n",
      "opt: adamw\n",
      "opt_eps: 1e-08\n",
      "opt_betas: None\n",
      "clip_grad: None\n",
      "momentum: 0.9\n",
      "weight_decay: 0.05\n",
      "weight_decay_end: None\n",
      "lr: 0.004\n",
      "layer_decay: 1.0\n",
      "min_lr: 1e-06\n",
      "warmup_epochs: 50\n",
      "warmup_steps: -1\n",
      "color_jitter: 0.4\n",
      "aa: rand-m9-mstd0.5-inc1\n",
      "smoothing: 0.1\n",
      "train_interpolation: bicubic\n",
      "crop_pct: None\n",
      "reprob: 0.25\n",
      "remode: pixel\n",
      "recount: 1\n",
      "resplit: False\n",
      "mixup: 0.8\n",
      "cutmix: 1.0\n",
      "cutmix_minmax: None\n",
      "mixup_prob: 1.0\n",
      "mixup_switch_prob: 0.5\n",
      "mixup_mode: batch\n",
      "finetune: \n",
      "head_init_scale: 1.0\n",
      "model_key: model|module\n",
      "model_prefix: \n",
      "eval_data_path: None\n",
      "nb_classes: 1000\n",
      "imagenet_default_mean_and_std: True\n",
      "data_set: IMNET\n",
      "output_dir: \n",
      "device: cuda\n",
      "seed: 0\n",
      "auto_resume: True\n",
      "save_ckpt: True\n",
      "save_ckpt_freq: 1\n",
      "save_ckpt_num: 3\n",
      "start_epoch: 0\n",
      "eval: False\n",
      "dist_eval: True\n",
      "disable_eval: False\n",
      "num_workers: 10\n",
      "pin_mem: True\n",
      "world_size: 1\n",
      "local_rank: -1\n",
      "dist_on_itp: False\n",
      "dist_url: env://\n",
      "use_amp: False\n",
      "enable_wandb: False\n",
      "project: convnext\n",
      "wandb_ckpt: False\n",
      "nsamples: 4096\n",
      "prune_metric: None\n",
      "prune_granularity: None\n",
      "blocksize: 1\n",
      "model: vit_tiny\n",
      "data_path: /rds/general/user/mm6322/home/imagenet\n",
      "resume: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "sparsity: 0.5\n",
      "batch_size: 1\n",
      "pruning_method: CAP\n",
      "prefix_dir: sparse-cap-acc-tmp/\n",
      "input_param_scale: 7\n",
      "log_rows: 20\n",
      "num_cols: 2\n",
      "scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in vars(args).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.output_dir:\n",
    "    Path(default_args_dict.output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from timm.data.constants import \\\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data import create_transform\n",
    "\n",
    "def build_dataset(is_train, args):\n",
    "    transform = build_transform(is_train, args)\n",
    "\n",
    "    print(\"Transform = \")\n",
    "    if isinstance(transform, tuple):\n",
    "        for trans in transform:\n",
    "            print(\" - - - - - - - - - - \")\n",
    "            for t in trans.transforms:\n",
    "                print(t)\n",
    "    else:\n",
    "        for t in transform.transforms:\n",
    "            print(t)\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    if args.data_set == 'CIFAR':\n",
    "        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform, download=True)\n",
    "        nb_classes = 100\n",
    "    elif args.data_set == 'IMNET':\n",
    "        print(\"reading from datapath\", args.data_path)\n",
    "        root = os.path.join(args.data_path, 'train' if is_train else 'val_dirs')\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = 1000\n",
    "    elif args.data_set == \"image_folder\":\n",
    "        root = args.data_path if is_train else args.eval_data_path\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = args.nb_classes\n",
    "        assert len(dataset.class_to_idx) == nb_classes\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    print(\"Number of the class = %d\" % nb_classes)\n",
    "\n",
    "    return dataset, nb_classes\n",
    "\n",
    "\n",
    "def build_transform(is_train, args):\n",
    "    resize_im = args.input_size > 32\n",
    "    imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n",
    "    mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n",
    "    std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n",
    "\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=args.input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            interpolation=args.train_interpolation,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                args.input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        # warping (no cropping) when evaluated at 384 or larger\n",
    "        if args.input_size >= 384:  \n",
    "            t.append(\n",
    "            transforms.Resize((args.input_size, args.input_size), \n",
    "                            interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "        )\n",
    "            print(f\"Warping {args.input_size} size input images...\")\n",
    "        else:\n",
    "            if args.crop_pct is None:\n",
    "                args.crop_pct = 224 / 256\n",
    "            size = int(args.input_size / args.crop_pct)\n",
    "            t.append(\n",
    "                # to maintain same ratio w.r.t. 224 images\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),  \n",
    "            )\n",
    "            t.append(transforms.CenterCrop(args.input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_train = torch.utils.data.DistributedSampler(\n",
    "#         dataset_train, num_replicas=1, rank=0, shuffle=True, seed=args.seed,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_train = torch.utils.data.DataLoader(\n",
    "#     dataset_train, sampler=sampler_train,\n",
    "#     batch_size=args.batch_size,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.12'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import numpy as np\n",
    "from timm.utils import get_state_dict\n",
    "\n",
    "from pathlib import Path\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "# from torch._six import inf\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "class TensorboardLogger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = SummaryWriter(logdir=log_dir)\n",
    "        self.step = 0\n",
    "\n",
    "    def set_step(self, step=None):\n",
    "        if step is not None:\n",
    "            self.step = step\n",
    "        else:\n",
    "            self.step += 1\n",
    "\n",
    "    def update(self, head='scalar', step=None, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.writer.add_scalar(head + \"/\" + k, v, self.step if step is None else step)\n",
    "\n",
    "    def flush(self):\n",
    "        self.writer.flush()\n",
    "\n",
    "\n",
    "class WandbLogger(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        try:\n",
    "            import wandb\n",
    "            self._wandb = wandb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use the Weights and Biases Logger please install wandb.\"\n",
    "                \"Run `pip install wandb` to install it.\"\n",
    "            )\n",
    "\n",
    "        # Initialize a W&B run \n",
    "        if self._wandb.run is None:\n",
    "            self._wandb.init(\n",
    "                project=args.project,\n",
    "                config=args\n",
    "            )\n",
    "\n",
    "    def log_epoch_metrics(self, metrics, commit=True):\n",
    "        \"\"\"\n",
    "        Log train/test metrics onto W&B.\n",
    "        \"\"\"\n",
    "        # Log number of model parameters as W&B summary\n",
    "        self._wandb.summary['n_parameters'] = metrics.get('n_parameters', None)\n",
    "        metrics.pop('n_parameters', None)\n",
    "\n",
    "        # Log current epoch\n",
    "        self._wandb.log({'epoch': metrics.get('epoch')}, commit=False)\n",
    "        metrics.pop('epoch')\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            if 'train' in k:\n",
    "                self._wandb.log({f'Global Train/{k}': v}, commit=False)\n",
    "            elif 'test' in k:\n",
    "                self._wandb.log({f'Global Test/{k}': v}, commit=False)\n",
    "\n",
    "        self._wandb.log({})\n",
    "\n",
    "    def log_checkpoints(self):\n",
    "        output_dir = self.args.output_dir\n",
    "        model_artifact = self._wandb.Artifact(\n",
    "            self._wandb.run.id + \"_model\", type=\"model\"\n",
    "        )\n",
    "\n",
    "        model_artifact.add_dir(output_dir)\n",
    "        self._wandb.log_artifact(model_artifact, aliases=[\"latest\", \"best\"])\n",
    "\n",
    "    def set_steps(self):\n",
    "        # Set global training step\n",
    "        self._wandb.define_metric('Rank-0 Batch Wise/*', step_metric='Rank-0 Batch Wise/global_train_step')\n",
    "        # Set epoch-wise step\n",
    "        self._wandb.define_metric('Global Train/*', step_metric='epoch')\n",
    "        self._wandb.define_metric('Global Test/*', step_metric='epoch')\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "def load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "        \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.weight$')\n",
    "    state_dict_keys = list(state_dict.keys())\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.weight\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "            \n",
    "    \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.bias$')\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.bias\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "                                              \n",
    "    \n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(model, prefix=prefix)\n",
    "\n",
    "    warn_missing_keys = []\n",
    "    ignore_missing_keys = []\n",
    "    for key in missing_keys:\n",
    "        keep_flag = True\n",
    "        for ignore_key in ignore_missing.split('|'):\n",
    "            if ignore_key in key:\n",
    "                keep_flag = False\n",
    "                break\n",
    "        if keep_flag:\n",
    "            warn_missing_keys.append(key)\n",
    "        else:\n",
    "            ignore_missing_keys.append(key)\n",
    "            \n",
    "\n",
    "    missing_keys = warn_missing_keys\n",
    "\n",
    "    if len(missing_keys) > 0:\n",
    "        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "            model.__class__.__name__, unexpected_keys))\n",
    "    if len(ignore_missing_keys) > 0:\n",
    "        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, ignore_missing_keys))\n",
    "    if len(error_msgs) > 0:\n",
    "        print('\\n'.join(error_msgs))\n",
    "\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "\n",
    "        ########################################################\n",
    "        ## Code I added \n",
    "        for param in parameters:\n",
    "            weight_copy = param.data.abs().clone()\n",
    "            mask = weight_copy.gt(0).float().cuda()\n",
    "            sparsity = mask.sum() / mask.numel()\n",
    "            if sparsity > 0.3:\n",
    "                # non-trivial sparsity \n",
    "                param.grad.data.mul_(mask)\n",
    "        ########################################################\n",
    "\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n",
    "                     start_warmup_value=0, warmup_steps=-1):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_steps > 0:\n",
    "        warmup_iters = warmup_steps\n",
    "    print(\"Set warmup steps = %d\" % warmup_iters)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = np.array(\n",
    "        [final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        to_save = {\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'scaler': loss_scaler.state_dict(),\n",
    "            'args': args,\n",
    "        }\n",
    "\n",
    "        if model_ema is not None:\n",
    "            to_save['model_ema'] = get_state_dict(model_ema)\n",
    "\n",
    "        save_on_master(to_save, checkpoint_path)\n",
    "\n",
    "    if is_main_process() and isinstance(epoch, int):\n",
    "        to_del = epoch - args.save_ckpt_num * args.save_ckpt_freq\n",
    "        old_ckpt = output_dir / ('checkpoint-%s.pth' % to_del)\n",
    "        if os.path.exists(old_ckpt):\n",
    "            os.remove(old_ckpt)\n",
    "\n",
    "\n",
    "def auto_load_model(args, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.auto_resume and len(args.resume) == 0:\n",
    "        import glob\n",
    "        all_checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint-*.pth'))\n",
    "        latest_ckpt = -1\n",
    "        for ckpt in all_checkpoints:\n",
    "            t = ckpt.split('-')[-1].split('.')[0]\n",
    "            if t.isdigit():\n",
    "                latest_ckpt = max(int(t), latest_ckpt)\n",
    "        if latest_ckpt >= 0:\n",
    "            args.resume = os.path.join(output_dir, 'checkpoint-%d.pth' % latest_ckpt)\n",
    "        print(\"Auto resume checkpoint: %s\" % args.resume)\n",
    "\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        print(\"Resume checkpoint %s\" % args.resume)\n",
    "        if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            if not isinstance(checkpoint['epoch'], str): # does not support resuming with 'best', 'best-ema'\n",
    "                args.start_epoch = checkpoint['epoch'] + 1\n",
    "            else:\n",
    "                assert args.eval, 'Does not support resuming with checkpoint-best'\n",
    "            if hasattr(args, 'model_ema') and args.model_ema:\n",
    "                if 'model_ema' in checkpoint.keys():\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model_ema'])\n",
    "                else:\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model'])\n",
    "            if 'scaler' in checkpoint:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            print(\"With optim & sched!\")\n",
    "\n",
    "def reg_scheduler(base_value, final_value, epochs, niter_per_ep, early_epochs=0, early_value=None, \n",
    "           mode='linear', early_mode='regular'):\n",
    "    early_schedule = np.array([])\n",
    "    early_iters = early_epochs * niter_per_ep\n",
    "    if early_value is None:\n",
    "        early_value = final_value\n",
    "    if early_epochs > 0:\n",
    "        print(f\"Set early value to {early_mode} {early_value}\")\n",
    "        if early_mode == 'regular':\n",
    "            early_schedule = np.array([early_value] * early_iters)\n",
    "        elif early_mode == 'linear':\n",
    "            early_schedule = np.linspace(early_value, base_value, early_iters)\n",
    "        elif early_mode == 'cosine':\n",
    "            early_schedule = np.array(\n",
    "            [base_value + 0.5 * (early_value - base_value) * (1 + math.cos(math.pi * i / early_iters)) for i in np.arange(early_iters)])\n",
    "    regular_epochs = epochs - early_epochs\n",
    "    iters = np.arange(regular_epochs * niter_per_ep)\n",
    "    schedule = np.linspace(base_value, final_value, len(iters))\n",
    "    schedule = np.concatenate((early_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def build_model(args, pretrained=False):\n",
    "    if args.model.startswith(\"convnext\"):\n",
    "        model = create_model(\n",
    "            args.model,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=args.nb_classes,\n",
    "            layer_scale_init_value=args.layer_scale_init_value,\n",
    "            head_init_scale=args.head_init_scale,\n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate=args.dropout,\n",
    "            )\n",
    "    else:\n",
    "        model = create_model(\n",
    "            args.model, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=args.nb_classes, \n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate =args.dropout\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCHS: 1\n"
     ]
    }
   ],
   "source": [
    "BATCHS = args.batch_size\n",
    "print(\"BATCHS:\",BATCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.resnet import resnet26d, resnet50d\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
    "    ),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_huge_patch16_224': _cfg(),\n",
    "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
    "    # hybrid models\n",
    "    'vit_small_resnet26d_224': _cfg(),\n",
    "    'vit_small_resnet50d_s3_224': _cfg(),\n",
    "    'vit_base_resnet26d_224': _cfg(),\n",
    "    'vit_base_resnet50d_224': _cfg(),\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        \n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "#         self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.query_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.key_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.value_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "#         query = self.query_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         key = self.key_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         value = self.value_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "        \n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        qkvs = self.qkv(x)\n",
    "        query = qkvs[:,:,0:self.dim]\n",
    "        key = qkvs[:,:,self.dim:2*self.dim]\n",
    "        value = qkvs[:,:,2*self.dim:] \n",
    "        query = query.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        key = key.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        value = value.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale     \n",
    "        attn = query.transpose(1,2) @ key.transpose(1,2).transpose(2,3)\n",
    "        attn = attn * self.scale\n",
    "\n",
    "\n",
    "        attn = attn.softmax(dim=(-1))\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        \n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        y = (attn@value.transpose(1,2)).transpose(1,2).reshape(B,N,C)\n",
    "        \n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "        y = self.proj(y)\n",
    "        y = self.proj_drop(y)\n",
    "#         return x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "#         self.norm1 = norm_layer(self.dim)\n",
    "        self.norm1 = norm_layer(self.dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(self.dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))      \n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def half_block(self,x):\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "#         assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "#             f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "#         x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x).reshape(self.batch_size,192,196,1).transpose(1,2)\n",
    "        x = x.squeeze(3)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# class HybridEmbed(nn.Module):\n",
    "#     \"\"\" CNN Feature Map Embedding\n",
    "#     Extract feature map from CNN, flatten, project to embedding dim.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         assert isinstance(backbone, nn.Module)\n",
    "#         img_size = to_2tuple(img_size)\n",
    "#         self.img_size = img_size\n",
    "#         self.backbone = backbone\n",
    "#         if feature_size is None:\n",
    "#             with torch.no_grad():\n",
    "#                 # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
    "#                 # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
    "#                 # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
    "#                 training = backbone.training\n",
    "#                 if training:\n",
    "#                     backbone.eval()\n",
    "#                 o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "#                 feature_size = o.shape[-2:]\n",
    "#                 feature_dim = o.shape[1]\n",
    "#                 backbone.train(training)\n",
    "#         else:\n",
    "#             feature_size = to_2tuple(feature_size)\n",
    "#             feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "#         self.num_patches = feature_size[0] * feature_size[1]\n",
    "#         self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)[-1]\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "#         x = self.proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        # I add these two lines\n",
    "        self.drop_rate=drop_rate\n",
    "        attn_drop_rate=drop_rate\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.depth = depth\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
    "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
    "        #self.repr_act = nn.Tanh()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    " \n",
    "        x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "        B = BATCHS\n",
    "    \n",
    "\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1)\n",
    "        \n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def split_convs(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "#       B = x.shape[0]\n",
    "        B = BATCHS\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = torch.cat((self.cls_token, x), dim=1)\n",
    "    \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "#         num_blocks = len(self.blocks) // 2\n",
    "#         for blk in self.blocks[:num_blocks]:\n",
    "#             x = blk(x)\n",
    "\n",
    "#         x = self.blocks[0].half_block(x)\n",
    "\n",
    "#         print(\"split_1, output.shape:\",x.shape, \"\\t num_blocks:\",num_blocks)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def split_2(self,x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks[0].half_block(x)\n",
    "        return x\n",
    "    \n",
    "    def split_n(self,x,n,half=None):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for layer_idx in range(n-1):\n",
    "            x = self.blocks[layer_idx](x)\n",
    "            \n",
    "        #n-th layer\n",
    "        if half:\n",
    "            x = self.blocks[n].half_block(x)\n",
    "        else:\n",
    "            x = self.blocks[n](x)\n",
    "            # last layer of transformer\n",
    "            if n == (self.depth - 1):\n",
    "                x = self.norm(x)\n",
    "                x = x[:, 0]\n",
    "                x = self.head(x)\n",
    "                \n",
    "        return x\n",
    "            \n",
    "          \n",
    "    def update_drop_path(self, drop_path_rate):\n",
    "        self.drop_path = drop_path_rate\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, self.depth)]\n",
    "        for i in range(self.depth):\n",
    "            self.blocks[i].drop_path.drop_prob = dp_rates[i]\n",
    "    \n",
    "    def update_dropout(self, drop_rate):\n",
    "        self.drop_rate = drop_rate\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = drop_rate\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# @register_model\n",
    "# def vit_tiny_tiny(pretrained=False, **kwargs):\n",
    "#     model = VisionTransformer(\n",
    "#         patch_size=16, embed_dim=48, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "@register_model\n",
    "def vit_tiny(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_small(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_base(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_large(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(args, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.VisionTransformer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 5717416\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_17560/3668749705.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "# total_batch_size = args.batch_size * args.update_freq * utils.get_world_size()\n",
    "total_batch_size = args.batch_size * args.update_freq\n",
    "# num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n",
    "\n",
    "# At most one of dropout and stochastic depth should be enabled.\n",
    "assert(args.dropout == 0 or args.drop_path == 0)\n",
    "# ConvNeXt does not support dropout.\n",
    "assert(args.dropout == 0 if args.model.startswith(\"convnext\") else True)\n",
    "\n",
    "import re\n",
    "\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state_dict', 'optimizer', 'loss_scaler', 'epoch', 'manager'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# cnt = 0\n",
    "# dl = DataLoader(dataset_train, batch_size=BATCHS, shuffle=True)\n",
    "# # data = dataset_train[cnt][0].unsqueeze(dim=0)\n",
    "# # label = dataset_train[cnt][1]\n",
    "# data,label = next(iter(dl))\n",
    "# print(data.shape)\n",
    "# print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5717416\n",
      "Number of zeros: 2654208\n",
      "Percentage of zeros: 46.42%\n"
     ]
    }
   ],
   "source": [
    "# compute the number of zeros in the model / total number of parameters\n",
    "total_params = 0\n",
    "zeros = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    zeros += (param.data == 0).sum().item()\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of zeros: {zeros}\")\n",
    "print(f\"Percentage of zeros: {zeros / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "# load JPEG image /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = Image.open(\"/rds/general/user/mm6322/home/imagenet/val/n12620546/ILSVRC2012_val_00011901.JPEG\")\n",
    "\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# result = model(data.view(data.shape[0],-1))\n",
    "print(\"data shape:\",data.shape)\n",
    "result = model(data)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = data.detach().clone()\n",
    "print(\"x.shape:\",x.shape)\n",
    "\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(    \n",
    "    model,               # model being run\n",
    "    x,                   # model input (or a tuple for multiple inputs)\n",
    "    args.prefix_dir + \"network_complete.onnx\",            # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=15,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                    'output': {0:'batch_size'},\n",
    "    },         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx:\t CONV \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.9203) \t max: tensor(13.6817)\n",
      "layer_idx:\t 0 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.0635) \t max: tensor(15.4783)\n",
      "layer_idx:\t 0 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.4229) \t max: tensor(17.4164)\n",
      "layer_idx:\t 1 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.8803) \t max: tensor(10.3352)\n",
      "layer_idx:\t 1 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.4997) \t max: tensor(9.3524)\n",
      "layer_idx:\t 2 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.2025) \t max: tensor(17.3773)\n",
      "layer_idx:\t 2 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.8159) \t max: tensor(16.2393)\n",
      "layer_idx:\t 3 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.4613) \t max: tensor(15.7312)\n",
      "layer_idx:\t 3 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.2997) \t max: tensor(14.8284)\n",
      "layer_idx:\t 4 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.5867) \t max: tensor(15.6392)\n",
      "layer_idx:\t 4 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.6180) \t max: tensor(14.4870)\n",
      "layer_idx:\t 5 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.0484) \t max: tensor(15.2520)\n",
      "layer_idx:\t 5 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.7885) \t max: tensor(14.6185)\n",
      "layer_idx:\t 6 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.8097) \t max: tensor(14.7460)\n",
      "layer_idx:\t 6 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.1806) \t max: tensor(14.0622)\n",
      "layer_idx:\t 7 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-7.9359) \t max: tensor(14.7281)\n",
      "layer_idx:\t 7 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.2735) \t max: tensor(18.5691)\n",
      "layer_idx:\t 8 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-11.1971) \t max: tensor(20.7566)\n",
      "layer_idx:\t 8 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.5824) \t max: tensor(25.8723)\n",
      "layer_idx:\t 9 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-14.3955) \t max: tensor(26.9132)\n",
      "layer_idx:\t 9 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.2325) \t max: tensor(27.7766)\n",
      "layer_idx:\t 10 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5998) \t max: tensor(29.1508)\n",
      "layer_idx:\t 10 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5393) \t max: tensor(30.7623)\n",
      "layer_idx:\t 11 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-16.1201) \t max: tensor(33.1553)\n",
      "layer_idx:\t 11 \t half: False \t inter_out.shape: torch.Size([1, 1000]) \t min: tensor(-3.9342) \t max: tensor(8.3160)\n"
     ]
    }
   ],
   "source": [
    "inter_out = model.split_convs(data)\n",
    "print(\"layer_idx:\\t\",\"CONV\",\"\\t half:\",str(False),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_out = model.split_n(data,layer_idx,half)\n",
    "        print(\"layer_idx:\\t\",layer_idx,\"\\t half:\",str(half),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for all layers\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),args.prefix_dir, \"input_convs.json\")\n",
    "data = dict(input_data = [((x).detach().numpy()).reshape([-1]).tolist()])\n",
    "json.dump( data, open(data_path, 'w' ))\n",
    "\n",
    "for i in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_i = model.split_n(x,i,half=half)\n",
    "        data_path = os.path.join(os.getcwd(),args.prefix_dir, f\"input_{i}_{str(half)}.json\")\n",
    "        data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "        json.dump( data, open(data_path, 'w' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# extract all onnx files of layers\n",
    "\n",
    "input_path = args.prefix_dir + \"network_complete.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function\n",
    "def activation_hook(layer_name, activation_stats):\n",
    "    def hook(module, input, output):\n",
    "        input_tensor = input[0]\n",
    "        activation_stats[layer_name] = {\n",
    "            # l1 norm\n",
    "            'norm': input_tensor.norm(),\n",
    "            'max': input_tensor.max(),\n",
    "            'min': input_tensor.min(),\n",
    "            'shape': input_tensor.shape\n",
    "    }\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # teleporing the mlp model\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "# # IMPORTANT: input_x_T/F is acutally the output of that block (output of the intermediate or the full block)\n",
    "\n",
    "# # find the input data for the mlp model\n",
    "# input_split0_False = json.load(open(args.prefix_dir + \"input_0_True.json\"))[\"input_data\"][0]\n",
    "# norm_split0_False = model.blocks[0].norm2\n",
    "# input_mlp_0 = norm_split0_False(torch.tensor(input_split0_False).view(1,197,192))\n",
    "# out_mlp_0 = original_mlp_0(input_mlp_0)\n",
    "# out_mlp_0 = model.blocks[0].drop_path(out_mlp_0)\n",
    "# out_split0_false = out_mlp_0 + torch.tensor(input_split0_False).view(1,197,192)\n",
    "\n",
    "# # True output of the current block == input of the next block\n",
    "# out_split0_false_orginal = json.load(open(args.prefix_dir + \"input_0_False.json\"))[\"input_data\"][0]\n",
    "\n",
    "# print(input_mlp_0.norm(),input_mlp_0.max(),input_mlp_0.min(),input_mlp_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor(input_split0_False).view(1,197,192)\n",
    "# print(t.norm(),t.max(),t.min(),t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register hooks to the layers before all activation functions\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "# activation_stats = {}\n",
    "# for i, layer in enumerate(original_mlp_0.children()):\n",
    "#     if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#         layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats))\n",
    "\n",
    "# # Run the mlp model\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_block0 = model.split_convs(torch.tensor(input_convs).view(1,3,224,224))\n",
    "# input_block0 = input_block0.view(1,197,192)\n",
    "# original_block0_false_pred = model.blocks[0](input_block0)\n",
    "# print(\"activation_stats:\",activation_stats)\n",
    "\n",
    "# original_loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# print(f\"Original loss: {original_loss}, Original prediction error: {0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = json.load(open(args.prefix_dir + \"input_0_True.json\"))['input_data'][0]\n",
    "# d = torch.tensor(d).view(1,197,192)\n",
    "# np.save(args.prefix_dir + \"input_block0_false.npy\", d.numpy())\n",
    "\n",
    "# # original_pred = model.blocks[0].mlp(model.blocks[0].norm2(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralteleportation.models.model_zoo.mlpcob import MLPCOB\n",
    "from neuralteleportation.neuralteleportationmodel import NeuralTeleportationModel\n",
    "from neuralteleportation.layers.neuralteleportation import COBForwardMixin, FlattenCOB\n",
    "from neuralteleportation.layers.neuron import LinearCOB\n",
    "from neuralteleportation.layers.activation import ReLUCOB, SigmoidCOB, GELUCOB, LeakyReLUCOB\n",
    "from neuralteleportation.layers.dropout import DropoutCOB\n",
    "from neuralteleportation.layers.neuron import LayerNormCOB\n",
    "from neuralteleportation.layers.merge import Add\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # self.add = Add()\n",
    "        self.norm2 = LayerNormCOB(192)\n",
    "        self.fc1 = LinearCOB(192, 768, bias=True)\n",
    "        self.act = GELUCOB()\n",
    "        self.fc2 = LinearCOB(768, 192, bias=True)\n",
    "        # self.drop1 = DropoutCOB(0.1)\n",
    "        # self.drop2 = DropoutCOB(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.norm2(x)\n",
    "        x2 = self.fc1(x1)\n",
    "        x3 = self.act(x2)\n",
    "        x4 = self.fc2(x3)\n",
    "        # x4 = self.add(x, x3)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teleported_model = LinearNet()\n",
    "# teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ln_weights(LN, model, block_idx):\n",
    "    original_mlp = model.blocks[block_idx].mlp\n",
    "    original_norm2 = model.blocks[block_idx].norm2\n",
    "\n",
    "    combined_dict = {}\n",
    "    combined_dict.update(original_mlp.state_dict())\n",
    "    for k,v in original_norm2.state_dict().items():\n",
    "        combined_dict[\"norm2.\" + k] = v\n",
    "    LN.network.load_state_dict(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_ln_weights(teleported_model, model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the difference between the two predictions\n",
    "\n",
    "# model_pred = teleported_model.network(d).detach().cpu().numpy()\n",
    "# diff = np.abs(original_pred - model_pred).norm()\n",
    "# print(f\"Prediction error: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "args.cob_range = 0.1\n",
    "args.steps = 600 * 1\n",
    "args.sample_type = \"centered\"\n",
    "args.center = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get initial weights\n",
    "# initial_weights = teleported_model.get_weights().detach()\n",
    "# initial_cob = teleported_model.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variable to store the best loss found\n",
    "global best_loss\n",
    "global cor_best_pred_error\n",
    "global cor_best_range\n",
    "# initialize best_loss\n",
    "best_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nevergrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "982a3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ack(solution,input_data=None, original_pred=None, layer_idx=None ,original_loss=None):\n",
    "    # layer_idx = 1\n",
    "    # input_data = None\n",
    "    # original_loss = 11.1694\n",
    "    # original_pred = None\n",
    "    \n",
    "    cob = solution.get_x()\n",
    "    cob = torch.tensor(cob).cpu()\n",
    "    \n",
    "    # Set up model with the new COB\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    # Load the weights of the original model\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "\n",
    "    teleported_model = teleported_model.teleport(cob, reset_teleportation=True)\n",
    "\n",
    "    # Reset activation stats and run a forward pass\n",
    "    activation_stats = {}\n",
    "    hook_handles = []\n",
    "    for i, layer in enumerate(teleported_model.network.children()):\n",
    "            if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "                handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "                hook_handles.append(handle)\n",
    "    teleported_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = teleported_model.network(input_data)\n",
    "    # After forward pass\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "    loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "    loss /= original_loss\n",
    "    pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "    pred_error /= np.abs(original_pred).mean()\n",
    "    total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "    if random.random() < 0.01:\n",
    "         print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "    activation_stats.clear()\n",
    "    del teleported_model, activation_stats, pred, loss, pred_error, cob\n",
    "    return total_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "272f78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 2 , \t  activation_stats: {'relu_1': {'norm': tensor(414.7610), 'max': tensor(4.6655), 'min': tensor(-6.8689), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(11.5343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n"
     ]
    }
   ],
   "source": [
    "# copy the model\n",
    "import copy\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "layer_idx = 2\n",
    "\n",
    "new_model = copy.deepcopy(model)\n",
    "\n",
    "original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "activation_stats_idx = {}\n",
    "for i,layer in enumerate(original_mlp_idx.children()):\n",
    "    if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "        layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "# run the mlp model to find original_loss\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "# Load the teleported model\n",
    "teleported_model_idx = LinearNet()\n",
    "teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# get initial weights and cob\n",
    "initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "global best_loss\n",
    "best_loss = 1e9\n",
    "\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "# Apply best COB and save model weights\n",
    "LN = LinearNet()\n",
    "LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "s = initial_cob_idx.size()\n",
    "\n",
    "ackley = functools.partial(\n",
    "    f_ack,\n",
    "    input_data=input_teleported_model,\n",
    "    original_pred=original_pred,\n",
    "    layer_idx=layer_idx,\n",
    "    original_loss = original_loss_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a5974f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zoopt] init solution 0, value: 1.0000584125518799\n",
      "[zoopt] expected remaining running time: 00:21:32\n",
      "pred_error: 0.008517669513821602 \t range_loss: 0.9999929070472717\n",
      "pred_error: 0.0018685802351683378 \t range_loss: 0.9999929070472717\n",
      "pred_error: 0.03384530171751976 \t range_loss: 0.9999929070472717\n",
      "pred_error: 0.0035040585789829493 \t range_loss: 0.9999929070472717\n",
      "pred_error: 0.04427925497293472 \t range_loss: 0.9999929070472717\n",
      "pred_error: 0.013353546150028706 \t range_loss: 0.9338342547416687\n",
      "pred_error: 0.016038963571190834 \t range_loss: 0.9338342547416687\n",
      "pred_error: 0.01876065693795681 \t range_loss: 0.9338342547416687\n",
      "pred_error: 0.020521363243460655 \t range_loss: 0.92113196849823\n",
      "pred_error: 0.017019206658005714 \t range_loss: 0.92113196849823\n",
      "pred_error: 0.016362760215997696 \t range_loss: 0.92113196849823\n",
      "pred_error: 0.013351554982364178 \t range_loss: 0.9365537762641907\n",
      "pred_error: 0.015917226672172546 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.010062373243272305 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.02220884896814823 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.04714033007621765 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.010016932152211666 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.02866450697183609 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.023841319605708122 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.011454597115516663 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.028147080913186073 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.015370802022516727 \t range_loss: 0.9829416871070862\n",
      "pred_error: 0.013566643930971622 \t range_loss: 0.92113196849823\n",
      "pred_error: 0.010353010147809982 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.01666388474404812 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.024194177240133286 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.02571270801126957 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.01935390569269657 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.013041476719081402 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.01208894420415163 \t range_loss: 0.92113196849823\n",
      "pred_error: 0.01457500085234642 \t range_loss: 0.9956276416778564\n",
      "pred_error: 0.009737381711602211 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.01965000294148922 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.012282885611057281 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.014079036191105843 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.01238045934587717 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.013463973999023438 \t range_loss: 0.9406307339668274\n",
      "pred_error: 0.013817036524415016 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.04313306510448456 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.015115906484425068 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.01747635379433632 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.009850178845226765 \t range_loss: 0.9211319088935852\n",
      "pred_error: 0.011215290054678917 \t range_loss: 0.9211319088935852\n"
     ]
    }
   ],
   "source": [
    "from zoopt import Dimension, ValueType, Dimension2, Objective, Parameter, Opt, ExpOpt, Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "args.pred_mul = 5\n",
    "\n",
    "dim_size = s[0]  # dimension\n",
    "dim = Dimension(dim_size, [[0, 1.5]]*dim_size, [True]*dim_size)  # or dim = Dimension2([(ValueType.CONTINUOUS, [-1, 1], 1e-6)]*dim_size)\n",
    "obj = Objective(ackley, dim)\n",
    "# perform optimization\n",
    "init_val = Solution(x = [1.0]*dim_size)\n",
    "solution = Opt.min(obj, Parameter(budget=20*dim_size, init_samples=[init_val]))  #exploration_rate=0.01,uncertain_bits=3\n",
    "# print the solution\n",
    "print(solution.get_x(), solution.get_value())\n",
    "# parallel optimization for time-consuming tasks\n",
    "# solution = Opt.min(obj, Parameter(budget=1*dim_size, parallel=False, server_num=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da284ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(obj.get_history_bestsofar())\n",
    "plt.savefig('figure.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55bc21c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fd82ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space for the COB\n",
    "import nevergrad as ng\n",
    "\n",
    "def gaussian(x, mu=1.0, sigma=0.5, a=2):\n",
    "    return a * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "def compute_loss(cob, input_data, original_pred, layer_idx, original_loss,a=None,sigma_gaussian=None):\n",
    "    # Set up model with the new COB\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "\n",
    "    # model.network.load_state_dict(original_mlp_0.state_dict())\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    # teleported_model.set_weights(initial_weights)\n",
    "    \n",
    "    if args.teleport_gaussian:\n",
    "        # check wheter all elements of cob are zero\n",
    "        # TODO: maybe uncomment\n",
    "        # if torch.all(cob == 0):\n",
    "        #     cob = torch.ones_like(cob)\n",
    "        if False:\n",
    "            pass\n",
    "        else:\n",
    "            cob = gaussian(cob, mu=1.0, sigma=sigma_gaussian, a=a)\n",
    "\n",
    "    teleported_model = teleported_model.teleport(cob, reset_teleportation=True)\n",
    "\n",
    "    # Reset activation stats and run a forward pass\n",
    "    activation_stats = {}\n",
    "    for i, layer in enumerate(teleported_model.network.children()):\n",
    "            if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "                layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "    teleported_model.eval()\n",
    "    pred = teleported_model.network(input_data)\n",
    "    # teleported_model.train()\n",
    "\n",
    "    # Compute loss based on activation stats\n",
    "    loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "    loss /= original_loss\n",
    "    # second term of loss - difference between the cob and the ones tensor\n",
    "    # loss += 10 * (cob - torch.ones_like(cob)).abs().mean()\n",
    "    pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "    pred_error /= np.abs(original_pred).mean()\n",
    "    total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "    if random.random() < 0.01:\n",
    "        print(f\"Loss: {loss}, Prediction Error: {pred_error}, Total Loss: {total_loss}\")\n",
    "\n",
    "    return total_loss, pred_error, loss\n",
    "\n",
    "# Define the function to minimize using Nevergrad\n",
    "def ng_loss_function(cob_flat,a=None,sigma_gaussian=None,input_teleported_model=None, original_pred=None, layer_idx=None, original_loss=None):\n",
    "    cob = torch.tensor(cob_flat)\n",
    "    loss, pred_error,range_loss = compute_loss(cob, input_teleported_model, original_pred, layer_idx, original_loss, a=a, sigma_gaussian=sigma_gaussian)\n",
    "\n",
    "    # if random.random() < 0.01:\n",
    "    # print(f\"Loss: {loss}, Prediction Error: {pred_error}\")\n",
    "\n",
    "    global best_loss, cor_best_pred_error, cor_best_range\n",
    "    if best_loss is None or loss < best_loss:\n",
    "        best_loss = loss\n",
    "        cor_best_range = range_loss\n",
    "        cor_best_pred_error = pred_error\n",
    "\n",
    "    return [loss.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization = ng.p.Instrumentation(\n",
    "#     ng.p.Array(shape=(initial_cob.size()),lower=0.5,upper=1.2)\n",
    "# )\n",
    "# # Define Nevergrad optimizer\n",
    "# # optimizer = ng.optimizers.OnePlusOne(parametrization=parametrization, budget=args.steps)\n",
    "# # tmp = ng.optimizers.Chaining([ng.optimizers.CMAbounded,ng.optimizers.CMA],[150])\n",
    "# optimizer = ng.optimizers.CMA(parametrization=parametrization, budget=args.steps)\n",
    "# # optimizer = ng.optimizers.OnePlusOne(parametrization=parametrization, budget=args.steps)\n",
    "# # optimizer.suggest(np.ones(initial_cob.size()))\n",
    "# # optimizer.ask()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"nevergrad\")\n",
    "# import cma\n",
    "# warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import cma\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"nevergrad\")\n",
    "warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)\n",
    "\n",
    "# switching off the INFO messages\n",
    "logging.getLogger('nevergrad').setLevel(logging.WARNING)\n",
    "# show only error messages of ezkl library\n",
    "logging.getLogger('ezklearn').setLevel(logging.ERROR)\n",
    "logging.getLogger('ezkl').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define input_teleported_model (used in ng_loss_function)\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "# input_teleported_model = model.split_n(input_convs,0,half=True)\n",
    "# # save npy file\n",
    "# np.save(args.prefix_dir + \"input_teleported_model.npy\", input_teleported_model.detach().numpy())\n",
    "\n",
    "# # define original_pred (used in ng_loss_function)\n",
    "# original_pred = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "\n",
    "# # Perform optimization using Nevergrad\n",
    "# import functools\n",
    "# ng_loss_function_partial = functools.partial(ng_loss_function,\\\n",
    "#                                                 input_teleported_model=input_teleported_model, \\\n",
    "#                                                 original_pred=original_pred, \\\n",
    "#                                                 layer_idx=0)\n",
    "# recommendation = optimizer.minimize(ng_loss_function_partial)\n",
    "\n",
    "# # Extract the best COB found\n",
    "# best_cob = recommendation.value[0][0]\n",
    "# best_cob = torch.tensor(best_cob)\n",
    "\n",
    "# print(f\"Best COB found: {best_cob}\")\n",
    "# print(f\"Loss associated with the best COB: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print best candidate\n",
    "# print(recommendation.value[0][0])\n",
    "# print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Apply best COB and save model weights\n",
    "# LN = LinearNet()\n",
    "# LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(LN, model, 0)\n",
    "# LN = LN.teleport(best_cob, reset_teleportation=True)\n",
    "# torch.save(LN.network.state_dict(), args.prefix_dir + 'block0_cob_activation_norm_teleported.pth')\n",
    "\n",
    "# # Export the optimized model to ONNX\n",
    "# torch.onnx.export(LN.network, input_teleported_model, args.prefix_dir + 'block0_cob_activation_norm_teleported.onnx', verbose=False, export_params=True, opset_version=12, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "\n",
    "# # Print final results\n",
    "# print(\"Best COB found using Nevergrad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file and write the header if it doesn't exist\n",
    "import csv\n",
    "\n",
    "csv_file_path = args.prefix_dir + 'all_settings.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            'EXPERIMENT SETTINGS',\n",
    "            'Model_Number', 'Layer_Index',\n",
    "            # 'Activation_Loss', 'Prediction_Error',\n",
    "            'Activation_Loss',\n",
    "            'Input_Scale', 'Param_Scale', 'Scale_Rebase_Multiplier',\n",
    "            'Lookup_Range_0', 'Lookup_Range_1', 'Logrows', 'Num_Rows', 'Num_cols', 'Total_Assignments',\n",
    "            'Total_Constant_Size', 'Model_Output_Scales', \n",
    "            'Required_Range_Checks_0','Required_Range_Checks_1', 'Proof_Time_Seconds', 'Max_Memory' ,'Mean_Squared_Error', 'Mean_Abs_Percent_Error',\n",
    "    ])\n",
    "        \n",
    "\n",
    "            # 'EXPERIMENT SETTINGS'\n",
    "            # 'Model_Number', 'Layer_Index',\n",
    "            # 'Activation_Loss',\n",
    "            # 'Input_Scale', 'Param_Scale', 'Scale_Rebase_Multiplier',\n",
    "            # 'Lookup_Range_0', 'Lookup_Range_1', 'Logrows', 'Num_Rows', 'Num_cols', 'Total_Assignments'\n",
    "            # 'Total_Constant_Size', 'Model_Output_Scales', \n",
    "            # 'Required_Range_Checks_0','Required_Range_Checks_1', 'Proof_Time_Seconds', 'Max_Memory' ,'Mean_Squared_Error', 'Mean_Abs_Percent_Error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= START =========\n",
      "input_param_scale: 7, num_cols: 4, max_log_rows: 20, param_visibility: fixed\n",
      "layer_idx: 0 , \t  activation_stats: {'relu_1': {'norm': tensor(476.6286), 'max': tensor(6.2171), 'min': tensor(-6.8579), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(13.0750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n",
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9999895691871643, Prediction Error: 2.0716381186502986e-05, Total Loss: 1.0000931024551392\n",
      "Loss: 0.9999895691871643, Prediction Error: 2.072716597467661e-05, Total Loss: 1.0000932216644287\n",
      "Loss: 0.9999885559082031, Prediction Error: 2.0719900930998847e-05, Total Loss: 1.0000921487808228\n",
      "Loss: 0.9999895095825195, Prediction Error: 2.0773213691427372e-05, Total Loss: 1.0000933408737183\n",
      "Loss: 0.9999884963035583, Prediction Error: 2.0748120732605457e-05, Total Loss: 1.0000922679901123\n",
      "Loss: 0.9999895095825195, Prediction Error: 2.0728219169541262e-05, Total Loss: 1.0000931024551392\n",
      "Loss: 0.9999885559082031, Prediction Error: 2.0706926079583354e-05, Total Loss: 1.0000921487808228\n",
      "Loss: 0.9999905228614807, Prediction Error: 2.07237244467251e-05, Total Loss: 1.0000941753387451\n",
      "Loss: 0.9999884963035583, Prediction Error: 2.068293724732939e-05, Total Loss: 1.0000919103622437\n",
      "Loss: 0.9999885559082031, Prediction Error: 2.0709814634756185e-05, Total Loss: 1.0000921487808228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 210\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# optimizer.suggest(np.ones(initial_cob_idx.size()),args.a_gaussian,args.sigma_gaussian)\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# optimizer.suggest(np.ones(initial_cob_idx.size()))\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Perform optimization using Nevergrad\u001b[39;00m\n\u001b[1;32m    203\u001b[0m ng_loss_function_partial \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    204\u001b[0m     ng_loss_function,\n\u001b[1;32m    205\u001b[0m     input_teleported_model\u001b[38;5;241m=\u001b[39minput_teleported_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     original_loss \u001b[38;5;241m=\u001b[39m original_loss_idx\n\u001b[1;32m    209\u001b[0m )\n\u001b[0;32m--> 210\u001b[0m recommendation \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mng_loss_function_partial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Extract the best COB found\u001b[39;00m\n\u001b[1;32m    213\u001b[0m best_cob \u001b[38;5;241m=\u001b[39m recommendation\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/nevergrad/optimization/base.py:678\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[0;34m(self, objective_function, executor, batch_mode, verbosity, constraint_violation)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finished_jobs:\n\u001b[1;32m    677\u001b[0m     x, job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finished_jobs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 678\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m constraint_violation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell(\n\u001b[1;32m    681\u001b[0m             x, result, [f(x\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m constraint_violation]\n\u001b[1;32m    682\u001b[0m         )  \u001b[38;5;66;03m# TODO: this is not parallelized, wtf!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/nevergrad/optimization/utils.py:145\u001b[0m, in \u001b[0;36mDelayedJob.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tp\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed:\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "Cell \u001b[0;32mIn[56], line 54\u001b[0m, in \u001b[0;36mng_loss_function\u001b[0;34m(cob_flat, a, sigma_gaussian, input_teleported_model, original_pred, layer_idx, original_loss)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mng_loss_function\u001b[39m(cob_flat,a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,sigma_gaussian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,input_teleported_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, original_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, layer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, original_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     cob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(cob_flat)\n\u001b[0;32m---> 54\u001b[0m     loss, pred_error,range_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_teleported_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_gaussian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_gaussian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# if random.random() < 0.01:\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# print(f\"Loss: {loss}, Prediction Error: {pred_error}\")\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m best_loss, cor_best_pred_error, cor_best_range\n",
      "Cell \u001b[0;32mIn[56], line 10\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(cob, input_data, original_pred, layer_idx, original_loss, a, sigma_gaussian)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(cob, input_data, original_pred, layer_idx, original_loss,a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,sigma_gaussian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Set up model with the new COB\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     teleported_model \u001b[38;5;241m=\u001b[39m LinearNet()\n\u001b[0;32m---> 10\u001b[0m     teleported_model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralTeleportationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteleported_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m197\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# model.network.load_state_dict(original_mlp_0.state_dict())\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     load_ln_weights(teleported_model, model, layer_idx)\n",
      "File \u001b[0;32m~/Phd research/nerual_transport/neuralteleportation/neuralteleportation/neuralteleportationmodel.py:32\u001b[0m, in \u001b[0;36mNeuralTeleportationModel.__init__\u001b[0;34m(self, network, input_shape)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrapher \u001b[38;5;241m=\u001b[39m NetworkGrapher(network, sample_input)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrapher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m was_training:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Phd research/nerual_transport/neuralteleportation/neuralteleportation/network_graph.py:88\u001b[0m, in \u001b[0;36mNetworkGrapher.get_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Use torch.jit to get the graph of the network.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    network graph : [{'in': [], 'idx': [], 'out' = [], 'module' nn.Module}, ... ]\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m trace \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_input,))\n\u001b[0;32m---> 88\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_pytorch_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph_from_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# remove empty keys\u001b[39;00m\n\u001b[1;32m     91\u001b[0m graph \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k}\n",
      "File \u001b[0;32m~/Phd research/nerual_transport/neuralteleportation/neuralteleportation/network_graph.py:127\u001b[0m, in \u001b[0;36mNetworkGrapher.extract_pytorch_graph\u001b[0;34m(inlined_graph)\u001b[0m\n\u001b[1;32m    124\u001b[0m layers \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inlined_graph\u001b[38;5;241m.\u001b[39mnodes()):\n\u001b[0;32m--> 127\u001b[0m     layers[node\u001b[38;5;241m.\u001b[39mscopeName()][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mextend([i\u001b[38;5;241m.\u001b[39munique() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m    128\u001b[0m     layers[node\u001b[38;5;241m.\u001b[39mscopeName()][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mextend([i\u001b[38;5;241m.\u001b[39munique() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39moutputs()])\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Remove indexes that are both in input and output\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "# args.input_param_scale = 12\n",
    "# args.num_cols = 2\n",
    "\n",
    "# array_param_visibility = [\"private\",\"fixed\"]\n",
    "# array_input_param_scale = [12,7]\n",
    "# array_num_cols = [2,4,8,16,32]\n",
    "# array_max_log_rows = [19,20,21,22,23]\n",
    "# array_scale_rebase = [1,2]\n",
    "\n",
    "# array_param_visibility = [\"private\",\"fixed\"]\n",
    "array_param_visibility = [\"fixed\"]\n",
    "# array_input_param_scale = [12,7]\n",
    "array_input_param_scale = [7]\n",
    "# array_num_cols = [1,2,4,8,16,32]\n",
    "array_num_cols = [4]\n",
    "# array_max_log_rows = [19,20,21,22,23]\n",
    "# array_max_log_rows = [19,20,21,22,23,24,25,-1]\n",
    "array_max_log_rows = [20]\n",
    "# array_scale_rebase = [1,2,-1]\n",
    "array_scale_rebase = [1]\n",
    "\n",
    "# iterate over all the possible combinations\n",
    "combinations = list(itertools.product(array_param_visibility, array_input_param_scale, array_num_cols, array_max_log_rows, array_scale_rebase))\n",
    "\n",
    "list_of_gaussian_teleportation = []\n",
    "list_of_uniform_teleportation = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "dic_uniform_teleportation = {\n",
    "    0: {\"down_range\": 0, \"up_range\": 2, \"sigma\": 0.15},\n",
    "    1: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.10},\n",
    "    2: {\"down_range\": 0.25, \"up_range\": 1.7, \"sigma\": 0.05},\n",
    "    3: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    4: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    5: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    6: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    7: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    8: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    9: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    10: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "    11: {\"down_range\": 0.4, \"up_range\": 1.6, \"sigma\": 0.15},\n",
    "}\n",
    "list_of_no_teleportation = [0]\n",
    "\n",
    "# with no gradient pytorch\n",
    "with torch.no_grad():\n",
    "    # iterate over all the possible combinations\n",
    "    for p in combinations:\n",
    "        param_visibility, input_param_scale, num_cols, max_log_rows, scale_rebase = p\n",
    "        # string experiment_settings as comma separated values\n",
    "        experiment_settings = f\"{param_visibility}/{input_param_scale}/{num_cols}/{max_log_rows}/{scale_rebase}\"\n",
    "        print(\"========= START =========\")\n",
    "        print(f\"input_param_scale: {input_param_scale}, num_cols: {num_cols}, max_log_rows: {max_log_rows}, param_visibility: {param_visibility}\")\n",
    "\n",
    "        # copy the model\n",
    "        new_model = copy.deepcopy(model)\n",
    "        \n",
    "        for layer_idx in range(model.depth):\n",
    "            # define the arguments of nevergrad\n",
    "            args.teleport_gaussian = True if layer_idx in list_of_gaussian_teleportation else False\n",
    "\n",
    "            if args.teleport_gaussian:\n",
    "                args.lower = (-1)\n",
    "                args.upper = 1\n",
    "            elif layer_idx in list_of_uniform_teleportation:\n",
    "                args.lower = dic_uniform_teleportation[layer_idx][\"down_range\"]\n",
    "                # args.lower = 0\n",
    "                args.upper = dic_uniform_teleportation[layer_idx][\"up_range\"]\n",
    "                # args.upper = 2\n",
    "            else:\n",
    "                args.lower = 1 - 1e-6\n",
    "                args.upper = 1 + 1e-6\n",
    "            args.steps = 300 * 10\n",
    "            args.sigma = 0.15\n",
    "            args.pred_mul = 5\n",
    "            args.a_gaussian = 5.5\n",
    "            args.sigma_gaussian = 0.5\n",
    "            args.optimizer_name = \"HSCMA\"\n",
    "\n",
    "            # check the experiment_settings and layer_idx exists in the csv file\n",
    "            with open(csv_file_path, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                exist_flag = False\n",
    "                for row in reader:\n",
    "                    if row[0] == experiment_settings and int(row[2]) == layer_idx:\n",
    "                        print(f\"Experiment settings: {experiment_settings} and layer_idx: {layer_idx} already exists in the csv file.\")\n",
    "                        exist_flag = True\n",
    "                        break\n",
    "            if exist_flag:\n",
    "                continue\n",
    "                \n",
    "    # #       TODO:\n",
    "    #         if layer_idx >=2:\n",
    "    #             continue\n",
    "\n",
    "            # Hook for the intermediate output of the block\n",
    "            original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "            activation_stats_idx = {}\n",
    "            for i,layer in enumerate(original_mlp_idx.children()):\n",
    "                if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "                    layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "\n",
    "            # run the mlp model to find original_loss\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "            # print activation stats\n",
    "            print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "            original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "            print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "            # Load the teleported model\n",
    "            teleported_model_idx = LinearNet()\n",
    "            teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "            load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "\n",
    "            # get initial weights and cob\n",
    "            initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "            initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "            # track best loss\n",
    "            global best_loss\n",
    "            best_loss = 1e9\n",
    "\n",
    "            # # nevergrad optimization\n",
    "            # parametrization = ng.p.Instrumentation(\n",
    "            #     ng.p.Array(shape=(initial_cob_idx.size()),lower=0.5,upper=1.2)\n",
    "            # )\n",
    "            # optimizer = ng.optimizers.CMA(parametrization=parametrization, budget=args.steps)\n",
    "            # optimizer.suggest(np.ones(initial_cob_idx.size()))\n",
    "\n",
    "            # define input_teleported_model (used in ng_loss_function)\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "            # input_teleported_model = model.split_n(input_convs,layer_idx,half=True)\n",
    "            # instead of model using the copy_model to consider the previous layer teleportation to the input of the current layer\n",
    "            input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "            # save npy file using in python checking script\n",
    "            np.save(args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\", input_teleported_model.detach().numpy())\n",
    "\n",
    "            # define original_pred (used in ng_loss_function)\n",
    "            input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "            original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "            # # Perform optimization using Nevergrad\n",
    "            # ng_loss_function_partial = functools.partial(ng_loss_function,\\\n",
    "            #                                             input_teleported_model=input_teleported_model, \\\n",
    "            #                                             original_pred=original_pred, \\\n",
    "            #                                             layer_idx=layer_idx)\n",
    "\n",
    "            # recommendation = optimizer.minimize(ng_loss_function_partial)\n",
    "\n",
    "            # # Extract the best COB found\n",
    "            # best_cob = recommendation.value[0][0]\n",
    "            # best_cob = torch.tensor(best_cob)\n",
    "            # print(\"BEST LOSS:\",best_loss)\n",
    "\n",
    "            # Apply best COB and save model weights\n",
    "            LN = LinearNet()\n",
    "            LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "            load_ln_weights(LN, model, layer_idx)\n",
    "            # check whether the teleportation .pth already exists\n",
    "            # TODO: uncomment\n",
    "            # if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "            #     print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "            #     LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "            #     best_loss = torch.tensor(best_loss).detach().cpu()\n",
    "            if False:\n",
    "                print(\"NO WAY\")\n",
    "            else:\n",
    "                # nevergrad optimization\n",
    "                # if args.teleport_gaussian and layer_idx != 2 and layer_idx != 3:\n",
    "                s = initial_cob_idx.size()\n",
    "                if args.teleport_gaussian:\n",
    "                    parametrization = ng.p.Instrumentation(\n",
    "                        ng.p.Array(lower=args.lower,upper=args.upper,shape=(s)).set_mutation(sigma=args.sigma),\n",
    "                        ng.p.Scalar(lower= args.a_gaussian - 3, upper= args.a_gaussian + 3).set_mutation(sigma=0.3),\n",
    "                        ng.p.Scalar(lower= args.sigma_gaussian - 0.3, upper= args.sigma_gaussian + 0.3).set_mutation(sigma=0.05),\n",
    "                    )\n",
    "                    optimizer_cls = ng.optimizers.registry[args.optimizer_name]\n",
    "                    optimizer = optimizer_cls(parametrization=parametrization, budget=args.steps)\n",
    "                    # optimizer.suggest(np.zeros(s),args.a_gaussian,args.sigma_gaussian)\n",
    "                # elif layer_idx!=2 and layer_idx != 3:\n",
    "                else:\n",
    "                    parametrization = ng.p.Instrumentation(\n",
    "                        ng.p.Array(lower=args.lower,upper=args.upper,shape=(s)).set_mutation(sigma=dic_uniform_teleportation[layer_idx][\"sigma\"]),\n",
    "                        # ng.p.Scalar(lower= 0, upper= 1e-9),\n",
    "                        # ng.p.Scalar(lower= 0, upper= 1e-9),\n",
    "                    )\n",
    "                    \n",
    "                    optimizer = ng.optimizers.CMA(parametrization=parametrization, budget=args.steps)\n",
    "                    # optimizer.suggest(np.ones(initial_cob_idx.size()),args.a_gaussian,args.sigma_gaussian)\n",
    "                    # optimizer.suggest(np.ones(initial_cob_idx.size()))\n",
    "                \n",
    "                # else:\n",
    "                #     print(\"INJAM LAYER_IDX 2 OR 3====\")\n",
    "\n",
    "                # Perform optimization using Nevergrad\n",
    "                ng_loss_function_partial = functools.partial(\n",
    "                    ng_loss_function,\n",
    "                    input_teleported_model=input_teleported_model,\n",
    "                    original_pred=original_pred,\n",
    "                    layer_idx=layer_idx,\n",
    "                    original_loss = original_loss_idx\n",
    "                )\n",
    "                recommendation = optimizer.minimize(ng_loss_function_partial)\n",
    "\n",
    "                # Extract the best COB found\n",
    "                best_cob = recommendation.value[0][0]\n",
    "                best_cob = torch.tensor(best_cob).detach().cpu()\n",
    "\n",
    "                if args.teleport_gaussian:\n",
    "                    best_a = recommendation.value[0][1]\n",
    "                    best_sigma_gaussian = recommendation.value[0][2]\n",
    "                else:\n",
    "                    best_a = None\n",
    "                    best_sigma_gaussian = None\n",
    "\n",
    "                print(\"BEST LOSS:\",best_loss, \"\\t cor_best_pred_error:\",cor_best_pred_error, \"\\t cor_best_range:\",cor_best_range)\n",
    "\n",
    "                # Apply best COB and save model weights\n",
    "                if layer_idx in list_of_no_teleportation:\n",
    "                    print(\"====== NO TELEPORTATION =====\")\n",
    "                else:\n",
    "                    if True:\n",
    "                        if args.teleport_gaussian:\n",
    "                            print(\"====== TELEPORTING USING GAUSSIAN =====\")\n",
    "                            LN = LN.teleport(gaussian(best_cob, mu=1.0, sigma=best_sigma_gaussian, a=best_a), reset_teleportation=True)\n",
    "                        else:\n",
    "                            print(\"====== TELEPORTING USING UNIFORM =====\")\n",
    "                            LN = LN.teleport(best_cob, reset_teleportation=True)\n",
    "                torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "\n",
    "            # Apply the teleportation to the new_model (Using for computing the next layer inputs)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k: v for k, v in sd.items() if 'norm2' not in k}\n",
    "            new_model.blocks[layer_idx].mlp.load_state_dict(sd)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k.replace('norm2.',''): v for k, v in sd.items() if 'norm2' in k}\n",
    "            new_model.blocks[layer_idx].norm2.load_state_dict(sd)\n",
    "\n",
    "            # Export the optimized model to ONNX\n",
    "            torch.onnx.export(LN.network, input_teleported_model, args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.onnx', verbose=False, export_params=True, opset_version=15, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "\n",
    "            # check the validation of the teleportation\n",
    "            \n",
    "            # 1.extract onnx corrosponding to the teleported model (in original onnx)\n",
    "            input_path = args.prefix_dir + f\"network_split_{layer_idx}_False.onnx\"\n",
    "            output_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "            input_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "            output_names = [f\"/blocks.{layer_idx}/mlp/fc2/Add_output_0\"]\n",
    "            onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "            \n",
    "            # # 2. run the python code\n",
    "            # print(\"===== RUNNING PYTHON CODE =====\")\n",
    "            # a = args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\"\n",
    "            # b = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "            # !python onnx_inference.py \\\n",
    "            #     --input {a} --model1 {output_path} --model2 {b}\n",
    "            # print(\"===== PYTHON CODE FINISHED =====\")\n",
    "            # time.sleep(5)\n",
    "\n",
    "            # ezkl to find the resources possible reduction\n",
    "            import ezkl\n",
    "            run_args = ezkl.PyRunArgs()\n",
    "            run_args.input_visibility = \"public\"\n",
    "            \n",
    "            # run_args.param_visibility = \"private\"\n",
    "            run_args.param_visibility = param_visibility\n",
    "\n",
    "            run_args.output_visibility = \"public\"\n",
    "\n",
    "            # run_args.input_scale = args.input_param_scale\n",
    "            run_args.input_scale = input_param_scale\n",
    "            # run_args.param_scale = args.input_param_scale\n",
    "            run_args.param_scale = input_param_scale\n",
    "\n",
    "            # run_args.logrows = args.log_rows\n",
    "            if max_log_rows != (-1):\n",
    "                run_args.logrows = max_log_rows\n",
    "\n",
    "            # run_args.num_inner_cols = args.num_cols\n",
    "            run_args.num_inner_cols = num_cols\n",
    "\n",
    "            # run_args.scale_rebase_multiplier = args.scale_rebase_multiplier\n",
    "            if scale_rebase != (-1):\n",
    "                run_args.scale_rebase_multiplier = scale_rebase\n",
    "\n",
    "            run_args.variables = [('batch_size', BATCHS)]\n",
    "\n",
    "            # get SRS\n",
    "            ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)\n",
    "\n",
    "            name0 = f'block{layer_idx}_cob_activation_norm'\n",
    "            name1 = f'block{layer_idx}_cob_activation_norm_teleported'\n",
    "            rng = [name0,name1]\n",
    "\n",
    "            for m in rng:\n",
    "                print(\"==== Model:\",m, \" in Layer:\",layer_idx,\"====\")\n",
    "\n",
    "                if m == name0:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "                    x = input_org.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "                    x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "\n",
    "                # Generate the calibration data\n",
    "                data = dict(input_data=[x])\n",
    "                cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "                json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "                # settings_path = f'{args.prefix_dir} settings_{model}.json'\n",
    "                settings_path = args.prefix_dir + f'settings_{m}_{layer_idx}.json'\n",
    "                # res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                # assert res == True\n",
    "\n",
    "                # try:\n",
    "                #     if scale_rebase != (-1):\n",
    "                #         res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale],scale_rebase_multiplier=[scale_rebase],max_logrows=max_log_rows)\n",
    "                #     elif max_log_rows != (-1):\n",
    "                #         res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale],max_logrows=max_log_rows)\n",
    "                #     else:\n",
    "                #         res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale])\n",
    "                # except:\n",
    "                #     print(\"ERROR in calibration: \",m,layer_idx)\n",
    "                #     res = False\n",
    "                #     # write in the csv file\n",
    "                #     with open(csv_file_path, mode='a', newline='') as file:\n",
    "                #         writer = csv.writer(file)\n",
    "                #         l = original_loss_idx if m == name0 else best_loss\n",
    "                #         writer.writerow([\n",
    "                #             experiment_settings,\n",
    "                #             m,layer_idx,\n",
    "                #             l,\n",
    "                #             None, None, None,\n",
    "                #             None, None, None, None, None, None,\n",
    "                #             None, None, None, None, None, None, None, None\n",
    "                #         ])\n",
    "                #     continue\n",
    "\n",
    "                # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier])\n",
    "                # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale])\n",
    "                # assert res == True\n",
    "\n",
    "                # extract the resources\n",
    "                # settings = json.load(open(settings_path))\n",
    "                # rg = settings.get('run_args', {})\n",
    "                # input_scale = rg.get('input_scale', None)\n",
    "                # param_scale = rg.get('param_scale', None)\n",
    "                # output_scale = settings.get('model_output_scales', None)\n",
    "                # scale_rebase_multiplier = rg.get('scale_rebase_multiplier', None)\n",
    "                # lookup_range = rg.get('lookup_range', [None, None])\n",
    "                # logrows = rg.get('logrows', None)\n",
    "                # num_rows = settings.get('num_rows', None)\n",
    "                # total_assignments = settings.get('total_assignments', None)\n",
    "                # num_cols = rg.get('num_inner_cols', None)\n",
    "                # total_constant_size = settings.get('total_const_size', None)\n",
    "                \n",
    "                # print(logrows, num_rows, num_cols ,lookup_range, scale_rebase_multiplier, output_scale)\n",
    "                # print(\"===============================\")\n",
    "\n",
    "                # 'EXPERIMENT SETTINGS'\n",
    "                # 'Model_Number', 'Layer_Index',\n",
    "                # 'Activation_Loss',\n",
    "                # 'Input_Scale', 'Param_Scale', 'Scale_Rebase_Multiplier',\n",
    "                # 'Lookup_Range_0', 'Lookup_Range_1', 'Logrows', 'Num_Rows', 'Num_cols', 'Total_Assignments'\n",
    "                # 'Total_Constant_Size', 'Model_Output_Scales', \n",
    "                # 'Required_Range_Checks_0','Required_Range_Checks_1', 'Proof_Time_Seconds', 'Max_Memory' ,'Mean_Squared_Error', 'Mean_Abs_Percent_Error'\n",
    "\n",
    "                # activation loss is equal to the original loss if m is equal to name0 else it is the best loss\n",
    "                activation_loss = original_loss_idx if m == name0 else best_loss\n",
    "\n",
    "                # write the results to the csv file\n",
    "                # with open(csv_file_path, mode='a', newline='') as file:\n",
    "                #     writer = csv.writer(file)\n",
    "                #     writer.writerow([\n",
    "                #         experiment_settings,\n",
    "                #         m,layer_idx,\n",
    "                #         activation_loss.item(),\n",
    "                #         input_scale, param_scale, scale_rebase_multiplier,\n",
    "                #         lookup_range[0], lookup_range[1], logrows, num_rows, num_cols, total_assignments,\n",
    "                #         total_constant_size, output_scale,\n",
    "                #           None, None, None, None, None, None\n",
    "                #     ])\n",
    "            \n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import functools\n",
    "\n",
    "\n",
    "# # args.input_param_scale = 12\n",
    "# # args.num_cols = 2\n",
    "\n",
    "# array_param_visibility = [\"private\",\"fixed\"]\n",
    "# array_input_param_scale = [12,7]\n",
    "# array_num_cols = [2,4,8,16,32]\n",
    "# # array_max_log_rows = [19,20,21,22,23]\n",
    "# array_max_log_rows = [20,21,22,23,24,25]\n",
    "# array_scale_rebase = [1,2]\n",
    "\n",
    "# # iterate over all the possible combinations\n",
    "# combinations = list(itertools.product(array_param_visibility, array_input_param_scale, array_num_cols, array_max_log_rows, array_scale_rebase))\n",
    "\n",
    "\n",
    "# for p in combinations:\n",
    "\n",
    "#     param_visibility, input_param_scale, num_cols, max_log_rows, scale_rebase = p\n",
    "#     # string experiment_settings as comma separated values\n",
    "#     experiment_settings = f\"{param_visibility}/{input_param_scale}/{num_cols}/{max_log_rows}/{scale_rebase}\"\n",
    "#     print(\"========= START =========\")\n",
    "#     print(f\"input_param_scale: {input_param_scale}, num_cols: {num_cols}, max_log_rows: {max_log_rows}, param_visibility: {param_visibility}\")\n",
    "\n",
    "    \n",
    "#     for layer_idx in range(model.depth):\n",
    "        \n",
    "#         # check the experiment_settings and layer_idx exists in the csv file\n",
    "#         with open(csv_file_path, mode='r') as file:\n",
    "#             reader = csv.reader(file)\n",
    "#             exist_flag = False\n",
    "#             for row in reader:\n",
    "#                 if row[0] == experiment_settings and int(row[2]) == layer_idx:\n",
    "#                     print(f\"Experiment settings: {experiment_settings} and layer_idx: {layer_idx} already exists in the csv file.\")\n",
    "#                     exist_flag = True\n",
    "#                     break\n",
    "\n",
    "#         if exist_flag:\n",
    "#             continue\n",
    "\n",
    "#         # Hook for the intermediate output of the block\n",
    "#         original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "#         activation_stats_idx = {}\n",
    "#         for i,layer in enumerate(original_mlp_idx.children()):\n",
    "#             if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#                 layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "\n",
    "#         # run the mlp model to find original_loss\n",
    "#         input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "#         original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "#         # print activation stats\n",
    "#         print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "#         original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "#         print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "#         # Load the teleported model\n",
    "#         teleported_model_idx = LinearNet()\n",
    "#         teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "#         load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "\n",
    "#         # get initial weights and cob\n",
    "#         initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "#         initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "#         # track best loss\n",
    "#         global best_loss\n",
    "#         best_loss = 1e9\n",
    "\n",
    "#         # nevergrad optimization\n",
    "#         parametrization = ng.p.Instrumentation(\n",
    "#             ng.p.Array(shape=(initial_cob_idx.size()),lower=0.5,upper=1.2)\n",
    "#         )\n",
    "#         optimizer = ng.optimizers.CMA(parametrization=parametrization, budget=args.steps)\n",
    "#         optimizer.suggest(np.ones(initial_cob_idx.size()))\n",
    "\n",
    "#         # define input_teleported_model (used in ng_loss_function)\n",
    "#         input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "#         input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "#         input_teleported_model = model.split_n(input_convs,layer_idx,half=True)\n",
    "#         # save npy file using in python checking script\n",
    "#         np.save(args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\", input_teleported_model.detach().numpy())\n",
    "\n",
    "#         # define original_pred (used in ng_loss_function)\n",
    "#         original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "\n",
    "#         # Perform optimization using Nevergrad\n",
    "#         ng_loss_function_partial = functools.partial(ng_loss_function,\\\n",
    "#                                                     input_teleported_model=input_teleported_model, \\\n",
    "#                                                     original_pred=original_pred, \\\n",
    "#                                                     layer_idx=layer_idx)\n",
    "\n",
    "#         recommendation = optimizer.minimize(ng_loss_function_partial)\n",
    "\n",
    "#         # Extract the best COB found\n",
    "#         best_cob = recommendation.value[0][0]\n",
    "#         best_cob = torch.tensor(best_cob)\n",
    "#         print(\"BEST LOSS:\",best_loss)\n",
    "\n",
    "#         # Apply best COB and save model weights\n",
    "#         LN = LinearNet()\n",
    "#         LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "#         load_ln_weights(LN, model, layer_idx)\n",
    "#         LN = LN.teleport(best_cob, reset_teleportation=True)\n",
    "#         torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "\n",
    "#         # Export the optimized model to ONNX\n",
    "#         torch.onnx.export(LN.network, input_teleported_model, args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.onnx', verbose=False, export_params=True, opset_version=15, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "\n",
    "#         # check the validation of the teleportation\n",
    "        \n",
    "#         # 1.extract onnx corrosponding to the teleported model (in original onnx)\n",
    "#         input_path = args.prefix_dir + f\"network_split_{layer_idx}_False.onnx\"\n",
    "#         output_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "#         input_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "#         output_names = [f\"/blocks.{layer_idx}/mlp/fc2/Add_output_0\"]\n",
    "#         onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "        \n",
    "#         # # 2. run the python code\n",
    "#         # print(\"===== RUNNING PYTHON CODE =====\")\n",
    "#         # a = args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\"\n",
    "#         # b = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "#         # !python onnx_inference.py \\\n",
    "#         #     --input {a} --model1 {output_path} --model2 {b}\n",
    "#         # print(\"===== PYTHON CODE FINISHED =====\")\n",
    "#         # time.sleep(5)\n",
    "\n",
    "#         # ezkl to find the resources possible reduction\n",
    "#         import ezkl\n",
    "#         run_args = ezkl.PyRunArgs()\n",
    "#         run_args.input_visibility = \"public\"\n",
    "        \n",
    "#         # run_args.param_visibility = \"private\"\n",
    "#         run_args.param_visibility = param_visibility\n",
    "\n",
    "#         run_args.output_visibility = \"public\"\n",
    "\n",
    "#         # run_args.input_scale = args.input_param_scale\n",
    "#         run_args.input_scale = input_param_scale\n",
    "#         # run_args.param_scale = args.input_param_scale\n",
    "#         run_args.param_scale = input_param_scale\n",
    "\n",
    "#         # run_args.logrows = args.log_rows\n",
    "#         run_args.logrows = max_log_rows\n",
    "\n",
    "#         # run_args.num_inner_cols = args.num_cols\n",
    "#         run_args.num_inner_cols = num_cols\n",
    "\n",
    "#         # run_args.scale_rebase_multiplier = args.scale_rebase_multiplier\n",
    "#         run_args.scale_rebase_multiplier = scale_rebase\n",
    "\n",
    "#         run_args.variables = [('batch_size', BATCHS)]\n",
    "\n",
    "#         # get SRS\n",
    "#         ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)\n",
    "\n",
    "#         name0 = f'block{layer_idx}_cob_activation_norm'\n",
    "#         name1 = f'block{layer_idx}_cob_activation_norm_teleported'\n",
    "#         rng = [name0,name1]\n",
    "        \n",
    "#         # Generate the calibration data\n",
    "#         x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "#         data = dict(input_data=[x])\n",
    "#         cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "#         json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "#         for m in rng:\n",
    "#             print(\"==== Model:\",m, \" in Layer:\",layer_idx,\"====\")\n",
    "\n",
    "#             if m == name0:\n",
    "#                 model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "#             else:\n",
    "#                 model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "\n",
    "#             # settings_path = f'{args.prefix_dir} settings_{model}.json'\n",
    "#             settings_path = args.prefix_dir + f'settings_{m}_{layer_idx}.json'\n",
    "\n",
    "#             res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "#             assert res == True\n",
    "\n",
    "#             try:\n",
    "#                 res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale],scale_rebase_multiplier=[scale_rebase],max_logrows=max_log_rows)\n",
    "#             except:\n",
    "#                 print(\"ERROR in calibration: \",m,layer_idx)\n",
    "#                 res = False\n",
    "#                 # write in the csv file\n",
    "#                 with open(csv_file_path, mode='a', newline='') as file:\n",
    "#                     writer = csv.writer(file)\n",
    "#                     l = original_loss_idx if m == name0 else best_loss\n",
    "#                     writer.writerow([\n",
    "#                         experiment_settings,\n",
    "#                         m,layer_idx,\n",
    "#                         l,\n",
    "#                         None, None, None,\n",
    "#                         None, None, None, None, None, None,\n",
    "#                         None, None, None, None, None, None, None, None\n",
    "#                     ])\n",
    "#                 continue\n",
    "\n",
    "#             # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier])\n",
    "#             # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale])\n",
    "\n",
    "#             assert res == True\n",
    "\n",
    "#             # extract the resources\n",
    "#             settings = json.load(open(settings_path))\n",
    "#             rg = settings.get('run_args', {})\n",
    "#             input_scale = rg.get('input_scale', None)\n",
    "#             param_scale = rg.get('param_scale', None)\n",
    "#             output_scale = settings.get('model_output_scales', None)\n",
    "#             scale_rebase_multiplier = rg.get('scale_rebase_multiplier', None)\n",
    "#             lookup_range = rg.get('lookup_range', [None, None])\n",
    "#             logrows = rg.get('logrows', None)\n",
    "#             num_rows = settings.get('num_rows', None)\n",
    "#             total_assignments = settings.get('total_assignments', None)\n",
    "#             num_cols = rg.get('num_inner_cols', None)\n",
    "#             total_constant_size = settings.get('total_const_size', None)\n",
    "            \n",
    "#             print(logrows, num_rows, num_cols ,lookup_range, scale_rebase_multiplier, output_scale)\n",
    "#             print(\"===============================\")\n",
    "\n",
    "#             # 'EXPERIMENT SETTINGS'\n",
    "#             # 'Model_Number', 'Layer_Index',\n",
    "#             # 'Activation_Loss',\n",
    "#             # 'Input_Scale', 'Param_Scale', 'Scale_Rebase_Multiplier',\n",
    "#             # 'Lookup_Range_0', 'Lookup_Range_1', 'Logrows', 'Num_Rows', 'Num_cols', 'Total_Assignments'\n",
    "#             # 'Total_Constant_Size', 'Model_Output_Scales', \n",
    "#             # 'Required_Range_Checks_0','Required_Range_Checks_1', 'Proof_Time_Seconds', 'Max_Memory' ,'Mean_Squared_Error', 'Mean_Abs_Percent_Error'\n",
    "\n",
    "#             # activation loss is equal to the original loss if m is equal to name0 else it is the best loss\n",
    "#             activation_loss = original_loss_idx if m == name0 else best_loss\n",
    "\n",
    "#             # write the results to the csv file\n",
    "#             with open(csv_file_path, mode='a', newline='') as file:\n",
    "#                 writer = csv.writer(file)\n",
    "#                 writer.writerow([\n",
    "#                     experiment_settings,\n",
    "#                     m,layer_idx,\n",
    "#                     activation_loss.item(),\n",
    "#                     input_scale, param_scale, scale_rebase_multiplier,\n",
    "#                     lookup_range[0], lookup_range[1], logrows, num_rows, num_cols, total_assignments,\n",
    "#                     total_constant_size, output_scale,\n",
    "#                     None, None, None, None, None, None\n",
    "#                 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract onnx of the teleported model\n",
    "# input_path = args.prefix_dir + \"network_split_0_False.onnx\"\n",
    "# output_path = args.prefix_dir + \"block0_cob_activation_norm.onnx\"\n",
    "# input_names = [\"/blocks.0/Add_2_output_0\"]\n",
    "# output_names = [\"/blocks.0/mlp/fc2/Add_output_0\"]\n",
    "# onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python onnx_inference.py --input {args.prefix_dir + 'input_teleported_model.npy'} \\\n",
    "#     --model1 {args.prefix_dir + 'block0_cob_activation_norm.onnx'} \\\n",
    "#     --model2 {args.prefix_dir + 'block0_cob_activation_norm_teleported.onnx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.input_param_scale: 7\n",
      "args.logrows: 20\n",
      "args.num_cols: 2\n",
      "args.scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "# print important args related to ezkl\n",
    "print(\"args.input_param_scale:\",args.input_param_scale)\n",
    "print(\"args.logrows:\",args.log_rows)\n",
    "print(\"args.num_cols:\",args.num_cols)\n",
    "print(\"args.scale_rebase_multiplier:\",args.scale_rebase_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future pending cb=[<builtins.PyDoneCallback object at 0x362e0d7b0>()]>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ezkl\n",
    "\n",
    "run_args = ezkl.PyRunArgs()\n",
    "run_args.input_visibility = \"public\"\n",
    "# TODO: change that to fixed\n",
    "run_args.param_visibility = \"fixed\"\n",
    "run_args.output_visibility = \"public\"\n",
    "run_args.input_scale = args.input_param_scale\n",
    "run_args.param_scale = args.input_param_scale\n",
    "run_args.logrows = args.log_rows\n",
    "run_args.num_inner_cols = args.num_cols\n",
    "run_args.scale_rebase_multiplier = args.scale_rebase_multiplier\n",
    "run_args.variables = [('batch_size', BATCHS)]\n",
    "\n",
    "ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "001240b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ezkl version: 100.100.101\n"
     ]
    }
   ],
   "source": [
    "# print ezkl version\n",
    "print(\"ezkl version:\",ezkl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source ~/.config/envman/PATH.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !RUST_LOG=trace\n",
    "\n",
    "# rng = ['block0_cob_activation_norm','block0_cob_activation_norm_teleported']\n",
    "\n",
    "# # Generate the calibration data\n",
    "# x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "# data = dict(input_data=[x])\n",
    "# cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "# json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "# for model in rng:\n",
    "\n",
    "#     print(\"==== Model:\",model, \"====\")\n",
    "\n",
    "#     model_path = args.prefix_dir + model + \".onnx\"\n",
    "#     settings_path = f'{args.prefix_dir} settings_{model}.json'\n",
    "\n",
    "#     res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "#     assert res == True\n",
    "\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier],max_logrows=args.log_rows)\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier])\n",
    "#     res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale])\n",
    "#     assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_82733/2906073395.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# recreate and initialize the model\n",
    "model = build_model(args, pretrained=False)\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# transfer the model to the cpu\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "\n",
    "# op - original_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tout = LN.network(input_teleported_model)\n",
    "# (original_pred - tout).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "926f9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the original predictions in original_pred_array\n",
    "original_pred_array = []\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):    \n",
    "    input_teleported_model = model.split_n(input_convs,layer_idx,half=True)\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_array.append(original_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f90ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_82733/3146665136.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 2.067991044896189e-05\n",
      "Layer 1 error: 0.020641284063458443\n",
      "Layer 2 error: 0.025099044665694237\n",
      "Layer 3 error: 0.03929218277335167\n",
      "Layer 4 error: 0.038576312363147736\n",
      "Layer 5 error: 0.05051187053322792\n",
      "Layer 6 error: 0.054717447608709335\n",
      "Layer 7 error: 0.06668603420257568\n",
      "Layer 8 error: 0.046443164348602295\n",
      "Layer 9 error: 0.03850443661212921\n",
      "Layer 10 error: 0.04973328113555908\n",
      "Layer 11 error: 0.042438797652721405\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (correct input (not teleported) for each layer)\n",
    "\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    input_layer = model.split_n(input_convs,layer_idx,half=True)\n",
    "    org_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_layer))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "\n",
    "    teleported_pred = teleported_model.network(input_layer)\n",
    "    error = (org_pred - teleported_pred).abs().mean()\n",
    "    error /= org_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82c28ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_82733/1111111726.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 2.067991044896189e-05\n",
      "Layer 1 error: 0.020641284063458443\n",
      "Layer 2 error: 0.025099044665694237\n",
      "Layer 3 error: 0.08037974685430527\n",
      "Layer 4 error: 0.0768611952662468\n",
      "Layer 5 error: 0.09685797244310379\n",
      "Layer 6 error: 0.10400263965129852\n",
      "Layer 7 error: 0.12368219345808029\n",
      "Layer 8 error: 0.13154716789722443\n",
      "Layer 9 error: 0.14072823524475098\n",
      "Layer 10 error: 0.1552278995513916\n",
      "Layer 11 error: 0.12587790191173553\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (teleported input for each layer)\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(new_model.depth):\n",
    "    input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "    input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "    teleported_pred = teleported_model.network(input_teleported_model)\n",
    "    \n",
    "    error = (original_pred - teleported_pred).abs().mean()\n",
    "    error /= original_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25440b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block0_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 0: 0.0\n",
      "block1_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 1: 0.020641300827264786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_82733/3984726627.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block2_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 2: 0.025095244869589806\n",
      "block3_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 3: 0.08038013428449631\n",
      "block4_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 4: 0.07686309516429901\n",
      "block5_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 5: 0.09686117619276047\n",
      "block6_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 6: 0.10400275141000748\n",
      "block7_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 7: 0.12368261814117432\n",
      "block8_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 8: 0.13154645264148712\n",
      "block9_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 9: 0.14072991907596588\n",
      "block10_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 10: 0.1552288830280304\n",
      "block11_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 11: 0.12587831914424896\n"
     ]
    }
   ],
   "source": [
    "teleportation_applied_layers = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "# the copy_model is used to consider the previous layer teleportation to the input of the current layer (only layers in the teleportation_applied_layers are gonna be teleported)\n",
    "copy_model = copy.deepcopy(model)\n",
    "copy_model.eval()\n",
    "for param in copy_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for layer_idx in teleportation_applied_layers:\n",
    "    # compute the original_pred_idx - IMPORTANT: PRVIOUS TELEPORTED LAYERS EFFECT THE INPUT OF THE CURRENT LAYER => pred_error WOULD BE DIFFERENT\n",
    "    input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "    input_convs = torch.tensor(input_convs).view(BATCHS,3,224,224)\n",
    "    input_teleported_model = copy_model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    # original_pred_idx = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_idx = original_pred_array[layer_idx]\n",
    "\n",
    "    LN = LinearNet()\n",
    "    LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "    # check whether the teleportation .pth already exists\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "        LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        # raise error\n",
    "        raise Exception(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exists.\")\n",
    "\n",
    "    # subsitude the teleported_model weights in the original model\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k: v for k, v in state_dic.items() if 'norm2' not in k}\n",
    "    copy_model.blocks[layer_idx].mlp.load_state_dict(state_dic)\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k.replace('norm2.',''): v for k, v in state_dic.items() if 'norm2' in k}\n",
    "    copy_model.blocks[layer_idx].norm2.load_state_dict(state_dic)\n",
    "\n",
    "    # compute the prediction error\n",
    "    new_pred = copy_model.blocks[layer_idx].mlp(copy_model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    diff = (original_pred_idx - new_pred).abs().mean()\n",
    "    print(f\"Prediction error in layer {layer_idx}: {diff/original_pred_idx.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the updated model .pth\n",
    "torch.save(copy_model.state_dict(), args.resume.replace(\".pth\",\"_teleported.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# export the updated model to onnx\n",
    "torch.onnx.export(copy_model, x,\\\n",
    "                args.prefix_dir + 'complete_model_teleported.onnx', \\\n",
    "                verbose=False, export_params=True, opset_version=15, do_constant_folding=True, \\\n",
    "                input_names=['input'], output_names=['output'], \\\n",
    "                dynamic_axes={'input' : {0 : 'batch_size'},'output': {0:'batch_size'},},\n",
    ")\n",
    "\n",
    "# define the shape\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "# export the splits of the model\n",
    "\n",
    "input_path = args.prefix_dir + \"complete_model_teleported.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs_teleported.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}_teleported.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "\n",
    "# TODO:\n",
    "# define the data_loader_test an iterator which only returns the img\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "# contain data and label in the dataset\n",
    "# label is shape 1,1000 which is one hot encoded\n",
    "label = torch.tensor(1).unsqueeze(0) \n",
    "data_set_test = torch.utils.data.TensorDataset(data,label)\n",
    "data_loader_test = torch.utils.data.DataLoader(data_set_test, batch_size=BATCHS, shuffle=True)\n",
    "# data_loader_test = torch.utils.data.DataLoader(da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import json\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_ezkl_output(witness_file, settings_file):\n",
    "    # convert the quantized ezkl output to float value\n",
    "    witness_output = json.load(open(witness_file))\n",
    "    outputs = witness_output['outputs']\n",
    "    with open(settings_file) as f:\n",
    "        settings = json.load(f)\n",
    "    ezkl_outputs = [[ezkl.felt_to_float(\n",
    "        outputs[i][j], settings['model_output_scales'][i]) for j in range(len(outputs[i]))] for i in range(len(outputs))]\n",
    "    return ezkl_outputs\n",
    "\n",
    "\n",
    "def get_onnx_output(model_file, input_file):\n",
    "    # generate the ML model output from the ONNX file\n",
    "    onnx_model = onnx.load(model_file)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    with open(input_file) as f:\n",
    "        inputs = json.load(f)\n",
    "    # reshape the input to the model\n",
    "    num_inputs = len(onnx_model.graph.input)\n",
    "\n",
    "    onnx_input = dict()\n",
    "    for i in range(num_inputs):\n",
    "        input_node = onnx_model.graph.input[i]\n",
    "        dims = []\n",
    "        elem_type = input_node.type.tensor_type.elem_type\n",
    "#         print(\"elem_type: \", elem_type)\n",
    "        for dim in input_node.type.tensor_type.shape.dim:\n",
    "            if dim.dim_value == 0:\n",
    "                dims.append(1)\n",
    "            else:\n",
    "                dims.append(dim.dim_value)\n",
    "        if elem_type == 6:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int32).reshape(dims)\n",
    "        elif elem_type == 7:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int64).reshape(dims)\n",
    "        elif elem_type == 9:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                bool).reshape(dims)\n",
    "        else:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.float32).reshape(dims)\n",
    "        onnx_input[input_node.name] = inputs_onnx\n",
    "    try:\n",
    "        onnx_session = onnxruntime.InferenceSession(model_file)\n",
    "        onnx_output = onnx_session.run(None, onnx_input)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "        # onnx_output = inputs['output_data']\n",
    "#     print(\"onnx \", onnx_output)\n",
    "    return onnx_output[0]\n",
    "\n",
    "\n",
    "def compare_outputs(zk_output, onnx_output):\n",
    "    # calculate percentage difference between the 2 outputs (which are lists)\n",
    "    res = []\n",
    "    contains_sublist = any(isinstance(sub, list) for sub in zk_output)\n",
    "    zip_object = zip(np.array(zk_output),\n",
    "                     np.array(onnx_output))\n",
    "    \n",
    "    num_eq_zk_onnx = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for (i, (list1_i, list2_i)) in enumerate(zip_object):\n",
    "        diff = list1_i - list2_i\n",
    "        # iterate and print the diffs  if they are greater than 0.0\n",
    "        res.append(np.linalg.norm(diff,axis=(-1)))\n",
    "        print(\"= index: \",i, \"\\t diff-norm: \",np.linalg.norm(diff,axis=(-1)),\"\\t zk_output: \",list1_i.shape, \"\\t onnx_output: \",list2_i.shape)\n",
    "        \n",
    "        if np.argmax(list1_i) == np.argmax(list2_i):\n",
    "            num_eq_zk_onnx += 1\n",
    "        num_total += 1\n",
    "    \n",
    "    print(\"Accuracy (zk_onnx): \\t\", num_eq_zk_onnx / num_total)\n",
    "    acc_zk_onnx = num_eq_zk_onnx / num_total\n",
    "    return res, acc_zk_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "10db441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = img.resize((224,224))\n",
    "# data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "\n",
    "# print(\"data.shape:\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t image.shape: torch.Size([1, 3, 224, 224])\n",
      "=====\n",
      "= index:  0 \t diff-norm:  20.417514179257797 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label): \t 0.0\n",
      "Accuracy (onnx_label): \t 0.0\n",
      "=====\n",
      "\n",
      "\n",
      "= index:  0 \t diff-norm:  20.417514179257797 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label) 0.0\n",
      "mean norm diff:  20.417514179257797\n",
      "max norm diff:  20.417514179257797\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ezkl_outputs = np.empty((0,1000))\n",
    "onnx_outputs = np.empty((0,1000))\n",
    "labels = np.array([])\n",
    "\n",
    "# Open log file for writing\n",
    "log_file_path = os.path.join(args.prefix_dir, \"log\", \"output_log.txt\")\n",
    "os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "\n",
    "    # Generate dataNum_input_convs.json  \n",
    "    for index, (image, label) in enumerate(data_loader_test):\n",
    "        # image = data\n",
    "        # label = torch.tensor(1).unsqueeze(0)\n",
    "        # index = 0\n",
    "        \n",
    "        print(\"index:\",index,\"\\t image.shape:\",image.shape)\n",
    "        log_file.write(f\"index: {index}\\timage.shape: {image.shape}\\n\")\n",
    "        log_file.flush()\n",
    "        \n",
    "        os.makedirs(args.prefix_dir + \"ezkl_inputs/\"+str(index),exist_ok=True)\n",
    "\n",
    "        # remove batch dimension\n",
    "        output = model(image)\n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        pre_witness_path = None\n",
    "\n",
    "        # computing witness (last witness is important)\n",
    "        for i in [\"convs\"] + [t for t in range(model.depth)]:\n",
    "            for half in [\"True\",\"False\"]:\n",
    "\n",
    "                if i==\"convs\" and half==\"False\":\n",
    "                    continue\n",
    "\n",
    "                # Define paths\n",
    "                if i == \"convs\":\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}.json\"\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_{half}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}_{half}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}_{half}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}_{half}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}_{half}.json\"\n",
    "\n",
    "                # Generating input data\n",
    "                if i == \"convs\":\n",
    "                    data = dict(input_data = [((image).detach().numpy()).reshape([-1]).tolist()])\n",
    "                else:\n",
    "                    inter_i = model.split_n(image,i,half=half)\n",
    "                    data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "                json.dump(data, open(data_path, 'w' ))\n",
    "\n",
    "\n",
    "                # Swapping (output pre_witness -> cur_input of data_path)\n",
    "                if i != \"convs\":\n",
    "                    with open(pre_witness_path, 'r') as prev_witness_file:\n",
    "                        prev_witness_data = json.load(prev_witness_file)\n",
    "                        outputs = prev_witness_data['outputs']\n",
    "                        tmp = {\"input_data\": outputs}\n",
    "                        with open(data_path, 'w') as data_file:\n",
    "                            json.dump(tmp, data_file) \n",
    "\n",
    "                # Generate setting\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "                \n",
    "\n",
    "                # Calibrating setting\n",
    "    #                 res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\", \\\n",
    "    #                                               scales=[run_args.input_scale],max_logrows=run_args.logrows, scale_rebase_multiplier=[1],lookup_safety_margin=1)\n",
    "                # Compile circuit\n",
    "                res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "                assert res == True\n",
    "\n",
    "                # Generating witness\n",
    "                res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "                assert os.path.isfile(witness_path)\n",
    "\n",
    "                # Update input_scale\n",
    "                settings = json.load(open(settings_path, 'r'))\n",
    "                run_args.input_scale = settings[\"model_output_scales\"][0]\n",
    "\n",
    "                # Update pre_witness_path\n",
    "                pre_witness_path = witness_path\n",
    "\n",
    "    # check accuracy\n",
    "    model_file = args.prefix_dir + \"network_complete.onnx\" \n",
    "    input_file = args.prefix_dir + f\"ezkl_inputs/{index}/input_convs.json\"\n",
    "    witness_file = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{model.depth-1}_False.json\"\n",
    "    settings_file = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{model.depth-1}_False.json\"\n",
    "\n",
    "    # get the ezkl output\n",
    "    ezkl_output = get_ezkl_output(witness_file, settings_file)\n",
    "    ezkl_output = np.array(ezkl_output).reshape(BATCHS,1000)\n",
    "    # get the onnx output\n",
    "    onnx_output = get_onnx_output(model_file, input_file)\n",
    "\n",
    "    ezkl_outputs = np.concatenate((ezkl_outputs,ezkl_output),axis=0)\n",
    "    onnx_outputs = np.concatenate((onnx_outputs,onnx_output),axis=0)\n",
    "    labels = np.concatenate((labels,label),axis=0)\n",
    "\n",
    "    print(\"=====\")\n",
    "    log_file.write(\"=====\\n\")\n",
    "    _,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "    \n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label): \\t\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label): \\t {acc_zk_label}\\n\")\n",
    "    \n",
    "    acc_onnx_label = np.sum( (labels) == np.argmax(onnx_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (onnx_label): \\t\", acc_onnx_label)\n",
    "    log_file.write(f\"Accuracy (onnx_label): \\t {acc_onnx_label}\\n\")\n",
    "    \n",
    "    print(\"=====\\n\\n\")\n",
    "    log_file.write(\"=====\\n\\n\")\n",
    "    log_file.flush()\n",
    "    \n",
    "    \n",
    "    # compare the outputs\n",
    "    percentage_difference,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "\n",
    "    # compare zk_output and truth_label\n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label)\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label) {acc_zk_label}\\n\")\n",
    "\n",
    "    # print the percentage difference\n",
    "    mean_percentage_difference = np.mean(np.abs(percentage_difference))\n",
    "    max_percentage_difference = np.max(np.abs(percentage_difference))\n",
    "    print(\"mean norm diff: \", mean_percentage_difference)\n",
    "    print(\"max norm diff: \", max_percentage_difference)\n",
    "    log_file.write(f\"mean norm diff: {mean_percentage_difference}\\n\")\n",
    "    log_file.write(f\"max norm diff: {max_percentage_difference}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_value: 1\n",
      "\n",
      "dim_proto: dim_value: 197\n",
      "\n",
      "dim_proto: dim_value: 192\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8e3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31b8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralteleportation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
