{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# def get_cpu_load():\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = psutil.cpu_percent(interval=1, percpu=True)\n",
    "#     return cpu_loads\n",
    "\n",
    "# def select_k_cpus_with_lowest_load(k):\n",
    "#     # Get CPU usage for each core\n",
    "#     cpu_loads = get_cpu_load()\n",
    "    \n",
    "#     # Create a list of tuples (core_id, load)\n",
    "#     cpu_load_tuples = list(enumerate(cpu_loads))\n",
    "    \n",
    "#     # Sort the list based on load\n",
    "#     sorted_cpu_loads = sorted(cpu_load_tuples, key=lambda x: x[1])\n",
    "    \n",
    "#     # Get the IDs of the K cores with lowest load\n",
    "#     selected_cpus = [core[0] for core in sorted_cpu_loads[:k]]\n",
    "    \n",
    "#     return selected_cpus\n",
    "\n",
    "# # Example usage\n",
    "# k = 8  # Number of CPUs with lowest load\n",
    "# selected_cpus = select_k_cpus_with_lowest_load(k)\n",
    "# print(f\"Selected {k} CPUs with lowest load:\", selected_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88b3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "\n",
    "# def set_cpu_affinity(pid, cpu_list):\n",
    "#     try:\n",
    "#         p = psutil.Process(pid)\n",
    "#         p.cpu_affinity(cpu_list)\n",
    "#         print(f\"CPU affinity for process {pid} set to: {cpu_list}\")\n",
    "#     except psutil.NoSuchProcess:\n",
    "#         print(f\"Process with PID {pid} does not exist.\")\n",
    "#     except psutil.AccessDenied:\n",
    "#         print(\"Permission denied. You may need sudo privileges to set CPU affinity.\")\n",
    "\n",
    "# # Get the process ID of the current process\n",
    "# pid = os.getpid()\n",
    "\n",
    "# # Set the CPU affinity to only CPU core 0\n",
    "# cpu_list = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "# set_cpu_affinity(pid, cpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as Fhtop\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "# check if notebook is in colab\n",
    "try:\n",
    "    # install ezkl\n",
    "    import google.colab\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
    "\n",
    "# rely on local installation of ezkl if the notebook is not in colab\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# uncomment for more descriptive logging \n",
    "import logging\n",
    "FORMAT = '%(levelname)s %(name)s %(asctime)-15s %(filename)s:%(lineno)d %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EZKL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import argparse\n",
    "\n",
    "def get_args_parser(initial_args=None):\n",
    "    parser = argparse.ArgumentParser('ConvNeXt training and evaluation script for image classification', add_help=False)\n",
    "#     parser.add_argument('--batch_size', default=256, type=int,\n",
    "#                         help='Per GPU batch size')\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--update_freq', default=1, type=int,\n",
    "                        help='gradient accumulation steps')\n",
    "\n",
    "    # Model parameters\n",
    "#     parser.add_argument('--model', default='convnext_tiny', type=str, metavar='MODEL',\n",
    "#                         help='Name of model to train')\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='image input size')\n",
    "    parser.add_argument('--layer_scale_init_value', default=1e-6, type=float,\n",
    "                        help=\"Layer scale initial values\")\n",
    "    \n",
    "    ########################## settings specific to this project ##########################\n",
    "    \n",
    "    # dropout and stochastic depth drop rate; set at most one to non-zero\n",
    "    parser.add_argument('--dropout', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    parser.add_argument('--drop_path', type=float, default=0, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.0)')\n",
    "    \n",
    "    # early / late dropout and stochastic depth settings\n",
    "    parser.add_argument('--drop_mode', type=str, default='standard', choices=['standard', 'early', 'late'], help='drop mode')\n",
    "    parser.add_argument('--drop_schedule', type=str, default='constant', choices=['constant', 'linear'], \n",
    "                        help='drop schedule for early dropout / s.d. only')\n",
    "    parser.add_argument('--cutoff_epoch', type=int, default=0, \n",
    "                        help='if drop_mode is early / late, this is the epoch where dropout ends / starts')\n",
    "    \n",
    "    ####################################################################################### \n",
    "    \n",
    "    # EMA related parameters\n",
    "    parser.add_argument('--model_ema', type=str2bool, default=False)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', type=str2bool, default=False, help='')\n",
    "    parser.add_argument('--model_ema_eval', type=str2bool, default=False, help='Using ema to eval during training.')\n",
    "\n",
    "    # Optimization parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "        weight decay. We use a cosine schedule for WD and using a larger decay by\n",
    "        the end of training improves performance for ViTs.\"\"\")\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=4e-3, metavar='LR',\n",
    "                        help='learning rate (default: 4e-3), with total batch size 4096')\n",
    "    parser.add_argument('--layer_decay', type=float, default=1.0)\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-6)')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=50, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--crop_pct', type=float, default=None)\n",
    "\n",
    "    # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', type=str2bool, default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # * Finetuning params\n",
    "    parser.add_argument('--finetune', default='',\n",
    "                        help='finetune from checkpoint')\n",
    "    parser.add_argument('--head_init_scale', default=1.0, type=float,\n",
    "                        help='classifier head initial scale, typically adjusted in fine-tuning')\n",
    "    parser.add_argument('--model_key', default='model|module', type=str,\n",
    "                        help='which key to load from saved state dict, usually model or model_ema')\n",
    "    parser.add_argument('--model_prefix', default='', type=str)\n",
    "\n",
    "    # Dataset parameters\n",
    "#     parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "#                         help='dataset path')\n",
    "    parser.add_argument('--eval_data_path', default=None, type=str,\n",
    "                        help='dataset path for evaluation')\n",
    "    parser.add_argument('--nb_classes', default=1000, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--imagenet_default_mean_and_std', type=str2bool, default=True)\n",
    "    parser.add_argument('--data_set', default='IMNET', choices=['CIFAR', 'IMNET', 'image_folder'],\n",
    "                        type=str, help='ImageNet dataset path')\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "#     parser.add_argument('--resume', default='',\n",
    "#                         help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt', type=str2bool, default=True)\n",
    "    parser.add_argument('--save_ckpt_freq', default=1, type=int)\n",
    "    parser.add_argument('--save_ckpt_num', default=3, type=int)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', type=str2bool, default=False,\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', type=str2bool, default=True,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--disable_eval', type=str2bool, default=False,\n",
    "                        help='Disabling evaluation during training')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', type=str2bool, default=True,\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', type=str2bool, default=False)\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--use_amp', type=str2bool, default=False, \n",
    "                        help=\"Use PyTorch's AMP (Automatic Mixed Precision) or not\")\n",
    "\n",
    "    # Weights and Biases arguments\n",
    "    parser.add_argument('--enable_wandb', type=str2bool, default=False,\n",
    "                        help=\"enable logging to Weights and Biases\")\n",
    "    parser.add_argument('--project', default='convnext', type=str,\n",
    "                        help=\"The name of the W&B project where you're sending the new run.\")\n",
    "    parser.add_argument('--wandb_ckpt', type=str2bool, default=False,\n",
    "                        help=\"Save model checkpoints as W&B Artifacts.\")\n",
    "\n",
    "    # arguments for pruning\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=4096)\n",
    "#     parser.add_argument(\"--sparsity\", type=float, default=0.)\n",
    "    parser.add_argument(\"--prune_metric\", type=str, choices=[\"magnitude\", \"wanda\"])\n",
    "    parser.add_argument(\"--prune_granularity\", type=str)\n",
    "    parser.add_argument(\"--blocksize\", type=int, default=1)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"\n",
    "    Converts string to bool type; enables command line \n",
    "    arguments in the format of '--arg1 true --arg2 false'\n",
    "    \"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def get_default_args():\n",
    "    parser = get_args_parser()\n",
    "    default_args = {}\n",
    "    for action in parser._actions:\n",
    "        # Check if action is an argument\n",
    "        if not action.option_strings:\n",
    "            continue\n",
    "        # Use the destination as the key and the default value as the value\n",
    "        default_args[action.dest] = action.default\n",
    "    return DefaultArgs(**default_args)\n",
    "\n",
    "# Example usage:\n",
    "default_args = get_default_args()\n",
    "args = default_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== IMPORTANT: INPUT FLAGS OF PYTHON CODE HERE =====\n",
    "\n",
    "# args.model = \"vit_tiny\"\n",
    "# args.data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "\n",
    "# args.resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# #args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "# # args.resume = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/model_weights/deit/deit_tiny_patch16_224-a1311bcf.pth\"\n",
    "\n",
    "\n",
    "# args.sparsity = 0.5\n",
    "# args.batch_size = 32\n",
    "\n",
    "# args.pruning_method = \"CAP\" # DENSE,CAP,WANDA\n",
    "\n",
    "# # args.prune_metric = \"wanda\"\n",
    "# # args.prune_granularity = \"row\"\n",
    "# # pruned_model_dir = \"/rds/general/user/mm6322/home/sparse_wanda/image_classifiers/outputs/vit_tiny/pruned_vit_tiny.pth\"\n",
    "\n",
    "# # Prefix directory\n",
    "# args.prefix_dir = \"sparse-cap-acc/\" #sparse-cap-acc, dense-acc, sparse-wanda-acc\n",
    "# os.makedirs(args.prefix_dir, exist_ok=True)\n",
    "\n",
    "# # EZKL HPs\n",
    "# args.input_param_scale = 7\n",
    "# args.log_rows = 20\n",
    "# args.num_cols = 4\n",
    "# args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- IN_JUPYTER: True ---- \n",
      "Model: vit_tiny\n",
      "Data Path: /rds/general/user/mm6322/home/imagenet\n",
      "Resume Path: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "Sparsity: 0.5\n",
      "Batch Size: 1\n",
      "Pruning Method: CAP\n",
      "Prefix Directory: sparse-cap-acc-tmp/\n",
      "Input Parameter Scale: 7\n",
      "Log Rows: 20\n",
      "Number of Columns: 2\n",
      "Scale Rebase Multiplier: 1\n",
      "-------- \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# Check if running in Jupyter Notebook\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        IN_JUPYTER = True\n",
    "    else:\n",
    "        IN_JUPYTER = False\n",
    "except:\n",
    "    IN_JUPYTER = False\n",
    "\n",
    "# Define default values\n",
    "default_model = \"vit_tiny\"\n",
    "default_data_path = \"/rds/general/user/mm6322/home/imagenet\"\n",
    "default_resume = \"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "# default_resume = \"/rds/general/user/mm6322/home/verifiable_NN_ezkl/examples/notebooks/CAP_pruned_models/Checkpoints/deit_tiny_patch16_224_sparsity=0.50_best.pth\"\n",
    "default_sparsity = 0.5\n",
    "default_batch_size = 1\n",
    "default_pruning_method = \"CAP\"\n",
    "default_prefix_dir = \"sparse-cap-acc-tmp/\"\n",
    "default_input_param_scale = 7\n",
    "default_log_rows = 20\n",
    "default_num_cols = 2\n",
    "default_scale_rebase_multiplier = 1\n",
    "\n",
    "# Parse command-line arguments if not in Jupyter Notebook\n",
    "if not IN_JUPYTER:\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('--model', default=default_model, type=str, help='Model type')\n",
    "    parser.add_argument('--data_path', default=default_data_path, type=str, help='Data path')\n",
    "    parser.add_argument('--resume', default=default_resume, type=str, help='Resume path')\n",
    "    parser.add_argument('--sparsity', default=default_sparsity, type=float, help='Sparsity value')\n",
    "    parser.add_argument('--batch_size', default=default_batch_size, type=int, help='Batch size')\n",
    "    parser.add_argument('--pruning_method', default=default_pruning_method, type=str, help='Pruning method')\n",
    "    parser.add_argument('--prefix_dir', default=default_prefix_dir, type=str, help='Prefix directory')\n",
    "    parser.add_argument('--input_param_scale', default=default_input_param_scale, type=int, help='Input parameter scale')\n",
    "    parser.add_argument('--log_rows', default=default_log_rows, type=int, help='Log rows')\n",
    "    parser.add_argument('--num_cols', default=default_num_cols, type=int, help='Number of columns')\n",
    "    parser.add_argument('--scale_rebase_multiplier', default=default_scale_rebase_multiplier, type=int, help='Scale rebase multiplier')\n",
    "\n",
    "    args = parser.parse_args(namespace=args)\n",
    "else:\n",
    "    # In Jupyter Notebook, define args with default values\n",
    "    args.model = default_model\n",
    "    args.data_path = default_data_path\n",
    "    args.resume = default_resume\n",
    "    args.sparsity = default_sparsity\n",
    "    args.batch_size = default_batch_size\n",
    "    args.pruning_method = default_pruning_method\n",
    "    args.prefix_dir = default_prefix_dir\n",
    "    args.input_param_scale = default_input_param_scale\n",
    "    args.log_rows = default_log_rows\n",
    "    args.num_cols = default_num_cols\n",
    "    args.scale_rebase_multiplier = default_scale_rebase_multiplier\n",
    "    \n",
    "    \n",
    "print(\"\\n ---- IN_JUPYTER:\",str(IN_JUPYTER),\"---- \")\n",
    "# Print values for verification\n",
    "print(\"Model:\", args.model)\n",
    "print(\"Data Path:\", args.data_path)\n",
    "print(\"Resume Path:\", args.resume)\n",
    "print(\"Sparsity:\", args.sparsity)\n",
    "print(\"Batch Size:\", args.batch_size)\n",
    "print(\"Pruning Method:\", args.pruning_method)\n",
    "print(\"Prefix Directory:\", args.prefix_dir)\n",
    "print(\"Input Parameter Scale:\", args.input_param_scale)\n",
    "print(\"Log Rows:\", args.log_rows)\n",
    "print(\"Number of Columns:\", args.num_cols)\n",
    "print(\"Scale Rebase Multiplier:\", args.scale_rebase_multiplier)\n",
    "print(\"-------- \\n\\n\\n\")\n",
    "\n",
    "os.makedirs(args.prefix_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 300\n",
      "update_freq: 1\n",
      "input_size: 224\n",
      "layer_scale_init_value: 1e-06\n",
      "dropout: 0\n",
      "drop_path: 0\n",
      "drop_mode: standard\n",
      "drop_schedule: constant\n",
      "cutoff_epoch: 0\n",
      "model_ema: False\n",
      "model_ema_decay: 0.9999\n",
      "model_ema_force_cpu: False\n",
      "model_ema_eval: False\n",
      "opt: adamw\n",
      "opt_eps: 1e-08\n",
      "opt_betas: None\n",
      "clip_grad: None\n",
      "momentum: 0.9\n",
      "weight_decay: 0.05\n",
      "weight_decay_end: None\n",
      "lr: 0.004\n",
      "layer_decay: 1.0\n",
      "min_lr: 1e-06\n",
      "warmup_epochs: 50\n",
      "warmup_steps: -1\n",
      "color_jitter: 0.4\n",
      "aa: rand-m9-mstd0.5-inc1\n",
      "smoothing: 0.1\n",
      "train_interpolation: bicubic\n",
      "crop_pct: None\n",
      "reprob: 0.25\n",
      "remode: pixel\n",
      "recount: 1\n",
      "resplit: False\n",
      "mixup: 0.8\n",
      "cutmix: 1.0\n",
      "cutmix_minmax: None\n",
      "mixup_prob: 1.0\n",
      "mixup_switch_prob: 0.5\n",
      "mixup_mode: batch\n",
      "finetune: \n",
      "head_init_scale: 1.0\n",
      "model_key: model|module\n",
      "model_prefix: \n",
      "eval_data_path: None\n",
      "nb_classes: 1000\n",
      "imagenet_default_mean_and_std: True\n",
      "data_set: IMNET\n",
      "output_dir: \n",
      "device: cuda\n",
      "seed: 0\n",
      "auto_resume: True\n",
      "save_ckpt: True\n",
      "save_ckpt_freq: 1\n",
      "save_ckpt_num: 3\n",
      "start_epoch: 0\n",
      "eval: False\n",
      "dist_eval: True\n",
      "disable_eval: False\n",
      "num_workers: 10\n",
      "pin_mem: True\n",
      "world_size: 1\n",
      "local_rank: -1\n",
      "dist_on_itp: False\n",
      "dist_url: env://\n",
      "use_amp: False\n",
      "enable_wandb: False\n",
      "project: convnext\n",
      "wandb_ckpt: False\n",
      "nsamples: 4096\n",
      "prune_metric: None\n",
      "prune_granularity: None\n",
      "blocksize: 1\n",
      "model: vit_tiny\n",
      "data_path: /rds/general/user/mm6322/home/imagenet\n",
      "resume: /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/deit_tiny_patch16_224_sparsity=0.50_best.pth\n",
      "sparsity: 0.5\n",
      "batch_size: 1\n",
      "pruning_method: CAP\n",
      "prefix_dir: sparse-cap-acc-tmp/\n",
      "input_param_scale: 7\n",
      "log_rows: 20\n",
      "num_cols: 2\n",
      "scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in vars(args).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.output_dir:\n",
    "    Path(default_args_dict.output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neuralteleportation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from timm.data.constants import \\\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data import create_transform\n",
    "\n",
    "def build_dataset(is_train, args):\n",
    "    transform = build_transform(is_train, args)\n",
    "\n",
    "    print(\"Transform = \")\n",
    "    if isinstance(transform, tuple):\n",
    "        for trans in transform:\n",
    "            print(\" - - - - - - - - - - \")\n",
    "            for t in trans.transforms:\n",
    "                print(t)\n",
    "    else:\n",
    "        for t in transform.transforms:\n",
    "            print(t)\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    if args.data_set == 'CIFAR':\n",
    "        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform, download=True)\n",
    "        nb_classes = 100\n",
    "    elif args.data_set == 'IMNET':\n",
    "        print(\"reading from datapath\", args.data_path)\n",
    "        root = os.path.join(args.data_path, 'train' if is_train else 'val_dirs')\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = 1000\n",
    "    elif args.data_set == \"image_folder\":\n",
    "        root = args.data_path if is_train else args.eval_data_path\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = args.nb_classes\n",
    "        assert len(dataset.class_to_idx) == nb_classes\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    print(\"Number of the class = %d\" % nb_classes)\n",
    "\n",
    "    return dataset, nb_classes\n",
    "\n",
    "\n",
    "def build_transform(is_train, args):\n",
    "    resize_im = args.input_size > 32\n",
    "    imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n",
    "    mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n",
    "    std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n",
    "\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=args.input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            interpolation=args.train_interpolation,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                args.input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        # warping (no cropping) when evaluated at 384 or larger\n",
    "        if args.input_size >= 384:  \n",
    "            t.append(\n",
    "            transforms.Resize((args.input_size, args.input_size), \n",
    "                            interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "        )\n",
    "            print(f\"Warping {args.input_size} size input images...\")\n",
    "        else:\n",
    "            if args.crop_pct is None:\n",
    "                args.crop_pct = 224 / 256\n",
    "            size = int(args.input_size / args.crop_pct)\n",
    "            t.append(\n",
    "                # to maintain same ratio w.r.t. 224 images\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),  \n",
    "            )\n",
    "            t.append(transforms.CenterCrop(args.input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_train = torch.utils.data.DistributedSampler(\n",
    "#         dataset_train, num_replicas=1, rank=0, shuffle=True, seed=args.seed,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_train = torch.utils.data.DataLoader(\n",
    "#     dataset_train, sampler=sampler_train,\n",
    "#     batch_size=args.batch_size,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.12'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import numpy as np\n",
    "from timm.utils import get_state_dict\n",
    "\n",
    "from pathlib import Path\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "# from torch._six import inf\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "class TensorboardLogger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = SummaryWriter(logdir=log_dir)\n",
    "        self.step = 0\n",
    "\n",
    "    def set_step(self, step=None):\n",
    "        if step is not None:\n",
    "            self.step = step\n",
    "        else:\n",
    "            self.step += 1\n",
    "\n",
    "    def update(self, head='scalar', step=None, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.writer.add_scalar(head + \"/\" + k, v, self.step if step is None else step)\n",
    "\n",
    "    def flush(self):\n",
    "        self.writer.flush()\n",
    "\n",
    "\n",
    "class WandbLogger(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        try:\n",
    "            import wandb\n",
    "            self._wandb = wandb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use the Weights and Biases Logger please install wandb.\"\n",
    "                \"Run `pip install wandb` to install it.\"\n",
    "            )\n",
    "\n",
    "        # Initialize a W&B run \n",
    "        if self._wandb.run is None:\n",
    "            self._wandb.init(\n",
    "                project=args.project,\n",
    "                config=args\n",
    "            )\n",
    "\n",
    "    def log_epoch_metrics(self, metrics, commit=True):\n",
    "        \"\"\"\n",
    "        Log train/test metrics onto W&B.\n",
    "        \"\"\"\n",
    "        # Log number of model parameters as W&B summary\n",
    "        self._wandb.summary['n_parameters'] = metrics.get('n_parameters', None)\n",
    "        metrics.pop('n_parameters', None)\n",
    "\n",
    "        # Log current epoch\n",
    "        self._wandb.log({'epoch': metrics.get('epoch')}, commit=False)\n",
    "        metrics.pop('epoch')\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            if 'train' in k:\n",
    "                self._wandb.log({f'Global Train/{k}': v}, commit=False)\n",
    "            elif 'test' in k:\n",
    "                self._wandb.log({f'Global Test/{k}': v}, commit=False)\n",
    "\n",
    "        self._wandb.log({})\n",
    "\n",
    "    def log_checkpoints(self):\n",
    "        output_dir = self.args.output_dir\n",
    "        model_artifact = self._wandb.Artifact(\n",
    "            self._wandb.run.id + \"_model\", type=\"model\"\n",
    "        )\n",
    "\n",
    "        model_artifact.add_dir(output_dir)\n",
    "        self._wandb.log_artifact(model_artifact, aliases=[\"latest\", \"best\"])\n",
    "\n",
    "    def set_steps(self):\n",
    "        # Set global training step\n",
    "        self._wandb.define_metric('Rank-0 Batch Wise/*', step_metric='Rank-0 Batch Wise/global_train_step')\n",
    "        # Set epoch-wise step\n",
    "        self._wandb.define_metric('Global Train/*', step_metric='epoch')\n",
    "        self._wandb.define_metric('Global Test/*', step_metric='epoch')\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "def load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "        \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.weight$')\n",
    "    state_dict_keys = list(state_dict.keys())\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.weight\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "            \n",
    "    \n",
    "    pattern = re.compile(r'^blocks\\.(\\d+)\\.attn\\.q\\.bias$')\n",
    "    \n",
    "    for key in state_dict_keys:\n",
    "        match = pattern.match(key)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            query_key = key\n",
    "            key_key = key.replace(\"q\",\"k\")\n",
    "            value_key = key.replace(\"q\",\"v\")\n",
    "            \n",
    "            new_name = \"blocks.\" + str(index) +\".attn.qkv.bias\"\n",
    "            state_dict[new_name] = torch.cat([state_dict[query_key], state_dict[key_key], state_dict[value_key]], dim=0)\n",
    "            \n",
    "            print(\"index:\",index,\"\\t new_name:\",new_name)\n",
    "            del state_dict[query_key], state_dict[key_key], state_dict[value_key]\n",
    "                                              \n",
    "    \n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(model, prefix=prefix)\n",
    "\n",
    "    warn_missing_keys = []\n",
    "    ignore_missing_keys = []\n",
    "    for key in missing_keys:\n",
    "        keep_flag = True\n",
    "        for ignore_key in ignore_missing.split('|'):\n",
    "            if ignore_key in key:\n",
    "                keep_flag = False\n",
    "                break\n",
    "        if keep_flag:\n",
    "            warn_missing_keys.append(key)\n",
    "        else:\n",
    "            ignore_missing_keys.append(key)\n",
    "            \n",
    "\n",
    "    missing_keys = warn_missing_keys\n",
    "\n",
    "    if len(missing_keys) > 0:\n",
    "        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "            model.__class__.__name__, unexpected_keys))\n",
    "    if len(ignore_missing_keys) > 0:\n",
    "        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, ignore_missing_keys))\n",
    "    if len(error_msgs) > 0:\n",
    "        print('\\n'.join(error_msgs))\n",
    "\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "\n",
    "        ########################################################\n",
    "        ## Code I added \n",
    "        for param in parameters:\n",
    "            weight_copy = param.data.abs().clone()\n",
    "            mask = weight_copy.gt(0).float().cuda()\n",
    "            sparsity = mask.sum() / mask.numel()\n",
    "            if sparsity > 0.3:\n",
    "                # non-trivial sparsity \n",
    "                param.grad.data.mul_(mask)\n",
    "        ########################################################\n",
    "\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n",
    "                     start_warmup_value=0, warmup_steps=-1):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_steps > 0:\n",
    "        warmup_iters = warmup_steps\n",
    "    print(\"Set warmup steps = %d\" % warmup_iters)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = np.array(\n",
    "        [final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        to_save = {\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'scaler': loss_scaler.state_dict(),\n",
    "            'args': args,\n",
    "        }\n",
    "\n",
    "        if model_ema is not None:\n",
    "            to_save['model_ema'] = get_state_dict(model_ema)\n",
    "\n",
    "        save_on_master(to_save, checkpoint_path)\n",
    "\n",
    "    if is_main_process() and isinstance(epoch, int):\n",
    "        to_del = epoch - args.save_ckpt_num * args.save_ckpt_freq\n",
    "        old_ckpt = output_dir / ('checkpoint-%s.pth' % to_del)\n",
    "        if os.path.exists(old_ckpt):\n",
    "            os.remove(old_ckpt)\n",
    "\n",
    "\n",
    "def auto_load_model(args, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.auto_resume and len(args.resume) == 0:\n",
    "        import glob\n",
    "        all_checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint-*.pth'))\n",
    "        latest_ckpt = -1\n",
    "        for ckpt in all_checkpoints:\n",
    "            t = ckpt.split('-')[-1].split('.')[0]\n",
    "            if t.isdigit():\n",
    "                latest_ckpt = max(int(t), latest_ckpt)\n",
    "        if latest_ckpt >= 0:\n",
    "            args.resume = os.path.join(output_dir, 'checkpoint-%d.pth' % latest_ckpt)\n",
    "        print(\"Auto resume checkpoint: %s\" % args.resume)\n",
    "\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        print(\"Resume checkpoint %s\" % args.resume)\n",
    "        if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            if not isinstance(checkpoint['epoch'], str): # does not support resuming with 'best', 'best-ema'\n",
    "                args.start_epoch = checkpoint['epoch'] + 1\n",
    "            else:\n",
    "                assert args.eval, 'Does not support resuming with checkpoint-best'\n",
    "            if hasattr(args, 'model_ema') and args.model_ema:\n",
    "                if 'model_ema' in checkpoint.keys():\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model_ema'])\n",
    "                else:\n",
    "                    model_ema.ema.load_state_dict(checkpoint['model'])\n",
    "            if 'scaler' in checkpoint:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            print(\"With optim & sched!\")\n",
    "\n",
    "def reg_scheduler(base_value, final_value, epochs, niter_per_ep, early_epochs=0, early_value=None, \n",
    "           mode='linear', early_mode='regular'):\n",
    "    early_schedule = np.array([])\n",
    "    early_iters = early_epochs * niter_per_ep\n",
    "    if early_value is None:\n",
    "        early_value = final_value\n",
    "    if early_epochs > 0:\n",
    "        print(f\"Set early value to {early_mode} {early_value}\")\n",
    "        if early_mode == 'regular':\n",
    "            early_schedule = np.array([early_value] * early_iters)\n",
    "        elif early_mode == 'linear':\n",
    "            early_schedule = np.linspace(early_value, base_value, early_iters)\n",
    "        elif early_mode == 'cosine':\n",
    "            early_schedule = np.array(\n",
    "            [base_value + 0.5 * (early_value - base_value) * (1 + math.cos(math.pi * i / early_iters)) for i in np.arange(early_iters)])\n",
    "    regular_epochs = epochs - early_epochs\n",
    "    iters = np.arange(regular_epochs * niter_per_ep)\n",
    "    schedule = np.linspace(base_value, final_value, len(iters))\n",
    "    schedule = np.concatenate((early_schedule, schedule))\n",
    "\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def build_model(args, pretrained=False):\n",
    "    if args.model.startswith(\"convnext\"):\n",
    "        model = create_model(\n",
    "            args.model,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=args.nb_classes,\n",
    "            layer_scale_init_value=args.layer_scale_init_value,\n",
    "            head_init_scale=args.head_init_scale,\n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate=args.dropout,\n",
    "            )\n",
    "    else:\n",
    "        model = create_model(\n",
    "            args.model, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=args.nb_classes, \n",
    "            drop_path_rate=args.drop_path,\n",
    "            drop_rate =args.dropout\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCHS: 1\n"
     ]
    }
   ],
   "source": [
    "BATCHS = args.batch_size\n",
    "print(\"BATCHS:\",BATCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.resnet import resnet26d, resnet50d\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
    "    ),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_huge_patch16_224': _cfg(),\n",
    "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
    "    # hybrid models\n",
    "    'vit_small_resnet26d_224': _cfg(),\n",
    "    'vit_small_resnet50d_s3_224': _cfg(),\n",
    "    'vit_base_resnet26d_224': _cfg(),\n",
    "    'vit_base_resnet50d_224': _cfg(),\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        \n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "#         self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.query_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.key_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.value_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "#         query = self.query_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         key = self.key_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "#         value = self.value_linear(x).reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "        \n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        qkvs = self.qkv(x)\n",
    "        query = qkvs[:,:,0:self.dim]\n",
    "        key = qkvs[:,:,self.dim:2*self.dim]\n",
    "        value = qkvs[:,:,2*self.dim:] \n",
    "        query = query.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        key = key.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        value = value.reshape(B,N,self.num_heads, C // self.num_heads)\n",
    "        \n",
    "\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale     \n",
    "        attn = query.transpose(1,2) @ key.transpose(1,2).transpose(2,3)\n",
    "        attn = attn * self.scale\n",
    "\n",
    "\n",
    "        attn = attn.softmax(dim=(-1))\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        \n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        y = (attn@value.transpose(1,2)).transpose(1,2).reshape(B,N,C)\n",
    "        \n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "        y = self.proj(y)\n",
    "        y = self.proj_drop(y)\n",
    "#         return x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "#         self.norm1 = norm_layer(self.dim)\n",
    "        self.norm1 = norm_layer(self.dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(self.dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))      \n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def half_block(self,x):\n",
    "        mean = torch.mean(x, dim=2)  # Calculate mean along the last dimension\n",
    "        mean = mean.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "\n",
    "        diff_squared = (x - mean) ** 2\n",
    "        std = torch.sqrt(torch.mean(diff_squared, dim=2))\n",
    "        std = std.unsqueeze(2).expand(-1,-1,x.shape[2])\n",
    "        \n",
    "        norm_x = (x - mean) \n",
    "        norm_x = norm_x / (std+1e-06)\n",
    "        norm_x = norm_x * self.norm1.weight.unsqueeze(0).unsqueeze(0)\n",
    "        norm_x = norm_x + self.norm1.bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.drop_path(self.attn(norm_x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "#         assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "#             f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "#         x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x).reshape(self.batch_size,192,196,1).transpose(1,2)\n",
    "        x = x.squeeze(3)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# class HybridEmbed(nn.Module):\n",
    "#     \"\"\" CNN Feature Map Embedding\n",
    "#     Extract feature map from CNN, flatten, project to embedding dim.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         assert isinstance(backbone, nn.Module)\n",
    "#         img_size = to_2tuple(img_size)\n",
    "#         self.img_size = img_size\n",
    "#         self.backbone = backbone\n",
    "#         if feature_size is None:\n",
    "#             with torch.no_grad():\n",
    "#                 # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
    "#                 # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
    "#                 # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
    "#                 training = backbone.training\n",
    "#                 if training:\n",
    "#                     backbone.eval()\n",
    "#                 o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "#                 feature_size = o.shape[-2:]\n",
    "#                 feature_dim = o.shape[1]\n",
    "#                 backbone.train(training)\n",
    "#         else:\n",
    "#             feature_size = to_2tuple(feature_size)\n",
    "#             feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "#         self.num_patches = feature_size[0] * feature_size[1]\n",
    "#         self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)[-1]\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "#         x = self.proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        # I add these two lines\n",
    "        self.drop_rate=drop_rate\n",
    "        attn_drop_rate=drop_rate\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.depth = depth\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
    "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
    "        #self.repr_act = nn.Tanh()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.batch_size = BATCHS\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    " \n",
    "        x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "        B = BATCHS\n",
    "    \n",
    "\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B,-1,-1)\n",
    "        \n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def split_convs(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "#       B = x.shape[0]\n",
    "        B = BATCHS\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = torch.cat((self.cls_token, x), dim=1)\n",
    "    \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "#         num_blocks = len(self.blocks) // 2\n",
    "#         for blk in self.blocks[:num_blocks]:\n",
    "#             x = blk(x)\n",
    "\n",
    "#         x = self.blocks[0].half_block(x)\n",
    "\n",
    "#         print(\"split_1, output.shape:\",x.shape, \"\\t num_blocks:\",num_blocks)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def split_2(self,x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks[0].half_block(x)\n",
    "        return x\n",
    "    \n",
    "    def split_n(self,x,n,half=None):\n",
    "        x = self.patch_embed(x)\n",
    "        B = BATCHS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for layer_idx in range(n-1):\n",
    "            x = self.blocks[layer_idx](x)\n",
    "            \n",
    "        #n-th layer\n",
    "        if half:\n",
    "            x = self.blocks[n].half_block(x)\n",
    "        else:\n",
    "            x = self.blocks[n](x)\n",
    "            # last layer of transformer\n",
    "            if n == (self.depth - 1):\n",
    "                x = self.norm(x)\n",
    "                x = x[:, 0]\n",
    "                x = self.head(x)\n",
    "                \n",
    "        return x\n",
    "            \n",
    "          \n",
    "    def update_drop_path(self, drop_path_rate):\n",
    "        self.drop_path = drop_path_rate\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, self.depth)]\n",
    "        for i in range(self.depth):\n",
    "            self.blocks[i].drop_path.drop_prob = dp_rates[i]\n",
    "    \n",
    "    def update_dropout(self, drop_rate):\n",
    "        self.drop_rate = drop_rate\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = drop_rate\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# @register_model\n",
    "# def vit_tiny_tiny(pretrained=False, **kwargs):\n",
    "#     model = VisionTransformer(\n",
    "#         patch_size=16, embed_dim=48, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "@register_model\n",
    "def vit_tiny(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_small(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_base(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_large(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(args, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.VisionTransformer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 5717416\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_29169/3668749705.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# total_batch_size = args.batch_size * args.update_freq * utils.get_world_size()\n",
    "total_batch_size = args.batch_size * args.update_freq\n",
    "# num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n",
    "\n",
    "# At most one of dropout and stochastic depth should be enabled.\n",
    "assert(args.dropout == 0 or args.drop_path == 0)\n",
    "# ConvNeXt does not support dropout.\n",
    "assert(args.dropout == 0 if args.model.startswith(\"convnext\") else True)\n",
    "\n",
    "import re\n",
    "\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state_dict', 'optimizer', 'loss_scaler', 'epoch', 'manager'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# cnt = 0\n",
    "# dl = DataLoader(dataset_train, batch_size=BATCHS, shuffle=True)\n",
    "# # data = dataset_train[cnt][0].unsqueeze(dim=0)\n",
    "# # label = dataset_train[cnt][1]\n",
    "# data,label = next(iter(dl))\n",
    "# print(data.shape)\n",
    "# print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5717416\n",
      "Number of zeros: 2654208\n",
      "Percentage of zeros: 46.42%\n"
     ]
    }
   ],
   "source": [
    "# compute the number of zeros in the model / total number of parameters\n",
    "total_params = 0\n",
    "zeros = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    zeros += (param.data == 0).sum().item()\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of zeros: {zeros}\")\n",
    "print(f\"Percentage of zeros: {zeros / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "# load JPEG image /Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = Image.open(\"/rds/general/user/mm6322/home/imagenet/val/n12620546/ILSVRC2012_val_00011901.JPEG\")\n",
    "\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# result = model(data.view(data.shape[0],-1))\n",
    "print(\"data shape:\",data.shape)\n",
    "result = model(data)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = data.detach().clone()\n",
    "print(\"x.shape:\",x.shape)\n",
    "\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(    \n",
    "    model,               # model being run\n",
    "    x,                   # model input (or a tuple for multiple inputs)\n",
    "    args.prefix_dir + \"network_complete.onnx\",            # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=15,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                    'output': {0:'batch_size'},\n",
    "    },         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx:\t CONV \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.9203) \t max: tensor(13.6817)\n",
      "layer_idx:\t 0 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.0635) \t max: tensor(15.4783)\n",
      "layer_idx:\t 0 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.4229) \t max: tensor(17.4164)\n",
      "layer_idx:\t 1 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.8803) \t max: tensor(10.3352)\n",
      "layer_idx:\t 1 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.4997) \t max: tensor(9.3524)\n",
      "layer_idx:\t 2 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.2025) \t max: tensor(17.3773)\n",
      "layer_idx:\t 2 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.8159) \t max: tensor(16.2393)\n",
      "layer_idx:\t 3 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-4.4613) \t max: tensor(15.7312)\n",
      "layer_idx:\t 3 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.2997) \t max: tensor(14.8284)\n",
      "layer_idx:\t 4 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.5867) \t max: tensor(15.6392)\n",
      "layer_idx:\t 4 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.6180) \t max: tensor(14.4870)\n",
      "layer_idx:\t 5 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-6.0484) \t max: tensor(15.2520)\n",
      "layer_idx:\t 5 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.7885) \t max: tensor(14.6185)\n",
      "layer_idx:\t 6 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.8097) \t max: tensor(14.7460)\n",
      "layer_idx:\t 6 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-5.1806) \t max: tensor(14.0622)\n",
      "layer_idx:\t 7 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-7.9359) \t max: tensor(14.7281)\n",
      "layer_idx:\t 7 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.2735) \t max: tensor(18.5691)\n",
      "layer_idx:\t 8 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-11.1971) \t max: tensor(20.7566)\n",
      "layer_idx:\t 8 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-12.5824) \t max: tensor(25.8723)\n",
      "layer_idx:\t 9 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-14.3955) \t max: tensor(26.9132)\n",
      "layer_idx:\t 9 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.2325) \t max: tensor(27.7766)\n",
      "layer_idx:\t 10 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5998) \t max: tensor(29.1508)\n",
      "layer_idx:\t 10 \t half: False \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-15.5393) \t max: tensor(30.7623)\n",
      "layer_idx:\t 11 \t half: True \t inter_out.shape: torch.Size([1, 197, 192]) \t min: tensor(-16.1201) \t max: tensor(33.1553)\n",
      "layer_idx:\t 11 \t half: False \t inter_out.shape: torch.Size([1, 1000]) \t min: tensor(-3.9342) \t max: tensor(8.3160)\n"
     ]
    }
   ],
   "source": [
    "inter_out = model.split_convs(data)\n",
    "print(\"layer_idx:\\t\",\"CONV\",\"\\t half:\",str(False),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_out = model.split_n(data,layer_idx,half)\n",
    "        print(\"layer_idx:\\t\",layer_idx,\"\\t half:\",str(half),\"\\t inter_out.shape:\",inter_out.shape,\"\\t min:\",inter_out.min(),\"\\t max:\",inter_out.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"network_complete.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"network_complete.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for all layers\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),args.prefix_dir, \"input_convs.json\")\n",
    "data = dict(input_data = [((x).detach().numpy()).reshape([-1]).tolist()])\n",
    "json.dump( data, open(data_path, 'w' ))\n",
    "\n",
    "for i in range(model.depth):\n",
    "    for half in [True,False]:\n",
    "        inter_i = model.split_n(x,i,half=half)\n",
    "        data_path = os.path.join(os.getcwd(),args.prefix_dir, f\"input_{i}_{str(half)}.json\")\n",
    "        data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "        json.dump( data, open(data_path, 'w' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# extract all onnx files of layers\n",
    "\n",
    "input_path = args.prefix_dir + \"network_complete.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function\n",
    "def activation_hook(layer_name, activation_stats):\n",
    "    def hook(module, input, output):\n",
    "        input_tensor = input[0]\n",
    "        activation_stats[layer_name] = {\n",
    "            # l1 norm\n",
    "            'norm': input_tensor.norm(),\n",
    "            'max': input_tensor.max(),\n",
    "            'min': input_tensor.min(),\n",
    "            'shape': input_tensor.shape\n",
    "    }\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # teleporing the mlp model\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "# # IMPORTANT: input_x_T/F is acutally the output of that block (output of the intermediate or the full block)\n",
    "\n",
    "# # find the input data for the mlp model\n",
    "# input_split0_False = json.load(open(args.prefix_dir + \"input_0_True.json\"))[\"input_data\"][0]\n",
    "# norm_split0_False = model.blocks[0].norm2\n",
    "# input_mlp_0 = norm_split0_False(torch.tensor(input_split0_False).view(1,197,192))\n",
    "# out_mlp_0 = original_mlp_0(input_mlp_0)\n",
    "# out_mlp_0 = model.blocks[0].drop_path(out_mlp_0)\n",
    "# out_split0_false = out_mlp_0 + torch.tensor(input_split0_False).view(1,197,192)\n",
    "\n",
    "# # True output of the current block == input of the next block\n",
    "# out_split0_false_orginal = json.load(open(args.prefix_dir + \"input_0_False.json\"))[\"input_data\"][0]\n",
    "\n",
    "# print(input_mlp_0.norm(),input_mlp_0.max(),input_mlp_0.min(),input_mlp_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor(input_split0_False).view(1,197,192)\n",
    "# print(t.norm(),t.max(),t.min(),t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register hooks to the layers before all activation functions\n",
    "# original_mlp_0 = model.blocks[0].mlp\n",
    "# activation_stats = {}\n",
    "# for i, layer in enumerate(original_mlp_0.children()):\n",
    "#     if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#         layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats))\n",
    "\n",
    "# # Run the mlp model\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_block0 = model.split_convs(torch.tensor(input_convs).view(1,3,224,224))\n",
    "# input_block0 = input_block0.view(1,197,192)\n",
    "# original_block0_false_pred = model.blocks[0](input_block0)\n",
    "# print(\"activation_stats:\",activation_stats)\n",
    "\n",
    "# original_loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# print(f\"Original loss: {original_loss}, Original prediction error: {0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = json.load(open(args.prefix_dir + \"input_0_True.json\"))['input_data'][0]\n",
    "# d = torch.tensor(d).view(1,197,192)\n",
    "# np.save(args.prefix_dir + \"input_block0_false.npy\", d.numpy())\n",
    "\n",
    "# # original_pred = model.blocks[0].mlp(model.blocks[0].norm2(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralteleportation.models.model_zoo.mlpcob import MLPCOB\n",
    "from neuralteleportation.neuralteleportationmodel import NeuralTeleportationModel\n",
    "from neuralteleportation.layers.neuralteleportation import COBForwardMixin, FlattenCOB\n",
    "from neuralteleportation.layers.neuron import LinearCOB\n",
    "from neuralteleportation.layers.activation import ReLUCOB, SigmoidCOB, GELUCOB, LeakyReLUCOB\n",
    "from neuralteleportation.layers.dropout import DropoutCOB\n",
    "from neuralteleportation.layers.neuron import LayerNormCOB\n",
    "from neuralteleportation.layers.merge import Add\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # self.add = Add()\n",
    "        self.norm2 = LayerNormCOB(192)\n",
    "        self.fc1 = LinearCOB(192, 768, bias=True)\n",
    "        self.act = GELUCOB()\n",
    "        self.fc2 = LinearCOB(768, 192, bias=True)\n",
    "        # self.drop1 = DropoutCOB(0.1)\n",
    "        # self.drop2 = DropoutCOB(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.norm2(x)\n",
    "        x2 = self.fc1(x1)\n",
    "        x3 = self.act(x2)\n",
    "        x4 = self.fc2(x3)\n",
    "        # x4 = self.add(x, x3)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teleported_model = LinearNet()\n",
    "# teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ln_weights(LN, model, block_idx):\n",
    "    original_mlp = model.blocks[block_idx].mlp\n",
    "    original_norm2 = model.blocks[block_idx].norm2\n",
    "\n",
    "    combined_dict = {}\n",
    "    combined_dict.update(original_mlp.state_dict())\n",
    "    for k,v in original_norm2.state_dict().items():\n",
    "        combined_dict[\"norm2.\" + k] = v\n",
    "    LN.network.load_state_dict(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variable to store the best loss found\n",
    "global best_loss\n",
    "global cor_best_pred_error\n",
    "global cor_best_range\n",
    "# initialize best_loss\n",
    "best_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "982a3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def f_ack(cob,input_data=None, original_pred=None, layer_idx=None ,original_loss=None, tm=None):    \n",
    "    \n",
    "    # # Set up model with the new COB\n",
    "    # teleported_model = LinearNet()\n",
    "    # teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    # deep copy tm to teleported_model\n",
    "    teleported_model = copy.deepcopy(tm)\n",
    "    # Load the weights of the original model\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    # teleport to the ones tensor\n",
    "    teleported_model = teleported_model.teleport(torch.ones_like(cob), reset_teleportation=True)\n",
    "\n",
    "    # apply the COB\n",
    "    teleported_model = teleported_model.teleport(cob, reset_teleportation=True)\n",
    "\n",
    "    # Reset activation stats and run a forward pass\n",
    "    activation_stats = {}\n",
    "    hook_handles = []\n",
    "    for i, layer in enumerate(teleported_model.network.children()):\n",
    "            if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "                handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "                hook_handles.append(handle)\n",
    "    teleported_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = teleported_model.network(input_data)\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "\n",
    "    # Calculate the range loss\n",
    "    loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "    loss /= original_loss\n",
    "    # Calculate the prediction error\n",
    "    pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "    pred_error /= np.abs(original_pred).mean()\n",
    "    total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "    if random.random() < 0.0005:\n",
    "         print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "    \n",
    "    activation_stats.clear()\n",
    "    del teleported_model, activation_stats, pred, loss, pred_error, cob\n",
    "    return total_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "272f78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy the model\n",
    "# import copy\n",
    "# import itertools\n",
    "# import functools\n",
    "\n",
    "# layer_idx = 1\n",
    "\n",
    "# # copy the model (the new_model will be used to apply the cob to prevent the original model from being modified)\n",
    "# new_model = copy.deepcopy(model)\n",
    "\n",
    "# # Register hooks to the layers before all activation functions\n",
    "# original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "# activation_stats_idx = {}\n",
    "# for i,layer in enumerate(original_mlp_idx.children()):\n",
    "#     if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "#         layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "\n",
    "# # run the mlp model to find original_loss\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "# print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "\n",
    "# # calculate the original loss\n",
    "# original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "# print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "# # Load the teleported model\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# # get initial weights and cob\n",
    "# initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "# initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "# global best_loss\n",
    "# best_loss = 1e9\n",
    "\n",
    "# # Load the input data to calculate the original_pred\n",
    "# input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "# input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "# input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "# input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "# original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "# # Apply best COB and save model weights\n",
    "# LN = LinearNet()\n",
    "# LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "# s = initial_cob_idx.size()\n",
    "\n",
    "# # add inputs to the function\n",
    "# ackley = functools.partial(\n",
    "#     f_ack,\n",
    "#     input_data=input_teleported_model,\n",
    "#     original_pred=original_pred,\n",
    "#     layer_idx=layer_idx,\n",
    "#     original_loss = original_loss_idx,\n",
    "#     tm = LN\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d068e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dic: cob\n",
    "@torch.no_grad()\n",
    "def rge(func, params_dict, sample_size, step_size, base=None):\n",
    "    if base == None:\n",
    "        base = func(params_dict[\"cob\"])\n",
    "    grads_dict = {}\n",
    "    for _ in range(sample_size):\n",
    "        perturbs_dict, perturbed_params_dict = {}, {}\n",
    "        for key, param in params_dict.items():\n",
    "            perturb = torch.randn_like(param)\n",
    "            perturb /= (torch.norm(perturb) + 1e-8)\n",
    "            perturb *= step_size\n",
    "            perturbs_dict[key] = perturb\n",
    "            perturbed_params_dict[key] = perturb + param\n",
    "        directional_derivative = (func(perturbed_params_dict[\"cob\"]) - base) / step_size\n",
    "        if len(grads_dict.keys()) == len(params_dict.keys()):\n",
    "            for key, perturb in perturbs_dict.items():\n",
    "                grads_dict[key] += perturb * directional_derivative / sample_size\n",
    "        else:\n",
    "            for key, perturb in perturbs_dict.items():\n",
    "                grads_dict[key] = perturb * directional_derivative / sample_size\n",
    "    return grads_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8979722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.pred_mul = 5\n",
    "# initial_cob_idx = torch.ones(s)\n",
    "# args.steps = int(s[0] * 10)\n",
    "# args.cob_lr = 10\n",
    "# args.zoo_sample_size = 100\n",
    "# args.zoo_step_size = 0.1\n",
    "# best_loss = 1e9\n",
    "\n",
    "# # grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, 10, 0.1)\n",
    "\n",
    "# # training the cob\n",
    "# # loop over the steps\n",
    "# for step in range(args.steps):\n",
    "#     # get the gradient of the cob\n",
    "#     grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, args.zoo_sample_size, args.zoo_step_size)\n",
    "#     # update the cob\n",
    "#     initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "#     # calculate the loss\n",
    "#     loss = ackley(initial_cob_idx)\n",
    "#     # update the best loss\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_cob = initial_cob_idx\n",
    "#         print(f\"Step: {step} \\t Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "958f1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply best_cob to find the pred_error and range_loss\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# teleported_model_idx = teleported_model_idx.teleport(best_cob, reset_teleportation=True)\n",
    "\n",
    "# # Reset activation stats and run a forward pass\n",
    "# activation_stats = {}\n",
    "# hook_handles = []\n",
    "# for i, layer in enumerate(teleported_model_idx.network.children()):\n",
    "#         if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "#             handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "#             hook_handles.append(handle)\n",
    "# teleported_model_idx.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = teleported_model_idx.network(input_teleported_model)\n",
    "# for handle in hook_handles:\n",
    "#     handle.remove()\n",
    "\n",
    "# # Calculate the range loss\n",
    "# loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# loss /= original_loss_idx\n",
    "# # Calculate the prediction error\n",
    "# pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "# pred_error /= np.abs(original_pred).mean()\n",
    "# total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "# print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "# print(f\"Total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b101ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cge(func, params_dict, mask_dict, step_size, base=None):\n",
    "    if base == None:\n",
    "        base = func(params_dict[\"cob\"])\n",
    "    grads_dict = {}\n",
    "    for key, param in params_dict.items():\n",
    "        if 'orig' in key:\n",
    "            mask_key = key.replace('orig', 'mask')\n",
    "            mask_flat = mask_dict[mask_key].flatten()\n",
    "        else:\n",
    "            mask_flat = torch.ones_like(param).flatten()\n",
    "        directional_derivative = torch.zeros_like(param)\n",
    "        directional_derivative_flat = directional_derivative.flatten()\n",
    "        for idx in mask_flat.nonzero().flatten():\n",
    "            perturbed_params_dict = copy.deepcopy(params_dict)\n",
    "            p_flat = perturbed_params_dict[key].flatten()\n",
    "            p_flat[idx] += step_size\n",
    "            directional_derivative_flat[idx] = (func(perturbed_params_dict[\"cob\"]) - base) / step_size\n",
    "        grads_dict[key] = directional_derivative.to(param.device)\n",
    "    return grads_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a51bd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.pred_mul = 10\n",
    "# args.steps = 800\n",
    "# initial_cob_idx = torch.ones(s)\n",
    "# # args.steps = int(s[0] * 10)\n",
    "# args.cob_lr = 0.1 # was 0.1\n",
    "# args.zoo_step_size = 0.001 # was 0.005 -> loss turned out to 57.2 (?)\n",
    "# #                                0.001 -> 56.2 (step 446)\n",
    "# #                                0.0001 -> 56.3 (lots of steps)\n",
    "# best_loss = 1e9\n",
    "\n",
    "\n",
    "# # grad_cob = rge(ackley, {\"cob\": initial_cob_idx}, 10, 0.1)\n",
    "\n",
    "# # training the cob\n",
    "# # loop over the steps\n",
    "# for step in range(args.steps):\n",
    "#     # get the gradient of the cob\n",
    "#     grad_cob = cge(ackley, {\"cob\": initial_cob_idx}, None, args.zoo_step_size)\n",
    "#     # update the cob\n",
    "#     initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "#     # calculate the loss\n",
    "#     loss = ackley(initial_cob_idx)\n",
    "#     # update the best loss\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_cob = initial_cob_idx\n",
    "#         print(f\"Step: {step} \\t Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4386ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply best_cob to find the pred_error and range_loss\n",
    "# teleported_model_idx = LinearNet()\n",
    "# teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "# load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "# teleported_model_idx = teleported_model_idx.teleport(best_cob, reset_teleportation=True)\n",
    "\n",
    "# # Reset activation stats and run a forward pass\n",
    "# activation_stats = {}\n",
    "# hook_handles = []\n",
    "# for i, layer in enumerate(teleported_model_idx.network.children()):\n",
    "#         if isinstance(layer, nn.ReLU) or isinstance(layer, ReLUCOB) or isinstance(layer, SigmoidCOB) or isinstance(layer, nn.Sigmoid) or isinstance(layer, GELUCOB) or isinstance(layer, nn.GELU) or isinstance(layer, LeakyReLUCOB) or isinstance(layer, nn.LeakyReLU):\n",
    "#             handle = layer.register_forward_hook(activation_hook(f'relu_{i}',activation_stats=activation_stats))\n",
    "#             hook_handles.append(handle)\n",
    "# teleported_model_idx.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = teleported_model_idx.network(input_teleported_model)\n",
    "# for handle in hook_handles:\n",
    "#     handle.remove()\n",
    "\n",
    "# # Calculate the range loss\n",
    "# loss = sum([stats['max'] - stats['min'] for stats in activation_stats.values()])\n",
    "# loss /= original_loss_idx\n",
    "# # Calculate the prediction error\n",
    "# pred_error = np.absolute(original_pred - pred.detach().cpu().numpy()).mean()\n",
    "# pred_error /= np.abs(original_pred).mean()\n",
    "# total_loss = loss + args.pred_mul * pred_error\n",
    "\n",
    "# print(f\"pred_error: {pred_error} \\t range_loss: {loss}\")\n",
    "# print(f\"Total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file and write the header if it doesn't exist\n",
    "import csv\n",
    "\n",
    "csv_file_path = args.prefix_dir + 'all_settings_deepzero.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            'EXPERIMENT SETTINGS',\n",
    "            'Model_Number', 'Layer_Index',\n",
    "            # 'Activation_Loss', 'Prediction_Error',\n",
    "            'Activation_Loss',\n",
    "            'Input_Scale', 'Param_Scale', 'Scale_Rebase_Multiplier',\n",
    "            'Lookup_Range_0', 'Lookup_Range_1', 'Logrows', 'Num_Rows', 'Num_cols', 'Total_Assignments',\n",
    "            'Total_Constant_Size', 'Model_Output_Scales', \n",
    "            'Required_Range_Checks_0','Required_Range_Checks_1', 'Proof_Time_Seconds', 'Max_Memory' ,'Mean_Squared_Error', 'Mean_Abs_Percent_Error',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= START =========\n",
      "input_param_scale: 16, num_cols: 64, max_log_rows: -1, param_visibility: fixed, lookup_margin: 2\n",
      "layer_idx: 0 , \t  activation_stats: {'relu_1': {'norm': tensor(476.6286), 'max': tensor(6.2171), 'min': tensor(-6.8579), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(13.0750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/layers/neuron.py:310: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(self.prev_cob == 1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_error: 2.068071989924647e-05 \t range_loss: 0.9999898076057434\n",
      "pred_error: 2.9191305657150224e-05 \t range_loss: 0.9999898076057434\n",
      "pred_error: 2.714591937547084e-05 \t range_loss: 0.9999898076057434\n",
      "Step: 0 \t Loss: 1.0335593223571777\n",
      "Step: 1 \t Loss: 0.9670328497886658\n",
      "pred_error: 0.0040275538340210915 \t range_loss: 0.9267262816429138\n",
      "Step: 2 \t Loss: 0.9376171827316284\n",
      "Step: 3 \t Loss: 0.9279061555862427\n",
      "Step: 4 \t Loss: 0.8983767628669739\n",
      "pred_error: 0.0046960292384028435 \t range_loss: 0.8514182567596436\n",
      "Step: 5 \t Loss: 0.8915551900863647\n",
      "Step: 6 \t Loss: 0.8797388076782227\n",
      "Step: 7 \t Loss: 0.8648985624313354\n",
      "Step: 8 \t Loss: 0.8569036722183228\n",
      "Step: 9 \t Loss: 0.8502700924873352\n",
      "pred_error: 0.005080376286059618 \t range_loss: 0.7994646430015564\n",
      "Step: 10 \t Loss: 0.8377221822738647\n",
      "Step: 12 \t Loss: 0.8341931104660034\n",
      "Step: 13 \t Loss: 0.8321618437767029\n",
      "Step: 14 \t Loss: 0.8311120271682739\n",
      "Step: 15 \t Loss: 0.8224620223045349\n",
      "Step: 16 \t Loss: 0.8142070770263672\n",
      "Step: 17 \t Loss: 0.8134172558784485\n",
      "pred_error: 0.004962821025401354 \t range_loss: 0.7637930512428284\n",
      "Step: 18 \t Loss: 0.8122773766517639\n",
      "pred_error: 0.005078814923763275 \t range_loss: 0.7614886164665222\n",
      "pred_error: 0.0053497436456382275 \t range_loss: 0.7595992088317871\n",
      "Step: 20 \t Loss: 0.8095033764839172\n",
      "Step: 21 \t Loss: 0.8081076741218567\n",
      "pred_error: 0.005205315537750721 \t range_loss: 0.7560545206069946\n",
      "Step: 22 \t Loss: 0.8024575114250183\n",
      "pred_error: 0.005181663203984499 \t range_loss: 0.7505838871002197\n",
      "Step: 23 \t Loss: 0.799782395362854\n",
      "Step: 24 \t Loss: 0.7985395789146423\n",
      "pred_error: 0.005452301353216171 \t range_loss: 0.7440119385719299\n",
      "Step: 25 \t Loss: 0.7935965657234192\n",
      "pred_error: 0.005276871845126152 \t range_loss: 0.7421079874038696\n",
      "Step: 27 \t Loss: 0.792155385017395\n",
      "pred_error: 0.005649136379361153 \t range_loss: 0.7373394966125488\n",
      "pred_error: 0.0056384955532848835 \t range_loss: 0.7373394966125488\n",
      "pred_error: 0.005644686054438353 \t range_loss: 0.7373394966125488\n",
      "Step: 31 \t Loss: 0.7897515892982483\n",
      "pred_error: 0.00539164524525404 \t range_loss: 0.7358366250991821\n",
      "Step: 32 \t Loss: 0.7879810333251953\n",
      "Step: 33 \t Loss: 0.7865201830863953\n",
      "Step: 34 \t Loss: 0.7828879952430725\n",
      "pred_error: 0.0053469110280275345 \t range_loss: 0.729416012763977\n",
      "Step: 35 \t Loss: 0.7809505462646484\n",
      "Step: 40 \t Loss: 0.7777238488197327\n",
      "pred_error: 0.005763207096606493 \t range_loss: 0.721572756767273\n",
      "pred_error: 0.00576293608173728 \t range_loss: 0.721572756767273\n",
      "Step: 42 \t Loss: 0.776839017868042\n",
      "Step: 43 \t Loss: 0.7752469182014465\n",
      "Step: 44 \t Loss: 0.7740675210952759\n",
      "Step: 45 \t Loss: 0.7721742987632751\n",
      "pred_error: 0.005741186439990997 \t range_loss: 0.7147624492645264\n",
      "pred_error: 0.005741158500313759 \t range_loss: 0.7147624492645264\n",
      "Step: 46 \t Loss: 0.7698358297348022\n",
      "pred_error: 0.005645656492561102 \t range_loss: 0.7133780717849731\n",
      "pred_error: 0.005645846016705036 \t range_loss: 0.7133780717849731\n",
      "pred_error: 0.0055569615215063095 \t range_loss: 0.7153224945068359\n",
      "pred_error: 0.0058358898386359215 \t range_loss: 0.7125796675682068\n",
      "Step: 49 \t Loss: 0.7686330676078796\n",
      "Step: 52 \t Loss: 0.7681944370269775\n",
      "Step: 53 \t Loss: 0.7675420045852661\n",
      "pred_error: 0.005741060711443424 \t range_loss: 0.7101314067840576\n",
      "Step: 54 \t Loss: 0.7669731974601746\n",
      "pred_error: 0.005863212514668703 \t range_loss: 0.7107354402542114\n",
      "pred_error: 0.006405937951058149 \t range_loss: 0.7077937126159668\n",
      "Step: 59 \t Loss: 0.7668614387512207\n",
      "Step: 60 \t Loss: 0.7647147178649902\n",
      "pred_error: 0.00613818084821105 \t range_loss: 0.7033308744430542\n",
      "pred_error: 0.0061378381215035915 \t range_loss: 0.7033308744430542\n",
      "Step: 61 \t Loss: 0.7624703049659729\n",
      "pred_error: 0.0062252553179860115 \t range_loss: 0.7021462321281433\n",
      "pred_error: 0.0064517115242779255 \t range_loss: 0.701262354850769\n",
      "pred_error: 0.0062497942708432674 \t range_loss: 0.7003797888755798\n",
      "pred_error: 0.006250125356018543 \t range_loss: 0.7003797888755798\n",
      "Step: 65 \t Loss: 0.7617834806442261\n",
      "pred_error: 0.006180328782647848 \t range_loss: 0.6999810338020325\n",
      "Step: 66 \t Loss: 0.7608795166015625\n",
      "Step: 67 \t Loss: 0.7595295906066895\n",
      "pred_error: 0.006672697141766548 \t range_loss: 0.695730984210968\n",
      "pred_error: 0.006472584325820208 \t range_loss: 0.6951844096183777\n",
      "Step: 72 \t Loss: 0.7594578266143799\n",
      "Step: 74 \t Loss: 0.7589011192321777\n",
      "pred_error: 0.006557885557413101 \t range_loss: 0.6933222413063049\n",
      "pred_error: 0.006777087692171335 \t range_loss: 0.6919340491294861\n",
      "Step: 76 \t Loss: 0.7580229043960571\n",
      "Step: 77 \t Loss: 0.7573918104171753\n",
      "pred_error: 0.006923271343111992 \t range_loss: 0.69001305103302\n",
      "Step: 83 \t Loss: 0.7565837502479553\n",
      "pred_error: 0.006911184173077345 \t range_loss: 0.6875980496406555\n",
      "Step: 86 \t Loss: 0.755270779132843\n",
      "Step: 87 \t Loss: 0.754051923751831\n",
      "pred_error: 0.00701178377494216 \t range_loss: 0.6839348077774048\n",
      "Step: 90 \t Loss: 0.7540135979652405\n",
      "pred_error: 0.007165307644754648 \t range_loss: 0.6833059787750244\n",
      "Step: 92 \t Loss: 0.7539271116256714\n",
      "Step: 93 \t Loss: 0.7531043887138367\n",
      "pred_error: 0.0070275054313242435 \t range_loss: 0.6828287839889526\n",
      "Step: 96 \t Loss: 0.7527326941490173\n",
      "Step: 97 \t Loss: 0.7514830827713013\n",
      "Step: 98 \t Loss: 0.750637412071228\n",
      "pred_error: 0.007570997811853886 \t range_loss: 0.6778808236122131\n",
      "pred_error: 0.007570905610918999 \t range_loss: 0.6778808236122131\n",
      "pred_error: 0.007570880465209484 \t range_loss: 0.6778808236122131\n",
      "pred_error: 0.007644646801054478 \t range_loss: 0.6771312952041626\n",
      "pred_error: 0.007567142602056265 \t range_loss: 0.6763530373573303\n",
      "pred_error: 0.0075348070822656155 \t range_loss: 0.6758363842964172\n",
      "pred_error: 0.0077333576045930386 \t range_loss: 0.6751342415809631\n",
      "pred_error: 0.007733314763754606 \t range_loss: 0.6751342415809631\n",
      "Step: 110 \t Loss: 0.7501866817474365\n",
      "Step: 111 \t Loss: 0.7493324279785156\n",
      "pred_error: 0.007729122415184975 \t range_loss: 0.6726041436195374\n",
      "pred_error: 0.008022762835025787 \t range_loss: 0.6711053848266602\n",
      "pred_error: 0.008023827336728573 \t range_loss: 0.6711053848266602\n",
      "Step: 116 \t Loss: 0.7492306232452393\n",
      "Step: 117 \t Loss: 0.7473916411399841\n",
      "pred_error: 0.008110837079584599 \t range_loss: 0.6662832498550415\n",
      "Step: 121 \t Loss: 0.7471197843551636\n",
      "pred_error: 0.008335161954164505 \t range_loss: 0.6637703776359558\n",
      "Step: 122 \t Loss: 0.7464628219604492\n",
      "Step: 123 \t Loss: 0.7460709810256958\n",
      "pred_error: 0.008347637951374054 \t range_loss: 0.6625946164131165\n",
      "pred_error: 0.00851418450474739 \t range_loss: 0.663038432598114\n",
      "pred_error: 0.008697817102074623 \t range_loss: 0.6630274057388306\n",
      "pred_error: 0.008582430891692638 \t range_loss: 0.6623610258102417\n",
      "pred_error: 0.008533701300621033 \t range_loss: 0.6637696623802185\n",
      "pred_error: 0.009076074697077274 \t range_loss: 0.6596195101737976\n",
      "pred_error: 0.008913177065551281 \t range_loss: 0.6588041186332703\n",
      "Step: 136 \t Loss: 0.7460252046585083\n",
      "Step: 138 \t Loss: 0.7458817958831787\n",
      "pred_error: 0.008965003304183483 \t range_loss: 0.656441867351532\n",
      "Step: 142 \t Loss: 0.7448061108589172\n",
      "Step: 143 \t Loss: 0.7444702386856079\n",
      "pred_error: 0.008998527191579342 \t range_loss: 0.6544895172119141\n",
      "Step: 146 \t Loss: 0.7427117824554443\n",
      "pred_error: 0.008945591747760773 \t range_loss: 0.6532564163208008\n",
      "pred_error: 0.008890209719538689 \t range_loss: 0.6542590260505676\n",
      "pred_error: 0.00906817615032196 \t range_loss: 0.6534777283668518\n",
      "pred_error: 0.009061641059815884 \t range_loss: 0.6558896899223328\n",
      "pred_error: 0.00927652046084404 \t range_loss: 0.6525329947471619\n",
      "pred_error: 0.009385730139911175 \t range_loss: 0.6515370011329651\n",
      "pred_error: 0.009544859640300274 \t range_loss: 0.64993816614151\n",
      "pred_error: 0.009545094333589077 \t range_loss: 0.64993816614151\n",
      "Step: 161 \t Loss: 0.7426820993423462\n",
      "pred_error: 0.009499067440629005 \t range_loss: 0.6476917266845703\n",
      "Step: 162 \t Loss: 0.7416975498199463\n",
      "Step: 165 \t Loss: 0.7413416504859924\n",
      "pred_error: 0.00963042862713337 \t range_loss: 0.6467093825340271\n",
      "pred_error: 0.01001647673547268 \t range_loss: 0.6454762816429138\n",
      "pred_error: 0.01001015305519104 \t range_loss: 0.6446516513824463\n",
      "pred_error: 0.010009885765612125 \t range_loss: 0.6446516513824463\n",
      "Step: 176 \t Loss: 0.7406759858131409\n",
      "pred_error: 0.00978113804012537 \t range_loss: 0.6428641676902771\n",
      "Step: 177 \t Loss: 0.7396206259727478\n",
      "pred_error: 0.009777731262147427 \t range_loss: 0.6432984471321106\n",
      "pred_error: 0.009974907152354717 \t range_loss: 0.6448187828063965\n",
      "pred_error: 0.009974563494324684 \t range_loss: 0.6448187828063965\n",
      "pred_error: 0.010083766654133797 \t range_loss: 0.6431180238723755\n",
      "pred_error: 0.010083810426294804 \t range_loss: 0.6431180238723755\n",
      "pred_error: 0.009966862387955189 \t range_loss: 0.6408196091651917\n",
      "pred_error: 0.010472719557583332 \t range_loss: 0.6407399773597717\n",
      "pred_error: 0.01006049569696188 \t range_loss: 0.640169084072113\n",
      "Step: 191 \t Loss: 0.738563060760498\n",
      "pred_error: 0.010479980148375034 \t range_loss: 0.6370790600776672\n",
      "pred_error: 0.01047997921705246 \t range_loss: 0.6370790600776672\n",
      "pred_error: 0.010479775257408619 \t range_loss: 0.6370790600776672\n",
      "pred_error: 0.010433604009449482 \t range_loss: 0.6379854679107666\n",
      "pred_error: 0.010648036375641823 \t range_loss: 0.6362623572349548\n",
      "pred_error: 0.01055224984884262 \t range_loss: 0.6352893114089966\n",
      "pred_error: 0.010678866878151894 \t range_loss: 0.6356485486030579\n",
      "pred_error: 0.010629862546920776 \t range_loss: 0.6327642202377319\n",
      "pred_error: 0.010629910044372082 \t range_loss: 0.6327642202377319\n",
      "Step: 216 \t Loss: 0.7376683950424194\n",
      "pred_error: 0.01046642754226923 \t range_loss: 0.6343848705291748\n",
      "pred_error: 0.010720997117459774 \t range_loss: 0.6341837048530579\n",
      "pred_error: 0.011011606082320213 \t range_loss: 0.6292047500610352\n",
      "pred_error: 0.01090631540864706 \t range_loss: 0.6298843026161194\n",
      "pred_error: 0.010986613109707832 \t range_loss: 0.6287573575973511\n",
      "pred_error: 0.010971514508128166 \t range_loss: 0.6280493140220642\n",
      "pred_error: 0.011361531913280487 \t range_loss: 0.6270519495010376\n",
      "pred_error: 0.011303965002298355 \t range_loss: 0.6266987323760986\n",
      "pred_error: 0.011185054667294025 \t range_loss: 0.6272293329238892\n",
      "pred_error: 0.011185048148036003 \t range_loss: 0.6272293329238892\n",
      "pred_error: 0.011238405480980873 \t range_loss: 0.6276302933692932\n",
      "pred_error: 0.011223072186112404 \t range_loss: 0.6257355809211731\n",
      "pred_error: 0.011290243826806545 \t range_loss: 0.6258697509765625\n",
      "pred_error: 0.011294608935713768 \t range_loss: 0.6254522800445557\n",
      "pred_error: 0.011294601485133171 \t range_loss: 0.6254522800445557\n",
      "pred_error: 0.011528941802680492 \t range_loss: 0.62652987241745\n",
      "pred_error: 0.011801456101238728 \t range_loss: 0.6241347193717957\n",
      "pred_error: 0.011801592074334621 \t range_loss: 0.6241347193717957\n",
      "Step: 253 \t Loss: 0.7375650405883789\n",
      "Step: 259 \t Loss: 0.7373089790344238\n",
      "pred_error: 0.011665410362184048 \t range_loss: 0.6206523180007935\n",
      "pred_error: 0.011630319990217686 \t range_loss: 0.6231741309165955\n",
      "pred_error: 0.011959352530539036 \t range_loss: 0.6203461289405823\n",
      "pred_error: 0.011966051533818245 \t range_loss: 0.6197996735572815\n",
      "Step: 265 \t Loss: 0.737248957157135\n",
      "pred_error: 0.011770850047469139 \t range_loss: 0.6195404529571533\n",
      "pred_error: 0.01177073922008276 \t range_loss: 0.6195404529571533\n",
      "Step: 266 \t Loss: 0.7358878254890442\n",
      "pred_error: 0.011639697477221489 \t range_loss: 0.6194908618927002\n",
      "pred_error: 0.011935834772884846 \t range_loss: 0.6203833222389221\n",
      "pred_error: 0.012100784108042717 \t range_loss: 0.6193268299102783\n",
      "pred_error: 0.012169462628662586 \t range_loss: 0.6216180324554443\n",
      "pred_error: 0.012220410630106926 \t range_loss: 0.6197748780250549\n",
      "pred_error: 0.01214596163481474 \t range_loss: 0.6182112097740173\n",
      "pred_error: 0.012010186910629272 \t range_loss: 0.6177142858505249\n",
      "pred_error: 0.012008560821413994 \t range_loss: 0.6177142858505249\n",
      "pred_error: 0.012114105746150017 \t range_loss: 0.6179961562156677\n",
      "pred_error: 0.012114062905311584 \t range_loss: 0.6179961562156677\n",
      "pred_error: 0.012045291252434254 \t range_loss: 0.6180631518363953\n",
      "pred_error: 0.012219290249049664 \t range_loss: 0.616562008857727\n",
      "pred_error: 0.012366554699838161 \t range_loss: 0.6159944534301758\n",
      "pred_error: 0.012134573422372341 \t range_loss: 0.6175089478492737\n",
      "pred_error: 0.012387768365442753 \t range_loss: 0.6162185072898865\n",
      "pred_error: 0.012387849390506744 \t range_loss: 0.6162185072898865\n",
      "pred_error: 0.012565724551677704 \t range_loss: 0.6158046722412109\n",
      "pred_error: 0.012591153383255005 \t range_loss: 0.614398717880249\n",
      "pred_error: 0.012853294610977173 \t range_loss: 0.6139906048774719\n",
      "pred_error: 0.012833108194172382 \t range_loss: 0.6140049695968628\n",
      "pred_error: 0.012677337042987347 \t range_loss: 0.6135332584381104\n",
      "pred_error: 0.012677335180342197 \t range_loss: 0.6135332584381104\n",
      "pred_error: 0.012677260674536228 \t range_loss: 0.6135332584381104\n",
      "pred_error: 0.012761138379573822 \t range_loss: 0.6122552156448364\n",
      "pred_error: 0.01266573928296566 \t range_loss: 0.6124999523162842\n",
      "pred_error: 0.012688045389950275 \t range_loss: 0.6133424639701843\n",
      "pred_error: 0.012644698843359947 \t range_loss: 0.612213671207428\n",
      "pred_error: 0.012651512399315834 \t range_loss: 0.6130204200744629\n",
      "pred_error: 0.0126514732837677 \t range_loss: 0.6130204200744629\n",
      "pred_error: 0.012529896572232246 \t range_loss: 0.6133172512054443\n",
      "pred_error: 0.012906597927212715 \t range_loss: 0.6128859519958496\n",
      "pred_error: 0.012790829874575138 \t range_loss: 0.6122391223907471\n",
      "pred_error: 0.012696107849478722 \t range_loss: 0.6120884418487549\n",
      "pred_error: 0.012578217312693596 \t range_loss: 0.6138591170310974\n",
      "pred_error: 0.01269080862402916 \t range_loss: 0.6122387051582336\n",
      "pred_error: 0.012692655436694622 \t range_loss: 0.6122387051582336\n",
      "pred_error: 0.012756485491991043 \t range_loss: 0.6119024157524109\n",
      "pred_error: 0.01289796270430088 \t range_loss: 0.6104447841644287\n",
      "pred_error: 0.012887001037597656 \t range_loss: 0.609932005405426\n",
      "pred_error: 0.012817217037081718 \t range_loss: 0.6099739670753479\n",
      "pred_error: 0.012954099103808403 \t range_loss: 0.6111961603164673\n",
      "pred_error: 0.01320861279964447 \t range_loss: 0.6101844310760498\n",
      "pred_error: 0.013088831678032875 \t range_loss: 0.6089420914649963\n",
      "pred_error: 0.013050848618149757 \t range_loss: 0.6082093119621277\n",
      "pred_error: 0.013166758231818676 \t range_loss: 0.6078664660453796\n",
      "pred_error: 0.013098843395709991 \t range_loss: 0.6090637445449829\n",
      "pred_error: 0.01305977813899517 \t range_loss: 0.6080565452575684\n",
      "pred_error: 0.013308350928127766 \t range_loss: 0.6076066493988037\n",
      "pred_error: 0.013261756859719753 \t range_loss: 0.6076661348342896\n",
      "pred_error: 0.013194897212088108 \t range_loss: 0.6077128648757935\n",
      "pred_error: 0.013249731622636318 \t range_loss: 0.6052509546279907\n",
      "pred_error: 0.01313062198460102 \t range_loss: 0.6052058339118958\n",
      "pred_error: 0.013472937047481537 \t range_loss: 0.6080131530761719\n",
      "pred_error: 0.013612943701446056 \t range_loss: 0.6060397028923035\n",
      "pred_error: 0.013483505696058273 \t range_loss: 0.6059550642967224\n",
      "pred_error: 0.013350967317819595 \t range_loss: 0.6059438586235046\n",
      "pred_error: 0.013208222575485706 \t range_loss: 0.6058984398841858\n",
      "pred_error: 0.013547717593610287 \t range_loss: 0.6061230897903442\n",
      "pred_error: 0.013717588037252426 \t range_loss: 0.6046572327613831\n",
      "pred_error: 0.013471362181007862 \t range_loss: 0.6039352416992188\n",
      "pred_error: 0.013425146229565144 \t range_loss: 0.6034825444221497\n",
      "pred_error: 0.013495026156306267 \t range_loss: 0.6023879051208496\n",
      "pred_error: 0.013547741807997227 \t range_loss: 0.603091299533844\n",
      "pred_error: 0.013449156656861305 \t range_loss: 0.6029670238494873\n",
      "pred_error: 0.013523349538445473 \t range_loss: 0.6035870313644409\n",
      "pred_error: 0.013551872223615646 \t range_loss: 0.6056182980537415\n",
      "pred_error: 0.013763864524662495 \t range_loss: 0.6033013463020325\n",
      "pred_error: 0.013602455146610737 \t range_loss: 0.6021060347557068\n",
      "pred_error: 0.0135634271427989 \t range_loss: 0.6027901768684387\n",
      "pred_error: 0.01365545392036438 \t range_loss: 0.6030574440956116\n",
      "pred_error: 0.013655552640557289 \t range_loss: 0.6030574440956116\n",
      "pred_error: 0.013752929866313934 \t range_loss: 0.6019326448440552\n",
      "pred_error: 0.013646475970745087 \t range_loss: 0.6034053564071655\n",
      "pred_error: 0.013824558816850185 \t range_loss: 0.6000014543533325\n",
      "pred_error: 0.014203079044818878 \t range_loss: 0.5995970368385315\n",
      "pred_error: 0.01401408202946186 \t range_loss: 0.6001981496810913\n",
      "pred_error: 0.014010314829647541 \t range_loss: 0.6001397371292114\n",
      "pred_error: 0.014010707847774029 \t range_loss: 0.6001397371292114\n",
      "pred_error: 0.014159217476844788 \t range_loss: 0.5992889404296875\n",
      "pred_error: 0.014135094359517097 \t range_loss: 0.5999066829681396\n",
      "pred_error: 0.014048412442207336 \t range_loss: 0.5995991230010986\n",
      "pred_error: 0.013922115787863731 \t range_loss: 0.599272608757019\n",
      "pred_error: 0.014174200594425201 \t range_loss: 0.5978626608848572\n",
      "pred_error: 0.01387873012572527 \t range_loss: 0.5999780893325806\n",
      "pred_error: 0.01407734863460064 \t range_loss: 0.5989060997962952\n",
      "pred_error: 0.014130039140582085 \t range_loss: 0.5991377830505371\n",
      "pred_error: 0.014130107127130032 \t range_loss: 0.5991377830505371\n",
      "pred_error: 0.014138779602944851 \t range_loss: 0.5996465086936951\n",
      "pred_error: 0.014166377484798431 \t range_loss: 0.5988742113113403\n",
      "pred_error: 0.014045405201613903 \t range_loss: 0.5996187925338745\n",
      "pred_error: 0.014254547655582428 \t range_loss: 0.598155677318573\n",
      "pred_error: 0.014373907819390297 \t range_loss: 0.5974103212356567\n",
      "pred_error: 0.01437442097812891 \t range_loss: 0.5974103212356567\n",
      "pred_error: 0.01420537754893303 \t range_loss: 0.5979794263839722\n",
      "pred_error: 0.014366952702403069 \t range_loss: 0.5970562696456909\n",
      "pred_error: 0.014400997199118137 \t range_loss: 0.5975237488746643\n",
      "pred_error: 0.014466957189142704 \t range_loss: 0.596741795539856\n",
      "pred_error: 0.014307945035398006 \t range_loss: 0.598466157913208\n",
      "pred_error: 0.014340153895318508 \t range_loss: 0.5976592302322388\n",
      "pred_error: 0.014354546554386616 \t range_loss: 0.5969066023826599\n",
      "pred_error: 0.014461714774370193 \t range_loss: 0.5964584350585938\n",
      "pred_error: 0.014452182687819004 \t range_loss: 0.5964584350585938\n",
      "pred_error: 0.014507230371236801 \t range_loss: 0.5968909859657288\n",
      "pred_error: 0.014395495876669884 \t range_loss: 0.5975853204727173\n",
      "pred_error: 0.014320967718958855 \t range_loss: 0.5969008803367615\n",
      "pred_error: 0.014218278229236603 \t range_loss: 0.5973045229911804\n",
      "pred_error: 0.014294268563389778 \t range_loss: 0.5955368280410767\n",
      "pred_error: 0.014207663014531136 \t range_loss: 0.5958737730979919\n",
      "pred_error: 0.014212075620889664 \t range_loss: 0.5968462824821472\n",
      "pred_error: 0.014436324127018452 \t range_loss: 0.5960221290588379\n",
      "pred_error: 0.01448164600878954 \t range_loss: 0.5964717864990234\n",
      "pred_error: 0.014486999250948429 \t range_loss: 0.5967537760734558\n",
      "pred_error: 0.014463101513683796 \t range_loss: 0.5967622995376587\n",
      "pred_error: 0.014463832601904869 \t range_loss: 0.5967622995376587\n",
      "pred_error: 0.014342614449560642 \t range_loss: 0.5964420437812805\n",
      "pred_error: 0.014342657290399075 \t range_loss: 0.5964420437812805\n",
      "pred_error: 0.014423394575715065 \t range_loss: 0.5963196754455566\n",
      "pred_error: 0.01464962400496006 \t range_loss: 0.595650851726532\n",
      "pred_error: 0.014649506658315659 \t range_loss: 0.595650851726532\n",
      "pred_error: 0.01469482108950615 \t range_loss: 0.595611035823822\n",
      "pred_error: 0.014391380362212658 \t range_loss: 0.5973957777023315\n",
      "pred_error: 0.014635021798312664 \t range_loss: 0.5952140688896179\n",
      "pred_error: 0.014565253630280495 \t range_loss: 0.5963733792304993\n",
      "pred_error: 0.014565045945346355 \t range_loss: 0.5963733792304993\n",
      "pred_error: 0.01517440378665924 \t range_loss: 0.5907943844795227\n",
      "pred_error: 0.015222227200865746 \t range_loss: 0.5911101698875427\n",
      "pred_error: 0.014987153001129627 \t range_loss: 0.5908700227737427\n",
      "pred_error: 0.015177500434219837 \t range_loss: 0.590528666973114\n",
      "pred_error: 0.015173688530921936 \t range_loss: 0.590038001537323\n",
      "pred_error: 0.01498259138315916 \t range_loss: 0.5912250280380249\n",
      "pred_error: 0.015361063182353973 \t range_loss: 0.5904218554496765\n",
      "pred_error: 0.015360802412033081 \t range_loss: 0.5904218554496765\n",
      "pred_error: 0.015135222114622593 \t range_loss: 0.5897671580314636\n",
      "pred_error: 0.015222063288092613 \t range_loss: 0.590229332447052\n",
      "pred_error: 0.015222468413412571 \t range_loss: 0.5919562578201294\n",
      "pred_error: 0.015223072841763496 \t range_loss: 0.5919562578201294\n",
      "pred_error: 0.01527026854455471 \t range_loss: 0.5894170999526978\n",
      "pred_error: 0.015174850821495056 \t range_loss: 0.5901329517364502\n",
      "pred_error: 0.015316047705709934 \t range_loss: 0.5903425216674805\n",
      "pred_error: 0.015273841097950935 \t range_loss: 0.5904146432876587\n",
      "pred_error: 0.01505004521459341 \t range_loss: 0.5896584987640381\n",
      "pred_error: 0.015185369178652763 \t range_loss: 0.5889140963554382\n",
      "pred_error: 0.01534753106534481 \t range_loss: 0.590688943862915\n",
      "pred_error: 0.015347464010119438 \t range_loss: 0.590688943862915\n",
      "pred_error: 0.015437022782862186 \t range_loss: 0.588241457939148\n",
      "pred_error: 0.015326807275414467 \t range_loss: 0.5878468155860901\n",
      "pred_error: 0.015404109843075275 \t range_loss: 0.5878337621688843\n",
      "pred_error: 0.015316787175834179 \t range_loss: 0.5877144932746887\n",
      "pred_error: 0.015316895209252834 \t range_loss: 0.5877144932746887\n",
      "pred_error: 0.015710866078734398 \t range_loss: 0.5879669785499573\n",
      "pred_error: 0.015710942447185516 \t range_loss: 0.5879669785499573\n",
      "pred_error: 0.015301480889320374 \t range_loss: 0.5881001353263855\n",
      "pred_error: 0.015446614474058151 \t range_loss: 0.5907153487205505\n",
      "pred_error: 0.01532680168747902 \t range_loss: 0.5884779095649719\n",
      "pred_error: 0.015344662591814995 \t range_loss: 0.5892283916473389\n",
      "pred_error: 0.015419923700392246 \t range_loss: 0.587411105632782\n",
      "pred_error: 0.015495946630835533 \t range_loss: 0.5876768231391907\n",
      "pred_error: 0.015587776899337769 \t range_loss: 0.5873523354530334\n",
      "pred_error: 0.015460718423128128 \t range_loss: 0.5865125060081482\n",
      "pred_error: 0.01564325951039791 \t range_loss: 0.5878196954727173\n",
      "pred_error: 0.015638452023267746 \t range_loss: 0.5878196954727173\n",
      "pred_error: 0.015692533925175667 \t range_loss: 0.5870053768157959\n",
      "pred_error: 0.015492971986532211 \t range_loss: 0.5868552923202515\n",
      "pred_error: 0.015492605045437813 \t range_loss: 0.5868552923202515\n",
      "pred_error: 0.015546530485153198 \t range_loss: 0.5868568420410156\n",
      "pred_error: 0.01554926298558712 \t range_loss: 0.5870600938796997\n",
      "pred_error: 0.015420421026647091 \t range_loss: 0.5863510966300964\n",
      "pred_error: 0.015587332658469677 \t range_loss: 0.5856637954711914\n",
      "pred_error: 0.01570107229053974 \t range_loss: 0.5874727368354797\n",
      "pred_error: 0.015478448942303658 \t range_loss: 0.5885345339775085\n",
      "pred_error: 0.01585717871785164 \t range_loss: 0.5865095257759094\n",
      "pred_error: 0.015725357457995415 \t range_loss: 0.5855162739753723\n",
      "pred_error: 0.015725409612059593 \t range_loss: 0.5855162739753723\n",
      "pred_error: 0.015590556897222996 \t range_loss: 0.5849327445030212\n",
      "pred_error: 0.015552319586277008 \t range_loss: 0.5851096510887146\n",
      "pred_error: 0.015476702712476254 \t range_loss: 0.5847434997558594\n",
      "pred_error: 0.015476752072572708 \t range_loss: 0.5847434997558594\n",
      "pred_error: 0.016105929389595985 \t range_loss: 0.5844064354896545\n",
      "pred_error: 0.015879295766353607 \t range_loss: 0.5841929912567139\n",
      "pred_error: 0.015888622030615807 \t range_loss: 0.584081768989563\n",
      "pred_error: 0.015719663351774216 \t range_loss: 0.5835862755775452\n",
      "pred_error: 0.01596955582499504 \t range_loss: 0.5834090709686279\n",
      "pred_error: 0.01586330682039261 \t range_loss: 0.5845580101013184\n",
      "pred_error: 0.015765352174639702 \t range_loss: 0.5835721492767334\n",
      "pred_error: 0.015804728493094444 \t range_loss: 0.5836953520774841\n",
      "pred_error: 0.015804795548319817 \t range_loss: 0.5836953520774841\n",
      "pred_error: 0.01606791466474533 \t range_loss: 0.5845454931259155\n",
      "pred_error: 0.016009418293833733 \t range_loss: 0.5836970210075378\n",
      "pred_error: 0.015860503539443016 \t range_loss: 0.584992527961731\n",
      "pred_error: 0.015767347067594528 \t range_loss: 0.5843437910079956\n",
      "pred_error: 0.01584562659263611 \t range_loss: 0.5833941102027893\n",
      "pred_error: 0.015949523076415062 \t range_loss: 0.5835022926330566\n",
      "pred_error: 0.015949713066220284 \t range_loss: 0.5835022926330566\n",
      "pred_error: 0.015909571200609207 \t range_loss: 0.5834118723869324\n",
      "pred_error: 0.015824774280190468 \t range_loss: 0.5828448534011841\n",
      "pred_error: 0.015767203643918037 \t range_loss: 0.5826611518859863\n",
      "pred_error: 0.016045359894633293 \t range_loss: 0.582817018032074\n",
      "pred_error: 0.016037972643971443 \t range_loss: 0.5822809934616089\n",
      "pred_error: 0.01592879183590412 \t range_loss: 0.5827771425247192\n",
      "pred_error: 0.016115766018629074 \t range_loss: 0.5842291116714478\n",
      "pred_error: 0.01622999832034111 \t range_loss: 0.583026111125946\n",
      "pred_error: 0.015962734818458557 \t range_loss: 0.5827910304069519\n",
      "pred_error: 0.01634673960506916 \t range_loss: 0.5818949341773987\n",
      "pred_error: 0.016291633248329163 \t range_loss: 0.5829211473464966\n",
      "pred_error: 0.016217337921261787 \t range_loss: 0.582621157169342\n",
      "pred_error: 0.016078772023320198 \t range_loss: 0.5824017524719238\n",
      "pred_error: 0.016078775748610497 \t range_loss: 0.5824017524719238\n",
      "pred_error: 0.015996651723980904 \t range_loss: 0.5835567116737366\n",
      "pred_error: 0.015997199341654778 \t range_loss: 0.5835567116737366\n",
      "pred_error: 0.016094136983156204 \t range_loss: 0.582251250743866\n",
      "pred_error: 0.016018928959965706 \t range_loss: 0.5817804336547852\n",
      "pred_error: 0.016288982704281807 \t range_loss: 0.5828571319580078\n",
      "pred_error: 0.016359787434339523 \t range_loss: 0.5830357670783997\n",
      "pred_error: 0.016136961057782173 \t range_loss: 0.5835120677947998\n",
      "pred_error: 0.01617491990327835 \t range_loss: 0.5819915533065796\n",
      "pred_error: 0.016175134107470512 \t range_loss: 0.5825098156929016\n",
      "pred_error: 0.01586839370429516 \t range_loss: 0.5827890634536743\n",
      "pred_error: 0.01619422435760498 \t range_loss: 0.5817584991455078\n",
      "pred_error: 0.016193510964512825 \t range_loss: 0.5817584991455078\n",
      "pred_error: 0.016003264114260674 \t range_loss: 0.5826178789138794\n",
      "pred_error: 0.0160032007843256 \t range_loss: 0.5826178789138794\n",
      "pred_error: 0.01634255424141884 \t range_loss: 0.5819221138954163\n",
      "pred_error: 0.016274474561214447 \t range_loss: 0.5819256901741028\n",
      "pred_error: 0.016358740627765656 \t range_loss: 0.5829495787620544\n",
      "pred_error: 0.016505548730492592 \t range_loss: 0.5807339549064636\n",
      "pred_error: 0.01627654954791069 \t range_loss: 0.5803043842315674\n",
      "pred_error: 0.01627659611403942 \t range_loss: 0.5803043842315674\n",
      "pred_error: 0.016207417473196983 \t range_loss: 0.5802461504936218\n",
      "pred_error: 0.016300255432724953 \t range_loss: 0.580129861831665\n",
      "pred_error: 0.016171583905816078 \t range_loss: 0.5826402902603149\n",
      "pred_error: 0.01650342158973217 \t range_loss: 0.5809696316719055\n",
      "pred_error: 0.016256386414170265 \t range_loss: 0.5814462304115295\n",
      "pred_error: 0.016267895698547363 \t range_loss: 0.5801975131034851\n",
      "pred_error: 0.016267815604805946 \t range_loss: 0.5801975131034851\n",
      "pred_error: 0.01656968891620636 \t range_loss: 0.5811575055122375\n",
      "pred_error: 0.01639395020902157 \t range_loss: 0.579810619354248\n",
      "pred_error: 0.01639392040669918 \t range_loss: 0.579810619354248\n",
      "pred_error: 0.016370592638850212 \t range_loss: 0.580453634262085\n",
      "BEST LOSS: 0.7358878\n",
      "==== Model: block0_cob_activation_norm  in Layer: 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 13:02:49,526 execute.rs:1044 num calibration batches: 1\n",
      "INFO tract_linalg.arm64 2024-09-16 13:02:49,539 arm64.rs:212 CPU optimisation: AppleM\n",
      "INFO tract_linalg.arm64 2024-09-16 13:02:49,540 arm64.rs:294 ARMv8.2 mmm_f16 and mmv_f16 activated\n",
      "INFO tract_linalg.arm64 2024-09-16 13:02:49,541 arm64.rs:315 ARMv8.2 tanh_f16 and sigmoid_f16 activated\n",
      "INFO tract_linalg.arm64 2024-09-16 13:02:49,542 arm64.rs:328 AMX optimisation activated\n",
      "ERROR ezkl.graph.model 2024-09-16 13:04:57,356 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:04:57,365 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 13:05:17,911 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:05:17,920 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 13:05:17,994 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 13:05:18,074 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 13:05:18,089 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error     | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000000483782 | 0.00004838407 | 0.00029587746 | -0.0002837777 | 0.00004309618  | 0.00004838407    | 0.00029587746 | 0             | 0.000000003106742  | 0.00026246207      | 0.0009010576           |\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 644815 64 [-635610, 576214] 1 [16]\n",
      "===============================\n",
      "==== Model: block0_cob_activation_norm_teleported  in Layer: 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 13:05:39,356 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 13:07:42,192 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:07:42,199 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 13:08:02,560 model.rs:1246 value (-454336) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:08:02,565 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 13:08:02,584 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 13:08:02,605 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 13:08:02,621 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.00000019549545 | -0.000022426248 | 0.00029560924 | -0.00025856495 | 0.000043380584 | 0.000022426248   | 0.00029560924 | 0             | 0.0000000031457021 | 0.00003521087      | 0.0006645845           |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 644823 64 [-411124, 291680] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 1 , \t  activation_stats: {'relu_1': {'norm': tensor(319.0262), 'max': tensor(6.5916), 'min': tensor(-4.5777), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(11.1694)\n",
      "====== NO OPTIMIZATION SINCE NO TELEPORTATION =====\n",
      "==== Model: block1_cob_activation_norm  in Layer: 1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 13:08:22,237 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 13:10:22,979 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:10:22,985 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 13:10:42,616 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:10:42,624 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 13:10:42,651 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 13:10:42,678 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 13:10:42,691 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000022606143 | -0.000042706728 | 0.00025475025 | -0.00023677945 | 0.000051323652 | 0.000042706728   | 0.00025475025 | 0             | 0.0000000041819175 | -0.000027651575    | 0.0006764947           |\n",
      "+-----------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 638317 64 [-424278, 610914] 1 [16]\n",
      "===============================\n",
      "==== Model: block1_cob_activation_norm_teleported  in Layer: 1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 13:11:02,501 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 13:12:57,833 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:12:57,839 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 13:13:16,519 model.rs:1246 value (-327872) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 13:13:16,524 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 13:13:16,542 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 13:13:16,560 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 13:13:16,576 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error     | median_error   | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.000002370056 | -0.00004467368 | 0.0002568364 | -0.00023817271 | 0.000051128263 | 0.00004467368    | 0.0002568364  | 0             | 0.0000000041670876 | -0.00003275327     | 0.0006839564           |\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 638318 64 [-424272, 610906] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 2 , \t  activation_stats: {'relu_1': {'norm': tensor(414.7610), 'max': tensor(4.6655), 'min': tensor(-6.8689), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(11.5343)\n",
      "Step: 0 \t Loss: 1.1848399639129639\n",
      "Step: 1 \t Loss: 1.1611366271972656\n",
      "pred_error: 0.02330457977950573 \t range_loss: 0.9280893802642822\n",
      "Step: 2 \t Loss: 1.1420512199401855\n",
      "pred_error: 0.02336311526596546 \t range_loss: 0.9084201455116272\n",
      "Step: 3 \t Loss: 1.1073503494262695\n",
      "Step: 4 \t Loss: 1.0944997072219849\n",
      "pred_error: 0.023688718676567078 \t range_loss: 0.8576059937477112\n",
      "Step: 5 \t Loss: 1.0877439975738525\n",
      "pred_error: 0.023711711168289185 \t range_loss: 0.8506283164024353\n",
      "Step: 6 \t Loss: 1.0851325988769531\n",
      "Step: 7 \t Loss: 1.0807740688323975\n",
      "Step: 8 \t Loss: 1.0730276107788086\n",
      "pred_error: 0.023739447817206383 \t range_loss: 0.8356340527534485\n",
      "pred_error: 0.023739362135529518 \t range_loss: 0.8356340527534485\n",
      "pred_error: 0.023739518597722054 \t range_loss: 0.8356340527534485\n",
      "Step: 9 \t Loss: 1.0624116659164429\n",
      "pred_error: 0.023668695241212845 \t range_loss: 0.8257323503494263\n",
      "Step: 10 \t Loss: 1.0591211318969727\n",
      "Step: 11 \t Loss: 1.0576112270355225\n",
      "Step: 12 \t Loss: 1.0519071817398071\n",
      "pred_error: 0.024009283632040024 \t range_loss: 0.8118135929107666\n",
      "Step: 13 \t Loss: 1.0493799448013306\n",
      "Step: 14 \t Loss: 1.0464003086090088\n",
      "pred_error: 0.024046894162893295 \t range_loss: 0.805932879447937\n",
      "Step: 15 \t Loss: 1.0434564352035522\n",
      "Step: 16 \t Loss: 1.0421451330184937\n",
      "Step: 17 \t Loss: 1.038959264755249\n",
      "Step: 18 \t Loss: 1.037550926208496\n",
      "Step: 19 \t Loss: 1.0372031927108765\n",
      "Step: 20 \t Loss: 1.0355781316757202\n",
      "Step: 22 \t Loss: 1.0311875343322754\n",
      "pred_error: 0.024357464164495468 \t range_loss: 0.7876115441322327\n",
      "Step: 23 \t Loss: 1.025542140007019\n",
      "Step: 24 \t Loss: 1.022242546081543\n",
      "pred_error: 0.02437332645058632 \t range_loss: 0.7785080671310425\n",
      "pred_error: 0.02437324821949005 \t range_loss: 0.7785080671310425\n",
      "Step: 25 \t Loss: 1.0183415412902832\n",
      "pred_error: 0.02472611516714096 \t range_loss: 0.7720620632171631\n",
      "pred_error: 0.0247261393815279 \t range_loss: 0.7720620632171631\n",
      "Step: 31 \t Loss: 1.0167056322097778\n",
      "Step: 32 \t Loss: 1.0135127305984497\n",
      "Step: 33 \t Loss: 1.009773850440979\n",
      "pred_error: 0.02466583624482155 \t range_loss: 0.7631171345710754\n",
      "pred_error: 0.02466556802392006 \t range_loss: 0.7631171345710754\n",
      "Step: 34 \t Loss: 1.0076525211334229\n",
      "pred_error: 0.02475418709218502 \t range_loss: 0.7601084113121033\n",
      "pred_error: 0.024754386395215988 \t range_loss: 0.7601084113121033\n",
      "Step: 36 \t Loss: 1.0071830749511719\n",
      "pred_error: 0.024903422221541405 \t range_loss: 0.7581421136856079\n",
      "Step: 37 \t Loss: 1.004750370979309\n",
      "pred_error: 0.024949301034212112 \t range_loss: 0.7556625604629517\n",
      "Step: 40 \t Loss: 1.0038057565689087\n",
      "Step: 41 \t Loss: 1.0018219947814941\n",
      "Step: 42 \t Loss: 1.000614881515503\n",
      "Step: 43 \t Loss: 0.9996945858001709\n",
      "pred_error: 0.025233855471014977 \t range_loss: 0.7473586797714233\n",
      "pred_error: 0.025233527645468712 \t range_loss: 0.7473586797714233\n",
      "Step: 44 \t Loss: 0.9984986186027527\n",
      "Step: 45 \t Loss: 0.9975740909576416\n",
      "Step: 46 \t Loss: 0.9965015649795532\n",
      "Step: 47 \t Loss: 0.9956628680229187\n",
      "pred_error: 0.025325926020741463 \t range_loss: 0.7427680492401123\n",
      "pred_error: 0.02541297674179077 \t range_loss: 0.7429972887039185\n",
      "pred_error: 0.025515450164675713 \t range_loss: 0.7428628206253052\n",
      "pred_error: 0.025515450164675713 \t range_loss: 0.7428628206253052\n",
      "pred_error: 0.025515275076031685 \t range_loss: 0.7428628206253052\n",
      "pred_error: 0.025515224784612656 \t range_loss: 0.7428628206253052\n",
      "Step: 51 \t Loss: 0.9952799081802368\n",
      "pred_error: 0.02558865025639534 \t range_loss: 0.7393907308578491\n",
      "pred_error: 0.025588596239686012 \t range_loss: 0.7393907308578491\n",
      "Step: 52 \t Loss: 0.9902108311653137\n",
      "Step: 53 \t Loss: 0.9874080419540405\n",
      "Step: 54 \t Loss: 0.9867796897888184\n",
      "pred_error: 0.02537255920469761 \t range_loss: 0.7330541014671326\n",
      "pred_error: 0.025372762233018875 \t range_loss: 0.7330541014671326\n",
      "pred_error: 0.025866547599434853 \t range_loss: 0.7314027547836304\n",
      "pred_error: 0.025907576084136963 \t range_loss: 0.7306185960769653\n",
      "pred_error: 0.025907857343554497 \t range_loss: 0.7306185960769653\n",
      "pred_error: 0.025945456698536873 \t range_loss: 0.7292620539665222\n",
      "pred_error: 0.025946350768208504 \t range_loss: 0.7292620539665222\n",
      "pred_error: 0.025849947705864906 \t range_loss: 0.7287281155586243\n",
      "pred_error: 0.02585035376250744 \t range_loss: 0.7287281155586243\n",
      "pred_error: 0.02585013397037983 \t range_loss: 0.7287281155586243\n",
      "Step: 62 \t Loss: 0.9847629070281982\n",
      "Step: 63 \t Loss: 0.9834339618682861\n",
      "Step: 64 \t Loss: 0.9804955720901489\n",
      "pred_error: 0.02568543143570423 \t range_loss: 0.7236407995223999\n",
      "pred_error: 0.02608386054635048 \t range_loss: 0.7218902111053467\n",
      "pred_error: 0.02596423402428627 \t range_loss: 0.7212691903114319\n",
      "Step: 70 \t Loss: 0.9797138571739197\n",
      "Step: 72 \t Loss: 0.9786595106124878\n",
      "Step: 73 \t Loss: 0.9770330190658569\n",
      "Step: 74 \t Loss: 0.9767066836357117\n",
      "pred_error: 0.02628369815647602 \t range_loss: 0.7163842916488647\n",
      "pred_error: 0.026190897449851036 \t range_loss: 0.7156895995140076\n",
      "pred_error: 0.026274770498275757 \t range_loss: 0.7147911190986633\n",
      "Step: 79 \t Loss: 0.9764037132263184\n",
      "Step: 80 \t Loss: 0.9747283458709717\n",
      "pred_error: 0.026239966973662376 \t range_loss: 0.7126782536506653\n",
      "pred_error: 0.02631385065615177 \t range_loss: 0.7130212783813477\n",
      "pred_error: 0.026313873007893562 \t range_loss: 0.7130212783813477\n",
      "Step: 83 \t Loss: 0.9737710356712341\n",
      "pred_error: 0.026337508112192154 \t range_loss: 0.710397481918335\n",
      "Step: 84 \t Loss: 0.9726318717002869\n",
      "pred_error: 0.026298746466636658 \t range_loss: 0.7096443772315979\n",
      "pred_error: 0.026298994198441505 \t range_loss: 0.7096443772315979\n",
      "Step: 86 \t Loss: 0.9726012349128723\n",
      "Step: 87 \t Loss: 0.9714715480804443\n",
      "Step: 88 \t Loss: 0.9692729711532593\n",
      "pred_error: 0.02655169554054737 \t range_loss: 0.7041846513748169\n",
      "Step: 90 \t Loss: 0.9686295986175537\n",
      "pred_error: 0.026482757180929184 \t range_loss: 0.7038031816482544\n",
      "pred_error: 0.026482444256544113 \t range_loss: 0.7038031816482544\n",
      "Step: 94 \t Loss: 0.9681792855262756\n",
      "pred_error: 0.02670375443994999 \t range_loss: 0.7011422514915466\n",
      "Step: 95 \t Loss: 0.9675430059432983\n",
      "pred_error: 0.026570409536361694 \t range_loss: 0.7025840878486633\n",
      "pred_error: 0.026716435328125954 \t range_loss: 0.7016687989234924\n",
      "pred_error: 0.026832345873117447 \t range_loss: 0.7007009387016296\n",
      "Step: 102 \t Loss: 0.9664215445518494\n",
      "Step: 107 \t Loss: 0.9660871028900146\n",
      "pred_error: 0.02734461985528469 \t range_loss: 0.6926413178443909\n",
      "Step: 108 \t Loss: 0.9646373987197876\n",
      "pred_error: 0.02725289575755596 \t range_loss: 0.6921084523200989\n",
      "Step: 109 \t Loss: 0.9619364738464355\n",
      "pred_error: 0.027108293026685715 \t range_loss: 0.6908535361289978\n",
      "pred_error: 0.027108293026685715 \t range_loss: 0.6908535361289978\n",
      "pred_error: 0.027108322829008102 \t range_loss: 0.6908535361289978\n",
      "pred_error: 0.027108091861009598 \t range_loss: 0.6908535361289978\n",
      "Step: 113 \t Loss: 0.9618303179740906\n",
      "Step: 114 \t Loss: 0.9602168798446655\n",
      "pred_error: 0.02759494259953499 \t range_loss: 0.6901229619979858\n",
      "pred_error: 0.027461176738142967 \t range_loss: 0.6896442174911499\n",
      "pred_error: 0.027243979275226593 \t range_loss: 0.6906031370162964\n",
      "pred_error: 0.02742324396967888 \t range_loss: 0.6879246830940247\n",
      "Step: 125 \t Loss: 0.9599010348320007\n",
      "pred_error: 0.027519598603248596 \t range_loss: 0.6870667934417725\n",
      "pred_error: 0.02754438854753971 \t range_loss: 0.6861011385917664\n",
      "pred_error: 0.027544395998120308 \t range_loss: 0.6861011385917664\n",
      "pred_error: 0.027544306591153145 \t range_loss: 0.6861011385917664\n",
      "pred_error: 0.02751963771879673 \t range_loss: 0.6857134699821472\n",
      "Step: 132 \t Loss: 0.95912766456604\n",
      "pred_error: 0.027423160150647163 \t range_loss: 0.684898316860199\n",
      "Step: 133 \t Loss: 0.9586693048477173\n",
      "pred_error: 0.027492526918649673 \t range_loss: 0.683743417263031\n",
      "Step: 134 \t Loss: 0.9572163820266724\n",
      "pred_error: 0.027517352253198624 \t range_loss: 0.6841551065444946\n",
      "pred_error: 0.02773856930434704 \t range_loss: 0.6824960708618164\n",
      "pred_error: 0.027868537232279778 \t range_loss: 0.681531548500061\n",
      "pred_error: 0.027756011113524437 \t range_loss: 0.6826962232589722\n",
      "pred_error: 0.027893483638763428 \t range_loss: 0.6804438233375549\n",
      "pred_error: 0.027982836589217186 \t range_loss: 0.6788895726203918\n",
      "Step: 149 \t Loss: 0.9567958116531372\n",
      "Step: 150 \t Loss: 0.9561622142791748\n",
      "pred_error: 0.02789403684437275 \t range_loss: 0.6772192716598511\n",
      "pred_error: 0.027891838923096657 \t range_loss: 0.6772192716598511\n",
      "pred_error: 0.027894141152501106 \t range_loss: 0.6772192716598511\n",
      "Step: 151 \t Loss: 0.9550561904907227\n",
      "Step: 152 \t Loss: 0.95389723777771\n",
      "pred_error: 0.028163431212306023 \t range_loss: 0.6755667328834534\n",
      "pred_error: 0.0281729344278574 \t range_loss: 0.6749675273895264\n",
      "pred_error: 0.028174500912427902 \t range_loss: 0.6749675273895264\n",
      "pred_error: 0.02816382423043251 \t range_loss: 0.6738802790641785\n",
      "pred_error: 0.02816407009959221 \t range_loss: 0.6738802790641785\n",
      "pred_error: 0.0281248576939106 \t range_loss: 0.6733769178390503\n",
      "pred_error: 0.028377285227179527 \t range_loss: 0.6723542213439941\n",
      "pred_error: 0.028230195865035057 \t range_loss: 0.6719473004341125\n",
      "Step: 166 \t Loss: 0.9526910185813904\n",
      "pred_error: 0.02883945032954216 \t range_loss: 0.6687887907028198\n",
      "pred_error: 0.028517985716462135 \t range_loss: 0.6685794591903687\n",
      "pred_error: 0.028554903343319893 \t range_loss: 0.6684472560882568\n",
      "pred_error: 0.028557199984788895 \t range_loss: 0.6684472560882568\n",
      "pred_error: 0.028669126331806183 \t range_loss: 0.6672917604446411\n",
      "Step: 181 \t Loss: 0.9526278972625732\n",
      "pred_error: 0.028608303517103195 \t range_loss: 0.6671131253242493\n",
      "pred_error: 0.028608812019228935 \t range_loss: 0.6671131253242493\n",
      "Step: 183 \t Loss: 0.9517298936843872\n",
      "pred_error: 0.028997372835874557 \t range_loss: 0.6673029661178589\n",
      "pred_error: 0.028996523469686508 \t range_loss: 0.6673029661178589\n",
      "pred_error: 0.02896350622177124 \t range_loss: 0.6650918126106262\n",
      "pred_error: 0.028879165649414062 \t range_loss: 0.6632948517799377\n",
      "Step: 190 \t Loss: 0.9485098123550415\n",
      "pred_error: 0.028743313625454903 \t range_loss: 0.6610767245292664\n",
      "pred_error: 0.029015416279435158 \t range_loss: 0.6620402932167053\n",
      "pred_error: 0.02906240150332451 \t range_loss: 0.6607310175895691\n",
      "pred_error: 0.02905319817364216 \t range_loss: 0.6604834794998169\n",
      "pred_error: 0.02905319817364216 \t range_loss: 0.6604834794998169\n",
      "pred_error: 0.029052982106804848 \t range_loss: 0.6604834794998169\n",
      "pred_error: 0.029264306649565697 \t range_loss: 0.6597976088523865\n",
      "pred_error: 0.029182199388742447 \t range_loss: 0.6608885526657104\n",
      "pred_error: 0.029393155127763748 \t range_loss: 0.6597097516059875\n",
      "pred_error: 0.02944047935307026 \t range_loss: 0.6597452759742737\n",
      "pred_error: 0.029317159205675125 \t range_loss: 0.6588340997695923\n",
      "pred_error: 0.029270095750689507 \t range_loss: 0.6586989164352417\n",
      "pred_error: 0.029146267101168633 \t range_loss: 0.6578433513641357\n",
      "pred_error: 0.029146289452910423 \t range_loss: 0.6578433513641357\n",
      "pred_error: 0.029439769685268402 \t range_loss: 0.6581558585166931\n",
      "pred_error: 0.029437966644763947 \t range_loss: 0.65842604637146\n",
      "pred_error: 0.029388830065727234 \t range_loss: 0.656561017036438\n",
      "pred_error: 0.029388954862952232 \t range_loss: 0.656561017036438\n",
      "pred_error: 0.02943010814487934 \t range_loss: 0.6570130586624146\n",
      "pred_error: 0.02938522771000862 \t range_loss: 0.6549484729766846\n",
      "Step: 227 \t Loss: 0.9482158422470093\n",
      "pred_error: 0.02942931465804577 \t range_loss: 0.655025064945221\n",
      "pred_error: 0.029429294168949127 \t range_loss: 0.655025064945221\n",
      "Step: 230 \t Loss: 0.9480819702148438\n",
      "Step: 231 \t Loss: 0.9472758173942566\n",
      "pred_error: 0.029427330940961838 \t range_loss: 0.654341459274292\n",
      "pred_error: 0.029746761545538902 \t range_loss: 0.6560419797897339\n",
      "pred_error: 0.029749125242233276 \t range_loss: 0.6545190811157227\n",
      "pred_error: 0.02967737801373005 \t range_loss: 0.6550050377845764\n",
      "pred_error: 0.02987722121179104 \t range_loss: 0.6523715257644653\n",
      "pred_error: 0.02971329726278782 \t range_loss: 0.6519917845726013\n",
      "pred_error: 0.029761943966150284 \t range_loss: 0.6509376168251038\n",
      "pred_error: 0.02982061170041561 \t range_loss: 0.6503748297691345\n",
      "pred_error: 0.029677672311663628 \t range_loss: 0.6516428589820862\n",
      "pred_error: 0.02967781387269497 \t range_loss: 0.6516428589820862\n",
      "pred_error: 0.029677720740437508 \t range_loss: 0.6516428589820862\n",
      "pred_error: 0.029820585623383522 \t range_loss: 0.6520506739616394\n",
      "pred_error: 0.029813237488269806 \t range_loss: 0.6510475873947144\n",
      "pred_error: 0.029965847730636597 \t range_loss: 0.6513775587081909\n",
      "pred_error: 0.029962781816720963 \t range_loss: 0.6516302227973938\n",
      "pred_error: 0.03005416877567768 \t range_loss: 0.6500735282897949\n",
      "pred_error: 0.03007173165678978 \t range_loss: 0.6514338850975037\n",
      "pred_error: 0.030072176828980446 \t range_loss: 0.6514338850975037\n",
      "pred_error: 0.030154157429933548 \t range_loss: 0.6513519883155823\n",
      "pred_error: 0.03015405312180519 \t range_loss: 0.6513519883155823\n",
      "pred_error: 0.030068080872297287 \t range_loss: 0.6513457298278809\n",
      "pred_error: 0.030087867751717567 \t range_loss: 0.6494039297103882\n",
      "pred_error: 0.03014947846531868 \t range_loss: 0.6467766761779785\n",
      "Step: 282 \t Loss: 0.9467617869377136\n",
      "pred_error: 0.03037038818001747 \t range_loss: 0.64725261926651\n",
      "Step: 300 \t Loss: 0.9464260339736938\n",
      "Step: 301 \t Loss: 0.9447697997093201\n",
      "Step: 302 \t Loss: 0.9447673559188843\n",
      "pred_error: 0.030147351324558258 \t range_loss: 0.6432933211326599\n",
      "pred_error: 0.03046736866235733 \t range_loss: 0.6439518332481384\n",
      "pred_error: 0.03046741895377636 \t range_loss: 0.6439518332481384\n",
      "pred_error: 0.03046761453151703 \t range_loss: 0.6439518332481384\n",
      "pred_error: 0.030412103980779648 \t range_loss: 0.6444517374038696\n",
      "pred_error: 0.030452078208327293 \t range_loss: 0.643498957157135\n",
      "pred_error: 0.030637288466095924 \t range_loss: 0.6428370475769043\n",
      "pred_error: 0.030655715614557266 \t range_loss: 0.6438725590705872\n",
      "pred_error: 0.030655425041913986 \t range_loss: 0.6438725590705872\n",
      "pred_error: 0.030736183747649193 \t range_loss: 0.6430643796920776\n",
      "pred_error: 0.030851857736706734 \t range_loss: 0.6413781046867371\n",
      "pred_error: 0.03071860782802105 \t range_loss: 0.642185389995575\n",
      "pred_error: 0.030718503519892693 \t range_loss: 0.642185389995575\n",
      "pred_error: 0.030504988506436348 \t range_loss: 0.6408805847167969\n",
      "pred_error: 0.030506324023008347 \t range_loss: 0.6407889127731323\n",
      "pred_error: 0.030506189912557602 \t range_loss: 0.6407889127731323\n",
      "pred_error: 0.030506396666169167 \t range_loss: 0.6407889127731323\n",
      "pred_error: 0.030506229028105736 \t range_loss: 0.6407889127731323\n",
      "pred_error: 0.030753260478377342 \t range_loss: 0.6410127878189087\n",
      "pred_error: 0.030834605917334557 \t range_loss: 0.6404702663421631\n",
      "pred_error: 0.030727732926607132 \t range_loss: 0.6399545669555664\n",
      "pred_error: 0.030701665207743645 \t range_loss: 0.6387138962745667\n",
      "pred_error: 0.031240403652191162 \t range_loss: 0.6379319429397583\n",
      "pred_error: 0.03113964945077896 \t range_loss: 0.6370819807052612\n",
      "pred_error: 0.031139720231294632 \t range_loss: 0.6370819807052612\n",
      "pred_error: 0.031139735132455826 \t range_loss: 0.6370819807052612\n",
      "pred_error: 0.0308921430259943 \t range_loss: 0.6371535658836365\n",
      "pred_error: 0.03089272417128086 \t range_loss: 0.6371535658836365\n",
      "pred_error: 0.03127644583582878 \t range_loss: 0.6357452869415283\n",
      "pred_error: 0.03127628564834595 \t range_loss: 0.6357452869415283\n",
      "pred_error: 0.03127658739686012 \t range_loss: 0.6357452869415283\n",
      "pred_error: 0.03127649053931236 \t range_loss: 0.6357452869415283\n",
      "pred_error: 0.03107878752052784 \t range_loss: 0.6353446841239929\n",
      "pred_error: 0.03107878565788269 \t range_loss: 0.6353446841239929\n",
      "pred_error: 0.031001728028059006 \t range_loss: 0.636926531791687\n",
      "pred_error: 0.03138616308569908 \t range_loss: 0.6354078054428101\n",
      "pred_error: 0.031360164284706116 \t range_loss: 0.6361370086669922\n",
      "pred_error: 0.031316254287958145 \t range_loss: 0.6359286308288574\n",
      "pred_error: 0.03131621330976486 \t range_loss: 0.6359286308288574\n",
      "pred_error: 0.03158799931406975 \t range_loss: 0.6360737681388855\n",
      "pred_error: 0.03158605843782425 \t range_loss: 0.6360737681388855\n",
      "pred_error: 0.03146934136748314 \t range_loss: 0.6357786655426025\n",
      "pred_error: 0.031282104551792145 \t range_loss: 0.6369852423667908\n",
      "pred_error: 0.03141370415687561 \t range_loss: 0.6348055601119995\n",
      "pred_error: 0.031201081350445747 \t range_loss: 0.6334930062294006\n",
      "pred_error: 0.03120097704231739 \t range_loss: 0.6334930062294006\n",
      "pred_error: 0.031073668971657753 \t range_loss: 0.6346324682235718\n",
      "pred_error: 0.031099019572138786 \t range_loss: 0.6351650953292847\n",
      "pred_error: 0.03135506063699722 \t range_loss: 0.6356489062309265\n",
      "pred_error: 0.03137820586562157 \t range_loss: 0.6362655758857727\n",
      "pred_error: 0.03111127018928528 \t range_loss: 0.6353594660758972\n",
      "pred_error: 0.03113151714205742 \t range_loss: 0.6355035901069641\n",
      "pred_error: 0.03126072883605957 \t range_loss: 0.6347401142120361\n",
      "pred_error: 0.03126061335206032 \t range_loss: 0.6347401142120361\n",
      "pred_error: 0.031345516443252563 \t range_loss: 0.6344391107559204\n",
      "pred_error: 0.03121163323521614 \t range_loss: 0.6336955428123474\n",
      "pred_error: 0.03146737813949585 \t range_loss: 0.6339614987373352\n",
      "pred_error: 0.03146743401885033 \t range_loss: 0.6339614987373352\n",
      "pred_error: 0.03141716867685318 \t range_loss: 0.6354455351829529\n",
      "pred_error: 0.031340092420578 \t range_loss: 0.634444534778595\n",
      "pred_error: 0.031219417229294777 \t range_loss: 0.6342322826385498\n",
      "pred_error: 0.031181255355477333 \t range_loss: 0.6338486075401306\n",
      "pred_error: 0.0313047356903553 \t range_loss: 0.6335511207580566\n",
      "pred_error: 0.031304534524679184 \t range_loss: 0.6335511207580566\n",
      "pred_error: 0.031232492998242378 \t range_loss: 0.633765459060669\n",
      "pred_error: 0.031221888959407806 \t range_loss: 0.6343121528625488\n",
      "pred_error: 0.03122180886566639 \t range_loss: 0.6343121528625488\n",
      "pred_error: 0.031221799552440643 \t range_loss: 0.6343121528625488\n",
      "pred_error: 0.031278111040592194 \t range_loss: 0.6337611675262451\n",
      "pred_error: 0.03146740794181824 \t range_loss: 0.6343685984611511\n",
      "pred_error: 0.031618136912584305 \t range_loss: 0.633306086063385\n",
      "pred_error: 0.03180696442723274 \t range_loss: 0.6326867938041687\n",
      "pred_error: 0.03146817162632942 \t range_loss: 0.631691575050354\n",
      "pred_error: 0.031514085829257965 \t range_loss: 0.6314167380332947\n",
      "pred_error: 0.03155504912137985 \t range_loss: 0.631854236125946\n",
      "pred_error: 0.031555160880088806 \t range_loss: 0.631854236125946\n",
      "pred_error: 0.031554996967315674 \t range_loss: 0.631854236125946\n",
      "pred_error: 0.03167872130870819 \t range_loss: 0.631566047668457\n",
      "pred_error: 0.03141489252448082 \t range_loss: 0.6323584318161011\n",
      "pred_error: 0.03141501173377037 \t range_loss: 0.6323584318161011\n",
      "pred_error: 0.03155304491519928 \t range_loss: 0.6316714286804199\n",
      "pred_error: 0.03156011551618576 \t range_loss: 0.6320666074752808\n",
      "pred_error: 0.03156008943915367 \t range_loss: 0.6320666074752808\n",
      "pred_error: 0.03156793490052223 \t range_loss: 0.6308955550193787\n",
      "pred_error: 0.031595539301633835 \t range_loss: 0.6309801340103149\n",
      "pred_error: 0.03159556910395622 \t range_loss: 0.6309801340103149\n",
      "pred_error: 0.0315033383667469 \t range_loss: 0.6317277550697327\n",
      "pred_error: 0.031489115208387375 \t range_loss: 0.6326925754547119\n",
      "pred_error: 0.031626999378204346 \t range_loss: 0.6329277157783508\n",
      "pred_error: 0.03192433342337608 \t range_loss: 0.6324416995048523\n",
      "Step: 456 \t Loss: 0.9442025423049927\n",
      "Step: 458 \t Loss: 0.9438042640686035\n",
      "pred_error: 0.031761690974235535 \t range_loss: 0.6321514248847961\n",
      "pred_error: 0.031761765480041504 \t range_loss: 0.6321514248847961\n",
      "pred_error: 0.03202414885163307 \t range_loss: 0.6307588219642639\n",
      "pred_error: 0.0318404883146286 \t range_loss: 0.630402684211731\n",
      "pred_error: 0.031544607132673264 \t range_loss: 0.6308257579803467\n",
      "pred_error: 0.03158726170659065 \t range_loss: 0.6312711238861084\n",
      "pred_error: 0.03158726915717125 \t range_loss: 0.6312711238861084\n",
      "pred_error: 0.03169243410229683 \t range_loss: 0.6308495402336121\n",
      "pred_error: 0.031584661453962326 \t range_loss: 0.6318191885948181\n",
      "pred_error: 0.031674109399318695 \t range_loss: 0.630754292011261\n",
      "pred_error: 0.03179644048213959 \t range_loss: 0.6294569969177246\n",
      "pred_error: 0.03179032728075981 \t range_loss: 0.6294201016426086\n",
      "pred_error: 0.031745992600917816 \t range_loss: 0.6296224594116211\n",
      "pred_error: 0.031944241374731064 \t range_loss: 0.6294580101966858\n",
      "pred_error: 0.03194331005215645 \t range_loss: 0.6294580101966858\n",
      "pred_error: 0.03188807889819145 \t range_loss: 0.626878023147583\n",
      "pred_error: 0.031700655817985535 \t range_loss: 0.6278442144393921\n",
      "pred_error: 0.03170069679617882 \t range_loss: 0.6278442144393921\n",
      "pred_error: 0.03185254707932472 \t range_loss: 0.6283260583877563\n",
      "pred_error: 0.03208920359611511 \t range_loss: 0.6276321411132812\n",
      "pred_error: 0.031928129494190216 \t range_loss: 0.6289989352226257\n",
      "pred_error: 0.03191642835736275 \t range_loss: 0.6277628540992737\n",
      "pred_error: 0.03197737783193588 \t range_loss: 0.6263546943664551\n",
      "pred_error: 0.03205841779708862 \t range_loss: 0.6260517835617065\n",
      "pred_error: 0.0321023091673851 \t range_loss: 0.6251479983329773\n",
      "pred_error: 0.03226638585329056 \t range_loss: 0.6255989670753479\n",
      "pred_error: 0.032489437609910965 \t range_loss: 0.62543123960495\n",
      "pred_error: 0.03233277052640915 \t range_loss: 0.6265562772750854\n",
      "pred_error: 0.03215803951025009 \t range_loss: 0.6263787150382996\n",
      "pred_error: 0.03214690461754799 \t range_loss: 0.6262983083724976\n",
      "pred_error: 0.03188705071806908 \t range_loss: 0.62760990858078\n",
      "pred_error: 0.032043151557445526 \t range_loss: 0.6265037655830383\n",
      "pred_error: 0.03233543038368225 \t range_loss: 0.6255067586898804\n",
      "pred_error: 0.03233513981103897 \t range_loss: 0.6255067586898804\n",
      "pred_error: 0.0324566587805748 \t range_loss: 0.6247894167900085\n",
      "pred_error: 0.032084640115499496 \t range_loss: 0.6236438155174255\n",
      "pred_error: 0.03217380866408348 \t range_loss: 0.6235746145248413\n",
      "pred_error: 0.03222094476222992 \t range_loss: 0.6252086758613586\n",
      "pred_error: 0.03234918415546417 \t range_loss: 0.6256868839263916\n",
      "pred_error: 0.03234736993908882 \t range_loss: 0.6256868839263916\n",
      "pred_error: 0.032358817756175995 \t range_loss: 0.6228978633880615\n",
      "pred_error: 0.03243807330727577 \t range_loss: 0.6239882707595825\n",
      "pred_error: 0.03219642862677574 \t range_loss: 0.624189019203186\n",
      "pred_error: 0.032202497124671936 \t range_loss: 0.623348593711853\n",
      "pred_error: 0.03220261260867119 \t range_loss: 0.623348593711853\n",
      "pred_error: 0.03237638622522354 \t range_loss: 0.6260319352149963\n",
      "pred_error: 0.03249933570623398 \t range_loss: 0.6265714168548584\n",
      "pred_error: 0.032212357968091965 \t range_loss: 0.6238064765930176\n",
      "pred_error: 0.03248552232980728 \t range_loss: 0.6240406036376953\n",
      "pred_error: 0.03241348639130592 \t range_loss: 0.6233909130096436\n",
      "pred_error: 0.032348357141017914 \t range_loss: 0.6227092742919922\n",
      "pred_error: 0.03259483352303505 \t range_loss: 0.6243860125541687\n",
      "pred_error: 0.03252175822854042 \t range_loss: 0.6221433877944946\n",
      "pred_error: 0.03256231173872948 \t range_loss: 0.6221911311149597\n",
      "pred_error: 0.032562315464019775 \t range_loss: 0.6221911311149597\n",
      "pred_error: 0.03242289647459984 \t range_loss: 0.6234766840934753\n",
      "pred_error: 0.03245920315384865 \t range_loss: 0.6231049299240112\n",
      "pred_error: 0.03236735239624977 \t range_loss: 0.6222002506256104\n",
      "pred_error: 0.032657381147146225 \t range_loss: 0.6232489943504333\n",
      "pred_error: 0.032457444816827774 \t range_loss: 0.6223765015602112\n",
      "pred_error: 0.03252355754375458 \t range_loss: 0.6223307251930237\n",
      "pred_error: 0.03261405974626541 \t range_loss: 0.6218216419219971\n",
      "pred_error: 0.0324568971991539 \t range_loss: 0.6213116645812988\n",
      "pred_error: 0.03240250051021576 \t range_loss: 0.6214892268180847\n",
      "pred_error: 0.032402586191892624 \t range_loss: 0.6214892268180847\n",
      "pred_error: 0.03240243345499039 \t range_loss: 0.6214892268180847\n",
      "pred_error: 0.032321155071258545 \t range_loss: 0.6232528686523438\n",
      "pred_error: 0.03260131925344467 \t range_loss: 0.6224862337112427\n",
      "pred_error: 0.03280598297715187 \t range_loss: 0.6237744688987732\n",
      "pred_error: 0.032903727144002914 \t range_loss: 0.6224706768989563\n",
      "pred_error: 0.0325414314866066 \t range_loss: 0.6219607591629028\n",
      "pred_error: 0.03261128440499306 \t range_loss: 0.6220871806144714\n",
      "pred_error: 0.032560382038354874 \t range_loss: 0.6228402853012085\n",
      "pred_error: 0.03274346888065338 \t range_loss: 0.6218077540397644\n",
      "pred_error: 0.032651547342538834 \t range_loss: 0.6206526160240173\n",
      "pred_error: 0.032597873359918594 \t range_loss: 0.6210968494415283\n",
      "pred_error: 0.03259782865643501 \t range_loss: 0.6210968494415283\n",
      "pred_error: 0.03276636078953743 \t range_loss: 0.6206064820289612\n",
      "pred_error: 0.03269753232598305 \t range_loss: 0.622552752494812\n",
      "pred_error: 0.03269854187965393 \t range_loss: 0.622552752494812\n",
      "pred_error: 0.032597173005342484 \t range_loss: 0.6216153502464294\n",
      "pred_error: 0.03244283050298691 \t range_loss: 0.6219590306282043\n",
      "pred_error: 0.03250838816165924 \t range_loss: 0.6217684745788574\n",
      "pred_error: 0.03245643526315689 \t range_loss: 0.621273934841156\n",
      "pred_error: 0.03262347728013992 \t range_loss: 0.6216604709625244\n",
      "pred_error: 0.03267909213900566 \t range_loss: 0.6212193965911865\n",
      "pred_error: 0.03256320580840111 \t range_loss: 0.6202951073646545\n",
      "pred_error: 0.032563213258981705 \t range_loss: 0.6202951073646545\n",
      "pred_error: 0.03241482004523277 \t range_loss: 0.6210862398147583\n",
      "pred_error: 0.032464176416397095 \t range_loss: 0.6220961213111877\n",
      "pred_error: 0.0327623188495636 \t range_loss: 0.6219457983970642\n",
      "pred_error: 0.03272778168320656 \t range_loss: 0.6232993602752686\n",
      "pred_error: 0.03269834443926811 \t range_loss: 0.621269702911377\n",
      "pred_error: 0.032698363065719604 \t range_loss: 0.621269702911377\n",
      "pred_error: 0.03269487991929054 \t range_loss: 0.621451199054718\n",
      "pred_error: 0.03269487991929054 \t range_loss: 0.621451199054718\n",
      "pred_error: 0.03269932419061661 \t range_loss: 0.6234455108642578\n",
      "pred_error: 0.032673828303813934 \t range_loss: 0.6222906112670898\n",
      "pred_error: 0.03271783888339996 \t range_loss: 0.6203555464744568\n",
      "pred_error: 0.032790157943964005 \t range_loss: 0.6220947504043579\n",
      "pred_error: 0.032871391624212265 \t range_loss: 0.6222980618476868\n",
      "pred_error: 0.03287143632769585 \t range_loss: 0.6222980618476868\n",
      "pred_error: 0.03256866708397865 \t range_loss: 0.6195860505104065\n",
      "pred_error: 0.0323580764234066 \t range_loss: 0.6205697655677795\n",
      "pred_error: 0.03289584442973137 \t range_loss: 0.6203101277351379\n",
      "pred_error: 0.032819412648677826 \t range_loss: 0.6199237704277039\n",
      "pred_error: 0.03279625624418259 \t range_loss: 0.620315432548523\n",
      "pred_error: 0.032667502760887146 \t range_loss: 0.6213260293006897\n",
      "pred_error: 0.03275004029273987 \t range_loss: 0.6213650703430176\n",
      "pred_error: 0.0327085517346859 \t range_loss: 0.6213163733482361\n",
      "pred_error: 0.03259497135877609 \t range_loss: 0.6188379526138306\n",
      "pred_error: 0.032594870775938034 \t range_loss: 0.6188379526138306\n",
      "pred_error: 0.03282434120774269 \t range_loss: 0.6191945672035217\n",
      "pred_error: 0.03289618715643883 \t range_loss: 0.6205261945724487\n",
      "pred_error: 0.033055152744054794 \t range_loss: 0.6186738014221191\n",
      "pred_error: 0.03281873092055321 \t range_loss: 0.6182655692100525\n",
      "pred_error: 0.03273576870560646 \t range_loss: 0.6196627616882324\n",
      "pred_error: 0.032735757529735565 \t range_loss: 0.6196627616882324\n",
      "pred_error: 0.03288077563047409 \t range_loss: 0.6193850636482239\n",
      "pred_error: 0.03276846557855606 \t range_loss: 0.6205963492393494\n",
      "pred_error: 0.03297119960188866 \t range_loss: 0.6186705827713013\n",
      "pred_error: 0.033157385885715485 \t range_loss: 0.6191975474357605\n",
      "pred_error: 0.033140864223241806 \t range_loss: 0.6186308860778809\n",
      "pred_error: 0.032963257282972336 \t range_loss: 0.6179478168487549\n",
      "pred_error: 0.032963648438453674 \t range_loss: 0.6179478168487549\n",
      "pred_error: 0.03267437964677811 \t range_loss: 0.6171438097953796\n",
      "Step: 749 \t Loss: 0.9436295032501221\n",
      "pred_error: 0.032640241086483 \t range_loss: 0.6172269582748413\n",
      "pred_error: 0.03284965828061104 \t range_loss: 0.6206187009811401\n",
      "pred_error: 0.033029574900865555 \t range_loss: 0.6201265454292297\n",
      "pred_error: 0.03290042281150818 \t range_loss: 0.6193658113479614\n",
      "pred_error: 0.032948676496744156 \t range_loss: 0.6193373799324036\n",
      "pred_error: 0.032947439700365067 \t range_loss: 0.6193373799324036\n",
      "pred_error: 0.032981809228658676 \t range_loss: 0.6184458136558533\n",
      "pred_error: 0.03298176825046539 \t range_loss: 0.6184458136558533\n",
      "pred_error: 0.03275397792458534 \t range_loss: 0.6201919913291931\n",
      "pred_error: 0.03275388106703758 \t range_loss: 0.6201919913291931\n",
      "pred_error: 0.03299599885940552 \t range_loss: 0.6186833381652832\n",
      "pred_error: 0.033003512769937515 \t range_loss: 0.6189423203468323\n",
      "pred_error: 0.032793283462524414 \t range_loss: 0.618884265422821\n",
      "pred_error: 0.03310597687959671 \t range_loss: 0.6170724034309387\n",
      "pred_error: 0.03333161026239395 \t range_loss: 0.6191186308860779\n",
      "pred_error: 0.033314984291791916 \t range_loss: 0.6185211539268494\n",
      "Step: 779 \t Loss: 0.942825198173523\n",
      "pred_error: 0.032643284648656845 \t range_loss: 0.6163923740386963\n",
      "Step: 780 \t Loss: 0.941809356212616\n",
      "pred_error: 0.03271114453673363 \t range_loss: 0.6166138052940369\n",
      "pred_error: 0.03282098472118378 \t range_loss: 0.6176039576530457\n",
      "pred_error: 0.03286391869187355 \t range_loss: 0.6197213530540466\n",
      "pred_error: 0.03318062424659729 \t range_loss: 0.618272602558136\n",
      "pred_error: 0.03281744569540024 \t range_loss: 0.6186555027961731\n",
      "pred_error: 0.03313969448208809 \t range_loss: 0.6185938119888306\n",
      "pred_error: 0.03311300650238991 \t range_loss: 0.6179695725440979\n",
      "pred_error: 0.03309793025255203 \t range_loss: 0.6180994510650635\n",
      "BEST LOSS: 0.94180936\n",
      "==== Model: block2_cob_activation_norm  in Layer: 2 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:12:54,449 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:15:36,213 model.rs:1246 value (-617472) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:15:36,225 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:16:01,865 model.rs:1246 value (-617472) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:16:01,871 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:16:01,899 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:16:01,929 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:16:01,947 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error   | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000008162118 | 0.000015750527 | 0.00027549267 | -0.00029866397 | 0.00004002754  | 0.000015750527   | 0.00029866397 | 0             | 0.000000002682428  | 0.00010109771      | 0.00079670444          |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 671208 64 [-636614, 432394] 1 [16]\n",
      "===============================\n",
      "==== Model: block2_cob_activation_norm_teleported  in Layer: 2 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:16:26,958 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:19:04,793 model.rs:1246 value (-35584) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:19:04,801 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:19:34,942 model.rs:1246 value (-35584) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:19:34,952 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:19:35,045 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:19:35,075 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:19:35,104 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error  | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.000000007360856 | 0.00002554059 | 0.0002527237 | -0.00028014183 | 0.000040324416 | 0.00002554059    | 0.00028014183 | 0             | 0.0000000027162694 | -0.00016391218     | 0.00077374355          |\n",
      "+-------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 671203 64 [-409346, 251202] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 3 , \t  activation_stats: {'relu_1': {'norm': tensor(501.2616), 'max': tensor(3.6280), 'min': tensor(-5.4008), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(9.0288)\n",
      "====== NO OPTIMIZATION SINCE NO TELEPORTATION =====\n",
      "==== Model: block3_cob_activation_norm  in Layer: 3 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:20:04,657 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:22:16,585 model.rs:1246 value (-590528) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:22:16,610 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:22:36,061 model.rs:1246 value (-590528) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:22:36,067 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:22:36,096 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:22:36,125 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:22:36,138 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000022542627 | -0.000016748905 | 0.00029869378 | -0.00035107136 | 0.00004945049  | 0.000016748905   | 0.00035107136 | 0             | 0.0000000041202632 | 0.0001190524       | 0.0013027088           |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 645016 64 [-500564, 336270] 1 [16]\n",
      "===============================\n",
      "==== Model: block3_cob_activation_norm_teleported  in Layer: 3 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:22:55,418 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:24:58,501 model.rs:1246 value (216320) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:24:58,516 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:25:17,398 model.rs:1246 value (216320) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:25:17,402 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:25:17,419 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:25:17,435 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:25:17,448 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000021946266 | -0.000021874905 | 0.00035524368 | -0.00036537647 | 0.000049379865 | 0.000021874905   | 0.00036537647 | 0             | 0.0000000041189345 | 0.00026250735      | 0.0010626403           |\n",
      "+------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 644829 64 [-499596, 336100] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 4 , \t  activation_stats: {'relu_1': {'norm': tensor(569.6558), 'max': tensor(2.8129), 'min': tensor(-5.8987), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(8.7116)\n",
      "====== NO OPTIMIZATION SINCE NO TELEPORTATION =====\n",
      "==== Model: block4_cob_activation_norm  in Layer: 4 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:25:37,227 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:27:37,815 model.rs:1246 value (354112) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:27:37,822 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:27:56,878 model.rs:1246 value (354112) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:27:56,891 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:27:56,928 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:27:56,969 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:27:56,984 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.00000028271268 | -0.000027433038 | 0.00030887127 | -0.00034918636 | 0.00003924855  | 0.000027433038   | 0.00034918636 | 0             | 0.0000000026895994 | -0.0000058611404   | 0.001038025            |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 623541 64 [-546704, 260716] 1 [16]\n",
      "===============================\n",
      "==== Model: block4_cob_activation_norm_teleported  in Layer: 4 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:28:16,514 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:30:17,684 model.rs:1246 value (380160) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:30:17,692 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:30:37,430 model.rs:1246 value (380160) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:30:37,441 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:30:37,463 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:30:37,480 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:30:37,494 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error    | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.00000028853867 | -0.000022560358 | 0.00030338764 | -0.00033723563 | 0.000038967733 | 0.000022560358   | 0.00033723563 | 0             | 0.0000000026536717 | -0.000060672795    | 0.0012155896           |\n",
      "+-------------------+-----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 623545 64 [-546468, 262710] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 5 , \t  activation_stats: {'relu_1': {'norm': tensor(619.9756), 'max': tensor(2.9422), 'min': tensor(-6.9142), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(9.8564)\n",
      "====== NO OPTIMIZATION SINCE NO TELEPORTATION =====\n",
      "==== Model: block5_cob_activation_norm  in Layer: 5 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:31:02,093 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:33:20,848 model.rs:1246 value (315392) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:33:20,880 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:33:42,364 model.rs:1246 value (315392) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:33:42,379 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:33:42,421 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:33:42,466 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:33:42,481 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000002367632 | 0.00008819997 | 0.0003267564 | -0.00036628544 | 0.000045873196 | 0.00008819997    | 0.00036628544 | 0             | 0.0000000036281333 | 0.00029594338      | 0.0011922804           |\n",
      "+-----------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 634374 64 [-640796, 272718] 1 [16]\n",
      "===============================\n",
      "==== Model: block5_cob_activation_norm_teleported  in Layer: 5 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 14:34:05,512 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 14:36:24,905 model.rs:1246 value (-228096) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:36:24,960 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 14:36:47,630 model.rs:1246 value (-228096) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 14:36:47,642 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 14:36:47,677 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 14:36:47,720 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 14:36:47,745 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error   | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000023498467 | 0.000115558505 | 0.00032514334 | -0.00035518408 | 0.000045444416 | 0.000115558505   | 0.00035518408 | 0             | 0.0000000035577317 | 0.00014846992      | 0.0012427195           |\n",
      "+------------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 634382 64 [-646462, 270956] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 6 , \t  activation_stats: {'relu_1': {'norm': tensor(712.5450), 'max': tensor(4.4912), 'min': tensor(-7.4060), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(11.8972)\n",
      "pred_error: 0.04675891995429993 \t range_loss: 0.996269702911377\n",
      "Step: 0 \t Loss: 1.411845088005066\n",
      "pred_error: 0.04649289324879646 \t range_loss: 0.9469148516654968\n",
      "pred_error: 0.04649278149008751 \t range_loss: 0.9469148516654968\n",
      "Step: 1 \t Loss: 1.3953335285186768\n",
      "Step: 2 \t Loss: 1.365050196647644\n",
      "pred_error: 0.04636736586689949 \t range_loss: 0.9013765454292297\n",
      "pred_error: 0.04636698216199875 \t range_loss: 0.9013765454292297\n",
      "Step: 3 \t Loss: 1.3410022258758545\n",
      "pred_error: 0.04632550850510597 \t range_loss: 0.8777517676353455\n",
      "Step: 4 \t Loss: 1.3282387256622314\n",
      "Step: 5 \t Loss: 1.3199533224105835\n",
      "pred_error: 0.0464705154299736 \t range_loss: 0.8552446961402893\n",
      "Step: 6 \t Loss: 1.3028762340545654\n",
      "Step: 7 \t Loss: 1.2900152206420898\n",
      "Step: 8 \t Loss: 1.2851481437683105\n",
      "Step: 9 \t Loss: 1.2784721851348877\n",
      "pred_error: 0.04681074246764183 \t range_loss: 0.8103704452514648\n",
      "Step: 10 \t Loss: 1.2740135192871094\n",
      "Step: 11 \t Loss: 1.2721772193908691\n",
      "Step: 12 \t Loss: 1.2632687091827393\n",
      "Step: 13 \t Loss: 1.2631890773773193\n",
      "Step: 14 \t Loss: 1.2568988800048828\n",
      "pred_error: 0.04704726114869118 \t range_loss: 0.7864276766777039\n",
      "Step: 15 \t Loss: 1.2479041814804077\n",
      "pred_error: 0.046974651515483856 \t range_loss: 0.7836464047431946\n",
      "pred_error: 0.04697462171316147 \t range_loss: 0.7836464047431946\n",
      "pred_error: 0.046974170953035355 \t range_loss: 0.7836464047431946\n",
      "Step: 17 \t Loss: 1.2451395988464355\n",
      "pred_error: 0.04765388369560242 \t range_loss: 0.7686001062393188\n",
      "Step: 18 \t Loss: 1.2419021129608154\n",
      "Step: 19 \t Loss: 1.2387397289276123\n",
      "Step: 20 \t Loss: 1.2330354452133179\n",
      "pred_error: 0.04730735719203949 \t range_loss: 0.7603802680969238\n",
      "pred_error: 0.04730716347694397 \t range_loss: 0.7603802680969238\n",
      "Step: 22 \t Loss: 1.2317352294921875\n",
      "Step: 24 \t Loss: 1.2252089977264404\n",
      "pred_error: 0.047867633402347565 \t range_loss: 0.7465370893478394\n",
      "pred_error: 0.04749153554439545 \t range_loss: 0.7514124512672424\n",
      "Step: 26 \t Loss: 1.2202996015548706\n",
      "pred_error: 0.04814733564853668 \t range_loss: 0.7442095279693604\n",
      "pred_error: 0.04832892864942551 \t range_loss: 0.7394408583641052\n",
      "Step: 30 \t Loss: 1.2187092304229736\n",
      "Step: 31 \t Loss: 1.2168607711791992\n",
      "pred_error: 0.048224516212940216 \t range_loss: 0.7346116900444031\n",
      "Step: 32 \t Loss: 1.2139531373977661\n",
      "pred_error: 0.04777763411402702 \t range_loss: 0.7361734509468079\n",
      "pred_error: 0.048056378960609436 \t range_loss: 0.7365793585777283\n",
      "pred_error: 0.04805651679635048 \t range_loss: 0.7365793585777283\n",
      "Step: 35 \t Loss: 1.2117886543273926\n",
      "Step: 37 \t Loss: 1.2086124420166016\n",
      "Step: 39 \t Loss: 1.2068034410476685\n",
      "pred_error: 0.04805469140410423 \t range_loss: 0.7262557744979858\n",
      "Step: 41 \t Loss: 1.2060950994491577\n",
      "pred_error: 0.04840927571058273 \t range_loss: 0.7220023274421692\n",
      "pred_error: 0.048408955335617065 \t range_loss: 0.7220023274421692\n",
      "pred_error: 0.048409316688776016 \t range_loss: 0.7220023274421692\n",
      "pred_error: 0.048409152776002884 \t range_loss: 0.7220023274421692\n",
      "pred_error: 0.04840948060154915 \t range_loss: 0.7220023274421692\n",
      "Step: 42 \t Loss: 1.2043629884719849\n",
      "pred_error: 0.048192307353019714 \t range_loss: 0.7224398851394653\n",
      "Step: 43 \t Loss: 1.203065037727356\n",
      "pred_error: 0.048204898834228516 \t range_loss: 0.7220447063446045\n",
      "Step: 45 \t Loss: 1.2025461196899414\n",
      "pred_error: 0.04864757880568504 \t range_loss: 0.7215610146522522\n",
      "pred_error: 0.04876706376671791 \t range_loss: 0.715353786945343\n",
      "Step: 48 \t Loss: 1.2024527788162231\n",
      "Step: 49 \t Loss: 1.198431372642517\n",
      "Step: 50 \t Loss: 1.1978379487991333\n",
      "pred_error: 0.04826781153678894 \t range_loss: 0.7151585817337036\n",
      "pred_error: 0.048267912119627 \t range_loss: 0.7151585817337036\n",
      "Step: 53 \t Loss: 1.1948583126068115\n",
      "pred_error: 0.04882952943444252 \t range_loss: 0.706561267375946\n",
      "Step: 54 \t Loss: 1.1944422721862793\n",
      "Step: 55 \t Loss: 1.1895825862884521\n",
      "pred_error: 0.048755791038274765 \t range_loss: 0.7069777846336365\n",
      "pred_error: 0.04889152571558952 \t range_loss: 0.7036712765693665\n",
      "pred_error: 0.04889187589287758 \t range_loss: 0.7036712765693665\n",
      "pred_error: 0.04889131337404251 \t range_loss: 0.7036712765693665\n",
      "pred_error: 0.04923630878329277 \t range_loss: 0.7001923322677612\n",
      "pred_error: 0.04881687089800835 \t range_loss: 0.7028151750564575\n",
      "Step: 66 \t Loss: 1.1852110624313354\n",
      "pred_error: 0.04870070144534111 \t range_loss: 0.7012971043586731\n",
      "pred_error: 0.049121592193841934 \t range_loss: 0.7015275955200195\n",
      "pred_error: 0.04938125982880592 \t range_loss: 0.6956146359443665\n",
      "pred_error: 0.04938102886080742 \t range_loss: 0.6956146359443665\n",
      "pred_error: 0.04942208528518677 \t range_loss: 0.6972419619560242\n",
      "pred_error: 0.049422211945056915 \t range_loss: 0.6972419619560242\n",
      "pred_error: 0.04952146112918854 \t range_loss: 0.6911154985427856\n",
      "Step: 78 \t Loss: 1.1836655139923096\n",
      "Step: 84 \t Loss: 1.1825873851776123\n",
      "pred_error: 0.049768950790166855 \t range_loss: 0.6848994493484497\n",
      "pred_error: 0.049506399780511856 \t range_loss: 0.6876596212387085\n",
      "Step: 88 \t Loss: 1.1818050146102905\n",
      "pred_error: 0.04971231892704964 \t range_loss: 0.6846818923950195\n",
      "Step: 89 \t Loss: 1.1802250146865845\n",
      "pred_error: 0.04938207566738129 \t range_loss: 0.6864042282104492\n",
      "pred_error: 0.049381908029317856 \t range_loss: 0.6864042282104492\n",
      "pred_error: 0.04958640784025192 \t range_loss: 0.6858162879943848\n",
      "pred_error: 0.04958640784025192 \t range_loss: 0.6858162879943848\n",
      "pred_error: 0.04958658292889595 \t range_loss: 0.6858162879943848\n",
      "pred_error: 0.04958647862076759 \t range_loss: 0.6858162879943848\n",
      "pred_error: 0.049553073942661285 \t range_loss: 0.6855711340904236\n",
      "pred_error: 0.04988282918930054 \t range_loss: 0.6848416328430176\n",
      "Step: 99 \t Loss: 1.179097056388855\n",
      "Step: 101 \t Loss: 1.178114414215088\n",
      "Step: 103 \t Loss: 1.1759921312332153\n",
      "pred_error: 0.04970012605190277 \t range_loss: 0.6802412271499634\n",
      "pred_error: 0.05010666325688362 \t range_loss: 0.6793292760848999\n",
      "pred_error: 0.050395749509334564 \t range_loss: 0.6777601838111877\n",
      "pred_error: 0.05027422681450844 \t range_loss: 0.6776149272918701\n",
      "pred_error: 0.0499512255191803 \t range_loss: 0.6775925755500793\n",
      "pred_error: 0.050794318318367004 \t range_loss: 0.6757481098175049\n",
      "pred_error: 0.050356823951005936 \t range_loss: 0.674347996711731\n",
      "Step: 117 \t Loss: 1.1751644611358643\n",
      "pred_error: 0.05029560998082161 \t range_loss: 0.6737753748893738\n",
      "Step: 120 \t Loss: 1.1751267910003662\n",
      "Step: 121 \t Loss: 1.1722664833068848\n",
      "pred_error: 0.05061463266611099 \t range_loss: 0.6727150082588196\n",
      "pred_error: 0.05063509941101074 \t range_loss: 0.6699584722518921\n",
      "pred_error: 0.05005025118589401 \t range_loss: 0.6721597909927368\n",
      "pred_error: 0.050050314515829086 \t range_loss: 0.6721597909927368\n",
      "Step: 132 \t Loss: 1.1721526384353638\n",
      "pred_error: 0.050157949328422546 \t range_loss: 0.6713455319404602\n",
      "pred_error: 0.05094107612967491 \t range_loss: 0.6693277359008789\n",
      "pred_error: 0.05071798339486122 \t range_loss: 0.6683875322341919\n",
      "Step: 143 \t Loss: 1.1712825298309326\n",
      "pred_error: 0.05044146999716759 \t range_loss: 0.6683325171470642\n",
      "pred_error: 0.0506843663752079 \t range_loss: 0.6677562594413757\n",
      "pred_error: 0.050684455782175064 \t range_loss: 0.6677562594413757\n",
      "pred_error: 0.05068433657288551 \t range_loss: 0.6677562594413757\n",
      "pred_error: 0.05084418132901192 \t range_loss: 0.6661887764930725\n",
      "pred_error: 0.05084413290023804 \t range_loss: 0.6661887764930725\n",
      "pred_error: 0.05084429308772087 \t range_loss: 0.6661887764930725\n",
      "Step: 153 \t Loss: 1.170050859451294\n",
      "Step: 154 \t Loss: 1.1690559387207031\n",
      "pred_error: 0.05008212849497795 \t range_loss: 0.6682345271110535\n",
      "pred_error: 0.05008213594555855 \t range_loss: 0.6682345271110535\n",
      "pred_error: 0.050667017698287964 \t range_loss: 0.6678206324577332\n",
      "pred_error: 0.05069763585925102 \t range_loss: 0.6658423542976379\n",
      "pred_error: 0.05069739744067192 \t range_loss: 0.6658423542976379\n",
      "pred_error: 0.05093778297305107 \t range_loss: 0.6681210398674011\n",
      "pred_error: 0.0505230575799942 \t range_loss: 0.6679155230522156\n",
      "pred_error: 0.05052293464541435 \t range_loss: 0.6679155230522156\n",
      "pred_error: 0.05105060338973999 \t range_loss: 0.6627107858657837\n",
      "pred_error: 0.051401834934949875 \t range_loss: 0.6615824699401855\n",
      "pred_error: 0.0511811301112175 \t range_loss: 0.6608837246894836\n",
      "pred_error: 0.051180873066186905 \t range_loss: 0.6608837246894836\n",
      "pred_error: 0.0511811189353466 \t range_loss: 0.6608837246894836\n",
      "Step: 177 \t Loss: 1.1668293476104736\n",
      "pred_error: 0.05062704533338547 \t range_loss: 0.6612496376037598\n",
      "pred_error: 0.05142471194267273 \t range_loss: 0.6595702767372131\n",
      "pred_error: 0.05108480527997017 \t range_loss: 0.6589224338531494\n",
      "pred_error: 0.05076893791556358 \t range_loss: 0.6601362824440002\n",
      "pred_error: 0.051651351153850555 \t range_loss: 0.6591371297836304\n",
      "pred_error: 0.05165111646056175 \t range_loss: 0.6591371297836304\n",
      "pred_error: 0.05165095999836922 \t range_loss: 0.6591371297836304\n",
      "pred_error: 0.05165107548236847 \t range_loss: 0.6591371297836304\n",
      "pred_error: 0.05101071670651436 \t range_loss: 0.6607130169868469\n",
      "pred_error: 0.051303181797266006 \t range_loss: 0.6626942157745361\n",
      "pred_error: 0.05134361982345581 \t range_loss: 0.6605994701385498\n",
      "pred_error: 0.050939369946718216 \t range_loss: 0.6613816618919373\n",
      "pred_error: 0.05113547295331955 \t range_loss: 0.6612444519996643\n",
      "pred_error: 0.05138956755399704 \t range_loss: 0.6600120067596436\n",
      "pred_error: 0.05140211060643196 \t range_loss: 0.6597049236297607\n",
      "pred_error: 0.05146899074316025 \t range_loss: 0.6568734049797058\n",
      "pred_error: 0.05146908760070801 \t range_loss: 0.6568734049797058\n",
      "Step: 205 \t Loss: 1.1663910150527954\n",
      "pred_error: 0.050822556018829346 \t range_loss: 0.6581676006317139\n",
      "pred_error: 0.05082232505083084 \t range_loss: 0.6581676006317139\n",
      "pred_error: 0.05074414238333702 \t range_loss: 0.6594833135604858\n",
      "pred_error: 0.05119524896144867 \t range_loss: 0.6623097062110901\n",
      "pred_error: 0.051350414752960205 \t range_loss: 0.6617379784584045\n",
      "pred_error: 0.0510646291077137 \t range_loss: 0.6596851944923401\n",
      "pred_error: 0.05137800797820091 \t range_loss: 0.6588253974914551\n",
      "pred_error: 0.05138865485787392 \t range_loss: 0.6561760902404785\n",
      "pred_error: 0.05132737010717392 \t range_loss: 0.6566816568374634\n",
      "pred_error: 0.0513271726667881 \t range_loss: 0.6566816568374634\n",
      "pred_error: 0.05161357671022415 \t range_loss: 0.6556892991065979\n",
      "Step: 234 \t Loss: 1.1662888526916504\n",
      "pred_error: 0.05132167041301727 \t range_loss: 0.6591283082962036\n",
      "pred_error: 0.05141523852944374 \t range_loss: 0.6603063344955444\n",
      "pred_error: 0.0514153391122818 \t range_loss: 0.6603063344955444\n",
      "pred_error: 0.05113817751407623 \t range_loss: 0.6564178466796875\n",
      "pred_error: 0.051182057708501816 \t range_loss: 0.6557784080505371\n",
      "pred_error: 0.05169220268726349 \t range_loss: 0.6548606157302856\n",
      "pred_error: 0.0512324757874012 \t range_loss: 0.6545814871788025\n",
      "pred_error: 0.051494862884283066 \t range_loss: 0.6543352007865906\n",
      "pred_error: 0.05151183158159256 \t range_loss: 0.6590690612792969\n",
      "pred_error: 0.05172566697001457 \t range_loss: 0.6510723233222961\n",
      "pred_error: 0.0516427680850029 \t range_loss: 0.6533678770065308\n",
      "pred_error: 0.05171172693371773 \t range_loss: 0.6593225002288818\n",
      "pred_error: 0.0518195778131485 \t range_loss: 0.6549067497253418\n",
      "pred_error: 0.051854949444532394 \t range_loss: 0.653360903263092\n",
      "pred_error: 0.05095987394452095 \t range_loss: 0.6638210415840149\n",
      "pred_error: 0.05133488401770592 \t range_loss: 0.6590474843978882\n",
      "pred_error: 0.05133511498570442 \t range_loss: 0.6590474843978882\n",
      "pred_error: 0.05208746716380119 \t range_loss: 0.6544308066368103\n",
      "pred_error: 0.052087489515542984 \t range_loss: 0.6544308066368103\n",
      "pred_error: 0.0512983612716198 \t range_loss: 0.6543881297111511\n",
      "Step: 274 \t Loss: 1.1643424034118652\n",
      "pred_error: 0.05201243609189987 \t range_loss: 0.6579412817955017\n",
      "pred_error: 0.051265086978673935 \t range_loss: 0.6589040160179138\n",
      "pred_error: 0.05172385275363922 \t range_loss: 0.6547988653182983\n",
      "pred_error: 0.051286257803440094 \t range_loss: 0.6548397541046143\n",
      "pred_error: 0.05128617212176323 \t range_loss: 0.6548397541046143\n",
      "pred_error: 0.05116748437285423 \t range_loss: 0.6545692682266235\n",
      "pred_error: 0.05111560598015785 \t range_loss: 0.6558365821838379\n",
      "pred_error: 0.05145430564880371 \t range_loss: 0.6553837656974792\n",
      "pred_error: 0.05130116268992424 \t range_loss: 0.654036283493042\n",
      "pred_error: 0.051612354815006256 \t range_loss: 0.6571635603904724\n",
      "pred_error: 0.05161239206790924 \t range_loss: 0.6571635603904724\n",
      "pred_error: 0.05161232873797417 \t range_loss: 0.6571635603904724\n",
      "pred_error: 0.05126286298036575 \t range_loss: 0.6544102430343628\n",
      "pred_error: 0.051758088171482086 \t range_loss: 0.6535059809684753\n",
      "pred_error: 0.05146963149309158 \t range_loss: 0.6529648303985596\n",
      "pred_error: 0.05168205872178078 \t range_loss: 0.6504811644554138\n",
      "pred_error: 0.051387719810009 \t range_loss: 0.6511590480804443\n",
      "pred_error: 0.05232814699411392 \t range_loss: 0.6518011093139648\n",
      "pred_error: 0.051677193492650986 \t range_loss: 0.6512823104858398\n",
      "pred_error: 0.051970455795526505 \t range_loss: 0.6527008414268494\n",
      "pred_error: 0.05160608887672424 \t range_loss: 0.6523245573043823\n",
      "pred_error: 0.05217013508081436 \t range_loss: 0.6533495187759399\n",
      "pred_error: 0.052169956266880035 \t range_loss: 0.6533495187759399\n",
      "pred_error: 0.051453493535518646 \t range_loss: 0.6573790311813354\n",
      "pred_error: 0.051453493535518646 \t range_loss: 0.6573790311813354\n",
      "pred_error: 0.051453374326229095 \t range_loss: 0.6573790311813354\n",
      "pred_error: 0.05145353451371193 \t range_loss: 0.6573790311813354\n",
      "pred_error: 0.05134059116244316 \t range_loss: 0.6545826196670532\n",
      "pred_error: 0.051984332501888275 \t range_loss: 0.6523386240005493\n",
      "pred_error: 0.05154390260577202 \t range_loss: 0.6584133505821228\n",
      "pred_error: 0.05147752910852432 \t range_loss: 0.6504753232002258\n",
      "pred_error: 0.05147683992981911 \t range_loss: 0.6504753232002258\n",
      "pred_error: 0.051477596163749695 \t range_loss: 0.6504753232002258\n",
      "pred_error: 0.05119306594133377 \t range_loss: 0.6556621789932251\n",
      "pred_error: 0.05160343647003174 \t range_loss: 0.6515036821365356\n",
      "pred_error: 0.05191527679562569 \t range_loss: 0.6507948040962219\n",
      "pred_error: 0.0518605038523674 \t range_loss: 0.6475345492362976\n",
      "pred_error: 0.05156911537051201 \t range_loss: 0.6538421511650085\n",
      "pred_error: 0.05156906321644783 \t range_loss: 0.6538421511650085\n",
      "pred_error: 0.05190632864832878 \t range_loss: 0.6464619040489197\n",
      "pred_error: 0.05207894369959831 \t range_loss: 0.6479014158248901\n",
      "pred_error: 0.051788974553346634 \t range_loss: 0.65110844373703\n",
      "pred_error: 0.05190173536539078 \t range_loss: 0.6500826478004456\n",
      "pred_error: 0.051901962608098984 \t range_loss: 0.6500826478004456\n",
      "pred_error: 0.0520939901471138 \t range_loss: 0.6538596153259277\n",
      "pred_error: 0.05233414098620415 \t range_loss: 0.6495801210403442\n",
      "pred_error: 0.05191396549344063 \t range_loss: 0.6494700908660889\n",
      "pred_error: 0.05140259489417076 \t range_loss: 0.6575159430503845\n",
      "pred_error: 0.05196620151400566 \t range_loss: 0.6482675671577454\n",
      "pred_error: 0.05188145488500595 \t range_loss: 0.6506734490394592\n",
      "pred_error: 0.05158894509077072 \t range_loss: 0.6523944139480591\n",
      "pred_error: 0.05190712958574295 \t range_loss: 0.6490150690078735\n",
      "pred_error: 0.05206163972616196 \t range_loss: 0.6490638256072998\n",
      "pred_error: 0.052003949880599976 \t range_loss: 0.6497599482536316\n",
      "pred_error: 0.05200517922639847 \t range_loss: 0.6497599482536316\n",
      "pred_error: 0.05171054229140282 \t range_loss: 0.6517292857170105\n",
      "pred_error: 0.0517105758190155 \t range_loss: 0.6517292857170105\n",
      "pred_error: 0.05171052739024162 \t range_loss: 0.6517292857170105\n",
      "pred_error: 0.052199628204107285 \t range_loss: 0.6499276161193848\n",
      "pred_error: 0.052199698984622955 \t range_loss: 0.6499276161193848\n",
      "pred_error: 0.05240030586719513 \t range_loss: 0.6485151648521423\n",
      "pred_error: 0.052400141954422 \t range_loss: 0.6485151648521423\n",
      "pred_error: 0.051802314817905426 \t range_loss: 0.648623526096344\n",
      "pred_error: 0.05212191492319107 \t range_loss: 0.6468340158462524\n",
      "Step: 392 \t Loss: 1.1625642776489258\n",
      "pred_error: 0.05151345580816269 \t range_loss: 0.6474305987358093\n",
      "pred_error: 0.05158434808254242 \t range_loss: 0.652807354927063\n",
      "pred_error: 0.05192863941192627 \t range_loss: 0.6497402787208557\n",
      "pred_error: 0.052123647183179855 \t range_loss: 0.6488313674926758\n",
      "pred_error: 0.05191495642066002 \t range_loss: 0.653644859790802\n",
      "pred_error: 0.0519150011241436 \t range_loss: 0.653644859790802\n",
      "pred_error: 0.051696110516786575 \t range_loss: 0.6509403586387634\n",
      "pred_error: 0.05156879499554634 \t range_loss: 0.6554587483406067\n",
      "pred_error: 0.051427848637104034 \t range_loss: 0.6540172100067139\n",
      "pred_error: 0.051427941769361496 \t range_loss: 0.6540172100067139\n",
      "pred_error: 0.05196193605661392 \t range_loss: 0.6506903767585754\n",
      "pred_error: 0.05172814801335335 \t range_loss: 0.6535820364952087\n",
      "pred_error: 0.05182386189699173 \t range_loss: 0.6502920985221863\n",
      "pred_error: 0.05203685164451599 \t range_loss: 0.6520847082138062\n",
      "pred_error: 0.05177992954850197 \t range_loss: 0.649797797203064\n",
      "pred_error: 0.051514387130737305 \t range_loss: 0.650365948677063\n",
      "pred_error: 0.05233348160982132 \t range_loss: 0.6498680710792542\n",
      "pred_error: 0.052333343774080276 \t range_loss: 0.6498680710792542\n",
      "pred_error: 0.0518404096364975 \t range_loss: 0.6483613848686218\n",
      "pred_error: 0.05202098563313484 \t range_loss: 0.6476843357086182\n",
      "pred_error: 0.05155486613512039 \t range_loss: 0.6471995711326599\n",
      "pred_error: 0.05166285112500191 \t range_loss: 0.6519736647605896\n",
      "pred_error: 0.05166376382112503 \t range_loss: 0.6519736647605896\n",
      "pred_error: 0.05164329707622528 \t range_loss: 0.6496625542640686\n",
      "pred_error: 0.05189025402069092 \t range_loss: 0.65343177318573\n",
      "pred_error: 0.05189013481140137 \t range_loss: 0.65343177318573\n",
      "pred_error: 0.0518900565803051 \t range_loss: 0.65343177318573\n",
      "pred_error: 0.05193522572517395 \t range_loss: 0.649087131023407\n",
      "pred_error: 0.05158449709415436 \t range_loss: 0.6493662595748901\n",
      "pred_error: 0.051584649831056595 \t range_loss: 0.6493662595748901\n",
      "pred_error: 0.052025552839040756 \t range_loss: 0.6510632634162903\n",
      "pred_error: 0.05178019776940346 \t range_loss: 0.6486173272132874\n",
      "pred_error: 0.05178019031882286 \t range_loss: 0.6486173272132874\n",
      "pred_error: 0.05176510289311409 \t range_loss: 0.6469718813896179\n",
      "pred_error: 0.05176509544253349 \t range_loss: 0.6469718813896179\n",
      "pred_error: 0.051764488220214844 \t range_loss: 0.6469718813896179\n",
      "pred_error: 0.05221448093652725 \t range_loss: 0.6495932936668396\n",
      "pred_error: 0.051858700811862946 \t range_loss: 0.652315616607666\n",
      "pred_error: 0.05165908485651016 \t range_loss: 0.6489257216453552\n",
      "pred_error: 0.05190493166446686 \t range_loss: 0.65130215883255\n",
      "pred_error: 0.051677312701940536 \t range_loss: 0.6516576409339905\n",
      "pred_error: 0.051633354276418686 \t range_loss: 0.6477378010749817\n",
      "pred_error: 0.0516333132982254 \t range_loss: 0.6477378010749817\n",
      "pred_error: 0.05127206817269325 \t range_loss: 0.6515415906906128\n",
      "pred_error: 0.05127206817269325 \t range_loss: 0.6515415906906128\n",
      "pred_error: 0.05213823541998863 \t range_loss: 0.6526117324829102\n",
      "pred_error: 0.05166656896471977 \t range_loss: 0.6516469717025757\n",
      "pred_error: 0.05200789123773575 \t range_loss: 0.6496824026107788\n",
      "pred_error: 0.052230458706617355 \t range_loss: 0.6498926877975464\n",
      "pred_error: 0.05223033204674721 \t range_loss: 0.6498926877975464\n",
      "pred_error: 0.05188130587339401 \t range_loss: 0.6485151648521423\n",
      "pred_error: 0.051438212394714355 \t range_loss: 0.6492469310760498\n",
      "Step: 491 \t Loss: 1.1623141765594482\n",
      "pred_error: 0.05177098140120506 \t range_loss: 0.6490099430084229\n",
      "pred_error: 0.05204881727695465 \t range_loss: 0.6493104100227356\n",
      "pred_error: 0.05204906314611435 \t range_loss: 0.6493104100227356\n",
      "pred_error: 0.05178435891866684 \t range_loss: 0.6492193341255188\n",
      "pred_error: 0.05231717973947525 \t range_loss: 0.6491266489028931\n",
      "pred_error: 0.051928263157606125 \t range_loss: 0.6490774750709534\n",
      "pred_error: 0.05209888890385628 \t range_loss: 0.6482534408569336\n",
      "pred_error: 0.05176997557282448 \t range_loss: 0.6467112302780151\n",
      "pred_error: 0.05208096280694008 \t range_loss: 0.6482558846473694\n",
      "pred_error: 0.05220910534262657 \t range_loss: 0.6501688957214355\n",
      "pred_error: 0.0517493337392807 \t range_loss: 0.6492680907249451\n",
      "pred_error: 0.05174929276108742 \t range_loss: 0.6492680907249451\n",
      "pred_error: 0.05202252417802811 \t range_loss: 0.6453322172164917\n",
      "pred_error: 0.052132729440927505 \t range_loss: 0.648369312286377\n",
      "pred_error: 0.05209669843316078 \t range_loss: 0.6543943285942078\n",
      "pred_error: 0.05159464105963707 \t range_loss: 0.650310218334198\n",
      "pred_error: 0.05200418457388878 \t range_loss: 0.6510112285614014\n",
      "pred_error: 0.05212851241230965 \t range_loss: 0.6483158469200134\n",
      "pred_error: 0.05168735235929489 \t range_loss: 0.6533603072166443\n",
      "pred_error: 0.0516250841319561 \t range_loss: 0.6496816277503967\n",
      "pred_error: 0.051825374364852905 \t range_loss: 0.6476691365242004\n",
      "pred_error: 0.051535576581954956 \t range_loss: 0.6518614292144775\n",
      "pred_error: 0.051838938146829605 \t range_loss: 0.6472717523574829\n",
      "pred_error: 0.051838770508766174 \t range_loss: 0.6472717523574829\n",
      "pred_error: 0.05209944397211075 \t range_loss: 0.6468167901039124\n",
      "pred_error: 0.05209953710436821 \t range_loss: 0.6468167901039124\n",
      "pred_error: 0.051665473729372025 \t range_loss: 0.6506310105323792\n",
      "pred_error: 0.05169730260968208 \t range_loss: 0.6487106084823608\n",
      "pred_error: 0.05196411907672882 \t range_loss: 0.6482381224632263\n",
      "pred_error: 0.05184458568692207 \t range_loss: 0.6504517793655396\n",
      "pred_error: 0.05218476802110672 \t range_loss: 0.6477044224739075\n",
      "pred_error: 0.05245135724544525 \t range_loss: 0.6486424207687378\n",
      "pred_error: 0.05197109654545784 \t range_loss: 0.6469281911849976\n",
      "pred_error: 0.051970891654491425 \t range_loss: 0.6469281911849976\n",
      "pred_error: 0.05209638178348541 \t range_loss: 0.6478562951087952\n",
      "pred_error: 0.0523553341627121 \t range_loss: 0.6466102004051208\n",
      "pred_error: 0.05190679803490639 \t range_loss: 0.6459447145462036\n",
      "pred_error: 0.0517706423997879 \t range_loss: 0.6461747884750366\n",
      "pred_error: 0.051770735532045364 \t range_loss: 0.6461747884750366\n",
      "pred_error: 0.05186569690704346 \t range_loss: 0.6467301249504089\n",
      "pred_error: 0.05233200639486313 \t range_loss: 0.6483202576637268\n",
      "pred_error: 0.052484359592199326 \t range_loss: 0.6479514837265015\n",
      "pred_error: 0.052178118377923965 \t range_loss: 0.646278977394104\n",
      "pred_error: 0.05217809975147247 \t range_loss: 0.646278977394104\n",
      "pred_error: 0.05222289264202118 \t range_loss: 0.6449134945869446\n",
      "pred_error: 0.05173113942146301 \t range_loss: 0.6479798555374146\n",
      "pred_error: 0.0517311617732048 \t range_loss: 0.6479798555374146\n",
      "pred_error: 0.051988281309604645 \t range_loss: 0.64970463514328\n",
      "pred_error: 0.051877912133932114 \t range_loss: 0.6479319930076599\n",
      "pred_error: 0.05186603590846062 \t range_loss: 0.6492316126823425\n",
      "pred_error: 0.05218708515167236 \t range_loss: 0.648833155632019\n",
      "pred_error: 0.052187006920576096 \t range_loss: 0.648833155632019\n",
      "pred_error: 0.05191321298480034 \t range_loss: 0.6531171798706055\n",
      "pred_error: 0.05191292241215706 \t range_loss: 0.6531171798706055\n",
      "pred_error: 0.052097972482442856 \t range_loss: 0.6439918279647827\n",
      "pred_error: 0.051611755043268204 \t range_loss: 0.6494192481040955\n",
      "pred_error: 0.05245491862297058 \t range_loss: 0.6462175846099854\n",
      "pred_error: 0.05245492234826088 \t range_loss: 0.6462175846099854\n",
      "pred_error: 0.05176866054534912 \t range_loss: 0.6462495923042297\n",
      "pred_error: 0.05231158807873726 \t range_loss: 0.6545209288597107\n",
      "pred_error: 0.052407193928956985 \t range_loss: 0.6488715410232544\n",
      "pred_error: 0.051970966160297394 \t range_loss: 0.6505861282348633\n",
      "pred_error: 0.05161573737859726 \t range_loss: 0.6465221047401428\n",
      "pred_error: 0.05203044414520264 \t range_loss: 0.6466280221939087\n",
      "pred_error: 0.05174298211932182 \t range_loss: 0.6490448117256165\n",
      "pred_error: 0.052109044045209885 \t range_loss: 0.6474648714065552\n",
      "pred_error: 0.051729634404182434 \t range_loss: 0.6494947671890259\n",
      "pred_error: 0.05223914608359337 \t range_loss: 0.6493192315101624\n",
      "pred_error: 0.05159560590982437 \t range_loss: 0.6542755961418152\n",
      "pred_error: 0.05195964500308037 \t range_loss: 0.651594877243042\n",
      "pred_error: 0.051553476601839066 \t range_loss: 0.649385929107666\n",
      "pred_error: 0.05188524350523949 \t range_loss: 0.649107038974762\n",
      "Step: 615 \t Loss: 1.1619174480438232\n",
      "pred_error: 0.05214449018239975 \t range_loss: 0.6478102207183838\n",
      "pred_error: 0.05183517932891846 \t range_loss: 0.6478565335273743\n",
      "pred_error: 0.05175371840596199 \t range_loss: 0.6495135426521301\n",
      "pred_error: 0.05155037343502045 \t range_loss: 0.6543677449226379\n",
      "pred_error: 0.05225135385990143 \t range_loss: 0.647111713886261\n",
      "pred_error: 0.0518127977848053 \t range_loss: 0.6491622924804688\n",
      "pred_error: 0.05189311131834984 \t range_loss: 0.6459351778030396\n",
      "pred_error: 0.05211386829614639 \t range_loss: 0.6456552743911743\n",
      "pred_error: 0.051951151341199875 \t range_loss: 0.6462002992630005\n",
      "pred_error: 0.052420198917388916 \t range_loss: 0.6506018042564392\n",
      "pred_error: 0.052359651774168015 \t range_loss: 0.648591935634613\n",
      "pred_error: 0.05235961079597473 \t range_loss: 0.648591935634613\n",
      "pred_error: 0.052480101585388184 \t range_loss: 0.6468071341514587\n",
      "pred_error: 0.05247984081506729 \t range_loss: 0.6468071341514587\n",
      "pred_error: 0.05199917033314705 \t range_loss: 0.6478118300437927\n",
      "pred_error: 0.051562871783971786 \t range_loss: 0.6517557501792908\n",
      "pred_error: 0.05188414454460144 \t range_loss: 0.6466691493988037\n",
      "pred_error: 0.051954369992017746 \t range_loss: 0.6470357775688171\n",
      "pred_error: 0.051783110946416855 \t range_loss: 0.6487160325050354\n",
      "pred_error: 0.051469482481479645 \t range_loss: 0.6497980356216431\n",
      "pred_error: 0.051768891513347626 \t range_loss: 0.6545920372009277\n",
      "pred_error: 0.05176887661218643 \t range_loss: 0.6545920372009277\n",
      "pred_error: 0.05176539719104767 \t range_loss: 0.6523417234420776\n",
      "pred_error: 0.05205470323562622 \t range_loss: 0.6499667763710022\n",
      "pred_error: 0.051643747836351395 \t range_loss: 0.6528072357177734\n",
      "pred_error: 0.05193120613694191 \t range_loss: 0.6548665165901184\n",
      "pred_error: 0.05155640095472336 \t range_loss: 0.6468342542648315\n",
      "pred_error: 0.05212511867284775 \t range_loss: 0.6514396071434021\n",
      "pred_error: 0.051808975636959076 \t range_loss: 0.6517416834831238\n",
      "pred_error: 0.051809098571538925 \t range_loss: 0.6517416834831238\n",
      "pred_error: 0.05180901661515236 \t range_loss: 0.6517416834831238\n",
      "pred_error: 0.05169649049639702 \t range_loss: 0.6496794819831848\n",
      "pred_error: 0.05209943652153015 \t range_loss: 0.6465910077095032\n",
      "pred_error: 0.0516568198800087 \t range_loss: 0.6497848629951477\n",
      "pred_error: 0.05165669694542885 \t range_loss: 0.6497848629951477\n",
      "pred_error: 0.052462898194789886 \t range_loss: 0.6486065983772278\n",
      "pred_error: 0.05183491110801697 \t range_loss: 0.6471619606018066\n",
      "pred_error: 0.05211751535534859 \t range_loss: 0.646149754524231\n",
      "pred_error: 0.051785726100206375 \t range_loss: 0.6539053320884705\n",
      "pred_error: 0.05253472551703453 \t range_loss: 0.6459659337997437\n",
      "pred_error: 0.05267145857214928 \t range_loss: 0.647232174873352\n",
      "pred_error: 0.0520009882748127 \t range_loss: 0.647452712059021\n",
      "pred_error: 0.051746100187301636 \t range_loss: 0.6507150530815125\n",
      "pred_error: 0.051746100187301636 \t range_loss: 0.6507150530815125\n",
      "pred_error: 0.05205709859728813 \t range_loss: 0.6467374563217163\n",
      "pred_error: 0.052190668880939484 \t range_loss: 0.6484139561653137\n",
      "pred_error: 0.05219065397977829 \t range_loss: 0.6484139561653137\n",
      "pred_error: 0.05197736993432045 \t range_loss: 0.6480017900466919\n",
      "pred_error: 0.051675181835889816 \t range_loss: 0.6500045657157898\n",
      "pred_error: 0.052162669599056244 \t range_loss: 0.6479173898696899\n",
      "pred_error: 0.05216259881854057 \t range_loss: 0.6479173898696899\n",
      "pred_error: 0.05213490501046181 \t range_loss: 0.6485884189605713\n",
      "pred_error: 0.052240125834941864 \t range_loss: 0.6476342678070068\n",
      "pred_error: 0.05245180428028107 \t range_loss: 0.647834837436676\n",
      "pred_error: 0.051737334579229355 \t range_loss: 0.6454147696495056\n",
      "pred_error: 0.05229061096906662 \t range_loss: 0.6477116346359253\n",
      "pred_error: 0.052016131579875946 \t range_loss: 0.6472918391227722\n",
      "pred_error: 0.05206058919429779 \t range_loss: 0.6464896202087402\n",
      "pred_error: 0.051763229072093964 \t range_loss: 0.6477447748184204\n",
      "pred_error: 0.051844704896211624 \t range_loss: 0.6482770442962646\n",
      "pred_error: 0.051730308681726456 \t range_loss: 0.6544683575630188\n",
      "pred_error: 0.05173027515411377 \t range_loss: 0.6544683575630188\n",
      "pred_error: 0.05173032730817795 \t range_loss: 0.6544683575630188\n",
      "pred_error: 0.05249188840389252 \t range_loss: 0.6471695899963379\n",
      "pred_error: 0.0521908663213253 \t range_loss: 0.6456931233406067\n",
      "pred_error: 0.0518873892724514 \t range_loss: 0.6474969387054443\n",
      "pred_error: 0.05169505253434181 \t range_loss: 0.6504496932029724\n",
      "pred_error: 0.05196113511919975 \t range_loss: 0.6507725715637207\n",
      "pred_error: 0.05162597447633743 \t range_loss: 0.6500611901283264\n",
      "pred_error: 0.051747072488069534 \t range_loss: 0.649459958076477\n",
      "pred_error: 0.05174687132239342 \t range_loss: 0.649459958076477\n",
      "pred_error: 0.05208762362599373 \t range_loss: 0.6500222086906433\n",
      "pred_error: 0.05171455442905426 \t range_loss: 0.6468108892440796\n",
      "pred_error: 0.0516875721514225 \t range_loss: 0.6482692360877991\n",
      "pred_error: 0.05225954204797745 \t range_loss: 0.6512746214866638\n",
      "pred_error: 0.051428765058517456 \t range_loss: 0.6508542895317078\n",
      "pred_error: 0.05172381177544594 \t range_loss: 0.6485721468925476\n",
      "pred_error: 0.05176327005028725 \t range_loss: 0.6491217613220215\n",
      "pred_error: 0.05176370218396187 \t range_loss: 0.6491217613220215\n",
      "pred_error: 0.05236148089170456 \t range_loss: 0.6528316736221313\n",
      "Step: 797 \t Loss: 1.161037564277649\n",
      "BEST LOSS: 1.1610376\n",
      "==== Model: block6_cob_activation_norm  in Layer: 6 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 15:29:01,955 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 15:31:38,639 model.rs:1246 value (-18624) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 15:31:38,645 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 15:32:01,245 model.rs:1246 value (-18624) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 15:32:01,255 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 15:32:01,283 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 15:32:01,305 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 15:32:01,322 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+--------------------+------------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error         | median_error     | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+--------------------+------------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000000008999839 | -0.0000021457672 | 0.00035792217 | -0.00048039854 | 0.000051266914 | 0.0000021457672  | 0.00048039854 | 0             | 0.0000000046482844 | -0.0001920712      | 0.0012473243           |\n",
      "+--------------------+------------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 656043 64 [-686376, 416250] 1 [16]\n",
      "===============================\n",
      "==== Model: block6_cob_activation_norm_teleported  in Layer: 6 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 15:32:28,836 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 15:35:13,811 model.rs:1246 value (1856) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 15:35:13,827 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 15:35:38,714 model.rs:1246 value (1856) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 15:35:38,723 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 15:35:38,743 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 15:35:38,777 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 15:35:38,794 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-------------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error        | median_error   | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-------------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.00000025414883 | 0.000046521425 | 0.00048853643 | -0.00047279894 | 0.000051373787 | 0.000046521425   | 0.00048853643 | 0             | 0.000000004685987  | -0.00009443487     | 0.001502892            |\n",
      "+-------------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 656046 64 [-495898, 218980] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 7 , \t  activation_stats: {'relu_1': {'norm': tensor(780.0437), 'max': tensor(7.7553), 'min': tensor(-7.5588), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(15.3141)\n",
      "Step: 0 \t Loss: 1.485593557357788\n",
      "Step: 1 \t Loss: 1.4524823427200317\n",
      "Step: 2 \t Loss: 1.416534185409546\n",
      "pred_error: 0.0510893389582634 \t range_loss: 0.9056371450424194\n",
      "Step: 3 \t Loss: 1.39580237865448\n",
      "pred_error: 0.051357071846723557 \t range_loss: 0.8822299242019653\n",
      "Step: 4 \t Loss: 1.3751246929168701\n",
      "pred_error: 0.051753442734479904 \t range_loss: 0.857588529586792\n",
      "Step: 5 \t Loss: 1.356931209564209\n",
      "Step: 6 \t Loss: 1.3385722637176514\n",
      "Step: 7 \t Loss: 1.3238329887390137\n",
      "Step: 8 \t Loss: 1.3105158805847168\n",
      "Step: 10 \t Loss: 1.303015947341919\n",
      "pred_error: 0.05374344810843468 \t range_loss: 0.7653759121894836\n",
      "Step: 12 \t Loss: 1.294980764389038\n",
      "Step: 14 \t Loss: 1.2875444889068604\n",
      "pred_error: 0.05393858626484871 \t range_loss: 0.7481562495231628\n",
      "Step: 15 \t Loss: 1.2867666482925415\n",
      "Step: 16 \t Loss: 1.2853596210479736\n",
      "Step: 17 \t Loss: 1.279670238494873\n",
      "pred_error: 0.05468209832906723 \t range_loss: 0.7328409552574158\n",
      "pred_error: 0.0541151724755764 \t range_loss: 0.7398958206176758\n",
      "Step: 19 \t Loss: 1.2691454887390137\n",
      "Step: 21 \t Loss: 1.2687327861785889\n",
      "pred_error: 0.054628778249025345 \t range_loss: 0.7224456071853638\n",
      "pred_error: 0.05526787415146828 \t range_loss: 0.7202372550964355\n",
      "Step: 23 \t Loss: 1.2682092189788818\n",
      "pred_error: 0.054744184017181396 \t range_loss: 0.7207669615745544\n",
      "pred_error: 0.05474429205060005 \t range_loss: 0.7207669615745544\n",
      "Step: 24 \t Loss: 1.2675230503082275\n",
      "Step: 25 \t Loss: 1.263847827911377\n",
      "Step: 26 \t Loss: 1.2614171504974365\n",
      "Step: 28 \t Loss: 1.2592025995254517\n",
      "Step: 29 \t Loss: 1.2587733268737793\n",
      "Step: 30 \t Loss: 1.2534923553466797\n",
      "pred_error: 0.05540310963988304 \t range_loss: 0.6994612216949463\n",
      "pred_error: 0.05547453463077545 \t range_loss: 0.6995441913604736\n",
      "Step: 34 \t Loss: 1.246884822845459\n",
      "pred_error: 0.05497889220714569 \t range_loss: 0.7045096755027771\n",
      "Step: 36 \t Loss: 1.2454822063446045\n",
      "pred_error: 0.054779842495918274 \t range_loss: 0.7019950151443481\n",
      "pred_error: 0.05477951839566231 \t range_loss: 0.7019950151443481\n",
      "Step: 38 \t Loss: 1.2425177097320557\n",
      "Step: 40 \t Loss: 1.2418272495269775\n",
      "pred_error: 0.05549895390868187 \t range_loss: 0.6868377923965454\n",
      "pred_error: 0.05605871230363846 \t range_loss: 0.686755359172821\n",
      "pred_error: 0.05605916678905487 \t range_loss: 0.686755359172821\n",
      "pred_error: 0.056058648973703384 \t range_loss: 0.686755359172821\n",
      "pred_error: 0.056151777505874634 \t range_loss: 0.6888180375099182\n",
      "pred_error: 0.05615164712071419 \t range_loss: 0.6888180375099182\n",
      "Step: 44 \t Loss: 1.2413842678070068\n",
      "Step: 45 \t Loss: 1.239929437637329\n",
      "Step: 46 \t Loss: 1.2386510372161865\n",
      "pred_error: 0.05549038201570511 \t range_loss: 0.6850665211677551\n",
      "Step: 49 \t Loss: 1.2361681461334229\n",
      "Step: 51 \t Loss: 1.2356619834899902\n",
      "pred_error: 0.05612003803253174 \t range_loss: 0.6744615435600281\n",
      "pred_error: 0.05611981451511383 \t range_loss: 0.6744615435600281\n",
      "pred_error: 0.05611986666917801 \t range_loss: 0.6744615435600281\n",
      "Step: 53 \t Loss: 1.2321434020996094\n",
      "pred_error: 0.056035116314888 \t range_loss: 0.6717898845672607\n",
      "pred_error: 0.05574049428105354 \t range_loss: 0.681501030921936\n",
      "pred_error: 0.05574037507176399 \t range_loss: 0.681501030921936\n",
      "Step: 55 \t Loss: 1.2310538291931152\n",
      "pred_error: 0.05621407553553581 \t range_loss: 0.6689130067825317\n",
      "pred_error: 0.055537931621074677 \t range_loss: 0.6807619333267212\n",
      "pred_error: 0.05553756654262543 \t range_loss: 0.6807619333267212\n",
      "pred_error: 0.05553775653243065 \t range_loss: 0.6807619333267212\n",
      "Step: 57 \t Loss: 1.2309606075286865\n",
      "pred_error: 0.05610130354762077 \t range_loss: 0.6699477434158325\n",
      "Step: 59 \t Loss: 1.225307822227478\n",
      "pred_error: 0.05630192533135414 \t range_loss: 0.6659037470817566\n",
      "pred_error: 0.05679720267653465 \t range_loss: 0.6672085523605347\n",
      "Step: 68 \t Loss: 1.223505973815918\n",
      "pred_error: 0.05658508092164993 \t range_loss: 0.6607005596160889\n",
      "pred_error: 0.05609928071498871 \t range_loss: 0.6662797331809998\n",
      "pred_error: 0.05609900876879692 \t range_loss: 0.6662797331809998\n",
      "Step: 72 \t Loss: 1.2234742641448975\n",
      "Step: 74 \t Loss: 1.2197636365890503\n",
      "pred_error: 0.057300325483083725 \t range_loss: 0.6522854566574097\n",
      "pred_error: 0.05716743692755699 \t range_loss: 0.6515451669692993\n",
      "pred_error: 0.05716743692755699 \t range_loss: 0.6515451669692993\n",
      "pred_error: 0.056646090000867844 \t range_loss: 0.6570783853530884\n",
      "pred_error: 0.05716116726398468 \t range_loss: 0.6489384770393372\n",
      "pred_error: 0.056604065001010895 \t range_loss: 0.6573472619056702\n",
      "Step: 85 \t Loss: 1.2192145586013794\n",
      "pred_error: 0.05711054801940918 \t range_loss: 0.6481081247329712\n",
      "pred_error: 0.056488387286663055 \t range_loss: 0.6593508720397949\n",
      "Step: 89 \t Loss: 1.217126727104187\n",
      "Step: 91 \t Loss: 1.2130128145217896\n",
      "pred_error: 0.05691560357809067 \t range_loss: 0.6539717316627502\n",
      "pred_error: 0.05726051330566406 \t range_loss: 0.6428693532943726\n",
      "pred_error: 0.05731022357940674 \t range_loss: 0.6441491842269897\n",
      "pred_error: 0.05731048807501793 \t range_loss: 0.6441491842269897\n",
      "pred_error: 0.05703839659690857 \t range_loss: 0.6517852544784546\n",
      "pred_error: 0.057038240134716034 \t range_loss: 0.6517852544784546\n",
      "pred_error: 0.05749587342143059 \t range_loss: 0.6404908299446106\n",
      "pred_error: 0.05745057016611099 \t range_loss: 0.642153263092041\n",
      "pred_error: 0.056976161897182465 \t range_loss: 0.6504737734794617\n",
      "pred_error: 0.057573527097702026 \t range_loss: 0.6378800868988037\n",
      "pred_error: 0.05726086348295212 \t range_loss: 0.6467310190200806\n",
      "pred_error: 0.057260867208242416 \t range_loss: 0.6467310190200806\n",
      "pred_error: 0.057260967791080475 \t range_loss: 0.6467310190200806\n",
      "pred_error: 0.057718414813280106 \t range_loss: 0.6370614171028137\n",
      "Step: 113 \t Loss: 1.212727427482605\n",
      "Step: 117 \t Loss: 1.2127059698104858\n",
      "pred_error: 0.05776700749993324 \t range_loss: 0.6350340843200684\n",
      "pred_error: 0.057016730308532715 \t range_loss: 0.6440740823745728\n",
      "Step: 119 \t Loss: 1.2122137546539307\n",
      "Step: 121 \t Loss: 1.2110157012939453\n",
      "pred_error: 0.05720560625195503 \t range_loss: 0.6456258296966553\n",
      "Step: 125 \t Loss: 1.2101469039916992\n",
      "pred_error: 0.057874370366334915 \t range_loss: 0.635037899017334\n",
      "pred_error: 0.05807970464229584 \t range_loss: 0.6309390664100647\n",
      "Step: 136 \t Loss: 1.2100706100463867\n",
      "Step: 138 \t Loss: 1.2090153694152832\n",
      "pred_error: 0.05711182579398155 \t range_loss: 0.6378975510597229\n",
      "pred_error: 0.057386402040719986 \t range_loss: 0.6380127668380737\n",
      "pred_error: 0.057386402040719986 \t range_loss: 0.6380127668380737\n",
      "pred_error: 0.057341787964105606 \t range_loss: 0.6374529004096985\n",
      "Step: 150 \t Loss: 1.2083826065063477\n",
      "pred_error: 0.05801549181342125 \t range_loss: 0.6287643909454346\n",
      "pred_error: 0.05750184506177902 \t range_loss: 0.6366304159164429\n",
      "pred_error: 0.057762790471315384 \t range_loss: 0.6331825256347656\n",
      "Step: 156 \t Loss: 1.206345558166504\n",
      "pred_error: 0.05735241249203682 \t range_loss: 0.6328210830688477\n",
      "pred_error: 0.05763016268610954 \t range_loss: 0.6315113306045532\n",
      "pred_error: 0.057554665952920914 \t range_loss: 0.631146252155304\n",
      "pred_error: 0.05815400555729866 \t range_loss: 0.627666711807251\n",
      "Step: 166 \t Loss: 1.205566167831421\n",
      "pred_error: 0.05820595100522041 \t range_loss: 0.6325532793998718\n",
      "pred_error: 0.05820585414767265 \t range_loss: 0.6325532793998718\n",
      "pred_error: 0.057670511305332184 \t range_loss: 0.6323056221008301\n",
      "pred_error: 0.05834164097905159 \t range_loss: 0.6279340982437134\n",
      "pred_error: 0.05785975232720375 \t range_loss: 0.6292188763618469\n",
      "Step: 176 \t Loss: 1.2055244445800781\n",
      "pred_error: 0.05781342089176178 \t range_loss: 0.6302104592323303\n",
      "pred_error: 0.05781339108943939 \t range_loss: 0.6302104592323303\n",
      "pred_error: 0.05785779282450676 \t range_loss: 0.6303648352622986\n",
      "pred_error: 0.057874444872140884 \t range_loss: 0.6269954442977905\n",
      "pred_error: 0.057874493300914764 \t range_loss: 0.6269954442977905\n",
      "pred_error: 0.05830450356006622 \t range_loss: 0.6257537603378296\n",
      "Step: 184 \t Loss: 1.2054741382598877\n",
      "pred_error: 0.057658907026052475 \t range_loss: 0.628882884979248\n",
      "pred_error: 0.058002352714538574 \t range_loss: 0.6264958381652832\n",
      "pred_error: 0.05840165168046951 \t range_loss: 0.6237847208976746\n",
      "pred_error: 0.05840178206562996 \t range_loss: 0.6237847208976746\n",
      "Step: 188 \t Loss: 1.2046115398406982\n",
      "Step: 190 \t Loss: 1.2032380104064941\n",
      "pred_error: 0.05778631567955017 \t range_loss: 0.6253752112388611\n",
      "pred_error: 0.05835045129060745 \t range_loss: 0.6256330013275146\n",
      "pred_error: 0.05790753662586212 \t range_loss: 0.626244843006134\n",
      "pred_error: 0.057894203811883926 \t range_loss: 0.6287318468093872\n",
      "pred_error: 0.0585218146443367 \t range_loss: 0.6256523132324219\n",
      "pred_error: 0.05824976786971092 \t range_loss: 0.6248287558555603\n",
      "pred_error: 0.057680390775203705 \t range_loss: 0.6272038221359253\n",
      "pred_error: 0.05852074921131134 \t range_loss: 0.6250563859939575\n",
      "pred_error: 0.057897936552762985 \t range_loss: 0.6276103258132935\n",
      "pred_error: 0.057827867567539215 \t range_loss: 0.628727912902832\n",
      "pred_error: 0.0578281432390213 \t range_loss: 0.628727912902832\n",
      "pred_error: 0.05777408182621002 \t range_loss: 0.626109778881073\n",
      "pred_error: 0.05777418985962868 \t range_loss: 0.626109778881073\n",
      "pred_error: 0.057915590703487396 \t range_loss: 0.6252686977386475\n",
      "Step: 215 \t Loss: 1.2008745670318604\n",
      "pred_error: 0.05794196575880051 \t range_loss: 0.6255683302879333\n",
      "pred_error: 0.05794203281402588 \t range_loss: 0.6255683302879333\n",
      "pred_error: 0.058471549302339554 \t range_loss: 0.6227643489837646\n",
      "pred_error: 0.05847126990556717 \t range_loss: 0.6227643489837646\n",
      "pred_error: 0.05864747241139412 \t range_loss: 0.6225228905677795\n",
      "Step: 235 \t Loss: 1.2005051374435425\n",
      "pred_error: 0.058737412095069885 \t range_loss: 0.6202975511550903\n",
      "pred_error: 0.05812149494886398 \t range_loss: 0.6212831139564514\n",
      "pred_error: 0.05812162905931473 \t range_loss: 0.6212831139564514\n",
      "pred_error: 0.05875419080257416 \t range_loss: 0.6190058588981628\n",
      "pred_error: 0.05826650932431221 \t range_loss: 0.6212977766990662\n",
      "pred_error: 0.05801890417933464 \t range_loss: 0.6217078566551208\n",
      "pred_error: 0.05886159464716911 \t range_loss: 0.6192632913589478\n",
      "pred_error: 0.05835612490773201 \t range_loss: 0.619949996471405\n",
      "pred_error: 0.058095380663871765 \t range_loss: 0.6208595037460327\n",
      "pred_error: 0.05862496420741081 \t range_loss: 0.620708703994751\n",
      "pred_error: 0.05812928453087807 \t range_loss: 0.6221704483032227\n",
      "pred_error: 0.05822938680648804 \t range_loss: 0.6197968125343323\n",
      "pred_error: 0.05821767449378967 \t range_loss: 0.6204875111579895\n",
      "pred_error: 0.05837816372513771 \t range_loss: 0.6193698644638062\n",
      "pred_error: 0.05822497978806496 \t range_loss: 0.6186938881874084\n",
      "pred_error: 0.0582248792052269 \t range_loss: 0.6186938881874084\n",
      "Step: 266 \t Loss: 1.2002876996994019\n",
      "pred_error: 0.058157779276371 \t range_loss: 0.618709921836853\n",
      "pred_error: 0.05879976227879524 \t range_loss: 0.6184144020080566\n",
      "pred_error: 0.058397095650434494 \t range_loss: 0.6193618774414062\n",
      "pred_error: 0.05841460078954697 \t range_loss: 0.6174138784408569\n",
      "pred_error: 0.05811971798539162 \t range_loss: 0.6238929629325867\n",
      "pred_error: 0.05870085954666138 \t range_loss: 0.6190710663795471\n",
      "pred_error: 0.05803089588880539 \t range_loss: 0.6213158965110779\n",
      "pred_error: 0.05827658995985985 \t range_loss: 0.6210964322090149\n",
      "pred_error: 0.05827658995985985 \t range_loss: 0.6210964322090149\n",
      "pred_error: 0.05816935747861862 \t range_loss: 0.619098424911499\n",
      "pred_error: 0.05843260511755943 \t range_loss: 0.618579089641571\n",
      "pred_error: 0.05843276530504227 \t range_loss: 0.618579089641571\n",
      "pred_error: 0.05853280425071716 \t range_loss: 0.6188241243362427\n",
      "Step: 292 \t Loss: 1.1971073150634766\n",
      "pred_error: 0.05804663151502609 \t range_loss: 0.6166410446166992\n",
      "pred_error: 0.058680832386016846 \t range_loss: 0.6156340837478638\n",
      "pred_error: 0.058768950402736664 \t range_loss: 0.6164861917495728\n",
      "pred_error: 0.05872693657875061 \t range_loss: 0.6186740398406982\n",
      "pred_error: 0.059071626514196396 \t range_loss: 0.6173462867736816\n",
      "pred_error: 0.05898060277104378 \t range_loss: 0.6152434349060059\n",
      "pred_error: 0.058113206177949905 \t range_loss: 0.6184258460998535\n",
      "pred_error: 0.05811237171292305 \t range_loss: 0.6184258460998535\n",
      "pred_error: 0.05811319872736931 \t range_loss: 0.6184258460998535\n",
      "pred_error: 0.05869008228182793 \t range_loss: 0.6196218729019165\n",
      "pred_error: 0.058895036578178406 \t range_loss: 0.6181163191795349\n",
      "pred_error: 0.05824350193142891 \t range_loss: 0.6172386407852173\n",
      "pred_error: 0.058977290987968445 \t range_loss: 0.616715133190155\n",
      "pred_error: 0.059228211641311646 \t range_loss: 0.6165072917938232\n",
      "pred_error: 0.05922821909189224 \t range_loss: 0.6165072917938232\n",
      "pred_error: 0.059049271047115326 \t range_loss: 0.6138726472854614\n",
      "pred_error: 0.05837557464838028 \t range_loss: 0.6218992471694946\n",
      "pred_error: 0.05873478204011917 \t range_loss: 0.6129353046417236\n",
      "pred_error: 0.05907805636525154 \t range_loss: 0.6117866635322571\n",
      "pred_error: 0.05849015340209007 \t range_loss: 0.617296576499939\n",
      "pred_error: 0.05923248827457428 \t range_loss: 0.6144108772277832\n",
      "pred_error: 0.05868662893772125 \t range_loss: 0.6163748502731323\n",
      "pred_error: 0.058383915573358536 \t range_loss: 0.6157770156860352\n",
      "pred_error: 0.05833296477794647 \t range_loss: 0.626264750957489\n",
      "pred_error: 0.0591830350458622 \t range_loss: 0.6101644039154053\n",
      "pred_error: 0.05918314680457115 \t range_loss: 0.6101644039154053\n",
      "pred_error: 0.05834505707025528 \t range_loss: 0.6207677125930786\n",
      "pred_error: 0.058401092886924744 \t range_loss: 0.6204046607017517\n",
      "pred_error: 0.05935753509402275 \t range_loss: 0.6136564016342163\n",
      "pred_error: 0.05856864154338837 \t range_loss: 0.6155579686164856\n",
      "pred_error: 0.0593191534280777 \t range_loss: 0.611644983291626\n",
      "pred_error: 0.059319064021110535 \t range_loss: 0.611644983291626\n",
      "pred_error: 0.05931869149208069 \t range_loss: 0.6116782426834106\n",
      "pred_error: 0.05847688391804695 \t range_loss: 0.6146368384361267\n",
      "pred_error: 0.058531563729047775 \t range_loss: 0.6142308115959167\n",
      "pred_error: 0.0585101954638958 \t range_loss: 0.6160143613815308\n",
      "pred_error: 0.05851013585925102 \t range_loss: 0.6160143613815308\n",
      "pred_error: 0.059032659977674484 \t range_loss: 0.6159107685089111\n",
      "pred_error: 0.05807678401470184 \t range_loss: 0.6265166997909546\n",
      "pred_error: 0.05859759822487831 \t range_loss: 0.6141791939735413\n",
      "pred_error: 0.05809083208441734 \t range_loss: 0.6211268901824951\n",
      "pred_error: 0.058753639459609985 \t range_loss: 0.6179870963096619\n",
      "pred_error: 0.05825811251997948 \t range_loss: 0.6192067861557007\n",
      "pred_error: 0.05825795605778694 \t range_loss: 0.6192067861557007\n",
      "pred_error: 0.05825809761881828 \t range_loss: 0.6192067861557007\n",
      "pred_error: 0.05904536321759224 \t range_loss: 0.613828718662262\n",
      "pred_error: 0.05843972787261009 \t range_loss: 0.6162500381469727\n",
      "pred_error: 0.058439768850803375 \t range_loss: 0.6162500381469727\n",
      "pred_error: 0.05905712768435478 \t range_loss: 0.6130997538566589\n",
      "pred_error: 0.05860380828380585 \t range_loss: 0.6165771484375\n",
      "pred_error: 0.058918584138154984 \t range_loss: 0.6123660802841187\n",
      "pred_error: 0.05830537527799606 \t range_loss: 0.6217838525772095\n",
      "pred_error: 0.05830562114715576 \t range_loss: 0.6217838525772095\n",
      "pred_error: 0.05905464291572571 \t range_loss: 0.6123424768447876\n",
      "pred_error: 0.0589526928961277 \t range_loss: 0.6125472784042358\n",
      "pred_error: 0.0589526891708374 \t range_loss: 0.6125472784042358\n",
      "pred_error: 0.05818638205528259 \t range_loss: 0.617495596408844\n",
      "pred_error: 0.05841775983572006 \t range_loss: 0.6178945302963257\n",
      "pred_error: 0.0585312657058239 \t range_loss: 0.6157143712043762\n",
      "pred_error: 0.05907539650797844 \t range_loss: 0.6109794974327087\n",
      "pred_error: 0.058627910912036896 \t range_loss: 0.6162738800048828\n",
      "pred_error: 0.05945183336734772 \t range_loss: 0.6095460057258606\n",
      "pred_error: 0.05848941579461098 \t range_loss: 0.6140628457069397\n",
      "pred_error: 0.05848941579461098 \t range_loss: 0.6140628457069397\n",
      "pred_error: 0.05865633860230446 \t range_loss: 0.6136708855628967\n",
      "pred_error: 0.05807251110672951 \t range_loss: 0.6252622604370117\n",
      "pred_error: 0.058072611689567566 \t range_loss: 0.6252622604370117\n",
      "pred_error: 0.05907290428876877 \t range_loss: 0.6117543578147888\n",
      "pred_error: 0.05906268209218979 \t range_loss: 0.6122177243232727\n",
      "pred_error: 0.05861017480492592 \t range_loss: 0.616057276725769\n",
      "pred_error: 0.058546971529722214 \t range_loss: 0.6146401166915894\n",
      "pred_error: 0.05854706093668938 \t range_loss: 0.6146401166915894\n",
      "pred_error: 0.05911431461572647 \t range_loss: 0.6125123500823975\n",
      "pred_error: 0.058421432971954346 \t range_loss: 0.6151671409606934\n",
      "pred_error: 0.059312853962183 \t range_loss: 0.6124691367149353\n",
      "pred_error: 0.05955701321363449 \t range_loss: 0.6124172806739807\n",
      "pred_error: 0.05843545123934746 \t range_loss: 0.6131061315536499\n",
      "pred_error: 0.05843544006347656 \t range_loss: 0.6131061315536499\n",
      "pred_error: 0.058313850313425064 \t range_loss: 0.621900737285614\n",
      "pred_error: 0.05831380933523178 \t range_loss: 0.621900737285614\n",
      "pred_error: 0.0589488260447979 \t range_loss: 0.6128284931182861\n",
      "pred_error: 0.05833127722144127 \t range_loss: 0.6196618676185608\n",
      "pred_error: 0.05903651937842369 \t range_loss: 0.6155610084533691\n",
      "pred_error: 0.05868098512291908 \t range_loss: 0.6155864000320435\n",
      "pred_error: 0.05868073180317879 \t range_loss: 0.6155864000320435\n",
      "pred_error: 0.05858873575925827 \t range_loss: 0.6146016716957092\n",
      "pred_error: 0.05858851224184036 \t range_loss: 0.6146016716957092\n",
      "pred_error: 0.058869484812021255 \t range_loss: 0.6136905550956726\n",
      "pred_error: 0.058869484812021255 \t range_loss: 0.6136905550956726\n",
      "pred_error: 0.058818716555833817 \t range_loss: 0.6120110750198364\n",
      "pred_error: 0.05881884694099426 \t range_loss: 0.6120110750198364\n",
      "pred_error: 0.0580744631588459 \t range_loss: 0.6230184435844421\n",
      "Step: 464 \t Loss: 1.196777105331421\n",
      "pred_error: 0.05852667614817619 \t range_loss: 0.6168760061264038\n",
      "pred_error: 0.05874655395746231 \t range_loss: 0.6146471500396729\n",
      "pred_error: 0.05874662846326828 \t range_loss: 0.6146471500396729\n",
      "pred_error: 0.05953804776072502 \t range_loss: 0.6106211543083191\n",
      "pred_error: 0.05879312753677368 \t range_loss: 0.6134918332099915\n",
      "pred_error: 0.05938395857810974 \t range_loss: 0.609997034072876\n",
      "pred_error: 0.05866194888949394 \t range_loss: 0.6148335337638855\n",
      "pred_error: 0.058767884969711304 \t range_loss: 0.6128525137901306\n",
      "pred_error: 0.059258103370666504 \t range_loss: 0.6116454601287842\n",
      "pred_error: 0.058983057737350464 \t range_loss: 0.6133188009262085\n",
      "pred_error: 0.05942212790250778 \t range_loss: 0.609074056148529\n",
      "pred_error: 0.05939624086022377 \t range_loss: 0.609037458896637\n",
      "pred_error: 0.05939631164073944 \t range_loss: 0.609037458896637\n",
      "pred_error: 0.058598946779966354 \t range_loss: 0.6134341955184937\n",
      "pred_error: 0.05842244252562523 \t range_loss: 0.6142100095748901\n",
      "pred_error: 0.05874134600162506 \t range_loss: 0.612636923789978\n",
      "pred_error: 0.058741506189107895 \t range_loss: 0.612636923789978\n",
      "pred_error: 0.05874139443039894 \t range_loss: 0.6122944355010986\n",
      "pred_error: 0.05956437438726425 \t range_loss: 0.6090490221977234\n",
      "pred_error: 0.05869128927588463 \t range_loss: 0.6104763746261597\n",
      "pred_error: 0.059219636023044586 \t range_loss: 0.6119716763496399\n",
      "pred_error: 0.05884240195155144 \t range_loss: 0.6125363111495972\n",
      "pred_error: 0.05867009237408638 \t range_loss: 0.6194137334823608\n",
      "pred_error: 0.058652348816394806 \t range_loss: 0.6166882514953613\n",
      "pred_error: 0.058736402541399 \t range_loss: 0.6130853295326233\n",
      "pred_error: 0.05902304872870445 \t range_loss: 0.6116695404052734\n",
      "pred_error: 0.05902312695980072 \t range_loss: 0.6116695404052734\n",
      "pred_error: 0.05954647809267044 \t range_loss: 0.6078136563301086\n",
      "pred_error: 0.05903324857354164 \t range_loss: 0.6102297902107239\n",
      "pred_error: 0.059033311903476715 \t range_loss: 0.6102297902107239\n",
      "pred_error: 0.058610327541828156 \t range_loss: 0.6170828342437744\n",
      "pred_error: 0.05856075510382652 \t range_loss: 0.6169387102127075\n",
      "pred_error: 0.05959579721093178 \t range_loss: 0.606277585029602\n",
      "pred_error: 0.058774568140506744 \t range_loss: 0.6115882396697998\n",
      "pred_error: 0.058774858713150024 \t range_loss: 0.6115882396697998\n",
      "pred_error: 0.05951337516307831 \t range_loss: 0.6061708331108093\n",
      "pred_error: 0.058718565851449966 \t range_loss: 0.6103992462158203\n",
      "pred_error: 0.05885985493659973 \t range_loss: 0.6113196015357971\n",
      "pred_error: 0.059667084366083145 \t range_loss: 0.6118714213371277\n",
      "pred_error: 0.05896008759737015 \t range_loss: 0.6113007664680481\n",
      "pred_error: 0.05969931185245514 \t range_loss: 0.6110649108886719\n",
      "pred_error: 0.0594782829284668 \t range_loss: 0.6088525056838989\n",
      "pred_error: 0.05900721997022629 \t range_loss: 0.6101394891738892\n",
      "pred_error: 0.05900721997022629 \t range_loss: 0.6101394891738892\n",
      "pred_error: 0.058399613946676254 \t range_loss: 0.6201306581497192\n",
      "pred_error: 0.058741021901369095 \t range_loss: 0.6111478805541992\n",
      "pred_error: 0.058234576135873795 \t range_loss: 0.623365581035614\n",
      "pred_error: 0.05905240774154663 \t range_loss: 0.6082608103752136\n",
      "pred_error: 0.059052370488643646 \t range_loss: 0.6082608103752136\n",
      "pred_error: 0.05937949940562248 \t range_loss: 0.6109799146652222\n",
      "pred_error: 0.059394463896751404 \t range_loss: 0.6104928851127625\n",
      "pred_error: 0.059394631534814835 \t range_loss: 0.6104928851127625\n",
      "pred_error: 0.0585859976708889 \t range_loss: 0.6135594248771667\n",
      "pred_error: 0.05941377207636833 \t range_loss: 0.609134316444397\n",
      "pred_error: 0.05941691994667053 \t range_loss: 0.6104320883750916\n",
      "pred_error: 0.05879626050591469 \t range_loss: 0.6106855273246765\n",
      "pred_error: 0.05879591777920723 \t range_loss: 0.6106855273246765\n",
      "pred_error: 0.05844787880778313 \t range_loss: 0.6227545142173767\n",
      "pred_error: 0.05908913165330887 \t range_loss: 0.6113890409469604\n",
      "pred_error: 0.05882221460342407 \t range_loss: 0.6134430170059204\n",
      "pred_error: 0.05878463014960289 \t range_loss: 0.613457441329956\n",
      "pred_error: 0.058784667402505875 \t range_loss: 0.613457441329956\n",
      "pred_error: 0.05941799283027649 \t range_loss: 0.6107969284057617\n",
      "pred_error: 0.05879094451665878 \t range_loss: 0.6124283075332642\n",
      "pred_error: 0.058859311044216156 \t range_loss: 0.612527072429657\n",
      "pred_error: 0.058341626077890396 \t range_loss: 0.6229163408279419\n",
      "pred_error: 0.058825165033340454 \t range_loss: 0.6085795164108276\n",
      "pred_error: 0.05882510170340538 \t range_loss: 0.6085795164108276\n",
      "pred_error: 0.05906955152750015 \t range_loss: 0.6128115653991699\n",
      "pred_error: 0.059088416397571564 \t range_loss: 0.6095626950263977\n",
      "pred_error: 0.05874443054199219 \t range_loss: 0.6122864484786987\n",
      "pred_error: 0.05888223275542259 \t range_loss: 0.6127947568893433\n",
      "pred_error: 0.05875874683260918 \t range_loss: 0.6092995405197144\n",
      "pred_error: 0.05816099792718887 \t range_loss: 0.6193740367889404\n",
      "pred_error: 0.05876399204134941 \t range_loss: 0.6118662357330322\n",
      "pred_error: 0.05860782787203789 \t range_loss: 0.616188645362854\n",
      "pred_error: 0.05925752595067024 \t range_loss: 0.6112560629844666\n",
      "pred_error: 0.05887039005756378 \t range_loss: 0.6133862137794495\n",
      "pred_error: 0.058980632573366165 \t range_loss: 0.6121850609779358\n",
      "pred_error: 0.05828222632408142 \t range_loss: 0.6198924779891968\n",
      "pred_error: 0.05881894752383232 \t range_loss: 0.6102334260940552\n",
      "pred_error: 0.05845147743821144 \t range_loss: 0.6181395649909973\n",
      "pred_error: 0.05865092575550079 \t range_loss: 0.6140074729919434\n",
      "pred_error: 0.059166766703128815 \t range_loss: 0.6086302995681763\n",
      "pred_error: 0.05916677787899971 \t range_loss: 0.6086302995681763\n",
      "pred_error: 0.058441635221242905 \t range_loss: 0.6232215762138367\n",
      "pred_error: 0.058939360082149506 \t range_loss: 0.6099465489387512\n",
      "pred_error: 0.058854393661022186 \t range_loss: 0.6122884154319763\n",
      "pred_error: 0.05885443091392517 \t range_loss: 0.6122884154319763\n",
      "pred_error: 0.05929728224873543 \t range_loss: 0.6115182042121887\n",
      "pred_error: 0.05929729714989662 \t range_loss: 0.6115182042121887\n",
      "pred_error: 0.05920153856277466 \t range_loss: 0.6100160479545593\n",
      "pred_error: 0.059207987040281296 \t range_loss: 0.6100160479545593\n",
      "pred_error: 0.058965153992176056 \t range_loss: 0.6102014780044556\n",
      "pred_error: 0.058964964002370834 \t range_loss: 0.6102014780044556\n",
      "pred_error: 0.05890080705285072 \t range_loss: 0.6113842129707336\n",
      "pred_error: 0.05830388143658638 \t range_loss: 0.620306670665741\n",
      "pred_error: 0.05830388143658638 \t range_loss: 0.620306670665741\n",
      "pred_error: 0.058598946779966354 \t range_loss: 0.6150252819061279\n",
      "pred_error: 0.05859898403286934 \t range_loss: 0.6150252819061279\n",
      "pred_error: 0.05941038206219673 \t range_loss: 0.6123790144920349\n",
      "pred_error: 0.059450868517160416 \t range_loss: 0.6107308268547058\n",
      "pred_error: 0.05873129144310951 \t range_loss: 0.6125134825706482\n",
      "pred_error: 0.059386350214481354 \t range_loss: 0.6098395586013794\n",
      "pred_error: 0.05938631296157837 \t range_loss: 0.6098395586013794\n",
      "pred_error: 0.058564186096191406 \t range_loss: 0.612179160118103\n",
      "pred_error: 0.059425514191389084 \t range_loss: 0.6104913949966431\n",
      "pred_error: 0.05814178287982941 \t range_loss: 0.6209061741828918\n",
      "pred_error: 0.05925451219081879 \t range_loss: 0.6123066544532776\n",
      "pred_error: 0.059502974152565 \t range_loss: 0.6079379916191101\n",
      "Step: 719 \t Loss: 1.1958988904953003\n",
      "pred_error: 0.059231698513031006 \t range_loss: 0.6127498149871826\n",
      "pred_error: 0.05923179164528847 \t range_loss: 0.6127498149871826\n",
      "pred_error: 0.058941882103681564 \t range_loss: 0.6090702414512634\n",
      "pred_error: 0.05952422320842743 \t range_loss: 0.6092386841773987\n",
      "pred_error: 0.05916842818260193 \t range_loss: 0.6125643849372864\n",
      "pred_error: 0.05916842818260193 \t range_loss: 0.6125643849372864\n",
      "pred_error: 0.05916842818260193 \t range_loss: 0.6125643849372864\n",
      "pred_error: 0.059167999774217606 \t range_loss: 0.6125643849372864\n",
      "pred_error: 0.05834835767745972 \t range_loss: 0.6151196956634521\n",
      "pred_error: 0.05887071415781975 \t range_loss: 0.6141330003738403\n",
      "pred_error: 0.058870829641819 \t range_loss: 0.6141330003738403\n",
      "pred_error: 0.05956556648015976 \t range_loss: 0.61115962266922\n",
      "pred_error: 0.05895645171403885 \t range_loss: 0.6103834509849548\n",
      "pred_error: 0.0589563362300396 \t range_loss: 0.6103834509849548\n",
      "pred_error: 0.059447865933179855 \t range_loss: 0.608238935470581\n",
      "pred_error: 0.05944785848259926 \t range_loss: 0.608238935470581\n",
      "pred_error: 0.05878525972366333 \t range_loss: 0.6120500564575195\n",
      "pred_error: 0.05923835188150406 \t range_loss: 0.6107502579689026\n",
      "pred_error: 0.0592227540910244 \t range_loss: 0.6095976233482361\n",
      "pred_error: 0.05877041071653366 \t range_loss: 0.6128671765327454\n",
      "pred_error: 0.05877033993601799 \t range_loss: 0.6128671765327454\n",
      "pred_error: 0.058364979922771454 \t range_loss: 0.6177935600280762\n",
      "pred_error: 0.059048060327768326 \t range_loss: 0.6096864938735962\n",
      "pred_error: 0.05871133878827095 \t range_loss: 0.6149346828460693\n",
      "pred_error: 0.05886291339993477 \t range_loss: 0.6090160608291626\n",
      "pred_error: 0.05848436802625656 \t range_loss: 0.6196525692939758\n",
      "pred_error: 0.05935042351484299 \t range_loss: 0.6076717972755432\n",
      "pred_error: 0.058709681034088135 \t range_loss: 0.617320716381073\n",
      "pred_error: 0.059576794505119324 \t range_loss: 0.6074212193489075\n",
      "pred_error: 0.059055719524621964 \t range_loss: 0.6121842265129089\n",
      "pred_error: 0.0592610202729702 \t range_loss: 0.6113948225975037\n",
      "BEST LOSS: 1.1958989\n",
      "==== Model: block7_cob_activation_norm  in Layer: 7 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 16:32:15,567 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 16:36:01,611 model.rs:1246 value (-196608) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 16:36:01,631 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 16:36:31,200 model.rs:1246 value (-196608) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 16:36:31,206 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 16:36:31,223 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 16:36:31,243 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 16:36:31,262 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error   | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000007581326 | 0.00006842613 | 0.000642363 | -0.00049084425 | 0.00006235357  | 0.00006842613    | 0.000642363   | 0             | 0.0000000069747355 | -0.0009197648      | 0.0023834805           |\n",
      "+-----------------+---------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 584818 64 [-700576, 718768] 1 [16]\n",
      "===============================\n",
      "==== Model: block7_cob_activation_norm_teleported  in Layer: 7 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 16:37:02,107 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 16:40:38,061 model.rs:1246 value (-158912) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 16:40:38,078 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 16:41:07,722 model.rs:1246 value (-158912) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 16:41:07,728 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 16:41:07,745 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 16:41:07,770 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 16:41:07,783 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+--------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error         | median_error  | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+--------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.000000034598195 | 0.00010097027 | 0.0005866159 | -0.00056034327 | 0.00006374875  | 0.00010097027    | 0.0005866159  | 0             | 0.0000000072517707 | -0.0000019145602   | 0.0012876609           |\n",
      "+--------------------+---------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 584822 64 [-567132, 358556] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 8 , \t  activation_stats: {'relu_1': {'norm': tensor(1188.2712), 'max': tensor(6.0490), 'min': tensor(-11.4612), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(17.5103)\n",
      "pred_error: 0.061904825270175934 \t range_loss: 0.9945582151412964\n",
      "Step: 0 \t Loss: 1.585130214691162\n",
      "pred_error: 0.06184700131416321 \t range_loss: 0.9666581153869629\n",
      "Step: 1 \t Loss: 1.5700312852859497\n",
      "Step: 2 \t Loss: 1.5506441593170166\n",
      "Step: 3 \t Loss: 1.5344512462615967\n",
      "Step: 4 \t Loss: 1.5193147659301758\n",
      "Step: 5 \t Loss: 1.514835238456726\n",
      "pred_error: 0.061745189130306244 \t range_loss: 0.8973859548568726\n",
      "Step: 6 \t Loss: 1.5033605098724365\n",
      "pred_error: 0.06183761730790138 \t range_loss: 0.8849843144416809\n",
      "Step: 7 \t Loss: 1.496503472328186\n",
      "Step: 8 \t Loss: 1.4918930530548096\n",
      "pred_error: 0.06175064668059349 \t range_loss: 0.8743813633918762\n",
      "Step: 9 \t Loss: 1.4869221448898315\n",
      "Step: 10 \t Loss: 1.4830303192138672\n",
      "Step: 11 \t Loss: 1.4802050590515137\n",
      "Step: 12 \t Loss: 1.4779200553894043\n",
      "pred_error: 0.06180432811379433 \t range_loss: 0.859876811504364\n",
      "Step: 13 \t Loss: 1.475904941558838\n",
      "pred_error: 0.0618736706674099 \t range_loss: 0.8571701645851135\n",
      "pred_error: 0.06187322735786438 \t range_loss: 0.8571701645851135\n",
      "Step: 14 \t Loss: 1.4739556312561035\n",
      "pred_error: 0.06181758642196655 \t range_loss: 0.8557790517807007\n",
      "Step: 15 \t Loss: 1.4716604948043823\n",
      "pred_error: 0.06181427836418152 \t range_loss: 0.8535159826278687\n",
      "Step: 16 \t Loss: 1.467978835105896\n",
      "pred_error: 0.061793699860572815 \t range_loss: 0.8500417470932007\n",
      "pred_error: 0.06179366260766983 \t range_loss: 0.8500417470932007\n",
      "Step: 17 \t Loss: 1.4648019075393677\n",
      "Step: 18 \t Loss: 1.4642393589019775\n",
      "pred_error: 0.061764758080244064 \t range_loss: 0.8465917110443115\n",
      "Step: 19 \t Loss: 1.4636030197143555\n",
      "Step: 20 \t Loss: 1.4615530967712402\n",
      "pred_error: 0.06191692873835564 \t range_loss: 0.8423812389373779\n",
      "Step: 21 \t Loss: 1.457877516746521\n",
      "pred_error: 0.061869993805885315 \t range_loss: 0.839177668094635\n",
      "pred_error: 0.06186998263001442 \t range_loss: 0.839177668094635\n",
      "Step: 22 \t Loss: 1.4537441730499268\n",
      "Step: 23 \t Loss: 1.453641653060913\n",
      "Step: 24 \t Loss: 1.450727105140686\n",
      "Step: 25 \t Loss: 1.44541597366333\n",
      "pred_error: 0.062074754387140274 \t range_loss: 0.8246683478355408\n",
      "pred_error: 0.06207475811243057 \t range_loss: 0.8246683478355408\n",
      "pred_error: 0.06207483261823654 \t range_loss: 0.8246683478355408\n",
      "Step: 26 \t Loss: 1.4426281452178955\n",
      "Step: 27 \t Loss: 1.439225196838379\n",
      "pred_error: 0.062074825167655945 \t range_loss: 0.8184769749641418\n",
      "Step: 28 \t Loss: 1.4365061521530151\n",
      "pred_error: 0.06209680438041687 \t range_loss: 0.8155375719070435\n",
      "Step: 29 \t Loss: 1.4363737106323242\n",
      "pred_error: 0.0621485710144043 \t range_loss: 0.8148875832557678\n",
      "Step: 30 \t Loss: 1.4348316192626953\n",
      "Step: 31 \t Loss: 1.4334805011749268\n",
      "pred_error: 0.06221483647823334 \t range_loss: 0.8113321661949158\n",
      "Step: 32 \t Loss: 1.4326317310333252\n",
      "Step: 33 \t Loss: 1.4305453300476074\n",
      "Step: 34 \t Loss: 1.4297113418579102\n",
      "Step: 36 \t Loss: 1.4286856651306152\n",
      "pred_error: 0.06252582371234894 \t range_loss: 0.8034274578094482\n",
      "Step: 37 \t Loss: 1.4268825054168701\n",
      "pred_error: 0.062443919479846954 \t range_loss: 0.8024432063102722\n",
      "Step: 38 \t Loss: 1.4255852699279785\n",
      "pred_error: 0.06240558996796608 \t range_loss: 0.8015293478965759\n",
      "Step: 39 \t Loss: 1.4238333702087402\n",
      "pred_error: 0.062376949936151505 \t range_loss: 0.8000637888908386\n",
      "Step: 41 \t Loss: 1.4233118295669556\n",
      "pred_error: 0.06258194893598557 \t range_loss: 0.7974908351898193\n",
      "Step: 42 \t Loss: 1.4222819805145264\n",
      "pred_error: 0.06256143003702164 \t range_loss: 0.7966693639755249\n",
      "Step: 43 \t Loss: 1.419661283493042\n",
      "Step: 44 \t Loss: 1.4189318418502808\n",
      "Step: 45 \t Loss: 1.4189250469207764\n",
      "pred_error: 0.0626634955406189 \t range_loss: 0.7935985326766968\n",
      "pred_error: 0.06266338378190994 \t range_loss: 0.7935985326766968\n",
      "pred_error: 0.06266326457262039 \t range_loss: 0.7935985326766968\n",
      "Step: 48 \t Loss: 1.414830207824707\n",
      "Step: 49 \t Loss: 1.4147493839263916\n",
      "Step: 50 \t Loss: 1.4144439697265625\n",
      "pred_error: 0.06305288523435593 \t range_loss: 0.7839188575744629\n",
      "pred_error: 0.06305248290300369 \t range_loss: 0.7839188575744629\n",
      "Step: 51 \t Loss: 1.4136812686920166\n",
      "pred_error: 0.06300964951515198 \t range_loss: 0.783584713935852\n",
      "pred_error: 0.06300964951515198 \t range_loss: 0.783584713935852\n",
      "pred_error: 0.06307902932167053 \t range_loss: 0.7838689684867859\n",
      "Step: 54 \t Loss: 1.4096243381500244\n",
      "pred_error: 0.06309686601161957 \t range_loss: 0.7786556482315063\n",
      "Step: 55 \t Loss: 1.4072216749191284\n",
      "Step: 56 \t Loss: 1.406403660774231\n",
      "pred_error: 0.06300826370716095 \t range_loss: 0.7763210535049438\n",
      "Step: 57 \t Loss: 1.4044499397277832\n",
      "Step: 60 \t Loss: 1.4026927947998047\n",
      "pred_error: 0.06313823163509369 \t range_loss: 0.7713103890419006\n",
      "Step: 61 \t Loss: 1.401310682296753\n",
      "pred_error: 0.06319308280944824 \t range_loss: 0.7693764567375183\n",
      "Step: 62 \t Loss: 1.4007724523544312\n",
      "pred_error: 0.06357788294553757 \t range_loss: 0.7668706178665161\n",
      "pred_error: 0.06357768923044205 \t range_loss: 0.7668706178665161\n",
      "Step: 68 \t Loss: 1.4002928733825684\n",
      "Step: 69 \t Loss: 1.3985638618469238\n",
      "Step: 70 \t Loss: 1.3974449634552002\n",
      "pred_error: 0.06378907710313797 \t range_loss: 0.7616081237792969\n",
      "pred_error: 0.06406503915786743 \t range_loss: 0.761036217212677\n",
      "Step: 78 \t Loss: 1.396549105644226\n",
      "Step: 79 \t Loss: 1.3954546451568604\n",
      "pred_error: 0.06398425996303558 \t range_loss: 0.7556111216545105\n",
      "Step: 80 \t Loss: 1.3947548866271973\n",
      "Step: 81 \t Loss: 1.3939719200134277\n",
      "Step: 82 \t Loss: 1.3923382759094238\n",
      "pred_error: 0.06403452903032303 \t range_loss: 0.7519930601119995\n",
      "pred_error: 0.06403452903032303 \t range_loss: 0.7519930601119995\n",
      "pred_error: 0.06401338428258896 \t range_loss: 0.7527169585227966\n",
      "Step: 85 \t Loss: 1.3923242092132568\n",
      "Step: 88 \t Loss: 1.3916349411010742\n",
      "Step: 89 \t Loss: 1.390756368637085\n",
      "pred_error: 0.06427151709794998 \t range_loss: 0.7480404376983643\n",
      "pred_error: 0.06470756232738495 \t range_loss: 0.7477092742919922\n",
      "pred_error: 0.06462670117616653 \t range_loss: 0.7469751238822937\n",
      "Step: 98 \t Loss: 1.3896360397338867\n",
      "pred_error: 0.06467942148447037 \t range_loss: 0.7428420782089233\n",
      "Step: 99 \t Loss: 1.3887572288513184\n",
      "pred_error: 0.06483933329582214 \t range_loss: 0.7409104108810425\n",
      "pred_error: 0.0652494877576828 \t range_loss: 0.7405684590339661\n",
      "Step: 106 \t Loss: 1.387923240661621\n",
      "pred_error: 0.06499603390693665 \t range_loss: 0.7379629015922546\n",
      "pred_error: 0.06551701575517654 \t range_loss: 0.7370901703834534\n",
      "Step: 113 \t Loss: 1.3878941535949707\n",
      "pred_error: 0.06558138132095337 \t range_loss: 0.7320803999900818\n",
      "Step: 114 \t Loss: 1.3858110904693604\n",
      "pred_error: 0.06570182740688324 \t range_loss: 0.7317172884941101\n",
      "pred_error: 0.06570171564817429 \t range_loss: 0.7317172884941101\n",
      "pred_error: 0.0657559409737587 \t range_loss: 0.7295058369636536\n",
      "pred_error: 0.06575620919466019 \t range_loss: 0.7295058369636536\n",
      "pred_error: 0.06593210995197296 \t range_loss: 0.7303315997123718\n",
      "pred_error: 0.06595401465892792 \t range_loss: 0.7295706868171692\n",
      "pred_error: 0.06603707373142242 \t range_loss: 0.7293812036514282\n",
      "pred_error: 0.06597425043582916 \t range_loss: 0.7293872237205505\n",
      "pred_error: 0.06601349264383316 \t range_loss: 0.7308882474899292\n",
      "pred_error: 0.06601349264383316 \t range_loss: 0.7308882474899292\n",
      "pred_error: 0.06617755442857742 \t range_loss: 0.7260762453079224\n",
      "pred_error: 0.06629853695631027 \t range_loss: 0.7246041297912598\n",
      "pred_error: 0.06629856675863266 \t range_loss: 0.7250163555145264\n",
      "Step: 138 \t Loss: 1.3855849504470825\n",
      "Step: 139 \t Loss: 1.384033203125\n",
      "Step: 140 \t Loss: 1.3834444284439087\n",
      "pred_error: 0.06630752980709076 \t range_loss: 0.7234659790992737\n",
      "pred_error: 0.06621955335140228 \t range_loss: 0.7235914468765259\n",
      "pred_error: 0.06641459465026855 \t range_loss: 0.7222146987915039\n",
      "pred_error: 0.06641489267349243 \t range_loss: 0.7222146987915039\n",
      "pred_error: 0.06659483909606934 \t range_loss: 0.7226662635803223\n",
      "pred_error: 0.06659449636936188 \t range_loss: 0.7226662635803223\n",
      "pred_error: 0.066701740026474 \t range_loss: 0.722159743309021\n",
      "pred_error: 0.06670169532299042 \t range_loss: 0.722159743309021\n",
      "pred_error: 0.06650024652481079 \t range_loss: 0.7203314304351807\n",
      "pred_error: 0.06650031358003616 \t range_loss: 0.7203314304351807\n",
      "Step: 150 \t Loss: 1.3833749294281006\n",
      "pred_error: 0.06649880111217499 \t range_loss: 0.7194985151290894\n",
      "pred_error: 0.06655428558588028 \t range_loss: 0.7190783619880676\n",
      "pred_error: 0.06655470281839371 \t range_loss: 0.7190783619880676\n",
      "pred_error: 0.0668569952249527 \t range_loss: 0.718315839767456\n",
      "Step: 160 \t Loss: 1.3823158740997314\n",
      "Step: 161 \t Loss: 1.3809535503387451\n",
      "pred_error: 0.06666190177202225 \t range_loss: 0.7150255441665649\n",
      "pred_error: 0.06665899604558945 \t range_loss: 0.7175937294960022\n",
      "pred_error: 0.06709141284227371 \t range_loss: 0.7150565981864929\n",
      "pred_error: 0.06704192608594894 \t range_loss: 0.7142267227172852\n",
      "pred_error: 0.06697681546211243 \t range_loss: 0.713797390460968\n",
      "pred_error: 0.06709008663892746 \t range_loss: 0.712872326374054\n",
      "pred_error: 0.06729290634393692 \t range_loss: 0.7129952311515808\n",
      "pred_error: 0.06724963337182999 \t range_loss: 0.7129157185554504\n",
      "pred_error: 0.06716567277908325 \t range_loss: 0.7132749557495117\n",
      "pred_error: 0.06719235330820084 \t range_loss: 0.7123953700065613\n",
      "pred_error: 0.0672992467880249 \t range_loss: 0.7111665606498718\n",
      "pred_error: 0.06729939579963684 \t range_loss: 0.7111665606498718\n",
      "pred_error: 0.0674499124288559 \t range_loss: 0.7089847326278687\n",
      "pred_error: 0.06733212620019913 \t range_loss: 0.7093958854675293\n",
      "pred_error: 0.06739205121994019 \t range_loss: 0.7094718813896179\n",
      "pred_error: 0.06731196492910385 \t range_loss: 0.7090006470680237\n",
      "pred_error: 0.067142054438591 \t range_loss: 0.7099186778068542\n",
      "pred_error: 0.0674571841955185 \t range_loss: 0.7104188203811646\n",
      "pred_error: 0.0676642581820488 \t range_loss: 0.7101821899414062\n",
      "pred_error: 0.06778810173273087 \t range_loss: 0.7120939493179321\n",
      "pred_error: 0.0676962360739708 \t range_loss: 0.7111315131187439\n",
      "pred_error: 0.06769762188196182 \t range_loss: 0.7111315131187439\n",
      "pred_error: 0.06759623438119888 \t range_loss: 0.7095592617988586\n",
      "pred_error: 0.06759601831436157 \t range_loss: 0.7095592617988586\n",
      "pred_error: 0.06759610027074814 \t range_loss: 0.7095592617988586\n",
      "pred_error: 0.06745205819606781 \t range_loss: 0.7096369862556458\n",
      "pred_error: 0.06741353869438171 \t range_loss: 0.7095274329185486\n",
      "pred_error: 0.06725329905748367 \t range_loss: 0.7110596895217896\n",
      "pred_error: 0.06725350767374039 \t range_loss: 0.7110596895217896\n",
      "pred_error: 0.06725340336561203 \t range_loss: 0.7110596895217896\n",
      "pred_error: 0.06740684807300568 \t range_loss: 0.7092954516410828\n",
      "pred_error: 0.06740702688694 \t range_loss: 0.7092954516410828\n",
      "pred_error: 0.06760092079639435 \t range_loss: 0.708824872970581\n",
      "pred_error: 0.06755968928337097 \t range_loss: 0.709123969078064\n",
      "Step: 219 \t Loss: 1.380866527557373\n",
      "pred_error: 0.06739860028028488 \t range_loss: 0.7094160914421082\n",
      "pred_error: 0.06756450980901718 \t range_loss: 0.7097343802452087\n",
      "pred_error: 0.06747499108314514 \t range_loss: 0.708772599697113\n",
      "pred_error: 0.06775836646556854 \t range_loss: 0.7079291343688965\n",
      "pred_error: 0.06765606999397278 \t range_loss: 0.7078776359558105\n",
      "pred_error: 0.06765607744455338 \t range_loss: 0.7078776359558105\n",
      "Step: 236 \t Loss: 1.3804922103881836\n",
      "pred_error: 0.06779292970895767 \t range_loss: 0.7058264017105103\n",
      "pred_error: 0.06779292225837708 \t range_loss: 0.7058264017105103\n",
      "pred_error: 0.06758520752191544 \t range_loss: 0.705168604850769\n",
      "pred_error: 0.06758509576320648 \t range_loss: 0.705168604850769\n",
      "pred_error: 0.06766495853662491 \t range_loss: 0.7062917351722717\n",
      "pred_error: 0.06766506284475327 \t range_loss: 0.7062917351722717\n",
      "pred_error: 0.06761840730905533 \t range_loss: 0.7061348557472229\n",
      "pred_error: 0.06761840730905533 \t range_loss: 0.7061348557472229\n",
      "pred_error: 0.06810171902179718 \t range_loss: 0.7066095471382141\n",
      "pred_error: 0.06810173392295837 \t range_loss: 0.7066095471382141\n",
      "pred_error: 0.06810171157121658 \t range_loss: 0.7066095471382141\n",
      "pred_error: 0.0681089237332344 \t range_loss: 0.7058661580085754\n",
      "pred_error: 0.06791851669549942 \t range_loss: 0.7056795358657837\n",
      "Step: 251 \t Loss: 1.3803176879882812\n",
      "pred_error: 0.06794491410255432 \t range_loss: 0.7080749869346619\n",
      "pred_error: 0.06795740872621536 \t range_loss: 0.7059046626091003\n",
      "pred_error: 0.06784900277853012 \t range_loss: 0.7061232924461365\n",
      "pred_error: 0.06774244457483292 \t range_loss: 0.7045136094093323\n",
      "pred_error: 0.06786296516656876 \t range_loss: 0.704571545124054\n",
      "pred_error: 0.06786303222179413 \t range_loss: 0.704571545124054\n",
      "pred_error: 0.06774451583623886 \t range_loss: 0.7050482630729675\n",
      "pred_error: 0.06784795969724655 \t range_loss: 0.7033873796463013\n",
      "pred_error: 0.06783848255872726 \t range_loss: 0.7032991647720337\n",
      "pred_error: 0.06818603724241257 \t range_loss: 0.7027139663696289\n",
      "pred_error: 0.06811229884624481 \t range_loss: 0.7040005326271057\n",
      "pred_error: 0.06802790611982346 \t range_loss: 0.7035574316978455\n",
      "pred_error: 0.06809300929307938 \t range_loss: 0.703842282295227\n",
      "pred_error: 0.06781841814517975 \t range_loss: 0.7031345963478088\n",
      "pred_error: 0.06795068830251694 \t range_loss: 0.7056117057800293\n",
      "pred_error: 0.06811797618865967 \t range_loss: 0.7047692537307739\n",
      "pred_error: 0.06798300892114639 \t range_loss: 0.7035776972770691\n",
      "pred_error: 0.06798308342695236 \t range_loss: 0.7035776972770691\n",
      "pred_error: 0.06798294186592102 \t range_loss: 0.7035776972770691\n",
      "pred_error: 0.06811974942684174 \t range_loss: 0.7019400000572205\n",
      "pred_error: 0.06802452355623245 \t range_loss: 0.7013003826141357\n",
      "pred_error: 0.06793596595525742 \t range_loss: 0.7014892101287842\n",
      "Step: 299 \t Loss: 1.3788127899169922\n",
      "Step: 300 \t Loss: 1.3785045146942139\n",
      "pred_error: 0.06805574893951416 \t range_loss: 0.700463056564331\n",
      "pred_error: 0.06796487420797348 \t range_loss: 0.7011683583259583\n",
      "pred_error: 0.06854430586099625 \t range_loss: 0.6987833976745605\n",
      "pred_error: 0.06855005770921707 \t range_loss: 0.700900137424469\n",
      "pred_error: 0.0685347244143486 \t range_loss: 0.6997414231300354\n",
      "pred_error: 0.06825262308120728 \t range_loss: 0.7020364999771118\n",
      "pred_error: 0.06825277209281921 \t range_loss: 0.7020364999771118\n",
      "pred_error: 0.06841902434825897 \t range_loss: 0.6989052295684814\n",
      "pred_error: 0.06834933161735535 \t range_loss: 0.6980918049812317\n",
      "pred_error: 0.068203404545784 \t range_loss: 0.7010663747787476\n",
      "pred_error: 0.06844402104616165 \t range_loss: 0.7002542018890381\n",
      "pred_error: 0.06856431812047958 \t range_loss: 0.7004168629646301\n",
      "pred_error: 0.06850947439670563 \t range_loss: 0.7005399465560913\n",
      "pred_error: 0.06836715340614319 \t range_loss: 0.7007578015327454\n",
      "pred_error: 0.06836714595556259 \t range_loss: 0.7007578015327454\n",
      "pred_error: 0.06836089491844177 \t range_loss: 0.7000434994697571\n",
      "pred_error: 0.06853874027729034 \t range_loss: 0.7007737755775452\n",
      "pred_error: 0.06856347620487213 \t range_loss: 0.6993703842163086\n",
      "pred_error: 0.06850486248731613 \t range_loss: 0.6982978582382202\n",
      "pred_error: 0.06827367842197418 \t range_loss: 0.7013996839523315\n",
      "pred_error: 0.06854138523340225 \t range_loss: 0.698513925075531\n",
      "pred_error: 0.06854112446308136 \t range_loss: 0.698513925075531\n",
      "pred_error: 0.06835171580314636 \t range_loss: 0.6982030868530273\n",
      "pred_error: 0.06851667910814285 \t range_loss: 0.6978833675384521\n",
      "pred_error: 0.06833740323781967 \t range_loss: 0.6978594064712524\n",
      "pred_error: 0.06833715736865997 \t range_loss: 0.6978594064712524\n",
      "pred_error: 0.06865019351243973 \t range_loss: 0.6977065801620483\n",
      "pred_error: 0.06879176944494247 \t range_loss: 0.6968582272529602\n",
      "pred_error: 0.06887811422348022 \t range_loss: 0.6972429752349854\n",
      "pred_error: 0.0688304752111435 \t range_loss: 0.6990185379981995\n",
      "pred_error: 0.06880156695842743 \t range_loss: 0.6970459818840027\n",
      "pred_error: 0.06880172342061996 \t range_loss: 0.6970459818840027\n",
      "pred_error: 0.06865166872739792 \t range_loss: 0.6962279081344604\n",
      "pred_error: 0.06861135363578796 \t range_loss: 0.6956745982170105\n",
      "pred_error: 0.0686018168926239 \t range_loss: 0.6958013772964478\n",
      "pred_error: 0.06860177218914032 \t range_loss: 0.6958013772964478\n",
      "pred_error: 0.06872981786727905 \t range_loss: 0.6953007578849792\n",
      "pred_error: 0.06902792304754257 \t range_loss: 0.6959214210510254\n",
      "pred_error: 0.06902787834405899 \t range_loss: 0.6959214210510254\n",
      "pred_error: 0.06848973035812378 \t range_loss: 0.6970957517623901\n",
      "pred_error: 0.06864872574806213 \t range_loss: 0.6985154747962952\n",
      "pred_error: 0.06877222657203674 \t range_loss: 0.6942309737205505\n",
      "pred_error: 0.06862141937017441 \t range_loss: 0.6933003067970276\n",
      "pred_error: 0.06845229119062424 \t range_loss: 0.6958311796188354\n",
      "pred_error: 0.06895507872104645 \t range_loss: 0.6962310671806335\n",
      "pred_error: 0.06882540136575699 \t range_loss: 0.6966133117675781\n",
      "pred_error: 0.06882534176111221 \t range_loss: 0.6966133117675781\n",
      "pred_error: 0.06882509589195251 \t range_loss: 0.6966133117675781\n",
      "pred_error: 0.06854994595050812 \t range_loss: 0.696045458316803\n",
      "pred_error: 0.068543441593647 \t range_loss: 0.6955634951591492\n",
      "pred_error: 0.06854334473609924 \t range_loss: 0.6955634951591492\n",
      "pred_error: 0.068808913230896 \t range_loss: 0.6964519023895264\n",
      "pred_error: 0.06873951852321625 \t range_loss: 0.6969912648200989\n",
      "pred_error: 0.06857231259346008 \t range_loss: 0.6971319913864136\n",
      "pred_error: 0.06870640069246292 \t range_loss: 0.6966487765312195\n",
      "pred_error: 0.0684971809387207 \t range_loss: 0.6969481706619263\n",
      "Step: 446 \t Loss: 1.3783537149429321\n",
      "pred_error: 0.06867952644824982 \t range_loss: 0.6992766857147217\n",
      "pred_error: 0.06837829202413559 \t range_loss: 0.6972288489341736\n",
      "pred_error: 0.06851339340209961 \t range_loss: 0.6949064135551453\n",
      "pred_error: 0.06851337850093842 \t range_loss: 0.6949064135551453\n",
      "pred_error: 0.06885714828968048 \t range_loss: 0.6946750283241272\n",
      "pred_error: 0.0685381069779396 \t range_loss: 0.6955426335334778\n",
      "pred_error: 0.06874757260084152 \t range_loss: 0.6936923861503601\n",
      "pred_error: 0.06874753534793854 \t range_loss: 0.6936923861503601\n",
      "pred_error: 0.06877351552248001 \t range_loss: 0.6938338875770569\n",
      "pred_error: 0.06877356022596359 \t range_loss: 0.6938338875770569\n",
      "pred_error: 0.06896452605724335 \t range_loss: 0.694590151309967\n",
      "pred_error: 0.06886940449476242 \t range_loss: 0.693737268447876\n",
      "pred_error: 0.0687028169631958 \t range_loss: 0.6943668127059937\n",
      "pred_error: 0.06868729740381241 \t range_loss: 0.6934456825256348\n",
      "pred_error: 0.0686386376619339 \t range_loss: 0.6956158876419067\n",
      "pred_error: 0.06865143775939941 \t range_loss: 0.6940053105354309\n",
      "pred_error: 0.06878933310508728 \t range_loss: 0.6951457262039185\n",
      "pred_error: 0.06878930330276489 \t range_loss: 0.6951457262039185\n",
      "pred_error: 0.06878932565450668 \t range_loss: 0.6951457262039185\n",
      "pred_error: 0.0688491091132164 \t range_loss: 0.6948278546333313\n",
      "pred_error: 0.0686626136302948 \t range_loss: 0.6942505836486816\n",
      "pred_error: 0.0688716247677803 \t range_loss: 0.6949465870857239\n",
      "pred_error: 0.06878216564655304 \t range_loss: 0.6937078833580017\n",
      "pred_error: 0.06878210604190826 \t range_loss: 0.6937078833580017\n",
      "pred_error: 0.0687345638871193 \t range_loss: 0.6933302879333496\n",
      "pred_error: 0.06910750269889832 \t range_loss: 0.6947870254516602\n",
      "pred_error: 0.06910139322280884 \t range_loss: 0.6948840022087097\n",
      "pred_error: 0.06918638199567795 \t range_loss: 0.6948590278625488\n",
      "pred_error: 0.06906456500291824 \t range_loss: 0.6940422654151917\n",
      "pred_error: 0.06906717270612717 \t range_loss: 0.6940422654151917\n",
      "pred_error: 0.06894110143184662 \t range_loss: 0.6941303014755249\n",
      "pred_error: 0.06894107162952423 \t range_loss: 0.6941303014755249\n",
      "pred_error: 0.06894104182720184 \t range_loss: 0.6941303014755249\n",
      "pred_error: 0.06882455199956894 \t range_loss: 0.6936314702033997\n",
      "pred_error: 0.06888791173696518 \t range_loss: 0.6979591846466064\n",
      "pred_error: 0.06906650960445404 \t range_loss: 0.6951593160629272\n",
      "pred_error: 0.06906654685735703 \t range_loss: 0.6951593160629272\n",
      "pred_error: 0.06878972053527832 \t range_loss: 0.6937481760978699\n",
      "pred_error: 0.0686504989862442 \t range_loss: 0.693293571472168\n",
      "pred_error: 0.0686504989862442 \t range_loss: 0.693293571472168\n",
      "pred_error: 0.0686504989862442 \t range_loss: 0.693293571472168\n",
      "pred_error: 0.06889021396636963 \t range_loss: 0.6926687359809875\n",
      "pred_error: 0.0691506639122963 \t range_loss: 0.694095253944397\n",
      "pred_error: 0.06909917294979095 \t range_loss: 0.693998396396637\n",
      "pred_error: 0.06913088262081146 \t range_loss: 0.6939493417739868\n",
      "pred_error: 0.06913088262081146 \t range_loss: 0.6939493417739868\n",
      "pred_error: 0.06877566874027252 \t range_loss: 0.6944047212600708\n",
      "pred_error: 0.06880343705415726 \t range_loss: 0.6953632831573486\n",
      "pred_error: 0.0687805563211441 \t range_loss: 0.6960462927818298\n",
      "pred_error: 0.06883568316698074 \t range_loss: 0.6947953104972839\n",
      "pred_error: 0.06862736493349075 \t range_loss: 0.6952000856399536\n",
      "pred_error: 0.068580262362957 \t range_loss: 0.6957191228866577\n",
      "pred_error: 0.06850805133581161 \t range_loss: 0.6959599852561951\n",
      "pred_error: 0.06897144764661789 \t range_loss: 0.6951963901519775\n",
      "pred_error: 0.06897133588790894 \t range_loss: 0.6951963901519775\n",
      "pred_error: 0.06857780367136002 \t range_loss: 0.6941788792610168\n",
      "pred_error: 0.06893891841173172 \t range_loss: 0.6968464851379395\n",
      "pred_error: 0.06885676831007004 \t range_loss: 0.6950621008872986\n",
      "pred_error: 0.06860239803791046 \t range_loss: 0.6950372457504272\n",
      "pred_error: 0.06859678030014038 \t range_loss: 0.696363091468811\n",
      "pred_error: 0.0686555802822113 \t range_loss: 0.6952245831489563\n",
      "pred_error: 0.06874603778123856 \t range_loss: 0.6950574517250061\n",
      "pred_error: 0.06858791410923004 \t range_loss: 0.6992731094360352\n",
      "pred_error: 0.06837686151266098 \t range_loss: 0.6967050433158875\n",
      "pred_error: 0.06837688386440277 \t range_loss: 0.6967050433158875\n",
      "pred_error: 0.06836293637752533 \t range_loss: 0.6983227133750916\n",
      "pred_error: 0.06842616945505142 \t range_loss: 0.694115161895752\n",
      "pred_error: 0.06842619180679321 \t range_loss: 0.694115161895752\n",
      "pred_error: 0.06865161657333374 \t range_loss: 0.6946914196014404\n",
      "pred_error: 0.06865102052688599 \t range_loss: 0.694947361946106\n",
      "pred_error: 0.06865085661411285 \t range_loss: 0.694947361946106\n",
      "pred_error: 0.06877346336841583 \t range_loss: 0.6945506930351257\n",
      "pred_error: 0.06869658082723618 \t range_loss: 0.6942840218544006\n",
      "Step: 620 \t Loss: 1.3768565654754639\n",
      "pred_error: 0.06836175918579102 \t range_loss: 0.6944546103477478\n",
      "pred_error: 0.0687422826886177 \t range_loss: 0.6957624554634094\n",
      "pred_error: 0.06892051547765732 \t range_loss: 0.6935465931892395\n",
      "pred_error: 0.06868107616901398 \t range_loss: 0.6928704977035522\n",
      "pred_error: 0.06866062432527542 \t range_loss: 0.6941922307014465\n",
      "pred_error: 0.06883789598941803 \t range_loss: 0.6943612098693848\n",
      "pred_error: 0.0688372477889061 \t range_loss: 0.6943612098693848\n",
      "pred_error: 0.06864023953676224 \t range_loss: 0.6937204599380493\n",
      "pred_error: 0.06856571137905121 \t range_loss: 0.6940953135490417\n",
      "pred_error: 0.06855320185422897 \t range_loss: 0.6936604380607605\n",
      "pred_error: 0.06855320185422897 \t range_loss: 0.6936604380607605\n",
      "pred_error: 0.06855320185422897 \t range_loss: 0.6936604380607605\n",
      "pred_error: 0.06862372905015945 \t range_loss: 0.6930963397026062\n",
      "pred_error: 0.06842684000730515 \t range_loss: 0.694604754447937\n",
      "pred_error: 0.06871087104082108 \t range_loss: 0.693971574306488\n",
      "pred_error: 0.06871094554662704 \t range_loss: 0.693971574306488\n",
      "pred_error: 0.06871084868907928 \t range_loss: 0.693971574306488\n",
      "pred_error: 0.0686502754688263 \t range_loss: 0.6936766505241394\n",
      "pred_error: 0.06853724271059036 \t range_loss: 0.6946326494216919\n",
      "pred_error: 0.0690615326166153 \t range_loss: 0.6950740814208984\n",
      "pred_error: 0.06897296756505966 \t range_loss: 0.6943099498748779\n",
      "pred_error: 0.06877155601978302 \t range_loss: 0.6951529383659363\n",
      "pred_error: 0.06877153366804123 \t range_loss: 0.6951529383659363\n",
      "pred_error: 0.06843094527721405 \t range_loss: 0.6942402124404907\n",
      "pred_error: 0.06873954087495804 \t range_loss: 0.6981533765792847\n",
      "pred_error: 0.06877375394105911 \t range_loss: 0.6974486708641052\n",
      "pred_error: 0.0687737986445427 \t range_loss: 0.6974486708641052\n",
      "pred_error: 0.0685749500989914 \t range_loss: 0.6964384317398071\n",
      "pred_error: 0.06849288195371628 \t range_loss: 0.6962067484855652\n",
      "pred_error: 0.0687301978468895 \t range_loss: 0.6946098208427429\n",
      "pred_error: 0.06873022019863129 \t range_loss: 0.6946098208427429\n",
      "pred_error: 0.06877218931913376 \t range_loss: 0.6931524276733398\n",
      "pred_error: 0.06854036450386047 \t range_loss: 0.6932544112205505\n",
      "pred_error: 0.06902938336133957 \t range_loss: 0.6933308243751526\n",
      "pred_error: 0.06869465857744217 \t range_loss: 0.6929647922515869\n",
      "pred_error: 0.0686945989727974 \t range_loss: 0.6929647922515869\n",
      "pred_error: 0.06891463696956635 \t range_loss: 0.6929989457130432\n",
      "pred_error: 0.06926076859235764 \t range_loss: 0.693000853061676\n",
      "pred_error: 0.06925997138023376 \t range_loss: 0.693000853061676\n",
      "pred_error: 0.06923191249370575 \t range_loss: 0.6915542483329773\n",
      "pred_error: 0.06912028044462204 \t range_loss: 0.6931331157684326\n",
      "pred_error: 0.0690762996673584 \t range_loss: 0.6911483407020569\n",
      "pred_error: 0.0688665360212326 \t range_loss: 0.6920112371444702\n",
      "pred_error: 0.06886650621891022 \t range_loss: 0.6920112371444702\n",
      "pred_error: 0.0690537542104721 \t range_loss: 0.6913411021232605\n",
      "pred_error: 0.06910194456577301 \t range_loss: 0.6913146376609802\n",
      "pred_error: 0.06892065703868866 \t range_loss: 0.6918037533760071\n",
      "pred_error: 0.0691206306219101 \t range_loss: 0.69321608543396\n",
      "pred_error: 0.06912065297365189 \t range_loss: 0.69321608543396\n",
      "pred_error: 0.06912054866552353 \t range_loss: 0.69321608543396\n",
      "pred_error: 0.06874483078718185 \t range_loss: 0.6959567070007324\n",
      "pred_error: 0.06874483078718185 \t range_loss: 0.6959567070007324\n",
      "pred_error: 0.0687447339296341 \t range_loss: 0.6959567070007324\n",
      "pred_error: 0.06895855814218521 \t range_loss: 0.6932773590087891\n",
      "pred_error: 0.06870482116937637 \t range_loss: 0.6932983994483948\n",
      "pred_error: 0.06846725195646286 \t range_loss: 0.6944990754127502\n",
      "pred_error: 0.06851755082607269 \t range_loss: 0.6955275535583496\n",
      "pred_error: 0.0689002200961113 \t range_loss: 0.69878089427948\n",
      "pred_error: 0.06878495961427689 \t range_loss: 0.6985567808151245\n",
      "pred_error: 0.06858330219984055 \t range_loss: 0.6938512921333313\n",
      "pred_error: 0.06835951656103134 \t range_loss: 0.6956352591514587\n",
      "pred_error: 0.06873037666082382 \t range_loss: 0.6966484189033508\n",
      "pred_error: 0.06873046606779099 \t range_loss: 0.6966484189033508\n",
      "pred_error: 0.06886366009712219 \t range_loss: 0.6939911842346191\n",
      "pred_error: 0.06878162175416946 \t range_loss: 0.6941336989402771\n",
      "pred_error: 0.06861422955989838 \t range_loss: 0.6937843561172485\n",
      "pred_error: 0.06891407072544098 \t range_loss: 0.6931619048118591\n",
      "pred_error: 0.06880990415811539 \t range_loss: 0.6945570707321167\n",
      "pred_error: 0.06880991905927658 \t range_loss: 0.6945570707321167\n",
      "pred_error: 0.06893381476402283 \t range_loss: 0.6927439570426941\n",
      "pred_error: 0.06871559470891953 \t range_loss: 0.6943100094795227\n",
      "pred_error: 0.06871555000543594 \t range_loss: 0.6943100094795227\n",
      "pred_error: 0.0688663050532341 \t range_loss: 0.6926274299621582\n",
      "pred_error: 0.06879732757806778 \t range_loss: 0.6925812363624573\n",
      "pred_error: 0.06902134418487549 \t range_loss: 0.6928542852401733\n",
      "pred_error: 0.06918376684188843 \t range_loss: 0.6936544179916382\n",
      "pred_error: 0.06923475861549377 \t range_loss: 0.6929075121879578\n",
      "pred_error: 0.0687575563788414 \t range_loss: 0.69158935546875\n",
      "pred_error: 0.06877289712429047 \t range_loss: 0.6917979121208191\n",
      "pred_error: 0.06877289712429047 \t range_loss: 0.6917979121208191\n",
      "pred_error: 0.06933707743883133 \t range_loss: 0.6904637217521667\n",
      "pred_error: 0.06921405345201492 \t range_loss: 0.691539466381073\n",
      "pred_error: 0.06901533901691437 \t range_loss: 0.6931068301200867\n",
      "pred_error: 0.06901528686285019 \t range_loss: 0.6931068301200867\n",
      "pred_error: 0.06913339346647263 \t range_loss: 0.6909412741661072\n",
      "pred_error: 0.06934715807437897 \t range_loss: 0.6910062432289124\n",
      "pred_error: 0.06883066892623901 \t range_loss: 0.6913901567459106\n",
      "pred_error: 0.06899134069681168 \t range_loss: 0.6903865933418274\n",
      "pred_error: 0.06899134814739227 \t range_loss: 0.6903865933418274\n",
      "pred_error: 0.06904960423707962 \t range_loss: 0.691827118396759\n",
      "pred_error: 0.06904963403940201 \t range_loss: 0.691827118396759\n",
      "pred_error: 0.06904948502779007 \t range_loss: 0.691827118396759\n",
      "pred_error: 0.06869585812091827 \t range_loss: 0.693027138710022\n",
      "pred_error: 0.06887516379356384 \t range_loss: 0.6949037313461304\n",
      "pred_error: 0.06887524574995041 \t range_loss: 0.6949037313461304\n",
      "pred_error: 0.06913904845714569 \t range_loss: 0.6921384334564209\n",
      "pred_error: 0.06891622394323349 \t range_loss: 0.6916235089302063\n",
      "pred_error: 0.06879926472902298 \t range_loss: 0.691700279712677\n",
      "BEST LOSS: 1.3768566\n",
      "==== Model: block8_cob_activation_norm  in Layer: 8 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 17:32:59,864 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 17:37:30,776 model.rs:1246 value (-97024) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 17:37:30,805 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 17:38:04,157 model.rs:1246 value (-97024) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 17:38:04,168 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 17:38:04,190 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 17:38:04,217 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 17:38:04,236 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error   | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000012372174 | -0.00004041195 | 0.0006183386 | -0.00071543455 | 0.00008166607  | 0.00004041195    | 0.00071543455 | 0             | 0.000000011993131  | 0.00006229167      | 0.0015281404           |\n",
      "+------------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 547123 64 [-1062178, 613306] 1 [16]\n",
      "===============================\n",
      "==== Model: block8_cob_activation_norm_teleported  in Layer: 8 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 17:38:38,606 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 17:43:31,297 model.rs:1246 value (-123584) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 17:43:31,331 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 17:44:12,769 model.rs:1246 value (-123584) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 17:44:12,778 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 17:44:12,824 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 17:44:12,860 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 17:44:12,889 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error   | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000007596311 | -0.000025987625 | 0.000600338 | -0.00068092346 | 0.000085770196 | 0.000025987625   | 0.00068092346 | 0             | 0.000000013387006  | 0.00022591533      | 0.0013724532           |\n",
      "+------------------+-----------------+-------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 547122 64 [-791036, 619000] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 9 , \t  activation_stats: {'relu_1': {'norm': tensor(939.6653), 'max': tensor(5.8807), 'min': tensor(-11.0276), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(16.9083)\n",
      "Step: 0 \t Loss: 1.67085862159729\n",
      "Step: 1 \t Loss: 1.63407301902771\n",
      "pred_error: 0.06943260878324509 \t range_loss: 0.9397481083869934\n",
      "Step: 2 \t Loss: 1.6077089309692383\n",
      "Step: 3 \t Loss: 1.5964823961257935\n",
      "Step: 4 \t Loss: 1.5765814781188965\n",
      "Step: 5 \t Loss: 1.5739269256591797\n",
      "Step: 6 \t Loss: 1.5652707815170288\n",
      "Step: 7 \t Loss: 1.560025691986084\n",
      "Step: 8 \t Loss: 1.5579123497009277\n",
      "Step: 9 \t Loss: 1.5500566959381104\n",
      "pred_error: 0.06963879615068436 \t range_loss: 0.8536688089370728\n",
      "Step: 10 \t Loss: 1.5440952777862549\n",
      "Step: 11 \t Loss: 1.5390347242355347\n",
      "Step: 12 \t Loss: 1.5266387462615967\n",
      "pred_error: 0.06987913697957993 \t range_loss: 0.827846109867096\n",
      "pred_error: 0.06987913697957993 \t range_loss: 0.827846109867096\n",
      "Step: 13 \t Loss: 1.5238158702850342\n",
      "Step: 14 \t Loss: 1.5188463926315308\n",
      "Step: 15 \t Loss: 1.5183886289596558\n",
      "pred_error: 0.06994212418794632 \t range_loss: 0.8189725279808044\n",
      "Step: 16 \t Loss: 1.5165047645568848\n",
      "Step: 17 \t Loss: 1.514028549194336\n",
      "Step: 18 \t Loss: 1.513864517211914\n",
      "Step: 19 \t Loss: 1.5100345611572266\n",
      "pred_error: 0.07031199336051941 \t range_loss: 0.8069144487380981\n",
      "Step: 20 \t Loss: 1.5049762725830078\n",
      "Step: 21 \t Loss: 1.5010119676589966\n",
      "pred_error: 0.07010719925165176 \t range_loss: 0.7999399900436401\n",
      "Step: 22 \t Loss: 1.4977054595947266\n",
      "pred_error: 0.07024415582418442 \t range_loss: 0.7952603697776794\n",
      "pred_error: 0.07024450600147247 \t range_loss: 0.7952603697776794\n",
      "Step: 23 \t Loss: 1.4936237335205078\n",
      "pred_error: 0.07018543034791946 \t range_loss: 0.7917693853378296\n",
      "Step: 24 \t Loss: 1.4894657135009766\n",
      "pred_error: 0.07037635147571564 \t range_loss: 0.7869117856025696\n",
      "pred_error: 0.0705360695719719 \t range_loss: 0.7856024503707886\n",
      "Step: 27 \t Loss: 1.4888696670532227\n",
      "Step: 28 \t Loss: 1.4852173328399658\n",
      "pred_error: 0.07053771615028381 \t range_loss: 0.7798407077789307\n",
      "Step: 29 \t Loss: 1.4842345714569092\n",
      "pred_error: 0.07066094130277634 \t range_loss: 0.7776278257369995\n",
      "pred_error: 0.07066058367490768 \t range_loss: 0.7776278257369995\n",
      "Step: 30 \t Loss: 1.482739806175232\n",
      "pred_error: 0.07057763636112213 \t range_loss: 0.776963472366333\n",
      "pred_error: 0.07057786732912064 \t range_loss: 0.776963472366333\n",
      "Step: 31 \t Loss: 1.4811427593231201\n",
      "pred_error: 0.0705861896276474 \t range_loss: 0.775280237197876\n",
      "Step: 32 \t Loss: 1.4803416728973389\n",
      "pred_error: 0.07050052285194397 \t range_loss: 0.7753370404243469\n",
      "Step: 33 \t Loss: 1.4769656658172607\n",
      "Step: 34 \t Loss: 1.4718962907791138\n",
      "pred_error: 0.07056976854801178 \t range_loss: 0.7661986351013184\n",
      "Step: 35 \t Loss: 1.471461296081543\n",
      "pred_error: 0.07044969499111176 \t range_loss: 0.766965925693512\n",
      "pred_error: 0.07089976221323013 \t range_loss: 0.7663902640342712\n",
      "pred_error: 0.07089932262897491 \t range_loss: 0.7664814591407776\n",
      "pred_error: 0.07100993394851685 \t range_loss: 0.7636124491691589\n",
      "Step: 41 \t Loss: 1.4688584804534912\n",
      "Step: 42 \t Loss: 1.466235876083374\n",
      "Step: 43 \t Loss: 1.465900182723999\n",
      "Step: 44 \t Loss: 1.4605774879455566\n",
      "pred_error: 0.07084042578935623 \t range_loss: 0.7521731853485107\n",
      "pred_error: 0.07084039598703384 \t range_loss: 0.7521731853485107\n",
      "Step: 45 \t Loss: 1.4589056968688965\n",
      "Step: 51 \t Loss: 1.4573029279708862\n",
      "pred_error: 0.07166966050863266 \t range_loss: 0.7425811886787415\n",
      "pred_error: 0.07167017459869385 \t range_loss: 0.7425811886787415\n",
      "Step: 54 \t Loss: 1.45648193359375\n",
      "Step: 55 \t Loss: 1.4546542167663574\n",
      "Step: 57 \t Loss: 1.453946828842163\n",
      "pred_error: 0.07143501192331314 \t range_loss: 0.7395967841148376\n",
      "Step: 58 \t Loss: 1.4523646831512451\n",
      "pred_error: 0.07153643667697906 \t range_loss: 0.7402573227882385\n",
      "Step: 61 \t Loss: 1.4487093687057495\n",
      "pred_error: 0.07149290293455124 \t range_loss: 0.7337803244590759\n",
      "pred_error: 0.0714927688241005 \t range_loss: 0.7337803244590759\n",
      "Step: 64 \t Loss: 1.4467005729675293\n",
      "pred_error: 0.07161521911621094 \t range_loss: 0.7305484414100647\n",
      "Step: 65 \t Loss: 1.4456984996795654\n",
      "pred_error: 0.0714394599199295 \t range_loss: 0.7313050031661987\n",
      "pred_error: 0.07143926620483398 \t range_loss: 0.7313050031661987\n",
      "pred_error: 0.07176787406206131 \t range_loss: 0.7284753322601318\n",
      "Step: 68 \t Loss: 1.4444772005081177\n",
      "pred_error: 0.07217265665531158 \t range_loss: 0.7253040671348572\n",
      "pred_error: 0.07201894372701645 \t range_loss: 0.7245820164680481\n",
      "Step: 76 \t Loss: 1.4435702562332153\n",
      "Step: 77 \t Loss: 1.4412364959716797\n",
      "Step: 78 \t Loss: 1.4408336877822876\n",
      "Step: 79 \t Loss: 1.440172791481018\n",
      "pred_error: 0.07203268259763718 \t range_loss: 0.7198459506034851\n",
      "Step: 82 \t Loss: 1.43947434425354\n",
      "Step: 83 \t Loss: 1.438476324081421\n",
      "pred_error: 0.07254275679588318 \t range_loss: 0.7135391235351562\n",
      "Step: 85 \t Loss: 1.4362684488296509\n",
      "Step: 86 \t Loss: 1.435731291770935\n",
      "pred_error: 0.07246502488851547 \t range_loss: 0.7114249467849731\n",
      "pred_error: 0.07313711941242218 \t range_loss: 0.7104783654212952\n",
      "Step: 95 \t Loss: 1.4333806037902832\n",
      "pred_error: 0.07306839525699615 \t range_loss: 0.7081096768379211\n",
      "pred_error: 0.07307234406471252 \t range_loss: 0.7080906629562378\n",
      "pred_error: 0.07307245582342148 \t range_loss: 0.7080906629562378\n",
      "pred_error: 0.07312975823879242 \t range_loss: 0.7057876586914062\n",
      "pred_error: 0.0731298178434372 \t range_loss: 0.7057876586914062\n",
      "pred_error: 0.07313022762537003 \t range_loss: 0.7057876586914062\n",
      "Step: 103 \t Loss: 1.4318926334381104\n",
      "pred_error: 0.0734463781118393 \t range_loss: 0.7007228136062622\n",
      "pred_error: 0.07322748005390167 \t range_loss: 0.7004638314247131\n",
      "pred_error: 0.07330085337162018 \t range_loss: 0.6988903284072876\n",
      "Step: 111 \t Loss: 1.428687572479248\n",
      "pred_error: 0.0735211968421936 \t range_loss: 0.6989142298698425\n",
      "pred_error: 0.07349186390638351 \t range_loss: 0.6974995136260986\n",
      "pred_error: 0.07374441623687744 \t range_loss: 0.6961350440979004\n",
      "pred_error: 0.07352019101381302 \t range_loss: 0.6954675912857056\n",
      "Step: 120 \t Loss: 1.4281716346740723\n",
      "pred_error: 0.07322034239768982 \t range_loss: 0.6959682106971741\n",
      "pred_error: 0.07325166463851929 \t range_loss: 0.6979963183403015\n",
      "pred_error: 0.07352012395858765 \t range_loss: 0.6956649422645569\n",
      "pred_error: 0.07365290075540543 \t range_loss: 0.6968221068382263\n",
      "pred_error: 0.07383620738983154 \t range_loss: 0.6943521499633789\n",
      "pred_error: 0.07383620738983154 \t range_loss: 0.6943521499633789\n",
      "pred_error: 0.073945052921772 \t range_loss: 0.694333553314209\n",
      "Step: 133 \t Loss: 1.4275784492492676\n",
      "Step: 134 \t Loss: 1.4268414974212646\n",
      "pred_error: 0.07328277081251144 \t range_loss: 0.6940138339996338\n",
      "pred_error: 0.07366865873336792 \t range_loss: 0.6924746036529541\n",
      "pred_error: 0.07378599792718887 \t range_loss: 0.6912416815757751\n",
      "pred_error: 0.07376275211572647 \t range_loss: 0.6910644769668579\n",
      "pred_error: 0.07375185191631317 \t range_loss: 0.6902210116386414\n",
      "pred_error: 0.07411550730466843 \t range_loss: 0.6894465088844299\n",
      "pred_error: 0.07411550730466843 \t range_loss: 0.6894465088844299\n",
      "Step: 143 \t Loss: 1.425694227218628\n",
      "pred_error: 0.07363682240247726 \t range_loss: 0.6893258690834045\n",
      "pred_error: 0.07390539348125458 \t range_loss: 0.6914249658584595\n",
      "pred_error: 0.07397763431072235 \t range_loss: 0.691112220287323\n",
      "pred_error: 0.07407664507627487 \t range_loss: 0.6875506043434143\n",
      "pred_error: 0.07407607138156891 \t range_loss: 0.6875506043434143\n",
      "pred_error: 0.07403040677309036 \t range_loss: 0.6882855892181396\n",
      "pred_error: 0.07390047609806061 \t range_loss: 0.6890144348144531\n",
      "pred_error: 0.07392801344394684 \t range_loss: 0.6888908743858337\n",
      "pred_error: 0.07413884252309799 \t range_loss: 0.6897180080413818\n",
      "pred_error: 0.07404303550720215 \t range_loss: 0.6894440054893494\n",
      "Step: 164 \t Loss: 1.4247455596923828\n",
      "pred_error: 0.07391724735498428 \t range_loss: 0.6855731010437012\n",
      "Step: 165 \t Loss: 1.4233827590942383\n",
      "pred_error: 0.07413526624441147 \t range_loss: 0.6867074370384216\n",
      "pred_error: 0.07413497567176819 \t range_loss: 0.6867074370384216\n",
      "pred_error: 0.07413506507873535 \t range_loss: 0.6867074370384216\n",
      "pred_error: 0.07413522154092789 \t range_loss: 0.6867074370384216\n",
      "pred_error: 0.07421869039535522 \t range_loss: 0.6877084970474243\n",
      "pred_error: 0.0742938295006752 \t range_loss: 0.6845495104789734\n",
      "pred_error: 0.07419684529304504 \t range_loss: 0.6847692131996155\n",
      "pred_error: 0.07419587671756744 \t range_loss: 0.6847692131996155\n",
      "pred_error: 0.07444357126951218 \t range_loss: 0.6833546161651611\n",
      "pred_error: 0.07421373575925827 \t range_loss: 0.6849185824394226\n",
      "pred_error: 0.07425341755151749 \t range_loss: 0.6810503602027893\n",
      "pred_error: 0.0744686871767044 \t range_loss: 0.6809738278388977\n",
      "pred_error: 0.07446828484535217 \t range_loss: 0.6809738278388977\n",
      "Step: 184 \t Loss: 1.4218227863311768\n",
      "pred_error: 0.07448126375675201 \t range_loss: 0.6812546849250793\n",
      "pred_error: 0.07448126375675201 \t range_loss: 0.6812546849250793\n",
      "pred_error: 0.07461642473936081 \t range_loss: 0.6833419799804688\n",
      "pred_error: 0.07435382902622223 \t range_loss: 0.6803208589553833\n",
      "pred_error: 0.07478106021881104 \t range_loss: 0.6776136159896851\n",
      "pred_error: 0.07441456615924835 \t range_loss: 0.6783058643341064\n",
      "pred_error: 0.07461031526327133 \t range_loss: 0.6786260008811951\n",
      "pred_error: 0.07466995716094971 \t range_loss: 0.6774402856826782\n",
      "pred_error: 0.07474132627248764 \t range_loss: 0.6769204139709473\n",
      "pred_error: 0.07491908967494965 \t range_loss: 0.6767240166664124\n",
      "pred_error: 0.07473596930503845 \t range_loss: 0.6748200058937073\n",
      "pred_error: 0.07461230456829071 \t range_loss: 0.6769568920135498\n",
      "pred_error: 0.0750918835401535 \t range_loss: 0.6754307746887207\n",
      "pred_error: 0.07516584545373917 \t range_loss: 0.6746468544006348\n",
      "pred_error: 0.07516574114561081 \t range_loss: 0.6746468544006348\n",
      "pred_error: 0.0751657783985138 \t range_loss: 0.6746468544006348\n",
      "pred_error: 0.07479238510131836 \t range_loss: 0.6765353083610535\n",
      "pred_error: 0.07493584603071213 \t range_loss: 0.6732103824615479\n",
      "pred_error: 0.07506231963634491 \t range_loss: 0.6743372082710266\n",
      "pred_error: 0.07542082667350769 \t range_loss: 0.6724913120269775\n",
      "pred_error: 0.07537434250116348 \t range_loss: 0.672127902507782\n",
      "pred_error: 0.07524092495441437 \t range_loss: 0.672558605670929\n",
      "pred_error: 0.07523996382951736 \t range_loss: 0.6726464033126831\n",
      "pred_error: 0.07523997873067856 \t range_loss: 0.6726464033126831\n",
      "Step: 244 \t Loss: 1.420885682106018\n",
      "pred_error: 0.07505108416080475 \t range_loss: 0.6703737378120422\n",
      "pred_error: 0.07567024230957031 \t range_loss: 0.669560432434082\n",
      "pred_error: 0.07567024230957031 \t range_loss: 0.669560432434082\n",
      "pred_error: 0.07567024230957031 \t range_loss: 0.669560432434082\n",
      "pred_error: 0.07546538859605789 \t range_loss: 0.6683312654495239\n",
      "pred_error: 0.07519061863422394 \t range_loss: 0.6701920032501221\n",
      "pred_error: 0.07547803223133087 \t range_loss: 0.6688253283500671\n",
      "pred_error: 0.07527989149093628 \t range_loss: 0.6685200929641724\n",
      "pred_error: 0.07563149183988571 \t range_loss: 0.6681274175643921\n",
      "Step: 257 \t Loss: 1.41945219039917\n",
      "pred_error: 0.0755486935377121 \t range_loss: 0.6721729636192322\n",
      "pred_error: 0.07599037140607834 \t range_loss: 0.6674405932426453\n",
      "pred_error: 0.07569526135921478 \t range_loss: 0.6671001315116882\n",
      "pred_error: 0.07553465664386749 \t range_loss: 0.6663254499435425\n",
      "pred_error: 0.07536445558071136 \t range_loss: 0.6736227869987488\n",
      "pred_error: 0.07575917989015579 \t range_loss: 0.6676771640777588\n",
      "pred_error: 0.07575946301221848 \t range_loss: 0.6676771640777588\n",
      "pred_error: 0.0757591649889946 \t range_loss: 0.6676771640777588\n",
      "pred_error: 0.07520328462123871 \t range_loss: 0.6701408624649048\n",
      "pred_error: 0.07550684362649918 \t range_loss: 0.6699894070625305\n",
      "pred_error: 0.07562883198261261 \t range_loss: 0.6692762970924377\n",
      "pred_error: 0.07548949867486954 \t range_loss: 0.669468343257904\n",
      "pred_error: 0.07560504227876663 \t range_loss: 0.6690585613250732\n",
      "pred_error: 0.07560504227876663 \t range_loss: 0.6690585613250732\n",
      "pred_error: 0.07554573565721512 \t range_loss: 0.6686530113220215\n",
      "pred_error: 0.07557849586009979 \t range_loss: 0.6676754355430603\n",
      "pred_error: 0.07561808824539185 \t range_loss: 0.6669934391975403\n",
      "Step: 292 \t Loss: 1.4185512065887451\n",
      "pred_error: 0.07564444094896317 \t range_loss: 0.6693295240402222\n",
      "pred_error: 0.07575079798698425 \t range_loss: 0.6677903532981873\n",
      "pred_error: 0.07556931674480438 \t range_loss: 0.6672897338867188\n",
      "pred_error: 0.07536566257476807 \t range_loss: 0.6676746606826782\n",
      "pred_error: 0.07536592334508896 \t range_loss: 0.6676746606826782\n",
      "pred_error: 0.07572843879461288 \t range_loss: 0.667819619178772\n",
      "pred_error: 0.07566753774881363 \t range_loss: 0.6668533682823181\n",
      "pred_error: 0.07587222009897232 \t range_loss: 0.6662376523017883\n",
      "pred_error: 0.07586389034986496 \t range_loss: 0.6672724485397339\n",
      "Step: 313 \t Loss: 1.4184465408325195\n",
      "pred_error: 0.07546789199113846 \t range_loss: 0.6644598841667175\n",
      "pred_error: 0.07546788454055786 \t range_loss: 0.6644598841667175\n",
      "pred_error: 0.07585934549570084 \t range_loss: 0.6680200099945068\n",
      "pred_error: 0.075970359146595 \t range_loss: 0.665741503238678\n",
      "pred_error: 0.07577234506607056 \t range_loss: 0.6644089221954346\n",
      "pred_error: 0.0755312517285347 \t range_loss: 0.6660131216049194\n",
      "pred_error: 0.07588673382997513 \t range_loss: 0.6645426750183105\n",
      "pred_error: 0.07556979358196259 \t range_loss: 0.6639971733093262\n",
      "pred_error: 0.07556979358196259 \t range_loss: 0.6639971733093262\n",
      "pred_error: 0.07537025958299637 \t range_loss: 0.6666800379753113\n",
      "pred_error: 0.07537030428647995 \t range_loss: 0.6666800379753113\n",
      "pred_error: 0.07537025958299637 \t range_loss: 0.6666800379753113\n",
      "pred_error: 0.0759149119257927 \t range_loss: 0.6649110317230225\n",
      "pred_error: 0.07597751915454865 \t range_loss: 0.6651734113693237\n",
      "pred_error: 0.0758245438337326 \t range_loss: 0.6636237502098083\n",
      "pred_error: 0.07582465559244156 \t range_loss: 0.6636237502098083\n",
      "pred_error: 0.07622546702623367 \t range_loss: 0.6656820774078369\n",
      "pred_error: 0.07606802880764008 \t range_loss: 0.6625104546546936\n",
      "pred_error: 0.07559750974178314 \t range_loss: 0.6659003496170044\n",
      "pred_error: 0.07609731703996658 \t range_loss: 0.6622588634490967\n",
      "pred_error: 0.07599232345819473 \t range_loss: 0.66412752866745\n",
      "pred_error: 0.07640354335308075 \t range_loss: 0.6641738414764404\n",
      "pred_error: 0.07599911838769913 \t range_loss: 0.6616696119308472\n",
      "pred_error: 0.07604550570249557 \t range_loss: 0.6618123650550842\n",
      "pred_error: 0.07617610692977905 \t range_loss: 0.6623256206512451\n",
      "pred_error: 0.07635170966386795 \t range_loss: 0.6604354977607727\n",
      "pred_error: 0.07593144476413727 \t range_loss: 0.6621725559234619\n",
      "pred_error: 0.07607400417327881 \t range_loss: 0.6609581112861633\n",
      "pred_error: 0.07607394456863403 \t range_loss: 0.6609581112861633\n",
      "pred_error: 0.07582557946443558 \t range_loss: 0.6607688665390015\n",
      "Step: 387 \t Loss: 1.4164053201675415\n",
      "Step: 388 \t Loss: 1.416062831878662\n",
      "pred_error: 0.07551228255033493 \t range_loss: 0.660940945148468\n",
      "pred_error: 0.07612299174070358 \t range_loss: 0.6631273627281189\n",
      "pred_error: 0.07591662555932999 \t range_loss: 0.6618782877922058\n",
      "pred_error: 0.0764772817492485 \t range_loss: 0.6611628532409668\n",
      "pred_error: 0.07642452418804169 \t range_loss: 0.6603183746337891\n",
      "pred_error: 0.07642363756895065 \t range_loss: 0.6604889631271362\n",
      "pred_error: 0.0760878250002861 \t range_loss: 0.6602476835250854\n",
      "pred_error: 0.07623305916786194 \t range_loss: 0.6617427468299866\n",
      "pred_error: 0.07604782283306122 \t range_loss: 0.6575301885604858\n",
      "Step: 407 \t Loss: 1.4148705005645752\n",
      "pred_error: 0.07625479251146317 \t range_loss: 0.6623628735542297\n",
      "pred_error: 0.07631035894155502 \t range_loss: 0.6644264459609985\n",
      "pred_error: 0.07631054520606995 \t range_loss: 0.6644264459609985\n",
      "pred_error: 0.07631032913923264 \t range_loss: 0.6644264459609985\n",
      "pred_error: 0.07646797597408295 \t range_loss: 0.6616343259811401\n",
      "pred_error: 0.07620607316493988 \t range_loss: 0.6622602939605713\n",
      "pred_error: 0.0764026865363121 \t range_loss: 0.660686731338501\n",
      "pred_error: 0.07640264928340912 \t range_loss: 0.660686731338501\n",
      "pred_error: 0.07622229307889938 \t range_loss: 0.6608644127845764\n",
      "pred_error: 0.0764027088880539 \t range_loss: 0.6603952050209045\n",
      "pred_error: 0.07610534876585007 \t range_loss: 0.6594789028167725\n",
      "pred_error: 0.0757780522108078 \t range_loss: 0.6618186831474304\n",
      "pred_error: 0.0763995423913002 \t range_loss: 0.6596038341522217\n",
      "pred_error: 0.07639946788549423 \t range_loss: 0.6596038341522217\n",
      "pred_error: 0.07608387619256973 \t range_loss: 0.6600984334945679\n",
      "pred_error: 0.07608640193939209 \t range_loss: 0.6600984334945679\n",
      "pred_error: 0.07630935311317444 \t range_loss: 0.6632831692695618\n",
      "pred_error: 0.07595142722129822 \t range_loss: 0.6604093313217163\n",
      "pred_error: 0.07614369690418243 \t range_loss: 0.661825954914093\n",
      "pred_error: 0.07625697553157806 \t range_loss: 0.6588440537452698\n",
      "pred_error: 0.07636105269193649 \t range_loss: 0.6580957770347595\n",
      "pred_error: 0.07619031518697739 \t range_loss: 0.6587225794792175\n",
      "pred_error: 0.07641720026731491 \t range_loss: 0.6582072377204895\n",
      "pred_error: 0.0764172300696373 \t range_loss: 0.6582072377204895\n",
      "pred_error: 0.07680145651102066 \t range_loss: 0.6590889096260071\n",
      "pred_error: 0.07666677236557007 \t range_loss: 0.6582421660423279\n",
      "pred_error: 0.07639101147651672 \t range_loss: 0.6580946445465088\n",
      "pred_error: 0.07661183923482895 \t range_loss: 0.6572971343994141\n",
      "pred_error: 0.07661169022321701 \t range_loss: 0.6572971343994141\n",
      "pred_error: 0.07661175727844238 \t range_loss: 0.6572971343994141\n",
      "pred_error: 0.07627236098051071 \t range_loss: 0.6573206782341003\n",
      "pred_error: 0.07627250999212265 \t range_loss: 0.6572158932685852\n",
      "pred_error: 0.07660100609064102 \t range_loss: 0.6585355997085571\n",
      "pred_error: 0.07608015835285187 \t range_loss: 0.6596643328666687\n",
      "pred_error: 0.07660071551799774 \t range_loss: 0.6586913466453552\n",
      "pred_error: 0.07651641219854355 \t range_loss: 0.6581428647041321\n",
      "pred_error: 0.07627209275960922 \t range_loss: 0.6576133966445923\n",
      "pred_error: 0.07597234100103378 \t range_loss: 0.6624618768692017\n",
      "pred_error: 0.0762762799859047 \t range_loss: 0.657877504825592\n",
      "pred_error: 0.07641661167144775 \t range_loss: 0.6577377319335938\n",
      "pred_error: 0.07648599147796631 \t range_loss: 0.6595126986503601\n",
      "pred_error: 0.07652640342712402 \t range_loss: 0.6569535732269287\n",
      "pred_error: 0.07638438045978546 \t range_loss: 0.6569231748580933\n",
      "pred_error: 0.07632488757371902 \t range_loss: 0.6561863422393799\n",
      "pred_error: 0.07660766690969467 \t range_loss: 0.6573271751403809\n",
      "pred_error: 0.07616575062274933 \t range_loss: 0.6637072563171387\n",
      "pred_error: 0.07642845809459686 \t range_loss: 0.6578220129013062\n",
      "pred_error: 0.07642847299575806 \t range_loss: 0.6578220129013062\n",
      "pred_error: 0.07626228779554367 \t range_loss: 0.6575782299041748\n",
      "pred_error: 0.07645316421985626 \t range_loss: 0.6616335511207581\n",
      "pred_error: 0.07645316421985626 \t range_loss: 0.6616335511207581\n",
      "pred_error: 0.07645310461521149 \t range_loss: 0.6616335511207581\n",
      "pred_error: 0.07680872082710266 \t range_loss: 0.6610976457595825\n",
      "pred_error: 0.07680880278348923 \t range_loss: 0.6610976457595825\n",
      "pred_error: 0.0765313133597374 \t range_loss: 0.6582486033439636\n",
      "pred_error: 0.07601235806941986 \t range_loss: 0.6612200140953064\n",
      "pred_error: 0.07628079503774643 \t range_loss: 0.6579686403274536\n",
      "pred_error: 0.07658923417329788 \t range_loss: 0.6570612192153931\n",
      "pred_error: 0.07658360153436661 \t range_loss: 0.6590723395347595\n",
      "pred_error: 0.07668215036392212 \t range_loss: 0.6544642448425293\n",
      "Step: 535 \t Loss: 1.41383957862854\n",
      "pred_error: 0.07704895734786987 \t range_loss: 0.6555910110473633\n",
      "pred_error: 0.07704893499612808 \t range_loss: 0.6555910110473633\n",
      "pred_error: 0.07673466950654984 \t range_loss: 0.655747652053833\n",
      "pred_error: 0.07675191015005112 \t range_loss: 0.6553411483764648\n",
      "pred_error: 0.07675663381814957 \t range_loss: 0.655815601348877\n",
      "pred_error: 0.07632268965244293 \t range_loss: 0.6569428443908691\n",
      "pred_error: 0.07662110775709152 \t range_loss: 0.6557782888412476\n",
      "pred_error: 0.07638464868068695 \t range_loss: 0.6574249863624573\n",
      "pred_error: 0.07648632675409317 \t range_loss: 0.6562455296516418\n",
      "pred_error: 0.07649440318346024 \t range_loss: 0.6562455296516418\n",
      "pred_error: 0.07624253630638123 \t range_loss: 0.6574468612670898\n",
      "pred_error: 0.07654017955064774 \t range_loss: 0.6571265459060669\n",
      "pred_error: 0.07654029130935669 \t range_loss: 0.6571265459060669\n",
      "pred_error: 0.07664366066455841 \t range_loss: 0.6561896204948425\n",
      "pred_error: 0.07639917731285095 \t range_loss: 0.6550299525260925\n",
      "pred_error: 0.07657386362552643 \t range_loss: 0.6541069746017456\n",
      "pred_error: 0.07657385617494583 \t range_loss: 0.6541069746017456\n",
      "pred_error: 0.07657919079065323 \t range_loss: 0.6570112705230713\n",
      "pred_error: 0.07693614810705185 \t range_loss: 0.6556347012519836\n",
      "pred_error: 0.076575368642807 \t range_loss: 0.6543713212013245\n",
      "pred_error: 0.07702235132455826 \t range_loss: 0.6573196649551392\n",
      "pred_error: 0.07673884928226471 \t range_loss: 0.6539770364761353\n",
      "pred_error: 0.07645389437675476 \t range_loss: 0.6534953117370605\n",
      "pred_error: 0.07707424461841583 \t range_loss: 0.6544804573059082\n",
      "pred_error: 0.07707424461841583 \t range_loss: 0.6544804573059082\n",
      "pred_error: 0.07673200964927673 \t range_loss: 0.6539677381515503\n",
      "pred_error: 0.0764978751540184 \t range_loss: 0.6534976959228516\n",
      "pred_error: 0.07667450606822968 \t range_loss: 0.6545235514640808\n",
      "pred_error: 0.07667450606822968 \t range_loss: 0.6545235514640808\n",
      "pred_error: 0.0766744613647461 \t range_loss: 0.6545235514640808\n",
      "pred_error: 0.07651325315237045 \t range_loss: 0.6563406586647034\n",
      "pred_error: 0.07651327550411224 \t range_loss: 0.6563406586647034\n",
      "pred_error: 0.07692476361989975 \t range_loss: 0.6552003622055054\n",
      "pred_error: 0.07686466723680496 \t range_loss: 0.6544124484062195\n",
      "pred_error: 0.076811283826828 \t range_loss: 0.6541402339935303\n",
      "pred_error: 0.0768112912774086 \t range_loss: 0.6541402339935303\n",
      "pred_error: 0.07638515532016754 \t range_loss: 0.6529486775398254\n",
      "pred_error: 0.07638511806726456 \t range_loss: 0.6529486775398254\n",
      "pred_error: 0.0767219066619873 \t range_loss: 0.6553929448127747\n",
      "pred_error: 0.07716577500104904 \t range_loss: 0.6556615233421326\n",
      "pred_error: 0.07623307406902313 \t range_loss: 0.6567342281341553\n",
      "pred_error: 0.07623298466205597 \t range_loss: 0.6567342281341553\n",
      "pred_error: 0.0766313448548317 \t range_loss: 0.6538758277893066\n",
      "pred_error: 0.07610853016376495 \t range_loss: 0.6572889685630798\n",
      "pred_error: 0.07661890238523483 \t range_loss: 0.6539028882980347\n",
      "pred_error: 0.07661940902471542 \t range_loss: 0.6539028882980347\n",
      "pred_error: 0.07671815156936646 \t range_loss: 0.6565544009208679\n",
      "pred_error: 0.076817087829113 \t range_loss: 0.6547374725341797\n",
      "pred_error: 0.07681584358215332 \t range_loss: 0.6573253870010376\n",
      "pred_error: 0.0769481435418129 \t range_loss: 0.6554901003837585\n",
      "pred_error: 0.07677359133958817 \t range_loss: 0.6531539559364319\n",
      "pred_error: 0.07677361369132996 \t range_loss: 0.6531539559364319\n",
      "pred_error: 0.07678939402103424 \t range_loss: 0.657450258731842\n",
      "pred_error: 0.07678931951522827 \t range_loss: 0.657450258731842\n",
      "pred_error: 0.07668877393007278 \t range_loss: 0.6526690125465393\n",
      "pred_error: 0.0768301710486412 \t range_loss: 0.653629720211029\n",
      "pred_error: 0.07696829736232758 \t range_loss: 0.6523799896240234\n",
      "pred_error: 0.07696829736232758 \t range_loss: 0.6523799896240234\n",
      "pred_error: 0.07677406072616577 \t range_loss: 0.6543968915939331\n",
      "pred_error: 0.07700584083795547 \t range_loss: 0.6528049111366272\n",
      "pred_error: 0.07700584828853607 \t range_loss: 0.6528049111366272\n",
      "pred_error: 0.07723934203386307 \t range_loss: 0.6522727608680725\n",
      "pred_error: 0.0771007090806961 \t range_loss: 0.6521506905555725\n",
      "pred_error: 0.07673005759716034 \t range_loss: 0.6512570381164551\n",
      "pred_error: 0.077457495033741 \t range_loss: 0.6515157222747803\n",
      "pred_error: 0.07743656635284424 \t range_loss: 0.6517846584320068\n",
      "pred_error: 0.07704002410173416 \t range_loss: 0.6549416184425354\n",
      "pred_error: 0.07718630135059357 \t range_loss: 0.6510630249977112\n",
      "pred_error: 0.07679472118616104 \t range_loss: 0.6510902643203735\n",
      "pred_error: 0.07739604264497757 \t range_loss: 0.6538459062576294\n",
      "pred_error: 0.07775145769119263 \t range_loss: 0.6505435705184937\n",
      "pred_error: 0.07775140553712845 \t range_loss: 0.6505435705184937\n",
      "pred_error: 0.07712697982788086 \t range_loss: 0.6494513750076294\n",
      "pred_error: 0.077235147356987 \t range_loss: 0.650505781173706\n",
      "pred_error: 0.07713711261749268 \t range_loss: 0.6532853245735168\n",
      "pred_error: 0.07713721692562103 \t range_loss: 0.6532853245735168\n",
      "pred_error: 0.07713715732097626 \t range_loss: 0.6532853245735168\n",
      "pred_error: 0.07760189473628998 \t range_loss: 0.6510336995124817\n",
      "pred_error: 0.07716549932956696 \t range_loss: 0.651945173740387\n",
      "pred_error: 0.07684094458818436 \t range_loss: 0.6537948250770569\n",
      "pred_error: 0.07733352482318878 \t range_loss: 0.6546679735183716\n",
      "pred_error: 0.0771370381116867 \t range_loss: 0.6531428694725037\n",
      "pred_error: 0.07713703066110611 \t range_loss: 0.6531428694725037\n",
      "pred_error: 0.07692108303308487 \t range_loss: 0.6514337062835693\n",
      "pred_error: 0.0771099105477333 \t range_loss: 0.6514722108840942\n",
      "pred_error: 0.07710998505353928 \t range_loss: 0.6514722108840942\n",
      "pred_error: 0.07706201076507568 \t range_loss: 0.6516655087471008\n",
      "pred_error: 0.07709448784589767 \t range_loss: 0.6519739031791687\n",
      "pred_error: 0.0771578699350357 \t range_loss: 0.6519330143928528\n",
      "pred_error: 0.07699814438819885 \t range_loss: 0.6532508134841919\n",
      "pred_error: 0.07737178355455399 \t range_loss: 0.6526395082473755\n",
      "pred_error: 0.07714313268661499 \t range_loss: 0.6542080640792847\n",
      "pred_error: 0.07714289426803589 \t range_loss: 0.6542080640792847\n",
      "pred_error: 0.07687142491340637 \t range_loss: 0.6539587378501892\n",
      "pred_error: 0.07703496515750885 \t range_loss: 0.6560735702514648\n",
      "pred_error: 0.07737445831298828 \t range_loss: 0.650934100151062\n",
      "pred_error: 0.077061727643013 \t range_loss: 0.6516279578208923\n",
      "pred_error: 0.07698007673025131 \t range_loss: 0.6531331539154053\n",
      "pred_error: 0.07698007673025131 \t range_loss: 0.6531331539154053\n",
      "pred_error: 0.07706214487552643 \t range_loss: 0.6551033854484558\n",
      "pred_error: 0.07679858058691025 \t range_loss: 0.6558672785758972\n",
      "pred_error: 0.07728927582502365 \t range_loss: 0.653281569480896\n",
      "pred_error: 0.07728922367095947 \t range_loss: 0.653281569480896\n",
      "pred_error: 0.07724079489707947 \t range_loss: 0.6526287198066711\n",
      "pred_error: 0.07687634229660034 \t range_loss: 0.6518787145614624\n",
      "pred_error: 0.07698656618595123 \t range_loss: 0.6532047390937805\n",
      "pred_error: 0.07673992961645126 \t range_loss: 0.6547577977180481\n",
      "pred_error: 0.07685627788305283 \t range_loss: 0.6540046334266663\n",
      "pred_error: 0.07691289484500885 \t range_loss: 0.654563307762146\n",
      "pred_error: 0.0769127607345581 \t range_loss: 0.654563307762146\n",
      "pred_error: 0.07727751135826111 \t range_loss: 0.6517518162727356\n",
      "pred_error: 0.07714191824197769 \t range_loss: 0.6526427268981934\n",
      "pred_error: 0.07689152657985687 \t range_loss: 0.6521766781806946\n",
      "pred_error: 0.07684428244829178 \t range_loss: 0.652044951915741\n",
      "pred_error: 0.0774388387799263 \t range_loss: 0.6534149050712585\n",
      "pred_error: 0.0774388387799263 \t range_loss: 0.6534149050712585\n",
      "pred_error: 0.07734695822000504 \t range_loss: 0.6516202688217163\n",
      "pred_error: 0.07698225975036621 \t range_loss: 0.6515759825706482\n",
      "pred_error: 0.07716114073991776 \t range_loss: 0.6529219746589661\n",
      "pred_error: 0.0772402435541153 \t range_loss: 0.6509008407592773\n",
      "pred_error: 0.07723909616470337 \t range_loss: 0.6509008407592773\n",
      "pred_error: 0.07692437618970871 \t range_loss: 0.6553774476051331\n",
      "pred_error: 0.07747186720371246 \t range_loss: 0.6513871550559998\n",
      "pred_error: 0.0771494060754776 \t range_loss: 0.6525440812110901\n",
      "pred_error: 0.07722066342830658 \t range_loss: 0.6504992246627808\n",
      "pred_error: 0.07722051441669464 \t range_loss: 0.6504992246627808\n",
      "pred_error: 0.07740046083927155 \t range_loss: 0.6525417566299438\n",
      "pred_error: 0.07777760177850723 \t range_loss: 0.650047242641449\n",
      "pred_error: 0.07777760177850723 \t range_loss: 0.650047242641449\n",
      "BEST LOSS: 1.4138396\n",
      "==== Model: block9_cob_activation_norm  in Layer: 9 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 18:35:23,789 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 18:40:18,025 model.rs:1246 value (393216) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 18:40:18,048 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 18:40:54,078 model.rs:1246 value (393216) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 18:40:54,084 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 18:40:54,105 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 18:40:54,123 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 18:40:54,136 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error     | median_error   | max_error    | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.000002296632 | 0.000037111342 | 0.0006939769 | -0.00067067146 | 0.00008371327  | 0.000037111342   | 0.0006939769  | 0             | 0.000000013730766  | 0.0030206195       | 0.0054899002           |\n",
      "+----------------+----------------+--------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 480312 64 [-1022058, 994510] 1 [16]\n",
      "===============================\n",
      "==== Model: block9_cob_activation_norm_teleported  in Layer: 9 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 18:41:29,894 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 18:46:14,538 model.rs:1246 value (-67264) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 18:46:14,545 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 18:46:49,552 model.rs:1246 value (-67264) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 18:46:49,557 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 18:46:49,573 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 18:46:49,597 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 18:46:49,608 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error  | max_error     | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000011552546 | 0.00013099611 | 0.00074386597 | -0.0006709099 | 0.00008297885  | 0.00013099611    | 0.00074386597 | 0             | 0.000000013559529  | -0.00018181656     | 0.0019571844           |\n",
      "+-----------------+---------------+---------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 480318 64 [-670326, 1009214] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 10 , \t  activation_stats: {'relu_1': {'norm': tensor(907.2410), 'max': tensor(4.6855), 'min': tensor(-8.9226), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(13.6081)\n",
      "pred_error: 0.07564935088157654 \t range_loss: 0.9955503940582275\n",
      "Step: 0 \t Loss: 1.722120761871338\n",
      "pred_error: 0.07559233903884888 \t range_loss: 0.9661923050880432\n",
      "pred_error: 0.07559285312891006 \t range_loss: 0.9661923050880432\n",
      "pred_error: 0.07559308409690857 \t range_loss: 0.9661923050880432\n",
      "pred_error: 0.07559266686439514 \t range_loss: 0.9661923050880432\n",
      "pred_error: 0.07559289038181305 \t range_loss: 0.9661923050880432\n",
      "Step: 1 \t Loss: 1.6990594863891602\n",
      "Step: 2 \t Loss: 1.6800744533538818\n",
      "pred_error: 0.07564790546894073 \t range_loss: 0.9235996603965759\n",
      "pred_error: 0.07564780116081238 \t range_loss: 0.9235996603965759\n",
      "Step: 3 \t Loss: 1.6677656173706055\n",
      "pred_error: 0.07570330053567886 \t range_loss: 0.9107280969619751\n",
      "Step: 4 \t Loss: 1.6578466892242432\n",
      "Step: 5 \t Loss: 1.6353402137756348\n",
      "Step: 6 \t Loss: 1.6350349187850952\n",
      "Step: 7 \t Loss: 1.6263363361358643\n",
      "Step: 8 \t Loss: 1.6165592670440674\n",
      "pred_error: 0.07614778727293015 \t range_loss: 0.8550803661346436\n",
      "Step: 9 \t Loss: 1.6139991283416748\n",
      "Step: 10 \t Loss: 1.6078388690948486\n",
      "pred_error: 0.07635706663131714 \t range_loss: 0.8442662954330444\n",
      "Step: 11 \t Loss: 1.6013462543487549\n",
      "Step: 12 \t Loss: 1.593214988708496\n",
      "Step: 13 \t Loss: 1.5900826454162598\n",
      "Step: 14 \t Loss: 1.5870386362075806\n",
      "Step: 15 \t Loss: 1.5848009586334229\n",
      "pred_error: 0.07704893499612808 \t range_loss: 0.8143107891082764\n",
      "Step: 16 \t Loss: 1.58070707321167\n",
      "Step: 18 \t Loss: 1.5766172409057617\n",
      "Step: 19 \t Loss: 1.5753328800201416\n",
      "pred_error: 0.07738890498876572 \t range_loss: 0.8021195530891418\n",
      "Step: 21 \t Loss: 1.5749574899673462\n",
      "Step: 22 \t Loss: 1.5640056133270264\n",
      "pred_error: 0.077422596514225 \t range_loss: 0.7897781729698181\n",
      "Step: 24 \t Loss: 1.562508463859558\n",
      "pred_error: 0.0776236429810524 \t range_loss: 0.7862704992294312\n",
      "pred_error: 0.07746820896863937 \t range_loss: 0.7897549271583557\n",
      "Step: 26 \t Loss: 1.5617246627807617\n",
      "pred_error: 0.07792088389396667 \t range_loss: 0.7825149893760681\n",
      "pred_error: 0.07792098820209503 \t range_loss: 0.7825149893760681\n",
      "Step: 27 \t Loss: 1.5591354370117188\n",
      "pred_error: 0.07760155946016312 \t range_loss: 0.7831156253814697\n",
      "Step: 28 \t Loss: 1.5565283298492432\n",
      "Step: 30 \t Loss: 1.5552992820739746\n",
      "pred_error: 0.07782071083784103 \t range_loss: 0.7770904898643494\n",
      "Step: 31 \t Loss: 1.5541090965270996\n",
      "pred_error: 0.07760585099458694 \t range_loss: 0.7780505418777466\n",
      "pred_error: 0.07817472517490387 \t range_loss: 0.7741998434066772\n",
      "pred_error: 0.07817456126213074 \t range_loss: 0.7741998434066772\n",
      "pred_error: 0.07809817790985107 \t range_loss: 0.7759358882904053\n",
      "Step: 34 \t Loss: 1.5539021492004395\n",
      "pred_error: 0.07791969180107117 \t range_loss: 0.7753333449363708\n",
      "Step: 36 \t Loss: 1.5483167171478271\n",
      "Step: 37 \t Loss: 1.5481925010681152\n",
      "Step: 38 \t Loss: 1.547647476196289\n",
      "Step: 40 \t Loss: 1.5456035137176514\n",
      "pred_error: 0.07870065420866013 \t range_loss: 0.7585938572883606\n",
      "Step: 41 \t Loss: 1.5426219701766968\n",
      "pred_error: 0.07840020954608917 \t range_loss: 0.7586201429367065\n",
      "pred_error: 0.07867981493473053 \t range_loss: 0.756163477897644\n",
      "pred_error: 0.07867424190044403 \t range_loss: 0.7567993402481079\n",
      "pred_error: 0.07870467752218246 \t range_loss: 0.7579396963119507\n",
      "pred_error: 0.0787048414349556 \t range_loss: 0.7579396963119507\n",
      "pred_error: 0.07881069928407669 \t range_loss: 0.7567858099937439\n",
      "pred_error: 0.07917769253253937 \t range_loss: 0.7510966658592224\n",
      "Step: 49 \t Loss: 1.538487434387207\n",
      "Step: 50 \t Loss: 1.5355470180511475\n",
      "pred_error: 0.07893214374780655 \t range_loss: 0.7462255358695984\n",
      "Step: 51 \t Loss: 1.5353503227233887\n",
      "pred_error: 0.07893882691860199 \t range_loss: 0.7459633946418762\n",
      "Step: 52 \t Loss: 1.5345027446746826\n",
      "Step: 53 \t Loss: 1.5341696739196777\n",
      "pred_error: 0.07960784435272217 \t range_loss: 0.7411816716194153\n",
      "pred_error: 0.07960770279169083 \t range_loss: 0.7411816716194153\n",
      "Step: 57 \t Loss: 1.5340464115142822\n",
      "Step: 58 \t Loss: 1.5332667827606201\n",
      "pred_error: 0.07950104027986526 \t range_loss: 0.7382553219795227\n",
      "Step: 59 \t Loss: 1.5328940153121948\n",
      "Step: 63 \t Loss: 1.5299179553985596\n",
      "pred_error: 0.07995786517858505 \t range_loss: 0.7366039752960205\n",
      "pred_error: 0.07995791733264923 \t range_loss: 0.7366039752960205\n",
      "Step: 71 \t Loss: 1.5286248922348022\n",
      "Step: 75 \t Loss: 1.5261504650115967\n",
      "pred_error: 0.07986275851726532 \t range_loss: 0.7312023639678955\n",
      "pred_error: 0.0801859125494957 \t range_loss: 0.7305788397789001\n",
      "pred_error: 0.07993866503238678 \t range_loss: 0.7297793030738831\n",
      "pred_error: 0.07993900775909424 \t range_loss: 0.7297793030738831\n",
      "pred_error: 0.07984302192926407 \t range_loss: 0.7303060293197632\n",
      "pred_error: 0.07989455759525299 \t range_loss: 0.7272705435752869\n",
      "Step: 86 \t Loss: 1.5252561569213867\n",
      "Step: 87 \t Loss: 1.5245144367218018\n",
      "pred_error: 0.08002348989248276 \t range_loss: 0.7242794632911682\n",
      "pred_error: 0.07988984137773514 \t range_loss: 0.7273426651954651\n",
      "pred_error: 0.08037865906953812 \t range_loss: 0.7256007194519043\n",
      "pred_error: 0.08037865906953812 \t range_loss: 0.7256007194519043\n",
      "pred_error: 0.08003528416156769 \t range_loss: 0.724614143371582\n",
      "pred_error: 0.08009909093379974 \t range_loss: 0.7238700985908508\n",
      "pred_error: 0.08009909093379974 \t range_loss: 0.7238700985908508\n",
      "Step: 94 \t Loss: 1.5211048126220703\n",
      "pred_error: 0.080475814640522 \t range_loss: 0.7232417464256287\n",
      "pred_error: 0.08037333190441132 \t range_loss: 0.7264779210090637\n",
      "Step: 103 \t Loss: 1.5203344821929932\n",
      "pred_error: 0.08005618304014206 \t range_loss: 0.7197697162628174\n",
      "pred_error: 0.0800565704703331 \t range_loss: 0.7197697162628174\n",
      "pred_error: 0.08027619868516922 \t range_loss: 0.7216181755065918\n",
      "pred_error: 0.08069372922182083 \t range_loss: 0.721447765827179\n",
      "pred_error: 0.08062838017940521 \t range_loss: 0.7201561331748962\n",
      "pred_error: 0.08062836527824402 \t range_loss: 0.7201561331748962\n",
      "pred_error: 0.08080455660820007 \t range_loss: 0.7139783501625061\n",
      "pred_error: 0.08073952794075012 \t range_loss: 0.7158458828926086\n",
      "pred_error: 0.08088328689336777 \t range_loss: 0.7176831364631653\n",
      "Step: 129 \t Loss: 1.5187286138534546\n",
      "pred_error: 0.08070789277553558 \t range_loss: 0.7137961983680725\n",
      "pred_error: 0.0812554880976677 \t range_loss: 0.7118352651596069\n",
      "pred_error: 0.08094415813684464 \t range_loss: 0.7108703851699829\n",
      "pred_error: 0.08094390481710434 \t range_loss: 0.7108703851699829\n",
      "pred_error: 0.0813978910446167 \t range_loss: 0.7162311673164368\n",
      "pred_error: 0.08120648562908173 \t range_loss: 0.7113202810287476\n",
      "Step: 146 \t Loss: 1.5158751010894775\n",
      "Step: 147 \t Loss: 1.515028715133667\n",
      "pred_error: 0.08195479214191437 \t range_loss: 0.7092596888542175\n",
      "pred_error: 0.08141649514436722 \t range_loss: 0.7061638832092285\n",
      "pred_error: 0.08141649514436722 \t range_loss: 0.7061638832092285\n",
      "pred_error: 0.08100118488073349 \t range_loss: 0.7091742157936096\n",
      "pred_error: 0.08155494928359985 \t range_loss: 0.7092896103858948\n",
      "pred_error: 0.08187530189752579 \t range_loss: 0.7073559165000916\n",
      "pred_error: 0.08132235705852509 \t range_loss: 0.7082054615020752\n",
      "pred_error: 0.08128958195447922 \t range_loss: 0.7060556411743164\n",
      "pred_error: 0.08157508820295334 \t range_loss: 0.7100628614425659\n",
      "pred_error: 0.08157560974359512 \t range_loss: 0.7100628614425659\n",
      "pred_error: 0.08106572180986404 \t range_loss: 0.7063512206077576\n",
      "pred_error: 0.08106575906276703 \t range_loss: 0.7063512206077576\n",
      "pred_error: 0.08120264858007431 \t range_loss: 0.7040273547172546\n",
      "pred_error: 0.08124739676713943 \t range_loss: 0.7059701085090637\n",
      "pred_error: 0.08126611262559891 \t range_loss: 0.707093358039856\n",
      "pred_error: 0.08171164244413376 \t range_loss: 0.7096151113510132\n",
      "pred_error: 0.08153413236141205 \t range_loss: 0.7042876482009888\n",
      "pred_error: 0.08118189871311188 \t range_loss: 0.7047904133796692\n",
      "pred_error: 0.08144193887710571 \t range_loss: 0.7066697478294373\n",
      "pred_error: 0.08138736337423325 \t range_loss: 0.7056871652603149\n",
      "pred_error: 0.08138541132211685 \t range_loss: 0.7024198174476624\n",
      "pred_error: 0.08159547299146652 \t range_loss: 0.7048534154891968\n",
      "Step: 198 \t Loss: 1.513454556465149\n",
      "pred_error: 0.08091390132904053 \t range_loss: 0.7043144702911377\n",
      "pred_error: 0.0818004459142685 \t range_loss: 0.7054688930511475\n",
      "pred_error: 0.08164730668067932 \t range_loss: 0.7048875093460083\n",
      "pred_error: 0.08173871785402298 \t range_loss: 0.7043827772140503\n",
      "Step: 215 \t Loss: 1.5127861499786377\n",
      "pred_error: 0.08120718598365784 \t range_loss: 0.7007142901420593\n",
      "pred_error: 0.08157549798488617 \t range_loss: 0.7030119895935059\n",
      "pred_error: 0.08202262967824936 \t range_loss: 0.7004351019859314\n",
      "pred_error: 0.08235260099172592 \t range_loss: 0.6977154612541199\n",
      "pred_error: 0.08209507912397385 \t range_loss: 0.6981743574142456\n",
      "pred_error: 0.08202201873064041 \t range_loss: 0.7022642493247986\n",
      "pred_error: 0.08202183991670609 \t range_loss: 0.7022642493247986\n",
      "pred_error: 0.08202242851257324 \t range_loss: 0.7022642493247986\n",
      "pred_error: 0.0817878320813179 \t range_loss: 0.7005559802055359\n",
      "pred_error: 0.08198779076337814 \t range_loss: 0.6979763507843018\n",
      "pred_error: 0.08228664100170135 \t range_loss: 0.7022161483764648\n",
      "pred_error: 0.08240567147731781 \t range_loss: 0.7035648226737976\n",
      "pred_error: 0.08225620537996292 \t range_loss: 0.6962035894393921\n",
      "pred_error: 0.08176370710134506 \t range_loss: 0.6963968873023987\n",
      "pred_error: 0.08202090859413147 \t range_loss: 0.6955054998397827\n",
      "Step: 255 \t Loss: 1.5121349096298218\n",
      "pred_error: 0.08164333552122116 \t range_loss: 0.6957032680511475\n",
      "pred_error: 0.08252411335706711 \t range_loss: 0.6987237334251404\n",
      "pred_error: 0.082857646048069 \t range_loss: 0.6956650018692017\n",
      "pred_error: 0.08182401210069656 \t range_loss: 0.6954641342163086\n",
      "pred_error: 0.08295580744743347 \t range_loss: 0.6932047605514526\n",
      "pred_error: 0.08261357247829437 \t range_loss: 0.6938996911048889\n",
      "pred_error: 0.08218006044626236 \t range_loss: 0.6969515681266785\n",
      "pred_error: 0.0824837014079094 \t range_loss: 0.6966007947921753\n",
      "pred_error: 0.08248378336429596 \t range_loss: 0.6966007947921753\n",
      "pred_error: 0.08263099938631058 \t range_loss: 0.6969850659370422\n",
      "pred_error: 0.0823642686009407 \t range_loss: 0.6967080235481262\n",
      "pred_error: 0.08236409723758698 \t range_loss: 0.6967080235481262\n",
      "pred_error: 0.08234410732984543 \t range_loss: 0.696590781211853\n",
      "pred_error: 0.08186899870634079 \t range_loss: 0.6983823776245117\n",
      "pred_error: 0.08225568383932114 \t range_loss: 0.6987894773483276\n",
      "pred_error: 0.0822620764374733 \t range_loss: 0.6987894773483276\n",
      "pred_error: 0.08226217329502106 \t range_loss: 0.6987894773483276\n",
      "pred_error: 0.08215352147817612 \t range_loss: 0.6972312927246094\n",
      "pred_error: 0.08210993558168411 \t range_loss: 0.701298177242279\n",
      "pred_error: 0.08237694203853607 \t range_loss: 0.6942838430404663\n",
      "pred_error: 0.08193840086460114 \t range_loss: 0.6989971995353699\n",
      "pred_error: 0.08230385929346085 \t range_loss: 0.6975023150444031\n",
      "pred_error: 0.08253873139619827 \t range_loss: 0.6965844631195068\n",
      "pred_error: 0.08253850787878036 \t range_loss: 0.6965844631195068\n",
      "pred_error: 0.08262665569782257 \t range_loss: 0.6940383315086365\n",
      "pred_error: 0.08226503431797028 \t range_loss: 0.6953839063644409\n",
      "pred_error: 0.08226499706506729 \t range_loss: 0.6953839063644409\n",
      "pred_error: 0.08218944817781448 \t range_loss: 0.6973039507865906\n",
      "pred_error: 0.08229269087314606 \t range_loss: 0.6996241211891174\n",
      "pred_error: 0.0823199599981308 \t range_loss: 0.6963391304016113\n",
      "pred_error: 0.08231988549232483 \t range_loss: 0.6963391304016113\n",
      "pred_error: 0.08213188499212265 \t range_loss: 0.6977154016494751\n",
      "pred_error: 0.08217544108629227 \t range_loss: 0.6953415870666504\n",
      "Step: 324 \t Loss: 1.5117982625961304\n",
      "pred_error: 0.08250587433576584 \t range_loss: 0.6971458196640015\n",
      "pred_error: 0.08227554708719254 \t range_loss: 0.699738621711731\n",
      "pred_error: 0.08291856944561005 \t range_loss: 0.6932132244110107\n",
      "pred_error: 0.0823952928185463 \t range_loss: 0.6936242580413818\n",
      "pred_error: 0.08191584050655365 \t range_loss: 0.6946858167648315\n",
      "pred_error: 0.08191590011119843 \t range_loss: 0.6946858167648315\n",
      "pred_error: 0.08249074220657349 \t range_loss: 0.6943192481994629\n",
      "pred_error: 0.08227121084928513 \t range_loss: 0.6974491477012634\n",
      "pred_error: 0.0821649432182312 \t range_loss: 0.694522500038147\n",
      "pred_error: 0.08216537535190582 \t range_loss: 0.694522500038147\n",
      "pred_error: 0.08245586603879929 \t range_loss: 0.6920645833015442\n",
      "pred_error: 0.08272358030080795 \t range_loss: 0.6943156123161316\n",
      "pred_error: 0.08208755403757095 \t range_loss: 0.6975857615470886\n",
      "pred_error: 0.08208755403757095 \t range_loss: 0.6975857615470886\n",
      "pred_error: 0.0822797641158104 \t range_loss: 0.6959981322288513\n",
      "pred_error: 0.08227542042732239 \t range_loss: 0.6930774450302124\n",
      "pred_error: 0.08227574080228806 \t range_loss: 0.6930774450302124\n",
      "pred_error: 0.08190691471099854 \t range_loss: 0.6963117718696594\n",
      "pred_error: 0.08190710842609406 \t range_loss: 0.6963117718696594\n",
      "pred_error: 0.0824151411652565 \t range_loss: 0.6953036189079285\n",
      "pred_error: 0.08288253843784332 \t range_loss: 0.6935362219810486\n",
      "pred_error: 0.08241911232471466 \t range_loss: 0.6932016015052795\n",
      "pred_error: 0.08256051689386368 \t range_loss: 0.6935669779777527\n",
      "pred_error: 0.08287561684846878 \t range_loss: 0.6951309442520142\n",
      "pred_error: 0.08207879960536957 \t range_loss: 0.6964693665504456\n",
      "pred_error: 0.08175282925367355 \t range_loss: 0.6969687342643738\n",
      "pred_error: 0.0819910392165184 \t range_loss: 0.6938695907592773\n",
      "pred_error: 0.08241398632526398 \t range_loss: 0.6940510272979736\n",
      "pred_error: 0.08157087117433548 \t range_loss: 0.7002524137496948\n",
      "pred_error: 0.08157079666852951 \t range_loss: 0.7002524137496948\n",
      "pred_error: 0.08157091587781906 \t range_loss: 0.7002524137496948\n",
      "pred_error: 0.08247477561235428 \t range_loss: 0.6965831518173218\n",
      "pred_error: 0.08224424719810486 \t range_loss: 0.6990177631378174\n",
      "pred_error: 0.08239118754863739 \t range_loss: 0.6944728493690491\n",
      "pred_error: 0.08240726590156555 \t range_loss: 0.6950538158416748\n",
      "pred_error: 0.08199330419301987 \t range_loss: 0.6952970027923584\n",
      "pred_error: 0.08225522935390472 \t range_loss: 0.6951722502708435\n",
      "pred_error: 0.08178364485502243 \t range_loss: 0.6964238882064819\n",
      "pred_error: 0.0817837044596672 \t range_loss: 0.6964238882064819\n",
      "pred_error: 0.08234690874814987 \t range_loss: 0.6977141499519348\n",
      "pred_error: 0.0826486125588417 \t range_loss: 0.694198489189148\n",
      "pred_error: 0.08229419589042664 \t range_loss: 0.6943014860153198\n",
      "pred_error: 0.0826725959777832 \t range_loss: 0.6926565170288086\n",
      "pred_error: 0.08246975392103195 \t range_loss: 0.6945715546607971\n",
      "pred_error: 0.08234848827123642 \t range_loss: 0.6945391297340393\n",
      "pred_error: 0.08234848827123642 \t range_loss: 0.6945391297340393\n",
      "pred_error: 0.08234858512878418 \t range_loss: 0.6945391297340393\n",
      "pred_error: 0.08207909017801285 \t range_loss: 0.6952028274536133\n",
      "pred_error: 0.08222600072622299 \t range_loss: 0.6951413750648499\n",
      "pred_error: 0.08205052465200424 \t range_loss: 0.6966975331306458\n",
      "pred_error: 0.0824570432305336 \t range_loss: 0.6961525082588196\n",
      "pred_error: 0.08219026029109955 \t range_loss: 0.695359468460083\n",
      "pred_error: 0.08228636533021927 \t range_loss: 0.695127010345459\n",
      "pred_error: 0.08228647708892822 \t range_loss: 0.695127010345459\n",
      "pred_error: 0.08253554254770279 \t range_loss: 0.6933329105377197\n",
      "pred_error: 0.0820741131901741 \t range_loss: 0.6927520632743835\n",
      "pred_error: 0.08207409828901291 \t range_loss: 0.6927520632743835\n",
      "pred_error: 0.08183953166007996 \t range_loss: 0.6954241394996643\n",
      "pred_error: 0.08246434479951859 \t range_loss: 0.6939468383789062\n",
      "pred_error: 0.08242590725421906 \t range_loss: 0.6947629451751709\n",
      "pred_error: 0.08239632844924927 \t range_loss: 0.6925724148750305\n",
      "pred_error: 0.08213727921247482 \t range_loss: 0.6983606815338135\n",
      "pred_error: 0.08210910856723785 \t range_loss: 0.6990476250648499\n",
      "pred_error: 0.08239007741212845 \t range_loss: 0.6919376850128174\n",
      "pred_error: 0.08196448534727097 \t range_loss: 0.6958042979240417\n",
      "pred_error: 0.08210168033838272 \t range_loss: 0.6934080719947815\n",
      "pred_error: 0.0819091871380806 \t range_loss: 0.6931065320968628\n",
      "Step: 493 \t Loss: 1.5116832256317139\n",
      "pred_error: 0.08174460381269455 \t range_loss: 0.6942371129989624\n",
      "pred_error: 0.08237853646278381 \t range_loss: 0.6947378516197205\n",
      "pred_error: 0.08237853646278381 \t range_loss: 0.6947378516197205\n",
      "pred_error: 0.08240882307291031 \t range_loss: 0.6922330856323242\n",
      "pred_error: 0.0824085995554924 \t range_loss: 0.6922330856323242\n",
      "pred_error: 0.08237019926309586 \t range_loss: 0.6948460936546326\n",
      "pred_error: 0.08190605044364929 \t range_loss: 0.6950855851173401\n",
      "pred_error: 0.08190605044364929 \t range_loss: 0.6950855851173401\n",
      "pred_error: 0.08225488662719727 \t range_loss: 0.6961295008659363\n",
      "pred_error: 0.08214108645915985 \t range_loss: 0.6962969303131104\n",
      "pred_error: 0.08214108645915985 \t range_loss: 0.6962969303131104\n",
      "pred_error: 0.08214104920625687 \t range_loss: 0.6962969303131104\n",
      "pred_error: 0.08214090019464493 \t range_loss: 0.6962969303131104\n",
      "pred_error: 0.08214090019464493 \t range_loss: 0.6962969303131104\n",
      "pred_error: 0.08196450769901276 \t range_loss: 0.694311797618866\n",
      "pred_error: 0.08219730108976364 \t range_loss: 0.6945154666900635\n",
      "pred_error: 0.08212094008922577 \t range_loss: 0.695932924747467\n",
      "pred_error: 0.08212082087993622 \t range_loss: 0.695932924747467\n",
      "pred_error: 0.08212089538574219 \t range_loss: 0.695932924747467\n",
      "pred_error: 0.08223570883274078 \t range_loss: 0.6992995142936707\n",
      "pred_error: 0.08215536922216415 \t range_loss: 0.6967171430587769\n",
      "pred_error: 0.08271019160747528 \t range_loss: 0.6951777338981628\n",
      "pred_error: 0.08209333568811417 \t range_loss: 0.6941629648208618\n",
      "pred_error: 0.08176632225513458 \t range_loss: 0.6974909901618958\n",
      "pred_error: 0.08176080882549286 \t range_loss: 0.6974909901618958\n",
      "pred_error: 0.0822809711098671 \t range_loss: 0.6974415183067322\n",
      "pred_error: 0.0822809711098671 \t range_loss: 0.6974415183067322\n",
      "pred_error: 0.0822809487581253 \t range_loss: 0.6974415183067322\n",
      "pred_error: 0.08250172436237335 \t range_loss: 0.6968427300453186\n",
      "pred_error: 0.08236342668533325 \t range_loss: 0.6967042684555054\n",
      "pred_error: 0.08183126896619797 \t range_loss: 0.695418655872345\n",
      "pred_error: 0.08211006969213486 \t range_loss: 0.6945120096206665\n",
      "pred_error: 0.0821099653840065 \t range_loss: 0.6945120096206665\n",
      "pred_error: 0.08223352581262589 \t range_loss: 0.696772038936615\n",
      "pred_error: 0.0823562890291214 \t range_loss: 0.694708526134491\n",
      "pred_error: 0.0818161591887474 \t range_loss: 0.6967892646789551\n",
      "pred_error: 0.08186260610818863 \t range_loss: 0.6963968873023987\n",
      "pred_error: 0.08187486976385117 \t range_loss: 0.6989670395851135\n",
      "pred_error: 0.08204321563243866 \t range_loss: 0.6957934498786926\n",
      "pred_error: 0.08242470771074295 \t range_loss: 0.6975190043449402\n",
      "pred_error: 0.08242467790842056 \t range_loss: 0.6975190043449402\n",
      "pred_error: 0.08217477798461914 \t range_loss: 0.6960247159004211\n",
      "pred_error: 0.08217497169971466 \t range_loss: 0.6960247159004211\n",
      "pred_error: 0.08170757442712784 \t range_loss: 0.6969946622848511\n",
      "pred_error: 0.08246715366840363 \t range_loss: 0.6970628499984741\n",
      "pred_error: 0.08234180510044098 \t range_loss: 0.6966420412063599\n",
      "pred_error: 0.08205096423625946 \t range_loss: 0.6947931051254272\n",
      "pred_error: 0.08236736059188843 \t range_loss: 0.6927125453948975\n",
      "pred_error: 0.08243077248334885 \t range_loss: 0.6957523822784424\n",
      "pred_error: 0.08224640041589737 \t range_loss: 0.6972506046295166\n",
      "pred_error: 0.08189109712839127 \t range_loss: 0.6980419754981995\n",
      "pred_error: 0.08210685104131699 \t range_loss: 0.6963908672332764\n",
      "pred_error: 0.08210690319538116 \t range_loss: 0.6963908672332764\n",
      "pred_error: 0.08260481059551239 \t range_loss: 0.6960393190383911\n",
      "pred_error: 0.08206168562173843 \t range_loss: 0.6950592994689941\n",
      "pred_error: 0.08209610730409622 \t range_loss: 0.6955784559249878\n",
      "pred_error: 0.08229051530361176 \t range_loss: 0.6995338797569275\n",
      "pred_error: 0.08253742754459381 \t range_loss: 0.6932323575019836\n",
      "pred_error: 0.08253742754459381 \t range_loss: 0.6932323575019836\n",
      "pred_error: 0.0822729840874672 \t range_loss: 0.6923874616622925\n",
      "Step: 598 \t Loss: 1.5108251571655273\n",
      "pred_error: 0.08208902925252914 \t range_loss: 0.6952919960021973\n",
      "pred_error: 0.08202944695949554 \t range_loss: 0.702674388885498\n",
      "pred_error: 0.08245449513196945 \t range_loss: 0.6979267001152039\n",
      "pred_error: 0.08245471119880676 \t range_loss: 0.6979267001152039\n",
      "pred_error: 0.08176159113645554 \t range_loss: 0.6971011757850647\n",
      "pred_error: 0.08176155388355255 \t range_loss: 0.6971011757850647\n",
      "pred_error: 0.0818043202161789 \t range_loss: 0.6972398161888123\n",
      "pred_error: 0.08228275924921036 \t range_loss: 0.6924569010734558\n",
      "pred_error: 0.0823216587305069 \t range_loss: 0.692747175693512\n",
      "pred_error: 0.08257534354925156 \t range_loss: 0.6966537237167358\n",
      "pred_error: 0.08238266408443451 \t range_loss: 0.6962657570838928\n",
      "pred_error: 0.08222071826457977 \t range_loss: 0.694839596748352\n",
      "pred_error: 0.0824199765920639 \t range_loss: 0.6935930848121643\n",
      "pred_error: 0.08232945203781128 \t range_loss: 0.6937474012374878\n",
      "pred_error: 0.08244097977876663 \t range_loss: 0.6913914084434509\n",
      "pred_error: 0.08200658112764359 \t range_loss: 0.6926722526550293\n",
      "pred_error: 0.08200682699680328 \t range_loss: 0.6926722526550293\n",
      "pred_error: 0.08242262154817581 \t range_loss: 0.6958792805671692\n",
      "pred_error: 0.08242284506559372 \t range_loss: 0.6958792805671692\n",
      "pred_error: 0.08265808969736099 \t range_loss: 0.6924732327461243\n",
      "pred_error: 0.08265801519155502 \t range_loss: 0.6924732327461243\n",
      "pred_error: 0.08261754363775253 \t range_loss: 0.6923109889030457\n",
      "pred_error: 0.08261796832084656 \t range_loss: 0.6923109889030457\n",
      "Step: 658 \t Loss: 1.5108060836791992\n",
      "pred_error: 0.08223126828670502 \t range_loss: 0.693515956401825\n",
      "pred_error: 0.08250422030687332 \t range_loss: 0.6962254047393799\n",
      "pred_error: 0.08292745053768158 \t range_loss: 0.6928890943527222\n",
      "pred_error: 0.08251385390758514 \t range_loss: 0.6918230056762695\n",
      "pred_error: 0.08300123363733292 \t range_loss: 0.6921818852424622\n",
      "pred_error: 0.08237478137016296 \t range_loss: 0.6925960183143616\n",
      "pred_error: 0.08237487822771072 \t range_loss: 0.6925960183143616\n",
      "pred_error: 0.08192720264196396 \t range_loss: 0.7007612586021423\n",
      "pred_error: 0.08231712877750397 \t range_loss: 0.6970962285995483\n",
      "pred_error: 0.08231712132692337 \t range_loss: 0.6970962285995483\n",
      "pred_error: 0.08231712877750397 \t range_loss: 0.6970962285995483\n",
      "pred_error: 0.08228899538516998 \t range_loss: 0.6944968104362488\n",
      "pred_error: 0.08238297700881958 \t range_loss: 0.6930927038192749\n",
      "pred_error: 0.08188818395137787 \t range_loss: 0.6943008899688721\n",
      "pred_error: 0.08272074908018112 \t range_loss: 0.6972164511680603\n",
      "pred_error: 0.0824437290430069 \t range_loss: 0.6932215094566345\n",
      "pred_error: 0.08164016902446747 \t range_loss: 0.6985683441162109\n",
      "pred_error: 0.08212922513484955 \t range_loss: 0.694929838180542\n",
      "pred_error: 0.08213026076555252 \t range_loss: 0.6958574056625366\n",
      "pred_error: 0.08232936263084412 \t range_loss: 0.6973235607147217\n",
      "pred_error: 0.08191806077957153 \t range_loss: 0.6941044926643372\n",
      "pred_error: 0.08246903866529465 \t range_loss: 0.6944724321365356\n",
      "pred_error: 0.08246909826993942 \t range_loss: 0.6944724321365356\n",
      "pred_error: 0.08261759579181671 \t range_loss: 0.6929073929786682\n",
      "pred_error: 0.082577645778656 \t range_loss: 0.691368579864502\n",
      "pred_error: 0.08206450939178467 \t range_loss: 0.6923032999038696\n",
      "pred_error: 0.08263268321752548 \t range_loss: 0.6954251527786255\n",
      "pred_error: 0.08259187638759613 \t range_loss: 0.6916121244430542\n",
      "pred_error: 0.08257274329662323 \t range_loss: 0.6918346285820007\n",
      "pred_error: 0.08239402621984482 \t range_loss: 0.6912336945533752\n",
      "pred_error: 0.08263074606657028 \t range_loss: 0.6931443810462952\n",
      "pred_error: 0.08257506787776947 \t range_loss: 0.691609263420105\n",
      "pred_error: 0.08257483690977097 \t range_loss: 0.691609263420105\n",
      "pred_error: 0.08269397914409637 \t range_loss: 0.6942753791809082\n",
      "pred_error: 0.08266357332468033 \t range_loss: 0.6906384825706482\n",
      "pred_error: 0.08222144842147827 \t range_loss: 0.6980519890785217\n",
      "pred_error: 0.0827246904373169 \t range_loss: 0.6928980350494385\n",
      "pred_error: 0.0825929343700409 \t range_loss: 0.6929340958595276\n",
      "pred_error: 0.08259300887584686 \t range_loss: 0.6929340958595276\n",
      "pred_error: 0.08262398838996887 \t range_loss: 0.6923719644546509\n",
      "pred_error: 0.08245449513196945 \t range_loss: 0.6913703680038452\n",
      "pred_error: 0.08288679271936417 \t range_loss: 0.6916375160217285\n",
      "pred_error: 0.08253253251314163 \t range_loss: 0.691031277179718\n",
      "pred_error: 0.08267533034086227 \t range_loss: 0.6913809180259705\n",
      "pred_error: 0.08229666203260422 \t range_loss: 0.6923249959945679\n",
      "pred_error: 0.08261589705944061 \t range_loss: 0.6923883557319641\n",
      "pred_error: 0.08261629194021225 \t range_loss: 0.6923883557319641\n",
      "pred_error: 0.08265912532806396 \t range_loss: 0.6898270845413208\n",
      "pred_error: 0.08265923708677292 \t range_loss: 0.6898270845413208\n",
      "pred_error: 0.08265575021505356 \t range_loss: 0.6914672255516052\n",
      "pred_error: 0.08229143917560577 \t range_loss: 0.6895884871482849\n",
      "pred_error: 0.08276647329330444 \t range_loss: 0.6927671432495117\n",
      "pred_error: 0.08276642113924026 \t range_loss: 0.6927671432495117\n",
      "pred_error: 0.08276638388633728 \t range_loss: 0.6927671432495117\n",
      "pred_error: 0.08322048932313919 \t range_loss: 0.6891963481903076\n",
      "pred_error: 0.08309802412986755 \t range_loss: 0.689657986164093\n",
      "pred_error: 0.08309806138277054 \t range_loss: 0.689657986164093\n",
      "pred_error: 0.08285579830408096 \t range_loss: 0.6897943019866943\n",
      "pred_error: 0.08300750702619553 \t range_loss: 0.6931446194648743\n",
      "pred_error: 0.08319175243377686 \t range_loss: 0.689691424369812\n",
      "pred_error: 0.08319185674190521 \t range_loss: 0.689691424369812\n",
      "pred_error: 0.08255375921726227 \t range_loss: 0.6898118257522583\n",
      "pred_error: 0.0825534388422966 \t range_loss: 0.6898118257522583\n",
      "pred_error: 0.08293973654508591 \t range_loss: 0.6878737211227417\n",
      "pred_error: 0.08249551057815552 \t range_loss: 0.6919177174568176\n",
      "pred_error: 0.08258236199617386 \t range_loss: 0.6912862658500671\n",
      "pred_error: 0.08282923698425293 \t range_loss: 0.6919502019882202\n",
      "pred_error: 0.08281788975000381 \t range_loss: 0.6926921010017395\n",
      "pred_error: 0.08290668576955795 \t range_loss: 0.6902807950973511\n",
      "pred_error: 0.0828358381986618 \t range_loss: 0.6890586614608765\n",
      "pred_error: 0.08283589035272598 \t range_loss: 0.6890586614608765\n",
      "pred_error: 0.08247757703065872 \t range_loss: 0.690981924533844\n",
      "pred_error: 0.08246294409036636 \t range_loss: 0.6941959857940674\n",
      "pred_error: 0.08291537314653397 \t range_loss: 0.6924452185630798\n",
      "pred_error: 0.08275460451841354 \t range_loss: 0.6958982348442078\n",
      "pred_error: 0.0826086476445198 \t range_loss: 0.6905500292778015\n",
      "pred_error: 0.0823398306965828 \t range_loss: 0.6922972798347473\n",
      "BEST LOSS: 1.5108061\n",
      "==== Model: block10_cob_activation_norm  in Layer: 10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 19:39:05,266 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 19:41:54,222 model.rs:1246 value (24384) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 19:41:54,228 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 19:42:17,779 model.rs:1246 value (24384) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 19:42:17,786 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 19:42:17,812 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 19:42:17,840 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 19:42:17,856 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error   | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.00000025132047 | -0.00004208088 | 0.0006685257 | -0.0006670952 | 0.0000545973   | 0.00004208088    | 0.0006685257  | 0             | 0.000000006134236  | 0.0000973302       | 0.0013384807           |\n",
      "+------------------+----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 539651 64 [-826962, 1168916] 1 [16]\n",
      "===============================\n",
      "==== Model: block10_cob_activation_norm_teleported  in Layer: 10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 19:42:42,123 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 19:45:32,032 model.rs:1246 value (-13056) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 19:45:32,050 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 19:45:56,048 model.rs:1246 value (-13056) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 19:45:56,054 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 19:45:56,088 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 19:45:56,142 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 19:45:56,157 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error      | median_error   | max_error     | min_error      | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| 0.0000019726579 | 0.000081807375 | 0.00051534176 | -0.00072050095 | 0.00005372495  | 0.000081807375   | 0.00072050095 | 0             | 0.0000000060218666 | 0.000038196365     | 0.0013509355           |\n",
      "+-----------------+----------------+---------------+----------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 539650 64 [-607314, 1190862] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n",
      "layer_idx: 11 , \t  activation_stats: {'relu_1': {'norm': tensor(872.8521), 'max': tensor(4.7341), 'min': tensor(-9.7593), 'shape': torch.Size([1, 197, 768])}}\n",
      "ORIGINAL LOSS: tensor(14.4935)\n",
      "Step: 0 \t Loss: 1.6197898387908936\n",
      "Step: 1 \t Loss: 1.590100884437561\n",
      "pred_error: 0.06707900017499924 \t range_loss: 0.9193113446235657\n",
      "Step: 2 \t Loss: 1.5699938535690308\n",
      "Step: 3 \t Loss: 1.5585746765136719\n",
      "Step: 4 \t Loss: 1.5569560527801514\n",
      "pred_error: 0.06697983294725418 \t range_loss: 0.8871577978134155\n",
      "Step: 5 \t Loss: 1.5520381927490234\n",
      "Step: 6 \t Loss: 1.5373609066009521\n",
      "pred_error: 0.06688646972179413 \t range_loss: 0.8684974908828735\n",
      "Step: 7 \t Loss: 1.530104398727417\n",
      "pred_error: 0.06683629006147385 \t range_loss: 0.8617414236068726\n",
      "Step: 8 \t Loss: 1.5260322093963623\n",
      "Step: 9 \t Loss: 1.5186681747436523\n",
      "Step: 10 \t Loss: 1.511795997619629\n",
      "pred_error: 0.06685760617256165 \t range_loss: 0.8432199358940125\n",
      "Step: 11 \t Loss: 1.505939245223999\n",
      "Step: 12 \t Loss: 1.502192735671997\n",
      "Step: 13 \t Loss: 1.4974796772003174\n",
      "Step: 14 \t Loss: 1.4961512088775635\n",
      "pred_error: 0.06688667833805084 \t range_loss: 0.8272844552993774\n",
      "Step: 15 \t Loss: 1.491553783416748\n",
      "Step: 16 \t Loss: 1.4842174053192139\n",
      "Step: 17 \t Loss: 1.482653021812439\n",
      "pred_error: 0.066782146692276 \t range_loss: 0.814831554889679\n",
      "pred_error: 0.06678202003240585 \t range_loss: 0.814831554889679\n",
      "Step: 18 \t Loss: 1.4764150381088257\n",
      "pred_error: 0.06679704785346985 \t range_loss: 0.8084473609924316\n",
      "Step: 19 \t Loss: 1.474806547164917\n",
      "Step: 20 \t Loss: 1.474134087562561\n",
      "Step: 21 \t Loss: 1.4701100587844849\n",
      "Step: 22 \t Loss: 1.4694486856460571\n",
      "Step: 23 \t Loss: 1.4672486782073975\n",
      "pred_error: 0.0669311061501503 \t range_loss: 0.7979347109794617\n",
      "pred_error: 0.06693100929260254 \t range_loss: 0.7979347109794617\n",
      "Step: 24 \t Loss: 1.4639631509780884\n",
      "pred_error: 0.06705666333436966 \t range_loss: 0.7933977246284485\n",
      "Step: 25 \t Loss: 1.4623311758041382\n",
      "pred_error: 0.06705222278833389 \t range_loss: 0.7918089628219604\n",
      "Step: 26 \t Loss: 1.458453893661499\n",
      "Step: 27 \t Loss: 1.4567359685897827\n",
      "pred_error: 0.06706076115369797 \t range_loss: 0.7861244082450867\n",
      "Step: 28 \t Loss: 1.4528194665908813\n",
      "pred_error: 0.06700032949447632 \t range_loss: 0.7828143835067749\n",
      "Step: 29 \t Loss: 1.4512969255447388\n",
      "pred_error: 0.06697978079319 \t range_loss: 0.7814938426017761\n",
      "pred_error: 0.06698019057512283 \t range_loss: 0.7814938426017761\n",
      "Step: 30 \t Loss: 1.4497672319412231\n",
      "Step: 31 \t Loss: 1.4461437463760376\n",
      "Step: 32 \t Loss: 1.4448250532150269\n",
      "Step: 33 \t Loss: 1.442190408706665\n",
      "Step: 34 \t Loss: 1.440009355545044\n",
      "Step: 35 \t Loss: 1.4389675855636597\n",
      "pred_error: 0.06707258522510529 \t range_loss: 0.7682409882545471\n",
      "Step: 36 \t Loss: 1.4385428428649902\n",
      "pred_error: 0.06701619178056717 \t range_loss: 0.7683811187744141\n",
      "Step: 38 \t Loss: 1.4373383522033691\n",
      "pred_error: 0.06727728992700577 \t range_loss: 0.7645645141601562\n",
      "Step: 39 \t Loss: 1.4358158111572266\n",
      "pred_error: 0.06722034513950348 \t range_loss: 0.7636080980300903\n",
      "Step: 40 \t Loss: 1.4348018169403076\n",
      "Step: 42 \t Loss: 1.4324593544006348\n",
      "Step: 43 \t Loss: 1.4283912181854248\n",
      "pred_error: 0.06723854690790176 \t range_loss: 0.7560058236122131\n",
      "Step: 44 \t Loss: 1.4260751008987427\n",
      "Step: 46 \t Loss: 1.4247561693191528\n",
      "Step: 47 \t Loss: 1.421639084815979\n",
      "pred_error: 0.06734275072813034 \t range_loss: 0.7482131719589233\n",
      "pred_error: 0.06734248995780945 \t range_loss: 0.7482131719589233\n",
      "Step: 48 \t Loss: 1.4200128316879272\n",
      "Step: 49 \t Loss: 1.4193469285964966\n",
      "pred_error: 0.06726493686437607 \t range_loss: 0.7466962933540344\n",
      "pred_error: 0.0674472525715828 \t range_loss: 0.7454312443733215\n",
      "pred_error: 0.06744768470525742 \t range_loss: 0.7454312443733215\n",
      "Step: 51 \t Loss: 1.4186244010925293\n",
      "Step: 52 \t Loss: 1.416898488998413\n",
      "pred_error: 0.06749651581048965 \t range_loss: 0.7419306039810181\n",
      "Step: 53 \t Loss: 1.4151623249053955\n",
      "Step: 54 \t Loss: 1.414233922958374\n",
      "pred_error: 0.06751757860183716 \t range_loss: 0.7390565872192383\n",
      "Step: 55 \t Loss: 1.4133919477462769\n",
      "pred_error: 0.06752338260412216 \t range_loss: 0.7381582260131836\n",
      "pred_error: 0.06764921545982361 \t range_loss: 0.7377604842185974\n",
      "pred_error: 0.0676492229104042 \t range_loss: 0.7377604842185974\n",
      "pred_error: 0.0676492229104042 \t range_loss: 0.7377604842185974\n",
      "Step: 58 \t Loss: 1.4123680591583252\n",
      "pred_error: 0.06773512810468674 \t range_loss: 0.7350144386291504\n",
      "pred_error: 0.06788506358861923 \t range_loss: 0.7343453168869019\n",
      "Step: 61 \t Loss: 1.4092211723327637\n",
      "pred_error: 0.06778085976839066 \t range_loss: 0.7314111590385437\n",
      "Step: 62 \t Loss: 1.4080923795700073\n",
      "Step: 63 \t Loss: 1.4066588878631592\n",
      "pred_error: 0.06772112101316452 \t range_loss: 0.729447603225708\n",
      "Step: 64 \t Loss: 1.405864953994751\n",
      "Step: 65 \t Loss: 1.4047263860702515\n",
      "pred_error: 0.06766149401664734 \t range_loss: 0.7281075119972229\n",
      "pred_error: 0.06766185909509659 \t range_loss: 0.7281075119972229\n",
      "pred_error: 0.06777244806289673 \t range_loss: 0.7279022932052612\n",
      "pred_error: 0.06777258217334747 \t range_loss: 0.7279022932052612\n",
      "Step: 67 \t Loss: 1.4045599699020386\n",
      "pred_error: 0.06773877888917923 \t range_loss: 0.7271721959114075\n",
      "Step: 70 \t Loss: 1.4043631553649902\n",
      "pred_error: 0.06792230904102325 \t range_loss: 0.7251385450363159\n",
      "Step: 71 \t Loss: 1.4024815559387207\n",
      "Step: 72 \t Loss: 1.4005959033966064\n",
      "Step: 73 \t Loss: 1.3999066352844238\n",
      "Step: 74 \t Loss: 1.399194598197937\n",
      "Step: 75 \t Loss: 1.3991105556488037\n",
      "Step: 77 \t Loss: 1.3983235359191895\n",
      "Step: 78 \t Loss: 1.3962808847427368\n",
      "pred_error: 0.06802083551883698 \t range_loss: 0.7169715166091919\n",
      "pred_error: 0.06804519891738892 \t range_loss: 0.7165207862854004\n",
      "pred_error: 0.06804519146680832 \t range_loss: 0.7165207862854004\n",
      "Step: 83 \t Loss: 1.3956794738769531\n",
      "pred_error: 0.06809619069099426 \t range_loss: 0.7147186398506165\n",
      "Step: 84 \t Loss: 1.3946609497070312\n",
      "pred_error: 0.06805731356143951 \t range_loss: 0.7140872478485107\n",
      "pred_error: 0.06814124435186386 \t range_loss: 0.7148774862289429\n",
      "pred_error: 0.06814109534025192 \t range_loss: 0.7148774862289429\n",
      "Step: 87 \t Loss: 1.392043113708496\n",
      "pred_error: 0.0681246891617775 \t range_loss: 0.7107990980148315\n",
      "Step: 88 \t Loss: 1.3900682926177979\n",
      "Step: 89 \t Loss: 1.389960765838623\n",
      "Step: 90 \t Loss: 1.3891746997833252\n",
      "Step: 92 \t Loss: 1.3888318538665771\n",
      "pred_error: 0.06817499548196793 \t range_loss: 0.7070848345756531\n",
      "Step: 93 \t Loss: 1.3884475231170654\n",
      "Step: 96 \t Loss: 1.3876667022705078\n",
      "pred_error: 0.06824445724487305 \t range_loss: 0.705220639705658\n",
      "pred_error: 0.06832308322191238 \t range_loss: 0.7057873010635376\n",
      "pred_error: 0.06832420825958252 \t range_loss: 0.7057873010635376\n",
      "Step: 100 \t Loss: 1.3874144554138184\n",
      "pred_error: 0.06826529651880264 \t range_loss: 0.7047594785690308\n",
      "Step: 101 \t Loss: 1.3868908882141113\n",
      "pred_error: 0.06838974356651306 \t range_loss: 0.7029934525489807\n",
      "Step: 102 \t Loss: 1.385290265083313\n",
      "pred_error: 0.06828638166189194 \t range_loss: 0.7024275064468384\n",
      "Step: 103 \t Loss: 1.38496994972229\n",
      "pred_error: 0.06836170703172684 \t range_loss: 0.7013550400733948\n",
      "Step: 108 \t Loss: 1.384397268295288\n",
      "pred_error: 0.06853955239057541 \t range_loss: 0.6990017890930176\n",
      "pred_error: 0.06853931397199631 \t range_loss: 0.6990017890930176\n",
      "Step: 109 \t Loss: 1.3821771144866943\n",
      "pred_error: 0.06829944252967834 \t range_loss: 0.6994857788085938\n",
      "Step: 113 \t Loss: 1.3811705112457275\n",
      "Step: 114 \t Loss: 1.379321813583374\n",
      "pred_error: 0.0683349072933197 \t range_loss: 0.6959726214408875\n",
      "pred_error: 0.06853563338518143 \t range_loss: 0.6962288618087769\n",
      "Step: 122 \t Loss: 1.3788774013519287\n",
      "Step: 123 \t Loss: 1.3781920671463013\n",
      "Step: 124 \t Loss: 1.3778831958770752\n",
      "Step: 125 \t Loss: 1.3776658773422241\n",
      "Step: 126 \t Loss: 1.3767317533493042\n",
      "pred_error: 0.0689145028591156 \t range_loss: 0.6893643140792847\n",
      "Step: 128 \t Loss: 1.3766179084777832\n",
      "pred_error: 0.06894632428884506 \t range_loss: 0.6871525049209595\n",
      "Step: 129 \t Loss: 1.375624179840088\n",
      "Step: 130 \t Loss: 1.3754560947418213\n",
      "pred_error: 0.06880997866392136 \t range_loss: 0.6873564124107361\n",
      "pred_error: 0.06889212876558304 \t range_loss: 0.6870959401130676\n",
      "Step: 133 \t Loss: 1.3754167556762695\n",
      "Step: 134 \t Loss: 1.375197172164917\n",
      "pred_error: 0.06882765144109726 \t range_loss: 0.6869191527366638\n",
      "pred_error: 0.06911799311637878 \t range_loss: 0.6844114065170288\n",
      "Step: 136 \t Loss: 1.37453031539917\n",
      "Step: 137 \t Loss: 1.3732821941375732\n",
      "pred_error: 0.06894759088754654 \t range_loss: 0.683807373046875\n",
      "pred_error: 0.0688810721039772 \t range_loss: 0.6860407590866089\n",
      "pred_error: 0.06910762935876846 \t range_loss: 0.684949517250061\n",
      "Step: 145 \t Loss: 1.372257947921753\n",
      "Step: 146 \t Loss: 1.371156930923462\n",
      "pred_error: 0.06895434111356735 \t range_loss: 0.6816150546073914\n",
      "Step: 147 \t Loss: 1.37037992477417\n",
      "Step: 148 \t Loss: 1.3693534135818481\n",
      "pred_error: 0.06930539011955261 \t range_loss: 0.679469108581543\n",
      "pred_error: 0.06929225474596024 \t range_loss: 0.6777358651161194\n",
      "pred_error: 0.06935257464647293 \t range_loss: 0.6787947416305542\n",
      "pred_error: 0.0693034827709198 \t range_loss: 0.6787752509117126\n",
      "pred_error: 0.06930374354124069 \t range_loss: 0.6787752509117126\n",
      "Step: 161 \t Loss: 1.3690801858901978\n",
      "Step: 162 \t Loss: 1.3690617084503174\n",
      "Step: 165 \t Loss: 1.366842269897461\n",
      "pred_error: 0.06923085451126099 \t range_loss: 0.6745312809944153\n",
      "pred_error: 0.06920028477907181 \t range_loss: 0.6760714054107666\n",
      "pred_error: 0.06920164823532104 \t range_loss: 0.6760714054107666\n",
      "pred_error: 0.06942976266145706 \t range_loss: 0.6750016212463379\n",
      "pred_error: 0.06945966929197311 \t range_loss: 0.674959123134613\n",
      "pred_error: 0.06965317577123642 \t range_loss: 0.6735427975654602\n",
      "pred_error: 0.06947643309831619 \t range_loss: 0.6726210713386536\n",
      "pred_error: 0.06947643309831619 \t range_loss: 0.6726211309432983\n",
      "pred_error: 0.06947650015354156 \t range_loss: 0.6726210713386536\n",
      "pred_error: 0.06958595663309097 \t range_loss: 0.67209392786026\n",
      "Step: 181 \t Loss: 1.3665926456451416\n",
      "pred_error: 0.06950883567333221 \t range_loss: 0.6715019345283508\n",
      "Step: 182 \t Loss: 1.3657965660095215\n",
      "pred_error: 0.06942709535360336 \t range_loss: 0.6715274453163147\n",
      "pred_error: 0.06942690908908844 \t range_loss: 0.6715274453163147\n",
      "pred_error: 0.0694870576262474 \t range_loss: 0.6716740131378174\n",
      "pred_error: 0.06970430165529251 \t range_loss: 0.6700388789176941\n",
      "Step: 190 \t Loss: 1.3654751777648926\n",
      "pred_error: 0.06961698830127716 \t range_loss: 0.6703930497169495\n",
      "Step: 196 \t Loss: 1.3650671243667603\n",
      "pred_error: 0.0699695497751236 \t range_loss: 0.669070839881897\n",
      "pred_error: 0.06988336890935898 \t range_loss: 0.6691774725914001\n",
      "Step: 206 \t Loss: 1.3650418519973755\n",
      "pred_error: 0.06991688907146454 \t range_loss: 0.6658720970153809\n",
      "Step: 207 \t Loss: 1.3643829822540283\n",
      "pred_error: 0.06976871937513351 \t range_loss: 0.6666958332061768\n",
      "Step: 208 \t Loss: 1.3642879724502563\n",
      "Step: 209 \t Loss: 1.3642399311065674\n",
      "Step: 210 \t Loss: 1.3628556728363037\n",
      "pred_error: 0.06979338824748993 \t range_loss: 0.6649225354194641\n",
      "pred_error: 0.06991288810968399 \t range_loss: 0.6669079065322876\n",
      "pred_error: 0.07004732638597488 \t range_loss: 0.6654151082038879\n",
      "pred_error: 0.0700468048453331 \t range_loss: 0.6654151082038879\n",
      "pred_error: 0.07004275918006897 \t range_loss: 0.665630042552948\n",
      "pred_error: 0.06997044384479523 \t range_loss: 0.6664456725120544\n",
      "pred_error: 0.06987224519252777 \t range_loss: 0.6642296314239502\n",
      "pred_error: 0.06987226009368896 \t range_loss: 0.6642296314239502\n",
      "pred_error: 0.06995736807584763 \t range_loss: 0.6636959910392761\n",
      "pred_error: 0.06995731592178345 \t range_loss: 0.6636959910392761\n",
      "pred_error: 0.0698479637503624 \t range_loss: 0.6649174094200134\n",
      "Step: 221 \t Loss: 1.361985683441162\n",
      "pred_error: 0.06991954892873764 \t range_loss: 0.6627889275550842\n",
      "Step: 222 \t Loss: 1.3619157075881958\n",
      "pred_error: 0.06988489627838135 \t range_loss: 0.6630657911300659\n",
      "pred_error: 0.0699792206287384 \t range_loss: 0.664375364780426\n",
      "pred_error: 0.0699792429804802 \t range_loss: 0.664375364780426\n",
      "Step: 226 \t Loss: 1.3618050813674927\n",
      "pred_error: 0.07041770964860916 \t range_loss: 0.6622781157493591\n",
      "pred_error: 0.07034716010093689 \t range_loss: 0.6619110703468323\n",
      "pred_error: 0.07038408517837524 \t range_loss: 0.6616965532302856\n",
      "pred_error: 0.07022710889577866 \t range_loss: 0.6618306040763855\n",
      "Step: 236 \t Loss: 1.3614307641983032\n",
      "pred_error: 0.07017898559570312 \t range_loss: 0.6596386432647705\n",
      "pred_error: 0.07017917931079865 \t range_loss: 0.6596386432647705\n",
      "Step: 237 \t Loss: 1.3605008125305176\n",
      "pred_error: 0.07012280821800232 \t range_loss: 0.6599651575088501\n",
      "pred_error: 0.07021888345479965 \t range_loss: 0.6596348285675049\n",
      "pred_error: 0.07060687243938446 \t range_loss: 0.660340428352356\n",
      "pred_error: 0.07033784687519073 \t range_loss: 0.6579888463020325\n",
      "pred_error: 0.07040999084711075 \t range_loss: 0.6581615209579468\n",
      "pred_error: 0.07046626508235931 \t range_loss: 0.658518373966217\n",
      "pred_error: 0.0703730583190918 \t range_loss: 0.6579210758209229\n",
      "pred_error: 0.07051115483045578 \t range_loss: 0.6569033265113831\n",
      "pred_error: 0.07061511278152466 \t range_loss: 0.6571556925773621\n",
      "pred_error: 0.07061494141817093 \t range_loss: 0.6571556925773621\n",
      "pred_error: 0.07057821750640869 \t range_loss: 0.6567167043685913\n",
      "pred_error: 0.07061280310153961 \t range_loss: 0.65541011095047\n",
      "Step: 272 \t Loss: 1.3604316711425781\n",
      "pred_error: 0.07058543711900711 \t range_loss: 0.6545771956443787\n",
      "pred_error: 0.07058307528495789 \t range_loss: 0.6545771956443787\n",
      "pred_error: 0.07058530300855637 \t range_loss: 0.6545771956443787\n",
      "Step: 275 \t Loss: 1.3596208095550537\n",
      "pred_error: 0.07049595564603806 \t range_loss: 0.6558802127838135\n",
      "pred_error: 0.07049649953842163 \t range_loss: 0.6558802127838135\n",
      "pred_error: 0.07080202549695969 \t range_loss: 0.6537879109382629\n",
      "pred_error: 0.0708111822605133 \t range_loss: 0.6546527743339539\n",
      "pred_error: 0.07084926217794418 \t range_loss: 0.6530807614326477\n",
      "pred_error: 0.07066962867975235 \t range_loss: 0.6542596817016602\n",
      "pred_error: 0.07084536552429199 \t range_loss: 0.6543691754341125\n",
      "pred_error: 0.07087178528308868 \t range_loss: 0.6528663635253906\n",
      "Step: 289 \t Loss: 1.3581949472427368\n",
      "Step: 290 \t Loss: 1.356683373451233\n",
      "pred_error: 0.07051926851272583 \t range_loss: 0.6514887809753418\n",
      "pred_error: 0.07051942497491837 \t range_loss: 0.6514887809753418\n",
      "pred_error: 0.07081402838230133 \t range_loss: 0.652203381061554\n",
      "pred_error: 0.07089131325483322 \t range_loss: 0.6531843543052673\n",
      "pred_error: 0.07095492631196976 \t range_loss: 0.652675211429596\n",
      "pred_error: 0.07113781571388245 \t range_loss: 0.654158353805542\n",
      "pred_error: 0.07083084434270859 \t range_loss: 0.6500993967056274\n",
      "pred_error: 0.07103347778320312 \t range_loss: 0.6513766646385193\n",
      "pred_error: 0.07108888030052185 \t range_loss: 0.6504188179969788\n",
      "pred_error: 0.07096469402313232 \t range_loss: 0.649409294128418\n",
      "pred_error: 0.07097145169973373 \t range_loss: 0.6486825942993164\n",
      "pred_error: 0.07093364000320435 \t range_loss: 0.6496425867080688\n",
      "pred_error: 0.07103399932384491 \t range_loss: 0.6480054259300232\n",
      "pred_error: 0.0711570754647255 \t range_loss: 0.6475247144699097\n",
      "pred_error: 0.07115703821182251 \t range_loss: 0.6475247144699097\n",
      "pred_error: 0.0711677148938179 \t range_loss: 0.6476547718048096\n",
      "pred_error: 0.07126649469137192 \t range_loss: 0.6472828984260559\n",
      "pred_error: 0.071280837059021 \t range_loss: 0.6468678116798401\n",
      "pred_error: 0.07126247137784958 \t range_loss: 0.6463705897331238\n",
      "pred_error: 0.07143516093492508 \t range_loss: 0.6454198360443115\n",
      "pred_error: 0.07143505662679672 \t range_loss: 0.6454198360443115\n",
      "pred_error: 0.0714498907327652 \t range_loss: 0.6453370451927185\n",
      "pred_error: 0.07144920527935028 \t range_loss: 0.6453370451927185\n",
      "pred_error: 0.07145270705223083 \t range_loss: 0.6450389623641968\n",
      "pred_error: 0.07152090221643448 \t range_loss: 0.6445639729499817\n",
      "pred_error: 0.07142583280801773 \t range_loss: 0.6467031836509705\n",
      "pred_error: 0.07131768018007278 \t range_loss: 0.6444268226623535\n",
      "pred_error: 0.07131706178188324 \t range_loss: 0.6444268226623535\n",
      "pred_error: 0.07137080281972885 \t range_loss: 0.6439318060874939\n",
      "pred_error: 0.07137080281972885 \t range_loss: 0.6439318060874939\n",
      "pred_error: 0.07135400176048279 \t range_loss: 0.6442239880561829\n",
      "pred_error: 0.07205165177583694 \t range_loss: 0.6437638998031616\n",
      "pred_error: 0.07178222388029099 \t range_loss: 0.6444932222366333\n",
      "pred_error: 0.07178222388029099 \t range_loss: 0.6444932222366333\n",
      "pred_error: 0.07158107310533524 \t range_loss: 0.6424252986907959\n",
      "pred_error: 0.07155627757310867 \t range_loss: 0.642111599445343\n",
      "pred_error: 0.07138199359178543 \t range_loss: 0.6439834237098694\n",
      "pred_error: 0.071382075548172 \t range_loss: 0.6439834237098694\n",
      "pred_error: 0.07138197869062424 \t range_loss: 0.6439834237098694\n",
      "pred_error: 0.07171875983476639 \t range_loss: 0.6419925689697266\n",
      "pred_error: 0.07181474566459656 \t range_loss: 0.642472505569458\n",
      "pred_error: 0.07189886271953583 \t range_loss: 0.6441876292228699\n",
      "pred_error: 0.07189904898405075 \t range_loss: 0.6441876292228699\n",
      "pred_error: 0.07205583155155182 \t range_loss: 0.6422379612922668\n",
      "pred_error: 0.07205593585968018 \t range_loss: 0.6422379612922668\n",
      "pred_error: 0.07169400155544281 \t range_loss: 0.6427379250526428\n",
      "pred_error: 0.07169461250305176 \t range_loss: 0.6427379250526428\n",
      "pred_error: 0.07173735648393631 \t range_loss: 0.6446022391319275\n",
      "pred_error: 0.07173758000135422 \t range_loss: 0.6446022391319275\n",
      "pred_error: 0.07167952507734299 \t range_loss: 0.6424088478088379\n",
      "pred_error: 0.07167952507734299 \t range_loss: 0.6424088478088379\n",
      "pred_error: 0.07207870483398438 \t range_loss: 0.6431098580360413\n",
      "pred_error: 0.0723835900425911 \t range_loss: 0.6416292786598206\n",
      "pred_error: 0.07208389788866043 \t range_loss: 0.6409686803817749\n",
      "pred_error: 0.07208382338285446 \t range_loss: 0.6409686803817749\n",
      "pred_error: 0.07215997576713562 \t range_loss: 0.642884373664856\n",
      "pred_error: 0.07202660292387009 \t range_loss: 0.6456392407417297\n",
      "pred_error: 0.07202649116516113 \t range_loss: 0.6456392407417297\n",
      "pred_error: 0.07196824997663498 \t range_loss: 0.6425521969795227\n",
      "pred_error: 0.07189085334539413 \t range_loss: 0.6433076858520508\n",
      "pred_error: 0.07204065471887589 \t range_loss: 0.6434110403060913\n",
      "pred_error: 0.07175767421722412 \t range_loss: 0.6407545804977417\n",
      "pred_error: 0.0716446116566658 \t range_loss: 0.641574501991272\n",
      "pred_error: 0.07164464890956879 \t range_loss: 0.641574501991272\n",
      "pred_error: 0.07173815369606018 \t range_loss: 0.641812801361084\n",
      "pred_error: 0.07173812389373779 \t range_loss: 0.641812801361084\n",
      "pred_error: 0.07195388525724411 \t range_loss: 0.641480028629303\n",
      "pred_error: 0.07195380330085754 \t range_loss: 0.641480028629303\n",
      "pred_error: 0.07195384800434113 \t range_loss: 0.641480028629303\n",
      "pred_error: 0.07189375907182693 \t range_loss: 0.6420230269432068\n",
      "pred_error: 0.0719887837767601 \t range_loss: 0.6412144303321838\n",
      "pred_error: 0.07198872417211533 \t range_loss: 0.6412144303321838\n",
      "pred_error: 0.07194145023822784 \t range_loss: 0.6405575275421143\n",
      "pred_error: 0.07204565405845642 \t range_loss: 0.6398013830184937\n",
      "pred_error: 0.07189545780420303 \t range_loss: 0.6415141224861145\n",
      "pred_error: 0.07211089134216309 \t range_loss: 0.6390746831893921\n",
      "pred_error: 0.07232064008712769 \t range_loss: 0.6388135552406311\n",
      "pred_error: 0.07206972688436508 \t range_loss: 0.6394689679145813\n",
      "pred_error: 0.07215152680873871 \t range_loss: 0.6374051570892334\n",
      "pred_error: 0.07252445816993713 \t range_loss: 0.6378933191299438\n",
      "pred_error: 0.0723874643445015 \t range_loss: 0.638725757598877\n",
      "pred_error: 0.07244663685560226 \t range_loss: 0.6367302536964417\n",
      "pred_error: 0.07244663685560226 \t range_loss: 0.6367302536964417\n",
      "pred_error: 0.07214175164699554 \t range_loss: 0.6382266879081726\n",
      "pred_error: 0.07214169204235077 \t range_loss: 0.6382266879081726\n",
      "pred_error: 0.07237373292446136 \t range_loss: 0.6379013061523438\n",
      "pred_error: 0.07214131951332092 \t range_loss: 0.6365635991096497\n",
      "pred_error: 0.07214115560054779 \t range_loss: 0.6365635991096497\n",
      "pred_error: 0.0722532719373703 \t range_loss: 0.6376749277114868\n",
      "pred_error: 0.07221150398254395 \t range_loss: 0.6380675435066223\n",
      "pred_error: 0.0720682442188263 \t range_loss: 0.6383534073829651\n",
      "pred_error: 0.07213535159826279 \t range_loss: 0.6377228498458862\n",
      "pred_error: 0.07231294363737106 \t range_loss: 0.6389487385749817\n",
      "pred_error: 0.07246199995279312 \t range_loss: 0.63688725233078\n",
      "pred_error: 0.07246189564466476 \t range_loss: 0.63688725233078\n",
      "pred_error: 0.07229044288396835 \t range_loss: 0.6356000304222107\n",
      "pred_error: 0.07205090671777725 \t range_loss: 0.637418806552887\n",
      "pred_error: 0.07205083966255188 \t range_loss: 0.637418806552887\n",
      "pred_error: 0.07215218991041183 \t range_loss: 0.6385137438774109\n",
      "pred_error: 0.07222051173448563 \t range_loss: 0.6386078596115112\n",
      "pred_error: 0.07222078740596771 \t range_loss: 0.6386078596115112\n",
      "pred_error: 0.07222051918506622 \t range_loss: 0.6386078596115112\n",
      "pred_error: 0.07219070196151733 \t range_loss: 0.636962890625\n",
      "pred_error: 0.0724276453256607 \t range_loss: 0.6369103193283081\n",
      "Step: 488 \t Loss: 1.3566303253173828\n",
      "Step: 489 \t Loss: 1.3553963899612427\n",
      "pred_error: 0.07179825752973557 \t range_loss: 0.6374156475067139\n",
      "Step: 490 \t Loss: 1.3550509214401245\n",
      "pred_error: 0.07188171148300171 \t range_loss: 0.6362331509590149\n",
      "pred_error: 0.0719260647892952 \t range_loss: 0.6359564661979675\n",
      "pred_error: 0.07217152416706085 \t range_loss: 0.6384530067443848\n",
      "pred_error: 0.07226065546274185 \t range_loss: 0.6385924816131592\n",
      "pred_error: 0.07230168581008911 \t range_loss: 0.6395615935325623\n",
      "pred_error: 0.07204961031675339 \t range_loss: 0.6374749541282654\n",
      "pred_error: 0.07193011790513992 \t range_loss: 0.6372286677360535\n",
      "pred_error: 0.07186926901340485 \t range_loss: 0.6372392773628235\n",
      "pred_error: 0.07194972038269043 \t range_loss: 0.6365345120429993\n",
      "pred_error: 0.07186257094144821 \t range_loss: 0.6369910836219788\n",
      "pred_error: 0.07239281386137009 \t range_loss: 0.635758638381958\n",
      "pred_error: 0.07241658866405487 \t range_loss: 0.6354958415031433\n",
      "pred_error: 0.0722823441028595 \t range_loss: 0.6369864344596863\n",
      "pred_error: 0.07241073995828629 \t range_loss: 0.6352353692054749\n",
      "pred_error: 0.07242894172668457 \t range_loss: 0.6353307962417603\n",
      "pred_error: 0.07240064442157745 \t range_loss: 0.6355636715888977\n",
      "pred_error: 0.07231050729751587 \t range_loss: 0.637285590171814\n",
      "pred_error: 0.07249733060598373 \t range_loss: 0.6365817189216614\n",
      "pred_error: 0.07237926125526428 \t range_loss: 0.6354404091835022\n",
      "pred_error: 0.07237932085990906 \t range_loss: 0.6354404091835022\n",
      "pred_error: 0.0722881406545639 \t range_loss: 0.6369829177856445\n",
      "pred_error: 0.07230834662914276 \t range_loss: 0.635948896408081\n",
      "pred_error: 0.0723075419664383 \t range_loss: 0.635335385799408\n",
      "pred_error: 0.0723898708820343 \t range_loss: 0.6338722705841064\n",
      "pred_error: 0.0724310353398323 \t range_loss: 0.6343545317649841\n",
      "pred_error: 0.07253304123878479 \t range_loss: 0.6352487206459045\n",
      "pred_error: 0.07269268482923508 \t range_loss: 0.635786771774292\n",
      "pred_error: 0.07267925888299942 \t range_loss: 0.6348461508750916\n",
      "pred_error: 0.07267940044403076 \t range_loss: 0.6348461508750916\n",
      "pred_error: 0.07254602015018463 \t range_loss: 0.6346370577812195\n",
      "pred_error: 0.07216079533100128 \t range_loss: 0.6337573528289795\n",
      "pred_error: 0.07250551879405975 \t range_loss: 0.6339276432991028\n",
      "pred_error: 0.07250549644231796 \t range_loss: 0.6339276432991028\n",
      "pred_error: 0.072391077876091 \t range_loss: 0.6324455738067627\n",
      "pred_error: 0.07239114493131638 \t range_loss: 0.6324455738067627\n",
      "pred_error: 0.07249639183282852 \t range_loss: 0.634103536605835\n",
      "pred_error: 0.07248350977897644 \t range_loss: 0.6355785727500916\n",
      "pred_error: 0.07247946411371231 \t range_loss: 0.6339871287345886\n",
      "pred_error: 0.07226689159870148 \t range_loss: 0.6336287260055542\n",
      "pred_error: 0.07239767909049988 \t range_loss: 0.6353342533111572\n",
      "pred_error: 0.07232639938592911 \t range_loss: 0.6333176493644714\n",
      "pred_error: 0.07232625037431717 \t range_loss: 0.6333176493644714\n",
      "pred_error: 0.07232624292373657 \t range_loss: 0.6333176493644714\n",
      "Step: 590 \t Loss: 1.3543636798858643\n",
      "pred_error: 0.07206551730632782 \t range_loss: 0.634809672832489\n",
      "Step: 593 \t Loss: 1.3541123867034912\n",
      "pred_error: 0.07231016457080841 \t range_loss: 0.6339918971061707\n",
      "pred_error: 0.0724153146147728 \t range_loss: 0.6331456303596497\n",
      "pred_error: 0.07248612493276596 \t range_loss: 0.6342961192131042\n",
      "pred_error: 0.07270803302526474 \t range_loss: 0.6334271430969238\n",
      "pred_error: 0.07250476628541946 \t range_loss: 0.6325663328170776\n",
      "pred_error: 0.07232246547937393 \t range_loss: 0.6325067281723022\n",
      "pred_error: 0.07227073609828949 \t range_loss: 0.6335700154304504\n",
      "pred_error: 0.0722706988453865 \t range_loss: 0.6335700154304504\n",
      "pred_error: 0.07250054180622101 \t range_loss: 0.63343745470047\n",
      "pred_error: 0.07242099940776825 \t range_loss: 0.6338562369346619\n",
      "pred_error: 0.07257477939128876 \t range_loss: 0.6338769793510437\n",
      "pred_error: 0.07258598506450653 \t range_loss: 0.632308304309845\n",
      "pred_error: 0.07258603721857071 \t range_loss: 0.632308304309845\n",
      "pred_error: 0.07234662026166916 \t range_loss: 0.6329253315925598\n",
      "pred_error: 0.07241835445165634 \t range_loss: 0.6342549324035645\n",
      "pred_error: 0.07254879176616669 \t range_loss: 0.6353652477264404\n",
      "pred_error: 0.07256349176168442 \t range_loss: 0.6335292458534241\n",
      "pred_error: 0.07234340161085129 \t range_loss: 0.6342020630836487\n",
      "pred_error: 0.07234271615743637 \t range_loss: 0.6342020630836487\n",
      "pred_error: 0.07234349846839905 \t range_loss: 0.6342020630836487\n",
      "pred_error: 0.07247571647167206 \t range_loss: 0.6334898471832275\n",
      "pred_error: 0.07232838124036789 \t range_loss: 0.6332384943962097\n",
      "pred_error: 0.07268298417329788 \t range_loss: 0.6328369975090027\n",
      "pred_error: 0.07268286496400833 \t range_loss: 0.6328369975090027\n",
      "pred_error: 0.07246126979589462 \t range_loss: 0.6324016451835632\n",
      "pred_error: 0.07246137410402298 \t range_loss: 0.6324016451835632\n",
      "pred_error: 0.07249237596988678 \t range_loss: 0.633385419845581\n",
      "pred_error: 0.07259245961904526 \t range_loss: 0.6322171092033386\n",
      "pred_error: 0.07253023982048035 \t range_loss: 0.6332367658615112\n",
      "pred_error: 0.07253028452396393 \t range_loss: 0.6332367658615112\n",
      "pred_error: 0.07250449061393738 \t range_loss: 0.6328527331352234\n",
      "pred_error: 0.07250440120697021 \t range_loss: 0.6328527331352234\n",
      "pred_error: 0.07237301021814346 \t range_loss: 0.6345849633216858\n",
      "pred_error: 0.07237301766872406 \t range_loss: 0.6345849633216858\n",
      "pred_error: 0.07236594706773758 \t range_loss: 0.6318858861923218\n",
      "pred_error: 0.07223676145076752 \t range_loss: 0.6328008770942688\n",
      "pred_error: 0.07268209010362625 \t range_loss: 0.6324935555458069\n",
      "pred_error: 0.07291390746831894 \t range_loss: 0.6319399476051331\n",
      "pred_error: 0.07278960198163986 \t range_loss: 0.6319569945335388\n",
      "pred_error: 0.07266117632389069 \t range_loss: 0.6326555609703064\n",
      "pred_error: 0.07258675992488861 \t range_loss: 0.6327323317527771\n",
      "pred_error: 0.07241033017635345 \t range_loss: 0.6311384439468384\n",
      "pred_error: 0.07253722846508026 \t range_loss: 0.6333135366439819\n",
      "pred_error: 0.07253734767436981 \t range_loss: 0.6333135366439819\n",
      "pred_error: 0.07262032479047775 \t range_loss: 0.6329904794692993\n",
      "pred_error: 0.07262302190065384 \t range_loss: 0.6327756643295288\n",
      "pred_error: 0.07248266786336899 \t range_loss: 0.6340184807777405\n",
      "pred_error: 0.07247768342494965 \t range_loss: 0.633094310760498\n",
      "pred_error: 0.07239296287298203 \t range_loss: 0.6310692429542542\n",
      "pred_error: 0.07268274575471878 \t range_loss: 0.6324018836021423\n",
      "pred_error: 0.07268271595239639 \t range_loss: 0.6324018836021423\n",
      "pred_error: 0.072637178003788 \t range_loss: 0.6321273446083069\n",
      "pred_error: 0.07263707369565964 \t range_loss: 0.6321273446083069\n",
      "pred_error: 0.07269268482923508 \t range_loss: 0.6335107684135437\n",
      "pred_error: 0.0726926401257515 \t range_loss: 0.6335107684135437\n",
      "pred_error: 0.07281733304262161 \t range_loss: 0.6305909752845764\n",
      "pred_error: 0.07264932245016098 \t range_loss: 0.6312363743782043\n",
      "pred_error: 0.0726611390709877 \t range_loss: 0.6316163539886475\n",
      "pred_error: 0.07276669889688492 \t range_loss: 0.6308205127716064\n",
      "pred_error: 0.0726020410656929 \t range_loss: 0.6311426758766174\n",
      "pred_error: 0.07281265407800674 \t range_loss: 0.6337623596191406\n",
      "pred_error: 0.07256857305765152 \t range_loss: 0.6314321160316467\n",
      "pred_error: 0.07257790863513947 \t range_loss: 0.6308265924453735\n",
      "pred_error: 0.07264259457588196 \t range_loss: 0.6309189796447754\n",
      "pred_error: 0.07273165881633759 \t range_loss: 0.632517397403717\n",
      "pred_error: 0.07273092865943909 \t range_loss: 0.632517397403717\n",
      "pred_error: 0.0726698562502861 \t range_loss: 0.6319354176521301\n",
      "pred_error: 0.07266990095376968 \t range_loss: 0.6319354176521301\n",
      "pred_error: 0.07256178557872772 \t range_loss: 0.630457878112793\n",
      "pred_error: 0.0725761130452156 \t range_loss: 0.6322386264801025\n",
      "pred_error: 0.07257416099309921 \t range_loss: 0.6306710839271545\n",
      "pred_error: 0.07285721600055695 \t range_loss: 0.6320794820785522\n",
      "pred_error: 0.07300074398517609 \t range_loss: 0.6309007406234741\n",
      "pred_error: 0.07300074398517609 \t range_loss: 0.6309007406234741\n",
      "pred_error: 0.07277509570121765 \t range_loss: 0.6312556266784668\n",
      "pred_error: 0.07277492433786392 \t range_loss: 0.6312556266784668\n",
      "Step: 735 \t Loss: 1.353891372680664\n",
      "pred_error: 0.0724046379327774 \t range_loss: 0.6302300691604614\n",
      "pred_error: 0.07276651263237 \t range_loss: 0.6316066384315491\n",
      "pred_error: 0.07285422831773758 \t range_loss: 0.6309120059013367\n",
      "pred_error: 0.07286867499351501 \t range_loss: 0.6317003965377808\n",
      "pred_error: 0.07279607653617859 \t range_loss: 0.6295569539070129\n",
      "pred_error: 0.07310259342193604 \t range_loss: 0.6291322112083435\n",
      "pred_error: 0.07306261360645294 \t range_loss: 0.6300686597824097\n",
      "pred_error: 0.0732620358467102 \t range_loss: 0.629143238067627\n",
      "pred_error: 0.07320442795753479 \t range_loss: 0.6289539337158203\n",
      "pred_error: 0.07300271093845367 \t range_loss: 0.6295062899589539\n",
      "pred_error: 0.07298999279737473 \t range_loss: 0.6308400630950928\n",
      "pred_error: 0.07320307940244675 \t range_loss: 0.6295952796936035\n",
      "pred_error: 0.07302593439817429 \t range_loss: 0.6296559572219849\n",
      "pred_error: 0.07292406260967255 \t range_loss: 0.6298904418945312\n",
      "pred_error: 0.07275038957595825 \t range_loss: 0.6311489343643188\n",
      "pred_error: 0.07283203303813934 \t range_loss: 0.6285408735275269\n",
      "pred_error: 0.07298274338245392 \t range_loss: 0.6296787261962891\n",
      "pred_error: 0.07302134484052658 \t range_loss: 0.6292030215263367\n",
      "pred_error: 0.07301431149244308 \t range_loss: 0.6295016407966614\n",
      "pred_error: 0.07333330065011978 \t range_loss: 0.6288087964057922\n",
      "pred_error: 0.07328701764345169 \t range_loss: 0.6290297508239746\n",
      "pred_error: 0.07318975031375885 \t range_loss: 0.6290188431739807\n",
      "pred_error: 0.07301975786685944 \t range_loss: 0.6280902624130249\n",
      "pred_error: 0.073116734623909 \t range_loss: 0.6291505694389343\n",
      "pred_error: 0.0732915997505188 \t range_loss: 0.6282591819763184\n",
      "pred_error: 0.0729861930012703 \t range_loss: 0.6284170150756836\n",
      "pred_error: 0.07298631966114044 \t range_loss: 0.6284170150756836\n",
      "pred_error: 0.07296106964349747 \t range_loss: 0.6293518543243408\n",
      "pred_error: 0.07333540916442871 \t range_loss: 0.6278841495513916\n",
      "pred_error: 0.07320620119571686 \t range_loss: 0.6280515789985657\n",
      "pred_error: 0.07320600003004074 \t range_loss: 0.6280515789985657\n",
      "BEST LOSS: 1.3538914\n",
      "==== Model: block11_cob_activation_norm  in Layer: 11 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 20:35:41,365 execute.rs:1044 num calibration batches: 1\n",
      "WARNING ezkl.circuit.table 2024-09-16 20:36:54,089 table.rs:187 Using 2 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-09-16 20:36:54,113 table.rs:187 Using 2 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-09-16 20:36:54,131 table.rs:187 Using 2 columns for non-linearity table.\n",
      "WARNING ezkl.circuit.table 2024-09-16 20:36:54,141 table.rs:187 Using 2 columns for non-linearity table.\n",
      "ERROR ezkl.graph.model 2024-09-16 20:37:26,411 model.rs:1246 value (130880) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 20:37:26,420 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 20:37:43,344 model.rs:1246 value (130880) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 20:37:43,359 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 20:37:43,382 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 20:37:43,424 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 20:37:43,438 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error    | min_error     | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000023401522 | -0.000017344952 | 0.0020785332 | -0.0051207542 | 0.000053248463 | 0.000017344952   | 0.0051207542  | 0             | 0.000000014794938  | 0.00006478574      | 0.0010136885           |\n",
      "+------------------+-----------------+--------------+---------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 534498 64 [-904508, 1382200] 1 [16]\n",
      "===============================\n",
      "==== Model: block11_cob_activation_norm_teleported  in Layer: 11 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 20:38:00,788 execute.rs:1044 num calibration batches: 1\n",
      "ERROR ezkl.graph.model 2024-09-16 20:39:46,174 model.rs:1246 value (148800) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 20:39:46,182 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.graph.model 2024-09-16 20:40:03,461 model.rs:1246 value (148800) out of range: (-96, 96)\n",
      "ERROR ezkl.execute 2024-09-16 20:40:03,468 execute.rs:1199 forward pass failed: \"failed to forward: [halo2] General synthesis error\"\n",
      "ERROR ezkl.execute 2024-09-16 20:40:03,487 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "ERROR ezkl.execute 2024-09-16 20:40:03,511 execute.rs:1160 circuit creation from run args failed: TensorError(SigBitTruncationError)\n",
      "WARNING ezkl.execute 2024-09-16 20:40:03,524 execute.rs:1362 \n",
      "\n",
      " <------------- Numerical Fidelity Report (input_scale: 16, param_scale: 16, scale_input_multiplier: 1) ------------->\n",
      "\n",
      "+------------------+-----------------+--------------+--------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| mean_error       | median_error    | max_error    | min_error    | mean_abs_error | median_abs_error | max_abs_error | min_abs_error | mean_squared_error | mean_percent_error | mean_abs_percent_error |\n",
      "+------------------+-----------------+--------------+--------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "| -0.0000011576112 | -0.000017151237 | 0.0024633408 | -0.005209923 | 0.000053146025 | 0.000017151237   | 0.005209923   | 0             | 0.000000015779474  | 0.00023337046      | 0.0013531893           |\n",
      "+------------------+-----------------+--------------+--------------+----------------+------------------+---------------+---------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 534501 64 [-580648, 1409976] 1 [16]\n",
      "===============================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import copy\n",
    "import ezkl\n",
    "\n",
    "array_param_visibility = [\"fixed\"]\n",
    "array_input_param_scale = [16]\n",
    "array_num_cols = [64]\n",
    "array_max_log_rows = [-1]\n",
    "array_scale_rebase = [-1]\n",
    "array_lookup_margin = [2]\n",
    "\n",
    "# iterate over all the possible combinations\n",
    "combinations = list(itertools.product(array_param_visibility, array_input_param_scale, array_num_cols, array_max_log_rows, array_scale_rebase, array_lookup_margin))\n",
    "global best_loss\n",
    "# the layer_idx which the logrows are equal to 20 are not gonna be teleported\n",
    "list_of_no_teleportation = [1,3,4,5]\n",
    "\n",
    "# with no gradient pytorch\n",
    "with torch.no_grad():\n",
    "    # iterate over all the possible combinations\n",
    "    for p in combinations:\n",
    "        param_visibility, input_param_scale, num_cols, max_log_rows, scale_rebase, lookup_margin = p\n",
    "        # string experiment_settings as comma separated values\n",
    "        experiment_settings = f\"{param_visibility}/{input_param_scale}/{num_cols}/{max_log_rows}/{scale_rebase}/{lookup_margin}\"\n",
    "        print(\"========= START =========\")\n",
    "        print(f\"input_param_scale: {input_param_scale}, num_cols: {num_cols}, max_log_rows: {max_log_rows}, param_visibility: {param_visibility}, lookup_margin: {lookup_margin}\")\n",
    "\n",
    "        # copy the model\n",
    "        new_model = copy.deepcopy(model)\n",
    "        \n",
    "        # generate compression-model and setting for all vit layers\n",
    "        for layer_idx in range(model.depth):\n",
    "\n",
    "            # if layer_idx >= 4:\n",
    "            #     continue\n",
    "\n",
    "            args.pred_mul = 10\n",
    "            # args.pred_mul = 20\n",
    "            args.steps = 800\n",
    "            # args.steps = 400\n",
    "            args.cob_lr = 0.1\n",
    "            args.zoo_step_size = 0.001 \n",
    "\n",
    "            # check the experiment_settings and layer_idx exists in the csv file\n",
    "            with open(csv_file_path, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                exist_flag = False\n",
    "                for row in reader:\n",
    "                    if row[0] == experiment_settings and int(row[2]) == layer_idx:\n",
    "                        print(f\"Experiment settings: {experiment_settings} and layer_idx: {layer_idx} already exists in the csv file.\")\n",
    "                        exist_flag = True\n",
    "                        break\n",
    "            # if exist_flag:\n",
    "            #     continue\n",
    "\n",
    "            # Hook for the intermediate output of the block\n",
    "            original_mlp_idx = model.blocks[layer_idx].mlp\n",
    "            activation_stats_idx = {}\n",
    "            for i,layer in enumerate(original_mlp_idx.children()):\n",
    "                if isinstance(layer, nn.ReLU) or isinstance(layer, nn.Sigmoid) or isinstance(layer, nn.GELU) or isinstance(layer, nn.LeakyReLU):\n",
    "                    layer.register_forward_hook(activation_hook(f'relu_{i}', activation_stats=activation_stats_idx))\n",
    "            # run the mlp model to find original_loss\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            original_block_idx_pred = model.split_n(torch.tensor(input_convs).view(BATCHS,3,224,224),layer_idx,half=False)\n",
    "            \n",
    "            # print activation stats\n",
    "            print(f\"layer_idx: {layer_idx} , \\t  activation_stats: {activation_stats_idx}\")\n",
    "            original_loss_idx = sum([stats['max'] - stats['min'] for stats in activation_stats_idx.values()])\n",
    "            print(\"ORIGINAL LOSS:\",original_loss_idx)\n",
    "\n",
    "            # # Load the teleported model\n",
    "            # teleported_model_idx = LinearNet()\n",
    "            # teleported_model_idx = NeuralTeleportationModel(teleported_model_idx, input_shape=(1, 197, 192))\n",
    "            # load_ln_weights(teleported_model_idx, model, layer_idx)\n",
    "\n",
    "            # # get initial weights and cob\n",
    "            # initial_weights_idx = teleported_model_idx.get_weights().detach()\n",
    "            # initial_cob_idx = teleported_model_idx.generate_random_cob(cob_range=args.cob_range, requires_grad=True,center=args.center,sampling_type=args.sample_type)\n",
    "\n",
    "            # track best loss\n",
    "            best_loss = 1e9\n",
    "\n",
    "            # define input_teleported_model (used in ng_loss_function)\n",
    "            input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "            input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "            input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "            # save npy file using in python checking script\n",
    "            np.save(args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\", input_teleported_model.detach().numpy())\n",
    "            # define original_pred (used in ng_loss_function)\n",
    "            input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "            np.save(args.prefix_dir + f\"input_org_{layer_idx}.npy\", input_org.detach().numpy())\n",
    "            original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "            # Apply best COB and save model weights\n",
    "            LN = LinearNet()\n",
    "            LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "            load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "            if layer_idx in list_of_no_teleportation:\n",
    "                print(\"====== NO OPTIMIZATION SINCE NO TELEPORTATION =====\")\n",
    "                best_loss = torch.tensor(best_loss).detach().cpu()\n",
    "                LN = LN.teleport(torch.ones_like(torch.ones(960)), reset_teleportation=True)\n",
    "                torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "            # check whether the teleportation .pth already exists\n",
    "            elif os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "                print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "                LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "                best_loss = torch.tensor(best_loss).detach().cpu()\n",
    "            else:\n",
    "                # zero-order derivative optimization\n",
    "                # if args.teleport_gaussian and layer_idx != 2 and layer_idx != 3:\n",
    "                initial_cob_idx = torch.ones(960)\n",
    "                # add inputs to the function\n",
    "                ackley = functools.partial(\n",
    "                    f_ack,\n",
    "                    input_data=input_teleported_model,\n",
    "                    original_pred=original_pred,\n",
    "                    layer_idx=layer_idx,\n",
    "                    original_loss = original_loss_idx,\\\n",
    "                    tm = LN\n",
    "                )\n",
    "\n",
    "                # training to find best_cob\n",
    "                best_cob = None\n",
    "                for step in range(args.steps):\n",
    "                    # get the gradient of the cob\n",
    "                    grad_cob = cge(ackley, {\"cob\": initial_cob_idx}, None, args.zoo_step_size)\n",
    "                    # update the cob\n",
    "                    initial_cob_idx -= args.cob_lr * grad_cob[\"cob\"]\n",
    "                    # calculate the loss\n",
    "                    loss = ackley(initial_cob_idx)\n",
    "                    # update the best loss\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_cob = initial_cob_idx\n",
    "                        print(f\"Step: {step} \\t Loss: {loss}\")\n",
    "\n",
    "                print(\"BEST LOSS:\",best_loss)\n",
    "\n",
    "                # Apply best COB and save model weights\n",
    "                if layer_idx in list_of_no_teleportation:\n",
    "                    print(\"====== NO TELEPORTATION =====\")\n",
    "                else:\n",
    "                    LN = LN.teleport(best_cob, reset_teleportation=True)\n",
    "                # save the .pth of the teleported model\n",
    "                torch.save(LN.network.state_dict(), args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth')\n",
    "\n",
    "            # Apply the teleportation to the new_model (Using for computing the next layer inputs)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k: v for k, v in sd.items() if 'norm2' not in k}\n",
    "            new_model.blocks[layer_idx].mlp.load_state_dict(sd)\n",
    "            sd = LN.network.state_dict()\n",
    "            sd = {k.replace('norm2.',''): v for k, v in sd.items() if 'norm2' in k}\n",
    "            new_model.blocks[layer_idx].norm2.load_state_dict(sd)\n",
    "\n",
    "            # Export the optimized model to ONNX\n",
    "            torch.onnx.export(LN.network, input_teleported_model, args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.onnx', verbose=False, export_params=True, opset_version=15, do_constant_folding=True, input_names=['input_0'], output_names=['output'])\n",
    "\n",
    "            # check the validation of the teleportation\n",
    "            \n",
    "            # 1.extract onnx corrosponding to the teleported model (in original onnx)\n",
    "            input_path = args.prefix_dir + f\"network_split_{layer_idx}_False.onnx\"\n",
    "            output_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "            input_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "            output_names = [f\"/blocks.{layer_idx}/mlp/fc2/Add_output_0\"]\n",
    "            onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "            \n",
    "            # # 2. run the python code\n",
    "            # print(\"===== RUNNING PYTHON CODE =====\")\n",
    "            # a = args.prefix_dir + f\"input_teleported_model_{layer_idx}.npy\"\n",
    "            # b = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "            # !python onnx_inference.py \\\n",
    "            #     --input {a} --model1 {output_path} --model2 {b}\n",
    "            # print(\"===== PYTHON CODE FINISHED =====\")\n",
    "            # time.sleep(5)\n",
    "\n",
    "            # ezkl to find the resources possible reduction\n",
    "            run_args = ezkl.PyRunArgs()\n",
    "            run_args.input_visibility = \"public\"\n",
    "            run_args.param_visibility = param_visibility\n",
    "            run_args.output_visibility = \"public\"\n",
    "            run_args.input_scale = input_param_scale\n",
    "            run_args.param_scale = input_param_scale\n",
    "\n",
    "            # run_args.logrows = args.log_rows\n",
    "            run_args.num_inner_cols = num_cols\n",
    "            run_args.variables = [('batch_size', BATCHS)]\n",
    "            if max_log_rows != (-1):\n",
    "                run_args.logrows = max_log_rows\n",
    "            if scale_rebase != (-1):\n",
    "                run_args.scale_rebase_multiplier = scale_rebase\n",
    "\n",
    "            # get SRS\n",
    "            # ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)\n",
    "\n",
    "            name0 = f'block{layer_idx}_cob_activation_norm'\n",
    "            name1 = f'block{layer_idx}_cob_activation_norm_teleported'\n",
    "            rng = [name0,name1]\n",
    "\n",
    "            for m in rng:\n",
    "                print(\"==== Model:\",m, \" in Layer:\",layer_idx,\"====\")\n",
    "\n",
    "                if m == name0:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm.onnx\"\n",
    "                    x = input_org.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"block{layer_idx}_cob_activation_norm_teleported.onnx\"\n",
    "                    x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "\n",
    "                # Generate the calibration data\n",
    "                data = dict(input_data=[x])\n",
    "                # cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "                cal_path = args.prefix_dir + f'cal_data_{m}.json'\n",
    "                json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "\n",
    "                settings_path = args.prefix_dir + f'settings_{m}_{layer_idx}.json'\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "\n",
    "                try:\n",
    "                    if scale_rebase != (-1):\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin, scale_rebase_multiplier=[scale_rebase],max_logrows=max_log_rows)\n",
    "                    elif max_log_rows != (-1):\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin, max_logrows=max_log_rows)\n",
    "                    else:\n",
    "                        res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[input_param_scale], lookup_safety_margin=lookup_margin)\n",
    "                except:\n",
    "                    print(\"ERROR in calibration: \",m,layer_idx)\n",
    "                    res = False\n",
    "                    # write in the csv file\n",
    "                    with open(csv_file_path, mode='a', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        l = original_loss_idx if m == name0 else best_loss\n",
    "                        writer.writerow([\n",
    "                            experiment_settings,\n",
    "                            m,layer_idx,\n",
    "                            l,\n",
    "                            None, None, None,\n",
    "                            None, None, None, None, None, None,\n",
    "                            None, None, None, None, None, None, None, None\n",
    "                        ])\n",
    "                    continue\n",
    "\n",
    "                # extract the resources\n",
    "                settings = json.load(open(settings_path))\n",
    "                rg = settings.get('run_args', {})\n",
    "                input_scale = rg.get('input_scale', None)\n",
    "                param_scale = rg.get('param_scale', None)\n",
    "                output_scale = settings.get('model_output_scales', None)\n",
    "                scale_rebase_multiplier = rg.get('scale_rebase_multiplier', None)\n",
    "                lookup_range = rg.get('lookup_range', [None, None])\n",
    "                logrows = rg.get('logrows', None)\n",
    "                num_rows = settings.get('num_rows', None)\n",
    "                total_assignments = settings.get('total_assignments', None)\n",
    "                num_cols = rg.get('num_inner_cols', None)\n",
    "                total_constant_size = settings.get('total_const_size', None)\n",
    "                \n",
    "                print(logrows, num_rows, num_cols ,lookup_range, scale_rebase_multiplier, output_scale)\n",
    "                print(\"===============================\")\n",
    "\n",
    "                # activation loss is equal to the original loss if m is equal to name0 else it is the best loss\n",
    "                activation_loss = original_loss_idx if m == name0 else best_loss\n",
    "\n",
    "                # write the results to the csv file\n",
    "                with open(csv_file_path, mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([\n",
    "                        experiment_settings,\n",
    "                        m,layer_idx,\n",
    "                        activation_loss.item(),\n",
    "                        input_scale, param_scale, scale_rebase_multiplier,\n",
    "                        lookup_range[0], lookup_range[1], logrows, num_rows, num_cols, total_assignments,\n",
    "                        total_constant_size, output_scale,\n",
    "                          None, None, None, None, None, None\n",
    "                    ])\n",
    "            \n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract onnx of the teleported model\n",
    "# input_path = args.prefix_dir + \"network_split_0_False.onnx\"\n",
    "# output_path = args.prefix_dir + \"block0_cob_activation_norm.onnx\"\n",
    "# input_names = [\"/blocks.0/Add_2_output_0\"]\n",
    "# output_names = [\"/blocks.0/mlp/fc2/Add_output_0\"]\n",
    "# onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python onnx_inference.py --input {args.prefix_dir + 'input_teleported_model.npy'} \\\n",
    "#     --model1 {args.prefix_dir + 'block0_cob_activation_norm.onnx'} \\\n",
    "#     --model2 {args.prefix_dir + 'block0_cob_activation_norm_teleported.onnx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.input_param_scale: 7\n",
      "args.logrows: 20\n",
      "args.num_cols: 2\n",
      "args.scale_rebase_multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "# print important args related to ezkl\n",
    "print(\"args.input_param_scale:\",args.input_param_scale)\n",
    "print(\"args.logrows:\",args.log_rows)\n",
    "print(\"args.num_cols:\",args.num_cols)\n",
    "print(\"args.scale_rebase_multiplier:\",args.scale_rebase_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 20:40:03,663 execute.rs:742 SRS already exists at that path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Future pending cb=[<builtins.PyDoneCallback object at 0x315f4b790>()]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ezkl\n",
    "\n",
    "run_args = ezkl.PyRunArgs()\n",
    "run_args.input_visibility = \"public\"\n",
    "# TODO: change that to fixed\n",
    "run_args.param_visibility = \"fixed\"\n",
    "run_args.output_visibility = \"public\"\n",
    "run_args.input_scale = args.input_param_scale\n",
    "run_args.param_scale = args.input_param_scale\n",
    "run_args.logrows = args.log_rows\n",
    "run_args.num_inner_cols = args.num_cols\n",
    "run_args.scale_rebase_multiplier = args.scale_rebase_multiplier\n",
    "run_args.variables = [('batch_size', BATCHS)]\n",
    "\n",
    "ezkl.get_srs(logrows=run_args.logrows, commitment=ezkl.PyCommitments.KZG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "001240b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ezkl version: 100.100.101\n"
     ]
    }
   ],
   "source": [
    "# print ezkl version\n",
    "print(\"ezkl version:\",ezkl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source ~/.config/envman/PATH.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !RUST_LOG=trace\n",
    "\n",
    "# rng = ['block0_cob_activation_norm','block0_cob_activation_norm_teleported']\n",
    "\n",
    "# # Generate the calibration data\n",
    "# x = input_teleported_model.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "# data = dict(input_data=[x])\n",
    "# cal_path = os.path.join(args.prefix_dir + 'cal_data.json')\n",
    "# json.dump(data, open(cal_path, 'w'))\n",
    "\n",
    "# for model in rng:\n",
    "\n",
    "#     print(\"==== Model:\",model, \"====\")\n",
    "\n",
    "#     model_path = args.prefix_dir + model + \".onnx\"\n",
    "#     settings_path = f'{args.prefix_dir} settings_{model}.json'\n",
    "\n",
    "#     res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "#     assert res == True\n",
    "\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier],max_logrows=args.log_rows)\n",
    "#     # res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale],scale_rebase_multiplier=[args.scale_rebase_multiplier])\n",
    "#     res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, \"resources\", scales=[args.input_param_scale])\n",
    "#     assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 20:40:03,766 execute.rs:640 read 134217988 bytes from file (vector of len = 134217988)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_29169/2906073395.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.resume, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t new_name: blocks.0.attn.qkv.weight\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.weight\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.weight\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.weight\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.weight\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.weight\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.weight\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.weight\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.weight\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.weight\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.weight\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.weight\n",
      "index: 0 \t new_name: blocks.0.attn.qkv.bias\n",
      "index: 1 \t new_name: blocks.1.attn.qkv.bias\n",
      "index: 2 \t new_name: blocks.2.attn.qkv.bias\n",
      "index: 3 \t new_name: blocks.3.attn.qkv.bias\n",
      "index: 4 \t new_name: blocks.4.attn.qkv.bias\n",
      "index: 5 \t new_name: blocks.5.attn.qkv.bias\n",
      "index: 6 \t new_name: blocks.6.attn.qkv.bias\n",
      "index: 7 \t new_name: blocks.7.attn.qkv.bias\n",
      "index: 8 \t new_name: blocks.8.attn.qkv.bias\n",
      "index: 9 \t new_name: blocks.9.attn.qkv.bias\n",
      "index: 10 \t new_name: blocks.10.attn.qkv.bias\n",
      "index: 11 \t new_name: blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "# recreate and initialize the model\n",
    "model = build_model(args, pretrained=False)\n",
    "if \"convnext\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "elif \"vit\" in args.model:\n",
    "    print(\"loading ...\")\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    \n",
    "    if args.pruning_method == \"CAP\":\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], prefix='', ignore_missing=\"relative_position_index\")\n",
    "    elif args.pruning_method == \"DENSE\":\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "elif \"deit\" in args.model:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# transfer the model to the cpu\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = model.blocks[0].mlp(model.blocks[0].norm2(input_teleported_model))\n",
    "\n",
    "# op - original_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tout = LN.network(input_teleported_model)\n",
    "# (original_pred - tout).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "926f9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO ezkl.execute 2024-09-16 20:40:05,055 execute.rs:647 file hash: 54ef75911da76d7a6b7ea341998aaf66cb06c679c53e0a88a4fe070dd3add963\n"
     ]
    }
   ],
   "source": [
    "# save all the original predictions in original_pred_array\n",
    "original_pred_array = []\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):    \n",
    "    input_teleported_model = model.split_n(input_convs,layer_idx,half=True)\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_array.append(original_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f90ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_29169/3146665136.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 0.016365107148885727\n",
      "Layer 1 error: 6.8462086346698925e-06\n",
      "Layer 2 error: 0.024285390973091125\n",
      "Layer 3 error: 1.3144852346158586e-05\n",
      "Layer 4 error: 1.2152940144005697e-05\n",
      "Layer 5 error: 1.4087309864407871e-05\n",
      "Layer 6 error: 0.03299365192651749\n",
      "Layer 7 error: 0.03475141152739525\n",
      "Layer 8 error: 0.0362166091799736\n",
      "Layer 9 error: 0.039391372352838516\n",
      "Layer 10 error: 0.04061271250247955\n",
      "Layer 11 error: 0.037957821041345596\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (correct input (not teleported) for each layer)\n",
    "\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    input_layer = model.split_n(input_convs,layer_idx,half=True)\n",
    "    org_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_layer))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "\n",
    "    teleported_pred = teleported_model.network(input_layer)\n",
    "    error = (org_pred - teleported_pred).abs().mean()\n",
    "    error /= org_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82c28ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_29169/1111111726.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 error: 0.016365107148885727\n",
      "Layer 1 error: 6.8462086346698925e-06\n",
      "Layer 2 error: 0.03295620530843735\n",
      "Layer 3 error: 0.023675469681620598\n",
      "Layer 4 error: 0.03997739031910896\n",
      "Layer 5 error: 0.043796613812446594\n",
      "Layer 6 error: 0.05183403193950653\n",
      "Layer 7 error: 0.058936070650815964\n",
      "Layer 8 error: 0.06905083358287811\n",
      "Layer 9 error: 0.0772804394364357\n",
      "Layer 10 error: 0.08275899291038513\n",
      "Layer 11 error: 0.07326808571815491\n"
     ]
    }
   ],
   "source": [
    "# list error of each tleported layer independently (teleported input for each layer)\n",
    "input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "input_convs = torch.tensor(input_convs).view(1,3,224,224)\n",
    "\n",
    "for layer_idx in range(new_model.depth):\n",
    "    input_teleported_model = new_model.split_n(input_convs,layer_idx,half=True)\n",
    "    input_org = model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    original_pred = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_org))\n",
    "\n",
    "    # Load the teleported model\n",
    "    teleported_model = LinearNet()\n",
    "    teleported_model = NeuralTeleportationModel(teleported_model, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(teleported_model, model, layer_idx)\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        teleported_model.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exist.\")\n",
    "        continue\n",
    "    teleported_pred = teleported_model.network(input_teleported_model)\n",
    "    \n",
    "    error = (original_pred - teleported_pred).abs().mean()\n",
    "    error /= original_pred.abs().mean()\n",
    "    print(f\"Layer {layer_idx} error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25440b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block0_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 0: 0.01636327989399433\n",
      "block1_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/ipykernel_29169/3984726627.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block2_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 2: 0.032954853028059006\n",
      "block3_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 3: 0.023675505071878433\n",
      "block4_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 4: 0.03997843340039253\n",
      "block5_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 5: 0.04379794001579285\n",
      "block6_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 6: 0.0518336296081543\n",
      "block7_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 7: 0.05893596634268761\n",
      "block8_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 8: 0.06905277818441391\n",
      "block9_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 9: 0.07727934420108795\n",
      "block10_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 10: 0.08275893330574036\n",
      "block11_cob_activation_norm_teleported.pth already exists.\n",
      "Prediction error in layer 11: 0.0732683539390564\n"
     ]
    }
   ],
   "source": [
    "teleportation_applied_layers = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "# the copy_model is used to consider the previous layer teleportation to the input of the current layer (only layers in the teleportation_applied_layers are gonna be teleported)\n",
    "copy_model = copy.deepcopy(model)\n",
    "copy_model.eval()\n",
    "for param in copy_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for layer_idx in teleportation_applied_layers:\n",
    "    # compute the original_pred_idx - IMPORTANT: PRVIOUS TELEPORTED LAYERS EFFECT THE INPUT OF THE CURRENT LAYER => pred_error WOULD BE DIFFERENT\n",
    "    input_convs = json.load(open(args.prefix_dir + \"input_convs.json\"))[\"input_data\"][0]\n",
    "    input_convs = torch.tensor(input_convs).view(BATCHS,3,224,224)\n",
    "    input_teleported_model = copy_model.split_n(input_convs,layer_idx,half=True)\n",
    "\n",
    "    # original_pred_idx = model.blocks[layer_idx].mlp(model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    original_pred_idx = original_pred_array[layer_idx]\n",
    "\n",
    "    LN = LinearNet()\n",
    "    LN = NeuralTeleportationModel(LN, input_shape=(1, 197, 192))\n",
    "    load_ln_weights(LN, model, layer_idx)\n",
    "\n",
    "    # check whether the teleportation .pth already exists\n",
    "    if os.path.exists(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'):\n",
    "        print(f\"block{layer_idx}_cob_activation_norm_teleported.pth already exists.\")\n",
    "        LN.network.load_state_dict(torch.load(args.prefix_dir + f'block{layer_idx}_cob_activation_norm_teleported.pth'))\n",
    "    else:\n",
    "        # raise error\n",
    "        raise Exception(f\"block{layer_idx}_cob_activation_norm_teleported.pth does not exists.\")\n",
    "\n",
    "    # subsitude the teleported_model weights in the original model\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k: v for k, v in state_dic.items() if 'norm2' not in k}\n",
    "    copy_model.blocks[layer_idx].mlp.load_state_dict(state_dic)\n",
    "    state_dic = LN.network.state_dict()\n",
    "    state_dic = {k.replace('norm2.',''): v for k, v in state_dic.items() if 'norm2' in k}\n",
    "    copy_model.blocks[layer_idx].norm2.load_state_dict(state_dic)\n",
    "\n",
    "    # compute the prediction error\n",
    "    new_pred = copy_model.blocks[layer_idx].mlp(copy_model.blocks[layer_idx].norm2(input_teleported_model))\n",
    "    diff = (original_pred_idx - new_pred).abs().mean()\n",
    "    print(f\"Prediction error in layer {layer_idx}: {diff/original_pred_idx.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the updated model .pth\n",
    "torch.save(copy_model.state_dict(), args.resume.replace(\".pth\",\"_teleported.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_proto: dim_param: \"batch_size\"\n",
      "\n",
      "dim_proto: dim_value: 3\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n",
      "dim_proto: dim_value: 224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# export the updated model to onnx\n",
    "torch.onnx.export(copy_model, x,\\\n",
    "                args.prefix_dir + 'complete_model_teleported.onnx', \\\n",
    "                verbose=False, export_params=True, opset_version=15, do_constant_folding=True, \\\n",
    "                input_names=['input'], output_names=['output'], \\\n",
    "                dynamic_axes={'input' : {0 : 'batch_size'},'output': {0:'batch_size'},},\n",
    ")\n",
    "\n",
    "# define the shape\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "for tensor in on.graph.input:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        print(\"dim_proto:\",dim_proto)\n",
    "        if dim_proto.HasField(\"dim_param\"): # and dim_proto.dim_param == 'batch_size':\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "for tensor in on.graph.output:\n",
    "    for dim_proto in tensor.type.tensor_type.shape.dim:\n",
    "        if dim_proto.HasField(\"dim_param\"):\n",
    "            dim_proto.Clear()\n",
    "            dim_proto.dim_value = BATCHS   # fixed batch size\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "\n",
    "on = onnx.load(args.prefix_dir + \"complete_model_teleported.onnx\")\n",
    "on = onnx.shape_inference.infer_shapes(on)\n",
    "onnx.save(on, args.prefix_dir + \"complete_model_teleported.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0 \t half: True \t input_names: ['/Add_output_0'] \t output_names: ['/blocks.0/Add_2_output_0']\n",
      "layer_idx: 0 \t half: False \t input_names: ['/blocks.0/Add_2_output_0'] \t output_names: ['/blocks.0/Add_3_output_0']\n",
      "layer_idx: 1 \t half: True \t input_names: ['/blocks.0/Add_3_output_0'] \t output_names: ['/blocks.1/Add_2_output_0']\n",
      "layer_idx: 1 \t half: False \t input_names: ['/blocks.1/Add_2_output_0'] \t output_names: ['/blocks.1/Add_3_output_0']\n",
      "layer_idx: 2 \t half: True \t input_names: ['/blocks.1/Add_3_output_0'] \t output_names: ['/blocks.2/Add_2_output_0']\n",
      "layer_idx: 2 \t half: False \t input_names: ['/blocks.2/Add_2_output_0'] \t output_names: ['/blocks.2/Add_3_output_0']\n",
      "layer_idx: 3 \t half: True \t input_names: ['/blocks.2/Add_3_output_0'] \t output_names: ['/blocks.3/Add_2_output_0']\n",
      "layer_idx: 3 \t half: False \t input_names: ['/blocks.3/Add_2_output_0'] \t output_names: ['/blocks.3/Add_3_output_0']\n",
      "layer_idx: 4 \t half: True \t input_names: ['/blocks.3/Add_3_output_0'] \t output_names: ['/blocks.4/Add_2_output_0']\n",
      "layer_idx: 4 \t half: False \t input_names: ['/blocks.4/Add_2_output_0'] \t output_names: ['/blocks.4/Add_3_output_0']\n",
      "layer_idx: 5 \t half: True \t input_names: ['/blocks.4/Add_3_output_0'] \t output_names: ['/blocks.5/Add_2_output_0']\n",
      "layer_idx: 5 \t half: False \t input_names: ['/blocks.5/Add_2_output_0'] \t output_names: ['/blocks.5/Add_3_output_0']\n",
      "layer_idx: 6 \t half: True \t input_names: ['/blocks.5/Add_3_output_0'] \t output_names: ['/blocks.6/Add_2_output_0']\n",
      "layer_idx: 6 \t half: False \t input_names: ['/blocks.6/Add_2_output_0'] \t output_names: ['/blocks.6/Add_3_output_0']\n",
      "layer_idx: 7 \t half: True \t input_names: ['/blocks.6/Add_3_output_0'] \t output_names: ['/blocks.7/Add_2_output_0']\n",
      "layer_idx: 7 \t half: False \t input_names: ['/blocks.7/Add_2_output_0'] \t output_names: ['/blocks.7/Add_3_output_0']\n",
      "layer_idx: 8 \t half: True \t input_names: ['/blocks.7/Add_3_output_0'] \t output_names: ['/blocks.8/Add_2_output_0']\n",
      "layer_idx: 8 \t half: False \t input_names: ['/blocks.8/Add_2_output_0'] \t output_names: ['/blocks.8/Add_3_output_0']\n",
      "layer_idx: 9 \t half: True \t input_names: ['/blocks.8/Add_3_output_0'] \t output_names: ['/blocks.9/Add_2_output_0']\n",
      "layer_idx: 9 \t half: False \t input_names: ['/blocks.9/Add_2_output_0'] \t output_names: ['/blocks.9/Add_3_output_0']\n",
      "layer_idx: 10 \t half: True \t input_names: ['/blocks.9/Add_3_output_0'] \t output_names: ['/blocks.10/Add_2_output_0']\n",
      "layer_idx: 10 \t half: False \t input_names: ['/blocks.10/Add_2_output_0'] \t output_names: ['/blocks.10/Add_3_output_0']\n",
      "layer_idx: 11 \t half: True \t input_names: ['/blocks.10/Add_3_output_0'] \t output_names: ['/blocks.11/Add_2_output_0']\n",
      "layer_idx: 11 \t half: False \t input_names: ['/blocks.11/Add_2_output_0'] \t output_names: ['output']\n"
     ]
    }
   ],
   "source": [
    "# export the splits of the model\n",
    "\n",
    "input_path = args.prefix_dir + \"complete_model_teleported.onnx\"\n",
    "\n",
    "# Convs layer\n",
    "output_path = args.prefix_dir + \"network_split_convs_teleported.onnx\"\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"/Add_output_0\"]\n",
    "\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names, check_model=True)\n",
    "input_names = output_names\n",
    "\n",
    "for layer_idx in range(model.depth):\n",
    "    for half in [True,False]:        \n",
    "        output_path = f\"{args.prefix_dir}network_split_{layer_idx}_{str(half)}_teleported.onnx\"\n",
    "        \n",
    "        if half:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_2_output_0\"]\n",
    "        else:\n",
    "            output_names = [f\"/blocks.{layer_idx}/Add_3_output_0\"]\n",
    "            \n",
    "            if layer_idx == (model.depth - 1):\n",
    "                output_names = [\"output\"]\n",
    "                \n",
    "        print(\"layer_idx:\",layer_idx,\"\\t half:\",str(half),\"\\t input_names:\",input_names,\"\\t output_names:\",output_names)\n",
    "                \n",
    "        onnx.utils.extract_model(input_path, output_path, input_names, output_names,check_model=True)\n",
    "        input_names = output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# # Loading the dataset\n",
    "# dataset_test, args.nb_classes = build_dataset(is_train=False, args=args)\n",
    "\n",
    "# # Create a random subset of indices for 10 samples\n",
    "# subset_indices = torch.randperm(len(dataset_test))[:args.batch_size*100]\n",
    "\n",
    "# # sampler_test = torch.utils.data.DistributedSampler(\n",
    "# #         dataset_test, num_replicas=1, rank=0, shuffle=True, seed=args.seed)\n",
    "# # Use SubsetRandomSampler to create a sampler for the subset\n",
    "# sampler_test = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, sampler=sampler_test,\n",
    "#     batch_size=BATCHS,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "\n",
    "# TODO:\n",
    "# define the data_loader_test an iterator which only returns the img\n",
    "img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "img = img.resize((224,224))\n",
    "data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "# contain data and label in the dataset\n",
    "# label is shape 1,1000 which is one hot encoded\n",
    "label = torch.tensor(1).unsqueeze(0) \n",
    "data_set_test = torch.utils.data.TensorDataset(data,label)\n",
    "data_loader_test = torch.utils.data.DataLoader(data_set_test, batch_size=BATCHS, shuffle=True)\n",
    "# data_loader_test = torch.utils.data.DataLoader(da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezkl\n",
    "import json\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_ezkl_output(witness_file, settings_file):\n",
    "    # convert the quantized ezkl output to float value\n",
    "    witness_output = json.load(open(witness_file))\n",
    "    outputs = witness_output['outputs']\n",
    "    with open(settings_file) as f:\n",
    "        settings = json.load(f)\n",
    "    ezkl_outputs = [[ezkl.felt_to_float(\n",
    "        outputs[i][j], settings['model_output_scales'][i]) for j in range(len(outputs[i]))] for i in range(len(outputs))]\n",
    "    return ezkl_outputs\n",
    "\n",
    "\n",
    "def get_onnx_output(model_file, input_file):\n",
    "    # generate the ML model output from the ONNX file\n",
    "    onnx_model = onnx.load(model_file)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    with open(input_file) as f:\n",
    "        inputs = json.load(f)\n",
    "    # reshape the input to the model\n",
    "    num_inputs = len(onnx_model.graph.input)\n",
    "\n",
    "    onnx_input = dict()\n",
    "    for i in range(num_inputs):\n",
    "        input_node = onnx_model.graph.input[i]\n",
    "        dims = []\n",
    "        elem_type = input_node.type.tensor_type.elem_type\n",
    "#         print(\"elem_type: \", elem_type)\n",
    "        for dim in input_node.type.tensor_type.shape.dim:\n",
    "            if dim.dim_value == 0:\n",
    "                dims.append(1)\n",
    "            else:\n",
    "                dims.append(dim.dim_value)\n",
    "        if elem_type == 6:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int32).reshape(dims)\n",
    "        elif elem_type == 7:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.int64).reshape(dims)\n",
    "        elif elem_type == 9:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                bool).reshape(dims)\n",
    "        else:\n",
    "            inputs_onnx = np.array(inputs['input_data'][i]).astype(\n",
    "                np.float32).reshape(dims)\n",
    "        onnx_input[input_node.name] = inputs_onnx\n",
    "    try:\n",
    "        onnx_session = onnxruntime.InferenceSession(model_file)\n",
    "        onnx_output = onnx_session.run(None, onnx_input)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "        # onnx_output = inputs['output_data']\n",
    "#     print(\"onnx \", onnx_output)\n",
    "    return onnx_output[0]\n",
    "\n",
    "\n",
    "def compare_outputs(zk_output, onnx_output):\n",
    "    # calculate percentage difference between the 2 outputs (which are lists)\n",
    "    res = []\n",
    "    contains_sublist = any(isinstance(sub, list) for sub in zk_output)\n",
    "    zip_object = zip(np.array(zk_output),\n",
    "                     np.array(onnx_output))\n",
    "    \n",
    "    num_eq_zk_onnx = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for (i, (list1_i, list2_i)) in enumerate(zip_object):\n",
    "        diff = list1_i - list2_i\n",
    "        # iterate and print the diffs  if they are greater than 0.0\n",
    "        res.append(np.linalg.norm(diff,axis=(-1)))\n",
    "        print(\"= index: \",i, \"\\t diff-norm: \",np.linalg.norm(diff,axis=(-1)),\"\\t zk_output: \",list1_i.shape, \"\\t onnx_output: \",list2_i.shape)\n",
    "        \n",
    "        if np.argmax(list1_i) == np.argmax(list2_i):\n",
    "            num_eq_zk_onnx += 1\n",
    "        num_total += 1\n",
    "    \n",
    "    print(\"Accuracy (zk_onnx): \\t\", num_eq_zk_onnx / num_total)\n",
    "    acc_zk_onnx = num_eq_zk_onnx / num_total\n",
    "    return res, acc_zk_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10db441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\"/Users/mm6322/Phd research/nerual_transport/neuralteleportation/neuralteleportation/experiments/sparse-cap-acc-tmp/ILSVRC2012_val_00000616.JPEG\")\n",
    "# img = img.resize((224,224))\n",
    "# data = transforms.ToTensor()(img).unsqueeze(0)\n",
    "# data_set_test = torch.utils.data.TensorDataset(data)\n",
    "\n",
    "# print(\"data.shape:\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f4a205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_pp max: tensor(8.7877)\n",
      "new_pp argmax: tensor(180)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    new_pp = new_model(img_input)\n",
    "    # print maximum value of the new_pp\n",
    "    print(\"new_pp max:\",torch.max(new_pp))\n",
    "    # print index of the maximum value of the new_pp\n",
    "    print(\"new_pp argmax:\",torch.argmax(new_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17c22f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_pp max: tensor(8.7614)\n",
      "org_pp argmax: tensor(180)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    org_pp = model(img_input)\n",
    "    # print maximum value of the new_pp\n",
    "    print(\"org_pp max:\",torch.max(org_pp))\n",
    "    # print index of the maximum value of the new_pp\n",
    "    print(\"org_pp argmax:\",torch.argmax(org_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3710ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b3924e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy_pp max: tensor(8.7877)\n",
      "copy_pp argmax: tensor(180)\n"
     ]
    }
   ],
   "source": [
    "for index, (img_input, label) in enumerate(data_loader_test):\n",
    "    copy_pp = copy_model(img_input)\n",
    "    print(\"copy_pp max:\",torch.max(copy_pp))\n",
    "    print(\"copy_pp argmax:\",torch.argmax(copy_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d24dd226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top5_org: torch.return_types.topk(\n",
      "values=tensor([[8.7614, 8.3986, 7.9848, 7.1704, 6.1396]]),\n",
      "indices=tensor([[180, 242, 243, 246, 179]]))\n",
      "top5_copy: torch.return_types.topk(\n",
      "values=tensor([[8.7877, 8.4531, 8.0164, 7.3628, 6.1197]]),\n",
      "indices=tensor([[180, 242, 243, 246, 179]]))\n"
     ]
    }
   ],
   "source": [
    "# print top-5 indexes of org_pp and copy_pp and the corrosponding values\n",
    "top5_org = torch.topk(org_pp, 5)\n",
    "top5_copy = torch.topk(copy_pp, 5)\n",
    "print(\"top5_org:\",top5_org)\n",
    "print(\"top5_copy:\",top5_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "856322aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_args.input_scale = 16\n",
    "run_args.param_scale = 16\n",
    "run_args.num_inner_cols = 64\n",
    "run_args.scale_rebase_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 \t image.shape: torch.Size([1, 3, 224, 224])\n",
      "=====\n",
      "= index:  0 \t diff-norm:  3.7335980104036293 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label): \t 0.0\n",
      "Accuracy (onnx_label): \t 0.0\n",
      "=====\n",
      "\n",
      "\n",
      "= index:  0 \t diff-norm:  3.7335980104036293 \t zk_output:  (1000,) \t onnx_output:  (1000,)\n",
      "Accuracy (zk_onnx): \t 1.0\n",
      "Accuracy (zk_label) 0.0\n",
      "mean norm diff:  3.7335980104036293\n",
      "max norm diff:  3.7335980104036293\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ezkl_outputs = np.empty((0,1000))\n",
    "onnx_outputs = np.empty((0,1000))\n",
    "labels = np.array([])\n",
    "\n",
    "# Open log file for writing\n",
    "log_file_path = os.path.join(args.prefix_dir, \"log\", \"output_log.txt\")\n",
    "os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "\n",
    "    # Generate dataNum_input_convs.json  \n",
    "    for index, (image, label) in enumerate(data_loader_test):\n",
    "        # image = data\n",
    "        # label = torch.tensor(1).unsqueeze(0)\n",
    "        # index = 0\n",
    "        \n",
    "        print(\"index:\",index,\"\\t image.shape:\",image.shape)\n",
    "        log_file.write(f\"index: {index}\\timage.shape: {image.shape}\\n\")\n",
    "        log_file.flush()\n",
    "        \n",
    "        os.makedirs(args.prefix_dir + \"ezkl_inputs/\"+str(index),exist_ok=True)\n",
    "\n",
    "        # remove batch dimension\n",
    "        output = model(image)\n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        pre_witness_path = None\n",
    "\n",
    "        # computing witness (last witness is important)\n",
    "        for i in [\"convs\"] + [t for t in range(model.depth)]:\n",
    "            for half in [\"True\",\"False\"]:\n",
    "\n",
    "                if i==\"convs\" and half==\"False\":\n",
    "                    continue\n",
    "\n",
    "                # Define paths\n",
    "                if i == \"convs\":\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}.json\"\n",
    "                else:\n",
    "                    model_path = args.prefix_dir + f\"network_split_{i}_{half}_teleported.onnx\"\n",
    "                    settings_path = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{i}_{half}.json\"\n",
    "                    data_path = args.prefix_dir + f\"ezkl_inputs/{index}/input_{i}_{half}.json\"\n",
    "                    compiled_model_path = args.prefix_dir + f\"ezkl_inputs/{index}/network_split_{i}_{half}.compiled\"\n",
    "                    witness_path = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{i}_{half}.json\"\n",
    "\n",
    "                # Generating input data\n",
    "                if i == \"convs\":\n",
    "                    data = dict(input_data = [((image).detach().numpy()).reshape([-1]).tolist()])\n",
    "                else:\n",
    "                    inter_i = model.split_n(image,i,half=half)\n",
    "                    data = dict(input_data = [((inter_i).detach().numpy()).reshape([-1]).tolist()])\n",
    "                json.dump(data, open(data_path, 'w' ))\n",
    "\n",
    "\n",
    "                # Swapping (output pre_witness -> cur_input of data_path)\n",
    "                if i != \"convs\":\n",
    "                    with open(pre_witness_path, 'r') as prev_witness_file:\n",
    "                        prev_witness_data = json.load(prev_witness_file)\n",
    "                        outputs = prev_witness_data['outputs']\n",
    "                        tmp = {\"input_data\": outputs}\n",
    "                        with open(data_path, 'w') as data_file:\n",
    "                            json.dump(tmp, data_file) \n",
    "\n",
    "                # Generate setting\n",
    "                res = ezkl.gen_settings(model_path, settings_path, py_run_args=run_args)\n",
    "                assert res == True\n",
    "                \n",
    "\n",
    "                # Calibrating setting\n",
    "                # res = ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\", \\\n",
    "                #                                   scales=[run_args.input_scale],max_logrows=run_args.logrows, scale_rebase_multiplier=[1],lookup_safety_margin=1)\n",
    "                \n",
    "                # Compile circuit\n",
    "                res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "                assert res == True\n",
    "\n",
    "                # Generating witness\n",
    "                res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "                assert os.path.isfile(witness_path)\n",
    "\n",
    "                # Update input_scale\n",
    "                settings = json.load(open(settings_path, 'r'))\n",
    "                run_args.input_scale = settings[\"model_output_scales\"][0]\n",
    "\n",
    "                # Update pre_witness_path\n",
    "                pre_witness_path = witness_path\n",
    "\n",
    "    # check accuracy\n",
    "    model_file = args.prefix_dir + \"network_complete.onnx\" \n",
    "    input_file = args.prefix_dir + f\"ezkl_inputs/{index}/input_convs.json\"\n",
    "    witness_file = args.prefix_dir + f\"ezkl_inputs/{index}/witness_split_{model.depth-1}_False.json\"\n",
    "    settings_file = args.prefix_dir + f\"ezkl_inputs/{index}/settings_split_{model.depth-1}_False.json\"\n",
    "\n",
    "    # get the ezkl output\n",
    "    ezkl_output = get_ezkl_output(witness_file, settings_file)\n",
    "    ezkl_output = np.array(ezkl_output).reshape(BATCHS,1000)\n",
    "    # get the onnx output\n",
    "    onnx_output = get_onnx_output(model_file, input_file)\n",
    "\n",
    "    ezkl_outputs = np.concatenate((ezkl_outputs,ezkl_output),axis=0)\n",
    "    onnx_outputs = np.concatenate((onnx_outputs,onnx_output),axis=0)\n",
    "    labels = np.concatenate((labels,label),axis=0)\n",
    "\n",
    "    print(\"=====\")\n",
    "    log_file.write(\"=====\\n\")\n",
    "    _,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "    \n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label): \\t\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label): \\t {acc_zk_label}\\n\")\n",
    "    \n",
    "    acc_onnx_label = np.sum( (labels) == np.argmax(onnx_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (onnx_label): \\t\", acc_onnx_label)\n",
    "    log_file.write(f\"Accuracy (onnx_label): \\t {acc_onnx_label}\\n\")\n",
    "    \n",
    "    print(\"=====\\n\\n\")\n",
    "    log_file.write(\"=====\\n\\n\")\n",
    "    log_file.flush()\n",
    "    \n",
    "    \n",
    "    # compare the outputs\n",
    "    percentage_difference,acc_zk_onnx = compare_outputs(ezkl_outputs, onnx_outputs)\n",
    "    log_file.write(f\"Accuracy (zk_onnx): \\t {acc_zk_onnx}\\n\")\n",
    "\n",
    "    # compare zk_output and truth_label\n",
    "    acc_zk_label = np.sum( (labels) == np.argmax(ezkl_outputs,axis=(-1)) ) / len(labels)\n",
    "    print(\"Accuracy (zk_label)\", acc_zk_label)\n",
    "    log_file.write(f\"Accuracy (zk_label) {acc_zk_label}\\n\")\n",
    "\n",
    "    # print the percentage difference\n",
    "    mean_percentage_difference = np.mean(np.abs(percentage_difference))\n",
    "    max_percentage_difference = np.max(np.abs(percentage_difference))\n",
    "    print(\"mean norm diff: \", mean_percentage_difference)\n",
    "    print(\"max norm diff: \", max_percentage_difference)\n",
    "    log_file.write(f\"mean norm diff: {mean_percentage_difference}\\n\")\n",
    "    log_file.write(f\"max norm diff: {max_percentage_difference}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5389361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac8208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralteleportation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
