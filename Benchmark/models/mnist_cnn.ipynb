{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model for MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load TensorFlow MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images_tf = train_images / 255.0\n",
    "test_images_tf = test_images / 255.0\n",
    "train_images_tf = train_images_tf.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images_tf = test_images_tf.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "train_images_tf_14 = tf.image.resize(train_images_tf, [14, 14]).numpy()\n",
    "test_images_tf_14 = tf.image.resize(test_images_tf, [14, 14]).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch format [batch_size, channels, height, width]\n",
    "train_images_pt = torch.tensor(train_images_tf).permute(0, 3, 1, 2).float()\n",
    "test_images_pt = torch.tensor(test_images_tf).permute(0, 3, 1, 2).float()\n",
    "\n",
    "\n",
    "train_images_pt_14 =  torch.tensor(test_images_tf_14).permute(0, 3, 1, 2).float()\n",
    "test_images_pt_14 =  torch.tensor(test_images_tf_14).permute(0, 3, 1, 2).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5_11_80_10 CNN Model for 14x14 input and 3x3 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 5)         50        \n",
      "                                                                 \n",
      " average_pooling2d_4 (Avera  (None, 6, 6, 5)           0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 11)          506       \n",
      "                                                                 \n",
      " average_pooling2d_5 (Avera  (None, 2, 2, 11)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 44)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 80)                3600      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                810       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4966 (19.40 KB)\n",
      "Trainable params: 4966 (19.40 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = [14, 5, 11, 80, 10, 3]\n",
    "\n",
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(5, 3, activation='relu', input_shape=(layers[0], layers[0], 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(11, 3, activation='relu'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(80, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2078 - accuracy: 0.9358 - val_loss: 0.1448 - val_accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model_tf.fit(train_images_tf_14, train_labels, epochs=1, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.1611 - accuracy: 0.9487 - 317ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.9487000107765198\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images_tf_14, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert it to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "layers = [14, 5, 11, 80, 10, 3]\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional encoder\n",
    "        self.conv1 = nn.Conv2d(1, layers[1], layers[-1]) \n",
    "        self.conv2 = nn.Conv2d(layers[1], layers[2], layers[-1]) \n",
    "\n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(11 * 2 * 2, layers[3]) # 256 * 120\n",
    "        self.fc2 = nn.Linear(layers[3], layers[4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # TODO: figure out the resize, currently work on batch_size = 1\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(x.size(0),layers[2],-1)  # 16 output channels\n",
    "        x = np.transpose(x, (0,2,1)).reshape(batch_size,-1)\n",
    "        #x = x.reshape(batch_size,-1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation function here, will use CrossEntropyLoss later\n",
    "        return x\n",
    "\n",
    "model_pt = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[6].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n",
      "TensorFlow Basic Model Output: [[-4.233818    3.8901105  -1.0666226   0.47903582 -0.4173532   0.30218112\n",
      "  -2.0122645  -1.617805    0.09114771 -1.5917833 ]]\n",
      "PyTorch Basic Model Output: tensor([[-4.2338,  3.8901, -1.0666,  0.4790, -0.4174,  0.3022, -2.0123, -1.6178,\n",
      "          0.0911, -1.5918]])\n"
     ]
    }
   ],
   "source": [
    "# Select the image for TensorFlow and PyTorch\n",
    "controlled_input_tf = test_images_tf_14[189:190]  # Reshape to (1, 784) for DNN\n",
    "controlled_input_pt = test_images_pt_14[189:190]\n",
    "\n",
    "# Test TensorFlow Model\n",
    "output_tf = model_tf.predict(controlled_input_tf) \n",
    "print(\"TensorFlow Basic Model Output:\", output_tf)\n",
    "\n",
    "# Test PyTorch Model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "print(\"PyTorch Basic Model Output:\", output_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 94.87%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming test_labels are already loaded\n",
    "test_dataset = TensorDataset(test_images_pt_14, torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = evaluate_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_tf(model, test_images, batch_size=256):\n",
    "    predictions = []\n",
    "    for i in range(0, len(test_images), batch_size):\n",
    "        batch = test_images[i:i+batch_size]\n",
    "        pred = model.predict(batch)\n",
    "        predictions.extend(np.argmax(pred, axis=1))\n",
    "    return predictions\n",
    "\n",
    "def get_predictions_pt(model, test_images, batch_size=256):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_images), batch_size):\n",
    "            batch = test_images[i:i+batch_size]\n",
    "            pred = model(batch)\n",
    "            predictions.extend(torch.argmax(pred, axis=1).tolist())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 945us/step\n",
      "8/8 [==============================] - 0s 958us/step\n",
      "8/8 [==============================] - 0s 978us/step\n",
      "8/8 [==============================] - 0s 916us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 999us/step\n",
      "8/8 [==============================] - 0s 887us/step\n",
      "8/8 [==============================] - 0s 988us/step\n",
      "8/8 [==============================] - 0s 934us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 923us/step\n",
      "8/8 [==============================] - 0s 856us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 913us/step\n",
      "8/8 [==============================] - 0s 867us/step\n",
      "8/8 [==============================] - 0s 948us/step\n",
      "8/8 [==============================] - 0s 863us/step\n",
      "8/8 [==============================] - 0s 926us/step\n",
      "8/8 [==============================] - 0s 854us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 982us/step\n",
      "8/8 [==============================] - 0s 984us/step\n",
      "8/8 [==============================] - 0s 934us/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 950us/step\n",
      "8/8 [==============================] - 0s 922us/step\n",
      "8/8 [==============================] - 0s 976us/step\n",
      "8/8 [==============================] - 0s 915us/step\n",
      "8/8 [==============================] - 0s 966us/step\n",
      "8/8 [==============================] - 0s 893us/step\n",
      "8/8 [==============================] - 0s 876us/step\n",
      "8/8 [==============================] - 0s 927us/step\n",
      "8/8 [==============================] - 0s 875us/step\n",
      "8/8 [==============================] - 0s 898us/step\n",
      "8/8 [==============================] - 0s 891us/step\n",
      "8/8 [==============================] - 0s 864us/step\n",
      "8/8 [==============================] - 0s 867us/step\n",
      "8/8 [==============================] - 0s 875us/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Number of mismatches: 0 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions_tf = get_predictions_tf(model_tf, test_images_tf_14)\n",
    "predictions_pt = get_predictions_pt(model_pt, test_images_pt_14)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(predictions_tf, predictions_pt))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 5, 11, 80, 10, 3]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/ZKML-Benchmark/ZKML-Benchmark/zkml_bench_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpg6tl7vgj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpg6tl7vgj/assets\n",
      "2024-07-31 19:49:35.184374: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-07-31 19:49:35.184549: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-07-31 19:49:35.188108: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpg6tl7vgj\n",
      "2024-07-31 19:49:35.189756: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-07-31 19:49:35.189770: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpg6tl7vgj\n",
      "2024-07-31 19:49:35.195667: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-07-31 19:49:35.197090: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-07-31 19:49:35.254211: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpg6tl7vgj\n",
      "2024-07-31 19:49:35.269015: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 80913 microseconds.\n",
      "2024-07-31 19:49:35.305153: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 9, Total Ops 19, % non-converted = 47.37 %\n",
      " * 9 ARITH ops\n",
      "\n",
      "- arith.constant:    9 occurrences  (f32: 8, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Tensorflow\n",
    "arch_folder = \"./input-conv2d-conv2d-dense-dense/\"\n",
    "os.makedirs(arch_folder, exist_ok=True)\n",
    "\n",
    "model_name = \"_\".join([str(x) for x in layers])\n",
    "model_tf.save(arch_folder + model_name + '.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(arch_folder + model_name + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model\n",
    "torch.save(model_pt, arch_folder + model_name + \".pt\")\n",
    "# Save only the state_dict\n",
    "torch.save(model_pt.state_dict(), arch_folder + model_name + \".pth\")\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model_pt, controlled_input_pt, arch_folder + model_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 784_6_16_10 Conv2d-Conv2D-Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3650 - accuracy: 0.9139 - val_loss: 0.0945 - val_accuracy: 0.9727\n"
     ]
    }
   ],
   "source": [
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='relu'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_tf.fit(train_images, train_labels, epochs=1, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d_6 (Avera  (None, 12, 12, 6)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " average_pooling2d_7 (Avera  (None, 4, 4, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5142 (20.09 KB)\n",
      "Trainable params: 5142 (20.09 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0906 - accuracy: 0.9728 - 391ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.9728000164031982\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional encoder\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
    "\n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 10) # 256 * 120\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # TODO: figure out the resize, currently work on batch_size = 1\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(x.size(0),16,-1)  # 16 output channels\n",
    "        x = np.transpose(x, (0,2,1)).reshape(batch_size,-1)\n",
    "        #x = x.reshape(batch_size,-1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)  # No activation function here, will use CrossEntropyLoss later\n",
    "        return x\n",
    "    \n",
    "\n",
    "model_pt = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select the image for TensorFlow\n",
    "controlled_input_tf = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_tf = np.expand_dims(controlled_input_tf, axis=-1)\n",
    "# controlled_input_tf = np.expand_dims(test_images[36], axis=0)\n",
    "print(controlled_input_tf.shape)\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "TF Basic Model Output: [[ -6.5935187   -4.563655     3.4562306    0.18523064 -15.529033\n",
      "  -11.604303   -18.729162     8.666261    -5.6039987   -0.50239336]]\n",
      "PT Basic Model Output: [[ -6.5935173   -4.5636544    3.4562306    0.18523028 -15.529034\n",
      "  -11.604305   -18.729158     8.666257    -5.6039977   -0.502393  ]]\n"
     ]
    }
   ],
   "source": [
    "# Test PyTorch Basic Model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "\n",
    "output_tf = model_tf.predict(controlled_input_tf) \n",
    "print(\"TF Basic Model Output:\", output_tf)\n",
    "print(\"PT Basic Model Output:\", output_pt.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 97.28%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming the TensorFlow MNIST data has already been loaded\n",
    "# Convert test_images to PyTorch tensor and permute\n",
    "test_images_pt = torch.tensor(test_images).unsqueeze(-1).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# Assuming test_labels are already loaded\n",
    "test_dataset = TensorDataset(test_images_pt, torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = evaluate_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_tf(model, test_images, batch_size=256):\n",
    "    predictions = []\n",
    "    for i in range(0, len(test_images), batch_size):\n",
    "        batch = test_images[i:i+batch_size]\n",
    "        pred = model.predict(batch)\n",
    "        predictions.extend(np.argmax(pred, axis=1))\n",
    "    return predictions\n",
    "\n",
    "def get_predictions_pt(model, test_images, batch_size=256):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_images), batch_size):\n",
    "            batch = test_images[i:i+batch_size]\n",
    "            pred = model(batch)\n",
    "            predictions.extend(torch.argmax(pred, axis=1).tolist())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Number of mismatches: 0 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "# add channel dimension to test_images\n",
    "predictions_tf = get_predictions_tf(model_tf, test_images.reshape(-1, 28, 28, 1))\n",
    "predictions_pt = get_predictions_pt(model_pt, test_images_pt)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(predictions_tf, predictions_pt))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Models for 784_6_16_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/ZKML-Benchmark/ZKML-Benchmark/zkml_bench_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpbykbu5ca/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpbykbu5ca/assets\n",
      "2024-07-31 19:56:58.740321: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-07-31 19:56:58.740479: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-07-31 19:56:58.741760: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpbykbu5ca\n",
      "2024-07-31 19:56:58.742828: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-07-31 19:56:58.742836: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpbykbu5ca\n",
      "2024-07-31 19:56:58.747665: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-07-31 19:56:58.795887: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpbykbu5ca\n",
      "2024-07-31 19:56:58.809515: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 67755 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 7, Total Ops 16, % non-converted = 43.75 %\n",
      " * 7 ARITH ops\n",
      "\n",
      "- arith.constant:    7 occurrences  (f32: 6, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Tensorflow\n",
    "arch_folder = \"./input-conv2d-conv2d-dense/\"\n",
    "os.makedirs(arch_folder, exist_ok=True)\n",
    "\n",
    "model_name = \"784_6_16_10\"\n",
    "model_tf.save(arch_folder + model_name + '.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(arch_folder + model_name + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "\n",
    "# Save entire model\n",
    "torch.save(model_pt, arch_folder + model_name + \".pt\")\n",
    "# Save only the state_dict\n",
    "torch.save(model_pt.state_dict(), arch_folder + model_name + \".pth\")\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model_pt, controlled_input_pt, arch_folder + model_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 784_6_16_120_84_10 Conv2d-Conv2D-Dense-Dense-Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 4s 17ms/step - loss: 0.4028 - accuracy: 0.8865 - val_loss: 0.0878 - val_accuracy: 0.9743\n"
     ]
    }
   ],
   "source": [
    "# Define the LeNet model in TensorFlow\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), activation='relu'),\n",
    "    tf.keras.layers.AvgPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_tf.fit(train_images, train_labels, epochs=1, batch_size=256, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0956 - accuracy: 0.9707 - 504ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.9707000255584717\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model_tf.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d_8 (Avera  (None, 12, 12, 6)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " average_pooling2d_9 (Avera  (None, 4, 4, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44426 (173.54 KB)\n",
      "Trainable params: 44426 (173.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional encoder\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
    "\n",
    "        # Fully connected layers / Dense block\n",
    "        self.fc1 = nn.Linear(16 *4 * 4,120) # 256 * 120\n",
    "        self.fc2 = nn.Linear(120, 84)         # 120 inputs, 84 outputs\n",
    "        self.fc3 = nn.Linear(84, 10)          # 84 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2)) # Convolution -> Sigmoid -> Avg Pool\n",
    "\n",
    "        # TODO: figure out the resize, currently work on batch_size = 1\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(x.size(0),16,-1)  # 16 output channels\n",
    "        x = np.transpose(x, (0,2,1)).reshape(batch_size,-1)\n",
    "        #x = x.reshape(batch_size,-1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, will use CrossEntropyLoss later\n",
    "        return x\n",
    "    \n",
    "\n",
    "model_pt = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights for the first Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[0].get_weights()\n",
    "model_pt.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second Conv2D layer from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[2].get_weights()\n",
    "model_pt.conv2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (3, 2, 0, 1))))\n",
    "model_pt.conv2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the first dense layer (fc1) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[5].get_weights()\n",
    "model_pt.fc1.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc1.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the second dense layer (fc2) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[6].get_weights()\n",
    "model_pt.fc2.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc2.bias = nn.Parameter(torch.from_numpy(biases))\n",
    "\n",
    "# Transfer weights for the third dense layer (fc3) from model_tf to model_pt\n",
    "weights, biases = model_tf.layers[7].get_weights()\n",
    "model_pt.fc3.weight = nn.Parameter(torch.from_numpy(np.transpose(weights, (1, 0))))\n",
    "model_pt.fc3.bias = nn.Parameter(torch.from_numpy(biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Select the image for TensorFlow\n",
    "controlled_input_tf = test_images[36][np.newaxis, ]  # No reshape needed as it's already in (28, 28, 1) format\n",
    "controlled_input_tf = np.expand_dims(controlled_input_tf, axis=-1)\n",
    "print(controlled_input_tf.shape)\n",
    "\n",
    "# controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 1, 2)\n",
    "controlled_input_pt = torch.tensor(controlled_input_tf).float().permute(0, 3, 2, 1)\n",
    "print(controlled_input_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlled_input_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n",
      "TF Basic Model Output: [[-0.7721681  -3.6355472   8.680198    5.5364165  -4.2197127  -2.9623535\n",
      "  -9.193442   12.8122635   0.41937524 -0.39005888]]\n",
      "PT Basic Model Output: [[ 3.7063713  -0.5799919   1.5933546  -5.1392236   6.149688   -3.4218006\n",
      "  10.209146   -0.81681114  2.215624    0.5799337 ]]\n"
     ]
    }
   ],
   "source": [
    "# Test PyTorch Basic Model\n",
    "model_pt.eval()  # Set PyTorch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_pt = model_pt(controlled_input_pt)\n",
    "\n",
    "output_tf = model_tf.predict(controlled_input_tf) \n",
    "print(\"TF Basic Model Output:\", output_tf)\n",
    "print(\"PT Basic Model Output:\", output_pt.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 97.07%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming the TensorFlow MNIST data has already been loaded\n",
    "# Convert test_images to PyTorch tensor and permute\n",
    "test_images_pt = torch.tensor(test_images).unsqueeze(-1).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# Assuming test_labels are already loaded\n",
    "test_dataset = TensorDataset(test_images_pt, torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = evaluate_model(model_pt, test_loader)\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Number of mismatches: 0 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions_tf = get_predictions_tf(model_tf, test_images.reshape(-1, 28, 28, 1))\n",
    "predictions_pt = get_predictions_pt(model_pt, test_images_pt)\n",
    "\n",
    "# Compare predictions\n",
    "mismatches = sum(p1 != p2 for p1, p2 in zip(predictions_tf, predictions_pt))\n",
    "print(f\"Number of mismatches: {mismatches} out of {len(test_images)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model for 784_6_16_120_84_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/ZKML-Benchmark/ZKML-Benchmark/zkml_bench_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpl_lds1gm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpl_lds1gm/assets\n",
      "2024-07-31 20:00:37.841647: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-07-31 20:00:37.841817: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-07-31 20:00:37.843963: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpl_lds1gm\n",
      "2024-07-31 20:00:37.845394: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-07-31 20:00:37.845405: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpl_lds1gm\n",
      "2024-07-31 20:00:37.850567: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-07-31 20:00:37.914869: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmpl_lds1gm\n",
      "2024-07-31 20:00:37.931948: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 87986 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 11, Total Ops 22, % non-converted = 50.00 %\n",
      " * 11 ARITH ops\n",
      "\n",
      "- arith.constant:   11 occurrences  (f32: 10, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Tensorflow\n",
    "arch_folder = \"./input-conv2d-conv2d-dense-dense-dense/\"\n",
    "os.makedirs(arch_folder, exist_ok=True)\n",
    "\n",
    "model_name = \"784_6_16_120_84_10\"\n",
    "model_tf.save(arch_folder + model_name + '.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(arch_folder + model_name + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "\n",
    "# Save entire model\n",
    "torch.save(model_pt, arch_folder + model_name + \".pt\")\n",
    "# Save only the state_dict\n",
    "torch.save(model_pt.state_dict(), arch_folder + model_name + \".pth\")\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model_pt, controlled_input_pt, arch_folder + model_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'784_6_16_120_84_10'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mm6322/Phd research/ZKML-Benchmark/ZKML-Benchmark/zkml_bench_env/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmprnxiuodj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmprnxiuodj/assets\n",
      "2024-07-31 20:15:11.917841: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-07-31 20:15:11.917855: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-07-31 20:15:11.918081: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmprnxiuodj\n",
      "2024-07-31 20:15:11.919092: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-07-31 20:15:11.919100: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmprnxiuodj\n",
      "2024-07-31 20:15:11.922356: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-07-31 20:15:11.951504: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/mh/wt0s4pwn1w52cl7dn_5mj92m0000gp/T/tmprnxiuodj\n",
      "2024-07-31 20:15:11.962419: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 44339 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 11, Total Ops 22, % non-converted = 50.00 %\n",
      " * 11 ARITH ops\n",
      "\n",
      "- arith.constant:   11 occurrences  (f32: 10, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "# make the 0.5 percent of the model weights zero and save\n",
    "import copy\n",
    "model_tf_pruned = copy.deepcopy(model_tf)\n",
    "\n",
    "# Prune the model\n",
    "pruning_percentage = 0.5\n",
    "for layer in model_tf_pruned.layers:\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):\n",
    "        weights = layer.get_weights()\n",
    "        weights[0] = weights[0] * (weights[0] > np.percentile(np.abs(weights[0]), pruning_percentage))\n",
    "        layer.set_weights(weights)\n",
    "\n",
    "# Save the pruned model\n",
    "model_tf_pruned.save(arch_folder + model_name + '_pruned.h5')\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf_pruned)\n",
    "tflite_model = converter.convert()\n",
    "# with open(arch_folder + model_name + '_pruned.tflite', 'wb') as f:\n",
    "# model_in_path = model_path+arch_folder+args.model + '_sparsity_'+str(args.sparsity) + '.tflite'\n",
    "# with open(arch_folder + model_name + '_sparsity_' + str(pruning_percentage) + '.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "if pruning_percentage > 0:\n",
    "    with open(arch_folder + '28_6_16_120_84_10_5_sparsity_0.5.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "else:\n",
    "    with open(arch_folder + '28_6_16_120_84_10_5.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
