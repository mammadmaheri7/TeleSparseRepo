datasets:
  - cifar10
  - cifar100

models:
  - MLPCOB
  - vgg16_bnCOB
  - resnet18COB
  - densenet121COB

optimizers:
  - cls: SGD
    lr: 0.01
    weight_decay: 0.01
    lr_scheduler:
      cls: OneCycleLR
      interval: step
      max_lr: 0.01
      steps_per_epoch: 196 # nb_batches (50 000) / batch_size (256)
      epochs: 100
  - cls: SGD
    lr: 0.01
    momentum: 0.9
    weight_decay: 0.01
    lr_scheduler:
      cls: OneCycleLR
      interval: step
      max_lr: 0.01
      steps_per_epoch: 196 # nb_batches (50 000) / batch_size (256)
      epochs: 100
  - cls: Adam
    lr: 0.001

initializers:
  - type: none # Uses PyTorch's default initializer
  - type: kaiming
    gain: 0.02
    non_linearity: relu # This is used by the kaiming initalizer to compute the correct gain
  - type: xavier
    gain: 0.02
  - type: normal
    gain: 0.02 # in case of the normal distribution gain is the std of the N(0, std)

training_params:
  epochs: 100
  batch_size: 256
  shuffle_batches: True
  drop_last_batch: True

teleportations:
  no_teleport: